[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Telling Stories with Data",
    "section": "",
    "text": "Preface\nThis book will help you tell stories with data. It establishes a foundation on which you can build and share knowledge about an aspect of the world that interests you based on data that you observe. Telling stories in small groups around a fire played a critical role in the development of humans and society (Wiessner 2014). Today our stories, based on data, can influence millions.\nIn this book we will explore, prod, push, manipulate, knead, and ultimately, try to understand the implications of, data. A variety of features drive the choices in this book.\nThe motto of the university from which I took my PhD is naturam primum cognoscere rerum or roughly “first to learn the nature of things”. But the original quote continues temporis aeterni quoniam, or roughly “for eternal time”. We will do both of these things. I focus on tools, approaches, and workflows that enable you to establish lasting and reproducible knowledge.\nWhen I talk of data in this book, it will typically be related to humans. Humans will be at the center of most of our stories, and we will tell social, cultural, and economic stories. In particular, throughout this book I will draw attention to inequity both in social phenomena and in data. Most data analysis reflects the world as it is. Many of the least well-off face a double burden in this regard: not only are they disadvantaged, but the extent is more difficult to measure. Respecting those whose data are in our dataset is a primary concern, and so is thinking of those who are systematically not in our dataset.\nWhile data are often specific to various contexts and disciplines, the approaches used to understand them tend to be similar. Data are also increasingly global, with resources and opportunities available from a variety of sources. Hence, I draw on examples from many disciplines and geographies.\nTo become knowledge, our findings must be communicated to, understood, and trusted by other people. Scientific and economic progress can only be made by building on the work of others. And this is only possible if we can understand what they did. Similarly, if we are to create knowledge about the world, then we must enable others to understand precisely what we did, what we found, and how we went about our tasks. As such, in this book I will be particularly prescriptive about communication and reproducibility.\nImproving the quality of quantitative work is an enormous challenge, yet it is the challenge of our time. Data are all around us, but there is little enduring knowledge being created. This book hopes to contribute, in some small way, to changing that."
  },
  {
    "objectID": "index.html#audience-and-assumed-background",
    "href": "index.html#audience-and-assumed-background",
    "title": "Telling Stories with Data",
    "section": "Audience and assumed background",
    "text": "Audience and assumed background\nThe typical person reading this book has some familiarity with first-year undergraduate statistics, for instance they have run a regression. But it is not targeted at a particular level, instead providing aspects relevant to almost any quantitative course. I have taught from this book at undergraduate, graduate, and professional levels. Everyone has unique needs, but hopefully some aspect of this book speaks to you.\nEnthusiasm and interest have taken people far. If you have those, then do not worry about too much else. Some of the most successful students have been those with no quantitative or coding background.\nThis book covers a lot of ground, but does not go into depth about any particular aspect. As such it especially complements more-detailed books such as: Data Science: A First Introduction (Timbers, Campbell, and Lee 2022), R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund [2016] 2023), An Introduction to Statistical Learning (James et al. [2013] 2021), and Statistical Rethinking (McElreath [2015] 2020). If you are interested in those books, then this might be a good one to start with."
  },
  {
    "objectID": "index.html#structure-and-content",
    "href": "index.html#structure-and-content",
    "title": "Telling Stories with Data",
    "section": "Structure and content",
    "text": "Structure and content\nThis book is structured around six parts: I) Foundations, II) Communication, III) Acquisition, IV) Preparation, V) Modeling, and VI) Applications.\nPart I—Foundations—begins with Chapter 1 which provides an overview of what I am trying to achieve with this book and why you should read it. Chapter 2 goes through three worked examples. The intention of these is that you can experience the full workflow recommended in this book without worrying too much about the specifics of what is happening. That workflow is: plan, simulate, acquire, model, and communicate. It is normal to not initially follow everything in this chapter, but you should go through it, typing out and executing the code yourself. If you only have time to read one chapter of this book, then I recommend that one. Chapter 3 introduces some key tools for reproducibility used in the workflow that I advocate. These are aspects like Quarto, R Projects, Git and GitHub, and using R in practice.\nPart II—Communication—considers written and static communication. Chapter 4 details the features that quantitative writing should have and how to write a crisp, quantitative research paper. Static communication in Chapter 5 introduces features like graphs, tables, and maps.\nPart III—Acquisition—focuses on turning our world into data. Chapter 6 begins with measurement, and then steps through essential concepts from sampling that govern our approach to data. It then considers datasets that are explicitly provided for us to use as data, for instance censuses and other government statistics. These are typically clean, well-documented, pre-packaged datasets. Chapter 7 covers aspects like using Application Programming Interfaces (APIs), scraping data, getting data from PDFs, and Optical Character Recognition (OCR). The idea is that data are available, but not necessarily designed to be datasets, and that we must go and get them. Finally, Chapter 8 covers aspects where more is expected of us. For instance, we may need to conduct an experiment, run an A/B test, or do some surveys.\nPart IV—Preparation—covers how to respectfully transform the original, unedited data into something that can be explored and shared. Chapter 9 begins by detailing some principles to follow when approaching the task of cleaning and preparing data, and then goes through specific steps to take and checks to implement. Chapter 10 focuses on methods of storing and retrieving those datasets, including the use of R data packages and parquet. It then continues onto considerations and steps to take when wanting to disseminate datasets as broadly as possible, while at the same time respecting those whose data they are based on.\nPart V—Modeling—begins with exploratory data analysis in Chapter 11. This is the critical process of coming to understand a dataset, but not something that typically finds itself into the final product. The process is an end in itself. In Chapter 12 the use of linear models to explore data is introduced. And Chapter 13 considers generalized linear models, including logistic, Poisson, and negative binomial regression. It also introduces multilevel modeling.\nPart VI—Applications—provides three applications of modeling. Chapter 14 focuses on making causal claims from observational data and covers approaches such as difference-in-differences, regression discontinuity, and instrumental variables. Chapter 15 introduces multilevel regression with post-stratification, which is where we use a statistical model to adjust a sample for known biases. Chapter 16 is focused on text-as-data.\nChapter 17 offers some concluding remarks, details some outstanding issues, and suggests some next steps.\nOnline appendices offer critical aspects that are either a little too unwieldy for the size constraints of the page, or likely to need more frequent updating than is reasonable for a printed book. Online Appendix A goes through some essential tasks in R, which is the statistical programming language used in this book. It can be a reference chapter and some students find themselves returning to it as they go through the rest of the book. Online Appendix B provides a list of datasets that may be useful for assessment. The core of this book is centered around Quarto, however its predecessor, R Markdown, has not yet been sunsetted and there is a lot of material available for it. As such, Online Appendix C contains R Markdown equivalents of the Quarto-specific aspects in Chapter 3. A set of papers is included in Online Appendix D. If you write these, you will be conducting original research on a topic that is of interest to you. Although open-ended research may be new to you, the extent to which you are able to: develop your own questions, use quantitative methods to explore them, and communicate your findings, is the measure of the success of this book. Online Appendix E covers aspects such as websites, web applications, and maps that can be interacted with. Online Appendix F provides an example of a datasheet. Online Appendix G gives a brief overview of SQL essentials. Online Appendix H provides a discussion of prediction-focused modeling. Online Appendix I considers how to make model estimates and forecasts more widely available. Finally, Online Appendix J provides ideas for using this book in class."
  },
  {
    "objectID": "index.html#pedagogy-and-key-features",
    "href": "index.html#pedagogy-and-key-features",
    "title": "Telling Stories with Data",
    "section": "Pedagogy and key features",
    "text": "Pedagogy and key features\nYou have to do the work. You should actively go through material and code yourself. King (2000) says “[a]mateurs sit and wait for inspiration, the rest of us just get up and go to work”. Do not passively read this book. My role is best described by Hamming ([1997] 2020, 2–3):\n\nI am, as it were, only a coach. I cannot run the mile for you; at best I can discuss styles and criticize yours. You know you must run the mile if the athletics course is to be of benefit to you—hence you must think carefully about what you hear and read in this book if it is to be effective in changing you—which must obviously be the purpose\\(\\dots\\)\n\nThis book is structured around a dense, introductory 12-week course. It provides enough material for advanced readers to be challenged, while establishing a core that all readers should master. Typical courses cover most of the material through to Chapter 13, and then pick another chapter that is of particular interest. But it depends on the backgrounds and interests of the students.\nFrom as early as Chapter 2 you will have a workflow—plan, simulate, acquire, model, and communicate—allowing you to tell a convincing story with data. In each subsequent chapter you will add depth to this workflow. This will allow you to speak with increasing sophistication and credibility. This workflow encompasses skills that are typically sought in both academia and industry. These include: communication, ethics, reproducibility, research question development, data collection, data cleaning, data protection and dissemination, exploratory data analysis, statistical modeling, and scaling.\nOne of the defining aspects of this book is that ethics and inequity concerns are integrated throughout, rather than being clustered in one, easily ignorable, chapter. These are critical, but it can be difficult to immediately see their value, hence their tight integration.\nThis book is also designed to enable you to build a portfolio of work that you could show to a potential employer. If you want a job in industry, then this is arguably the most important thing that you should be doing. Robinson and Nolis (2020, 55) describe how a portfolio is a collection of projects that show what you can do and is something that can help be successful in a job search.\nIn the novel The Last Samurai (DeWitt 2000, 326), a character says:\n\n[A] scholar should be able to look at any word in a passage and instantly think of another passage where it occurred; \\(\\dots\\) [so a] text was like a pack of icebergs each word a snowy peak with a huge frozen mass of cross-references beneath the surface.\n\nIn an analogous way, this book not only provides text and instruction that is self-contained, but also helps develop the critical masses of knowledge on which expertise is built. No chapter positions itself as the last word, instead they are written in relation to other work.\nEach chapter has the following features:\n\nA list of required materials that you should go through before you read that chapter. To be clear, you should first read that material and then return to this book. Each chapter also contains extensive references. If you are particularly interested in the topic, then you should use these as a starting place for further exploration.\nA summary of the key concepts and skills that are developed in that chapter. Technical chapters additionally contain a list of the software and packages that are used in the chapter. The combination of these features acts as a checklist for your learning, and you should return to them after completing the chapter.\n“Scales” where I provide a small scenario and ask you to work through the workflow advocated in this book. This will probably take 15-30 minutes. Hilary Hahn, the American violinist, publicly documents herself practicing the violin, often scales or similar exercises, almost every day. I recommend you do something similar, and these are designed to enable that.\nA series of short questions that you should complete after going through the required materials, but before going through the chapter, to test your knowledge. After completing the chapter, you should go back through the questions to make sure that you understand each aspect. An answer guide is available on request.\nA tutorial question to further encourage you to actively engage with the material. You could consider forming small groups to discuss your answers to these questions.\n\nSome chapters additionally feature:\n\nA section called “Oh, you think we have good data on that!” which focuses on a particular setting where it is often assumed that there are unimpeachable and unambiguous data but the reality tends to be quite far from that.\nA section called “Shoulders of giants”, which focuses on some of those who created the intellectual foundation on which we build."
  },
  {
    "objectID": "index.html#software-information-and-conventions",
    "href": "index.html#software-information-and-conventions",
    "title": "Telling Stories with Data",
    "section": "Software information and conventions",
    "text": "Software information and conventions\nThe software that I primarily use in this book is R (R Core Team 2023). This language was chosen because it is open source, widely used, general enough to cover the entire workflow, yet specific enough to have plenty of well-developed features. I do not assume that you have used R before, and so another reason for selecting R for this book is the community of R users. The community is especially welcoming of newcomers and there is a lot of complementary beginner-friendly material available.\nIf you do not have a programming language, then R is a great one to start with. Please do go through Online Appendix A.\nThe ability to code is useful well beyond this book. If you have a preferred programming language already, then it would not hurt to also pick up R. That said, if you have a good reason to prefer another open-source programming language (for instance you use Python daily at work) then you may wish to stick with that. However, all examples in this book are in R.\nPlease download R and RStudio onto your own computer. You can download R for free here, and you can download RStudio Desktop for free here. Please also create an account on Posit Cloud here. This will allow you to run R in the cloud. And download Quarto here.\nPackages are in typewriter text, for instance, tidyverse, while functions are also in typewriter text, but include brackets, for instance filter()."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Telling Stories with Data",
    "section": "About the author",
    "text": "About the author\nI am an assistant professor at the University of Toronto, jointly appointed in the Faculty of Information and the Department of Statistical Sciences. I am also the assistant director of the Canadian Statistical Sciences Institute (CANSSI) Ontario, a senior fellow at Massey College, a faculty affiliate at the Schwartz Reisman Institute for Technology and Society, and a co-lead of the Data Sciences Institute Thematic Program in Reproducibility. I hold a PhD in Economics from the Australian National University where I focused on economic history and was supervised by John Tang (chair), Martine Mariotti, Tim Hatton, and Zach Ward.\nMy research investigates how we can develop workflows that improve the trustworthiness of data science. I am particularly interested in the role of testing in data science.\nI enjoy teaching and I aim to help students from a wide range of backgrounds learn how to use data to tell convincing stories. I try to develop students that are skilled not only in using statistical methods across various disciplines, but also appreciate their limitations, and think deeply about the broader contexts of their work. I teach in both the Faculty of Information and the Department of Statistical Sciences at both undergraduate and graduate levels. I am a RStudio Certified Tidyverse Trainer.\nI am married to Monica Alexander and we have two children. I probably spend too much money on books, and certainly too much time at libraries. If you have any book recommendations of your own, then I would love to hear them."
  },
  {
    "objectID": "index.html#land-acknowledgment",
    "href": "index.html#land-acknowledgment",
    "title": "Telling Stories with Data",
    "section": "Land acknowledgment",
    "text": "Land acknowledgment\nThis book was primarily written on the traditional lands of the Mississaugas of the Credit, the Huron-Wendat, and the Seneca. Data have long been used to oppress and harm, and the acknowledgment of the history of this land serves as a reminder of the need to try to be better in our own use of data."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Telling Stories with Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany people generously gave code, data, examples, guidance, opportunities, thoughts, and time that helped develop this book.\nThank you to David Grubbs, Curtis Hill, Robin Lloyd-Starkes, and the team at Taylor & Francis for editing and publishing this book, and providing invaluable guidance and support. I am grateful to Erica Orloff who thoroughly edited this book. Thank you to Isabella Ghement, who thoroughly went through an early draft of this book and provided detailed feedback that improved it.\nThank you to Annie Collins, who went through every word in this book, improving many of them, and helped to sharpen my thinking on much of the content covered in it. One of the joys of teaching is the chance to work with talented people like Annie as they start their careers.\nThank you to Emily Riederer, who provided detailed comments on the initial plans for the book. She then returned to the manuscript after it was drafted and went through it in minute detail. Her thoughtful comments greatly improved the book. More broadly her work changed the way I thought about much of the content of this book.\nI was fortunate to have many reviewers who read entire chapters, sometimes two, three, or even more. They very much went above and beyond and provided excellent suggestions for improving this book. For this I am indebted to Albert Rapp, Alex Hayes, Alex Luscombe (who also suggested the Police Violence “Oh, you think\\(\\dots\\)” entry), Ariel Mundo, Benjamin Haibe-Kains, Dan Ryan, Erik Drysdale, Florence Vallée-Dubois, Jack Bailey, Jae Hattrick-Simpers, Jon Khan, Jonathan Keane (who also generously shared their parquet expertise), Lauren Kennedy (who also generously shared code, data, and expertise, to develop my thoughts about MRP), Liam Welsh, Liza Bolton (who also helped develop my ideas around how this book should be taught), Luis Correia, Matt Ratto, Matthias Berger, Michael Moon, Roberto Lentini, Ryan Briggs, and Taylor Wright.\nMany people made specific suggestions that greatly improved things. All these people contribute to the spirit of generosity that characterizes the open source programming languages communities this book builds on. I am grateful to them all. A Mahfouz made me realize that that covering Poisson regression was critical. Aaron Miller suggested the FINER framework. Alison Presmanes Hill suggested Wordbank. Chris Warshaw suggested the Democracy Fund Voter Study Group survey data. Christina Wei pointed out many code errors. Claire Battershill directed me toward many books about writing. Ella Kaye suggested, and rightly insisted on, moving to Quarto. Faria Khandaker suggested what became the “R essentials” chapter. Hareem Naveed generously shared her industry experience. Heath Priston provided assistance with Toronto homelessness data. Jessica Gronsbell gave invaluable suggestions around statistical practice. Keli Chiu reinforced the importance of text-as-data. Leslie Root came up with the idea of “Oh, you think we have good data on that!”. Michael Chong shaped my approach to EDA. Michael Donnelly, Peter Hepburn, and Léo Raymond-Belzile provided pointers to classic papers that I was unaware of, in political science, sociology, and statistics, respectively. Nick Horton suggested the Hadley Wickham video in Chapter 11. Paul Hodgetts taught me how to make R packages and made the cover art for this book. Radu Craiu ensured sampling was afforded its appropriate place. Sharla Gelfand the approaches I advocate around how to use R. Thomas William Rosenthal made me realize the potential of Shiny. Tom Cardoso and Zane Schwartz were excellent sources of data put together by journalists. Yanbo Tang assisted with Nancy Reid’s “Shoulders of giants” entry. Finally, Chris Maddison and Maia Balint suggested the closing poem.\nThank you to my PhD supervisory panel John Tang, Martine Mariotti, Tim Hatton, and Zach Ward. They gave me the freedom to explore the intellectual space that was of interest to me, the support to follow through on those interests, and the guidance to ensure that it all resulted in something tangible. What I learned during those years served as the foundation for this book.\nThis book has greatly benefited from the notes and teaching materials of others that are freely available online, including: Chris Bail, Scott Cunningham, Andrew Heiss (who independently taught a course with the same name as this book, well before the book was available), Lisa Lendway, Grant McDermott, Nathan Matias, David Mimno, and Ed Rubin. Thank you to these people. The changed norm of academics making their materials freely available online is a great one and one that I hope the free online version of this book, available here, helps contribute to.\nThank you to Samantha-Jo Caetano who helped develop some of the assessment items. And also, to Lisa Romkey and Alan Chong, who allowed me to adapt some aspects of their rubric. The catalysts for aspects of the Chapter 4 tutorial were McPhee (2017, 186) and Chelsea Parlett-Pelleriti. The idea behind the “Interactive communication” tutorial was work by Mauricio Vargas Sepúlveda (“Pachá”) and Andrew Whitby.\nI am grateful for the corrections of: Amy Farrow, Arsh Lakhanpal, Cesar Villarreal Guzman, Chloe Thierstein, Finn Korol-O’Dwyer, Flavia López, Gregory Power, Hong Shi, Jayden Jung, John Hayes, Joyce Xuan, Laura Cline, Lorena Almaraz De La Garza, Matthew Robertson, Michaela Drouillard, Mounica Thanam, Reem Alasadi, Rob Zimmerman, Tayedza Chikumbirike, Wijdan Tariq, Yang Wu, and Yewon Han.\nKelly Lyons provided support, guidance, mentorship, and friendship. Every day she demonstrates what an academic should be, and more broadly, what one should aspire to be as a person.\nGreg Wilson provided a structure to think about teaching and suggested the “Scales” style exercises. He was the catalyst for this book, and provided helpful comments on drafts. Every day he demonstrates how to contribute to the intellectual community.\nThank you to Elle Côté for enabling this book to be written by looking after first one, and then two, children during a pandemic.\nAs at Christmas 2021 this book was a disparate collection of partially completed notes; thank you to Mum and Dad, who dropped everything and came over from the other side of the world for two months to give me the opportunity to rewrite it all and put together a cohesive draft.\nThank you to Marija Taflaga and the ANU Australian Politics Studies Centre at the School of Politics and International Relations for funding a two-week “writing retreat” in Canberra.\nFinally, thank you to Monica Alexander. Without you I would not have written a book; I would not have even thought it possible. Many of the best ideas in this book are yours, and those that are not, you made better, by reading everything many times. Thank you for your inestimable help with writing this book, providing the base on which it builds (remember in the library showing me many times how to get certain rows in R!), giving me the time that I needed to write, encouragement when it turned out that writing a book just meant endlessly rewriting that which was perfect the day before, reading everything in this book many times, making coffee or cocktails as appropriate, looking after the children, and more.\nYou can contact me at: rohan.alexander@utoronto.ca.\n\nRohan Alexander\nToronto, Canada\nMay 2023\n\n\n\n\n\nDeWitt, Helen. 2000. The Last Samurai. 1st ed. United States: Talk Mirimax Books.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2013) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus; Giroux.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data Science. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWiessner, Polly. 2014. “Embers of Society: Firelight Talk Among the Ju/’hoansi Bushmen.” Proceedings of the National Academy of Sciences 111 (39): 14027–35. https://doi.org/10.1073/pnas.1404212111."
  },
  {
    "objectID": "01-introduction.html#on-telling-stories",
    "href": "01-introduction.html#on-telling-stories",
    "title": "1  Telling stories with data",
    "section": "1.1 On telling stories",
    "text": "1.1 On telling stories\nOne of the first things that many parents regularly do when their children are born is read stories to them. In doing so they carry on a tradition that has occurred for millennia. Myths, fables, and fairy tales can be seen and heard all around us. Not only are they entertaining, but they enable us to learn about the world. While The Very Hungry Caterpillar by Eric Carle may seem quite far from the world of dealing with data, there are similarities. Both aim to tell a story and impart knowledge.\nWhen using data we try to tell a convincing story. It may be as exciting as predicting elections, as banal as increasing internet advertising click rates, as serious as finding the cause of a disease, or as fun as forecasting basketball games. In any case the key elements are the same. The early twentieth century English author, E. M. Forster, described the aspects common to all novels as: story, people, plot, fantasy, prophecy, pattern, and rhythm (Forster 1927). Similarly, regardless of the setting, there are common concerns when we tell stories with data:\n\nWhat is the dataset? Who generated the dataset and why?\nWhat is the process that underpins the dataset? Given that process, what is missing from the dataset or has been poorly measured? Could other datasets have been generated, and if so, how different could they have been to the one that we have?\nWhat is the dataset trying to say, and how can we let it say this? What else could it say? How do we decide between these?\nWhat are we hoping others will see from this dataset, and how can we convince them of this? How much work must we do to convince them?\nWho is affected by the processes and outcomes, related to this dataset? To what extent are they represented in the dataset, and have they been involved in the analysis?\n\nIn the past, certain elements of telling stories with data were easier. For instance, experimental design has a long and robust tradition within agricultural and medical sciences, physics, and chemistry. Student’s t-distribution was identified in the early 1900s by a chemist, William Sealy Gosset, who worked at Guinness, a beer manufacturer (Boland 1984). It would have been relatively straightforward for him to randomly sample the beer and change one aspect at a time.\nMany of the fundamentals of the statistical methods that we use today were developed in such settings. In those circumstances, it was typically possible to establish control groups and randomize, and there were fewer ethical concerns. A story told with the resulting data was likely to be fairly convincing.\nUnfortunately, little of this applies these days, given the diversity of settings to which statistical methods are applied. On the other hand, we have many advantages. For instance, we have well-developed statistical techniques, easier access to large datasets, and open-source statistical languages such as R. But the difficulty of conducting traditional experiments means that we must also turn to other aspects to tell a convincing story."
  },
  {
    "objectID": "01-introduction.html#workflow-components",
    "href": "01-introduction.html#workflow-components",
    "title": "1  Telling stories with data",
    "section": "1.2 Workflow components",
    "text": "1.2 Workflow components\nThere are five core components to the workflow needed to tell stories with data:\n\nPlan and sketch an endpoint.\nSimulate and consider that simulated data.\nAcquire and prepare the actual data.\nExplore and understand the actual data.\nShare what was done and what was found.\n\nWe begin by planning and sketching an endpoint because this ensures that we think carefully about where we want to go. It forces us to deeply consider our situation, acts to keep us focused and efficient, and helps reduce scope creep. In Alice’s Adventures in Wonderland by Lewis Carroll, Alice asks the Cheshire Cat which way she should go. The Cheshire Cat replies by asking where Alice would like to go. And when Alice replies that she does not mind, so long as she gets somewhere, the Cheshire Cat says then the direction does not matter because one will always get somewhere if one “walks long enough”. The issue, in our case, is that we typically cannot afford to walk aimlessly for long. While it may be that the endpoint needs to change, it is important that this is a deliberate, reasoned decision. And that is only possible given an initial objective. There is no need to spend too much time on this to get a lot of value from it. Often ten minutes with paper and pen are enough.\nThe next step is to simulate data, because that forces us into the details. It helps with cleaning and preparing the dataset because it focuses us on the classes in the dataset and the distribution of the values that we expect. For instance, if we were interested in the effect of age-groups on political preferences, then we may expect that our age-group variable would be a factor, with four possible values: “18-29”, “30-44”, “45-59”, “60+”. The process of simulation provides us with clear features that our real dataset should satisfy. We could use these features to define tests that would guide our data cleaning and preparation. For instance, we could check our real dataset for age-groups that are not one of those four values. When those tests pass, we could be confident that our age-group variable only contains values that we expect.\nSimulating data is also important when we turn to statistical modeling. When we are at that stage, we are concerned with whether the model reflects what is in the dataset. The issue is that if we go straight to modeling the real dataset, then we do not know whether we have a problem with our model. We initially simulate data so that we precisely know the underlying data generation process. We then apply the model to the simulated dataset. If we get out what we put in, then we know that our model is performing appropriately, and can turn to the real dataset. Without that initial application to simulated data, it would be more difficult to have confidence in our model.\nSimulation is often cheap—almost free given modern computing resources and statistical programming languages—and fast. It provides “an intimate feeling for the situation”, (Hamming [1997] 2020, 239). Start with a simulation that just contains the essentials, get that working, and then complicate it.\nAcquiring and preparing the data that we are interested in is an often-overlooked stage of the workflow. This is surprising because it can be one of the most difficult stages and requires many decisions to be made. It is increasingly the subject of research, and it has been found that decisions made during this stage can affect statistical results (Huntington-Klein et al. 2021; Dolatsara et al. 2021).\nAt this stage of the workflow, it is common to feel a little overwhelmed. Typically, the data we can acquire leave us a little scared. There may be too little of it, in which case we worry about how we are going to be able to make our statistical machinery work. Alternatively, we may have the opposite problem and be worried about how we can even begin to deal with such a large amount of data.\n\nPerhaps all the dragons in our lives are princesses who are only waiting to see us act, just once, with beauty and courage. Perhaps everything that frightens us is, in its deepest essence, something helpless that wants our love.\nRilke ([1929] 2014)\n\nDeveloping comfort in this stage of the workflow unlocks the rest of it. The dataset that is needed to tell a convincing story is in there. But, like a sculptor, we need to iteratively remove everything that is not the data that we need, and to then shape that which is.\nAfter we have a dataset, we then want to explore and understand certain relationships in that dataset. We typically begin the process with descriptive statistics and then move to statistical models. The use of statistical models to understand the implications of our data is not free of bias, nor are they “truth”; they do what we tell them to do. When telling stories with data, statistical models are tools and approaches that we use to explore our dataset, in the same way that we may use graphs and tables. They are not something that will provide us with a definitive result but will enable us to understand the dataset more clearly in a particular way.\nBy the time we get to this step in the workflow, to a large extent, the model will reflect the decisions that were made in earlier stages, especially acquisition and cleaning, as much as it reflects any type of underlying data generating process. Sophisticated modelers know that their statistical models are like the bit of the iceberg above the surface: they build on, and are only possible due to, the majority that is underneath, in this case, the data. But when an expert at the whole data science workflow uses modeling, they recognize that the results that are obtained are additionally due to choices about whose data matters, decisions about how to measure and record the data, and other aspects that reflect the world as it is, well before the data are available to their specific workflow.\nFinally, we must share what we did and what we found, at as high a fidelity as is possible. Talking about knowledge that only you have does not make you knowledgeable, and that includes knowledge that only “past you” has. When communicating, we need to be clear about the decisions that we made, why we made them, our findings, and the weaknesses of our approach. We are aiming to uncover something important so we should write down everything in the first instance, although this written communication may be supplemented with other forms of communication later. There are so many decisions that we need to make in this workflow that we want to be sure that we are open about the entire thing—start to finish. This means much more than just the statistical modeling and creation of the graphs and tables, but everything. Without this, stories based on data lack credibility.\nThe world is not a rational meritocracy where everything is carefully and judiciously evaluated. Instead, we use shortcuts, hacks, and heuristics, based on our experience. Unclear communication will render even the best work moot, because it will not be thoroughly engaged with. While there is a minimum when it comes to communication, there is no upper limit to how impressive it can be. When it is the culmination of a thought-out workflow, it can even obtain a certain sprezzatura, or studied carelessness. Achieving such mastery requires years of work."
  },
  {
    "objectID": "01-introduction.html#telling-stories-with-data",
    "href": "01-introduction.html#telling-stories-with-data",
    "title": "1  Telling stories with data",
    "section": "1.3 Telling stories with data",
    "text": "1.3 Telling stories with data\nA compelling story based on data can likely be told in around ten-to-twenty pages. Anything less than this, and it is likely too light on some of the details. And while it is easy to write much more, often some reflection enables succinctness or for multiple stories to be separated.\nIt is possible to tell convincing stories even when it is not possible to conduct traditional experiments. These approaches do not rely on “big data”—which is not a cure-all (Meng 2018; Bradley et al. 2021)—but instead on better using the data that are available. Research and independent learning, a blend of theory and application, all combined with practical skills, a sophisticated workflow, and an appreciation for what one does not know, is often enough to create lasting knowledge.\nThe best stories based on data tend to be multi-disciplinary. They take from whatever field they need to, but almost always draw on statistics, computer science, economics, and engineering (to name a few). As such, an end-to-end workflow requires a blend of skills from these areas. The best way to learn these skills is to use real-world data to conduct research projects where you:\n\ndevelop research questions;\nobtain and clean relevant datasets;\nexplore the data to answer those questions; and\ncommunicate in a meaningful way.\n\nThe key elements of telling convincing stories with data are:\n\nCommunication.\nReproducibility.\nEthics.\nQuestions.\nMeasurement.\nData collection.\nData cleaning.\nExploratory data analysis.\nModeling.\nScaling.\n\nThese elements can be considered within a few different categories including: doing good research (ethics and questions), coming up with credible answers (measurement, collection, cleaning, exploratory data analysis, and modeling), and creating compelling explanations (communication, reproducibility, and scaling). These elements are the foundation on which the workflow is built (Figure 1.1).\n\n\n\nFigure 1.1: The workflow builds on various elements\n\n\nThis is a lot to master, but communication is the most important. Simple analysis, communicated well, is more valuable than complicated analysis communicated poorly. This is because the latter cannot be understood or trusted by others. A lack of clear communication sometimes reflects a failure by the researcher to understand what is going on, or even what they are doing. And so, while the level of the analysis should match the dataset, instrumentation, task, and skillset, when a trade-off is required between clarity and complication, it can be sensible to err on the side of clarity.\nClear communication means writing in plain language with the help of tables, graphs, and models, in a way that brings the audience along with you. It means setting out what was done and why, as well as what was found. The minimum standard is that this is done to an extent such that another person can independently do what you did and find what you found. One challenge is that as you immerse yourself in the data, it can be difficult to remember what it was like when you first came to it. But that is where most of your audience will be coming from. Learning to provide an appropriate level of nuance and detail can be challenging, but is made easier by focusing on writing for the benefit of the audience.\nReproducibility is required to create lasting knowledge about the world. It means that everything that was done—all of it, end-to-end—can be independently redone. Ideally, autonomous end-to-end reproducibility is possible; anyone can get the code, data, and environment to verify everything that was done (Heil et al. 2021). Unrestricted access to code is almost always possible. While that is the default expectation for data also, it is not always reasonable. For instance, studies in psychology may have small, personally identifying samples. One way forward is to openly share simulated data with similar properties, along with defining a process by which the real data could be accessed, given appropriate bona fides. Statistical models are commonly subject to an extensive suite of manual checks. Another aspect of reproducibility is that we similarly need to include a broad swathe of automated testing.\nActive consideration of ethics is needed because the dataset likely concerns humans. This means considering things like: who is in the dataset, who is missing, and why? To what extent will our story perpetuate the past? And is this something that ought to happen? Even if the dataset does not concern humans, the story is likely being put together by humans, and we affect almost everything else. This means that we have a responsibility to use data ethically, with concern for environmental impact and inequity.\nThere are many definitions of ethics, but when it comes to telling stories with data, at a minimum it means considering the full context of the dataset (D’Ignazio and Klein 2020). In jurisprudence, a textual approach to law means literally considering the words of the law as they are printed, while a purposive approach means laws are interpreted within a broader context. An ethical approach to telling stories with data means adopting the latter approach, and considering the social, cultural, historical, and political forces that shape our world, and hence our data (Crawford 2021).\nCuriosity provides internal motivation to explore a dataset, and associated process, to a proper extent. Questions tend to beget questions, and these usually improve and refine as the process of coming to understand a dataset carries on. In contrast to the stock Popperian approach of hypothesis testing often taught, questions are typically developed through a continuous and evolving process (Franklin 2005). Finding an initial question can be challenging. It is especially tough to operationalize research questions into measurable variables that are reasonably available. Selecting an area of interest can help, as can sketching a broad claim with the intent of evolving it into a specific question, and finally, bringing together two different areas.\nDeveloping a comfort and ease in the messiness of real-world data means getting to ask new questions each time the data update. And knowing a dataset in detail tends to surface unexpected groupings or values that you can then work with subject-area experts to understand. Becoming a bit of a “hybrid” by developing a base of knowledge across a variety of areas is especially valuable, as is becoming comfortable with the possibility of initially asking dumb questions.\nMeasurement and data collection are about deciding how our world will become data. They are challenging. The world is so vibrant that it is difficult to reduce it to something that is possible to consistently measure and collect. Take, for instance, someone’s height. We can, probably, all agree that we should take our shoes off before we measure height. But our height changes over the course of the day. And measuring someone’s height with a tape measure will give different results to using a laser. If we are comparing heights between people or over time, it therefore becomes important to measure at the same time each day, using the same method. But that quickly becomes unfeasible.\nMost of the questions we are interested in will use data that are more complicated than height. How do we measure how sad someone is? How do we measure pain? Who decides what we will measure and how we will measure it? There is a certain arrogance required to think that we can reduce the world to a value and then compare these. Ultimately, we must, but it is difficult to consistently define what is to be measured. This process is not value-free. The only way to reasonably come to terms with this brutal reduction is to deeply understand and respect what we are measuring and collecting. What is the central essence, and what can be stripped away?\nPablo Picasso, the twentieth century Spanish painter, has a series of drawings where he depicts the outline of an animal using only one line (Figure 1.2). Despite their simplicity, we recognize which animal is being depicted—the drawing is sufficient to tell the animal is a dog, not a cat. Could this be used to determine whether the dog is sick? Probably not. We would likely want a different depiction. The decision as to which things should be measured, and then of those things that we decide to consider, which features should be measured and collected, and which to ignore, turns on context and purpose.\n\n\n\nFigure 1.2: This drawing is clearly a dog, even though it is just one line\n\n\nData cleaning and preparation is a critical part of using data. We need to massage the data available to us into a dataset that we can use. This requires making a lot of decisions. The data cleaning and preparation stage is critical, and worthy of as much attention and care as any other.\nFollowing Kennedy et al. (2022) consider a survey that collected information about a potentially sensitive topic, gender, using four options: “man”, “woman”, “prefer not to say”, and “other”, where “other” dissolved into an open textbox. When we come to that dataset, we are likely to find that most responses are either “man” or “woman”. We need to decide what to do about “prefer not to say”. If we drop it from our dataset, then we are actively ignoring these respondents. If we do not drop it, then it makes our analysis more complicated. Similarly, we need to decide how to deal with the open text responses. Again, we could drop these responses, but this ignores the experiences of some of our respondents. Another option is to merge this with “prefer not to say”, but that shows a disregard for our respondents, because they specifically did not choose that option.\nThere is no easy, nor always-correct, choice in many data cleaning and preparation situations. It depends on context and purpose. Data cleaning and preparation involves making many choices like this, and it is vital to record every step so that others can understand what was done and why. Data never speak for themselves; they are the puppets of the ventriloquists that cleaned and prepared them.\nThe process of coming to understand the look and feel of a dataset is termed exploratory data analysis (EDA). This is an open-ended process. We need to understand the shape of our dataset before we can formally model it. The process of EDA is an iterative one that involves producing summary statistics, graphs, tables, and sometimes even some modeling. It is a process that never formally finishes and requires a variety of skills.\nIt is difficult to delineate where EDA ends and formal statistical modeling begins, especially when considering how beliefs and understanding develop (Hullman and Gelman 2021). But at its core, it starts with the data, and involves immersing ourselves in it (Cook, Reid, and Tanaka 2021). EDA is not typically explicitly included in our final story. But it has a central role in how we come to understand the story we are telling. It is critical that all the steps taken during EDA are recorded and shared.\nStatistical modeling has a long and robust history. Our knowledge of statistics has been built over hundreds of years. Statistics is not a series of dry theorems and proofs but is instead a way of exploring the world. It is analogous to “a knowledge of foreign languages or of algebra: it may prove of use at any time under any circumstances” (Bowley 1901, 4). A statistical model is not a recipe to be naively followed in an if-this-then-that way but is instead a way of understanding data (James et al. [2013] 2021). Modeling is usually required to infer statistical patterns from data. More formally, statistical inference is “the process of using data to infer the distribution that generated the data” (Wasserman 2005, 87).\nStatistical significance is not the same as scientific significance, and we are realizing the cost of what has been the dominant paradigm. Using an arbitrary pass/fail statistical test on our data is rarely appropriate. Instead, the proper use for statistical modeling is as a kind of echolocation. We listen to what comes back to us from the model, to help learn about the shape of the world, while recognizing that it is only one representation of the world.\nThe use of statistical programming languages, such as R, enables us to rapidly scale our work. This refers to both inputs and outputs. It is basically just as easy to consider ten observations as 1,000, or even 1,000,000. This enables us to more quickly see the extent to which our stories apply. It is also the case that our outputs can be consumed as easily by one person as by ten, or 100. Using an Application Programming Interface (API) it is even possible for our stories to be considered many thousands of times each second."
  },
  {
    "objectID": "01-introduction.html#how-do-our-worlds-become-data",
    "href": "01-introduction.html#how-do-our-worlds-become-data",
    "title": "1  Telling stories with data",
    "section": "1.4 How do our worlds become data?",
    "text": "1.4 How do our worlds become data?\n\nThere is the famous story by Eddington about some people who went fishing in the sea with a net. Upon examining the size of the fish they had caught, they decided there was a minimum size to the fish in the sea! Their conclusion arose from the tool used and not from reality.\nHamming ([1997] 2020, 177)\n\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could forecast perfectly a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex world from which they were derived.\nThere are different approximations of “plausibly measurable”. Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data.\nMuch of statistics is focused on considering, thoroughly, the data that we have. That was appropriate for when our data were agricultural, astronomical, or from the physical sciences. This is not to say that systemic bias cannot exist or have an impact in non-human contexts, but with the rise of data science, partly because of the value of its application to datasets generated by humans, we must also actively consider what is not in our dataset. Who is systematically missing from our dataset? Whose data do not fit nicely into the approach that we are using and are hence being inappropriately simplified? If the process of the world becoming data requires abstraction and simplification, then we need to be clear about when we can reasonably simplify and when it would be inappropriate.\nThe process of our world becoming data necessarily involves measurement. Paradoxically, often those that do the measurement and are deeply immersed in the details have less trust in the data than those who are removed from it. Even seemingly clear tasks, such as measuring distance, defining boundaries, and counting populations, are surprisingly difficult in practice. Turning our world into data requires many decisions and imposes much error. Among many other considerations, we need to decide what will be measured, how accurately we will do this, and who will be doing the measurement.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nAn important example of how something seemingly simple quickly becomes difficult is maternal-related deaths. That refers to the number of women who die while pregnant, or soon after a termination, from a cause related to the pregnancy or its management (World Health Organization 2019). It is difficult but critical to turn the tragedy of such a death into cause-specific data because that helps mitigate future deaths. Some countries have well-developed civil registration and vital statistics (CRVS), which collect data about every death. But many countries lack a CRVS, resulting in unrecorded deaths. Even if a death is recorded, defining a cause of death may be difficult, especially when there is a lack of qualified medical personal or equipment. Maternal deaths are particularly difficult because there are typically many causes. Some CRVS systems have a checkbox on the death registration form to specify whether the death should be counted as maternal. But even some developed countries have only recently adopted this. For instance, it was only introduced in the United States in 2003, and even in 2015 Alabama, California, and West Virginia had not adopted the standard question (MacDorman and Declercq 2018). This means there is a risk that maternal deaths are under-reported or misclassified.\n\n\nWe typically use various instruments to turn the world into data. In astronomy, the development of better telescopes, and eventually satellites and probes, enabled new understanding of other worlds. Similarly, we have new instruments for turning our own world into data being developed each day. Where once a census was a generation-defining event, now we have regular surveys, transaction data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories.\nOur world imperfectly becomes data. If we are to use data nonetheless to learn about the world, then we need to actively seek to understand their imperfections and the implications of those imperfections."
  },
  {
    "objectID": "01-introduction.html#what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world",
    "href": "01-introduction.html#what-is-data-science-and-how-should-we-use-it-to-learn-about-the-world",
    "title": "1  Telling stories with data",
    "section": "1.5 What is data science and how should we use it to learn about the world?",
    "text": "1.5 What is data science and how should we use it to learn about the world?\nThere is no agreed definition of data science. Wickham, Çetinkaya-Rundel, and Grolemund ([2016] 2023) say it “…allows you to turn raw data into understanding, insight, and knowledge”. Similarly, Leek and Peng (2020) contend that it is “\\(\\dots\\)the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience”. Baumer, Kaplan, and Horton (2021) consider it a “\\(\\dots\\)science of extracting meaningful information from data”. And Timbers, Campbell, and Lee (2022) define it as “the process of generating insight from data through reproducible and auditable processes”. From an earlier age Foster (1968) points clearly to what we now call data science when he says: “(s)tatistics are concerned with the processing and analysis of masses of data and with the development of mathematical methods of extracting information from data. Combine all this activity with computer methods and you have something more than the sum of its parts.”\nCraiu (2019) argues that the lack of certainty as to what data science is might not matter because “\\(\\dots\\)who can really say what makes someone a poet or a scientist?” He goes on to broadly say that a data scientist is “\\(\\dots\\)someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.”\nIn any case, alongside specific, technical definitions, there is value in having a simple definition, even if we lose a bit of specificity. Probability is often informally defined as “counting things” (McElreath [2015] 2020, 10). In a similar informal sense, data science can be defined as something like: humans measuring things, typically related to other humans, and using sophisticated averaging to explain and predict. We revisit this in Chapter 17 to provide a more detailed definition.\nThat may sound a touch cute, but Francis Edgeworth, the nineteenth century statistician and economist, considered statistics to be the science “of those Means which are presented by social phenomena”, so it finds itself in good company (Edgeworth 1885). In any case, one feature of this definition is that it does not treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, yet there are many cases in statistics where data kind of just appear; they belong to nobody. But that is never actually the case.\nData are generated, and then must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.\nMuch of data science focuses on the “science”, but it is important to also focus on the “data”. And that is another characteristic of that cutesy definition of data science. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And frequently it is the specifics of those data that require the most time, that update most often, and that are worthy of our most full attention.\nJordan (2019) describes being in a medical office and being given some probability, based on prenatal initial screening, that his child, then a fetus, had Down syndrome. By way of background, one can do a test to know for sure, but that test comes with the risk of the fetus not surviving, so this initial screening is done and then parents typically use the probability of Down syndrome from that initial screening to decide whether to do the conclusive test. Jordan (2019) found the probabilities provided by the initial screening were being determined based on a study done a decade earlier in the United Kingdom. The issue was that in the ensuing ten years, imaging technology had improved so the initial screening was not expecting such high-resolution images, and there had been a subsequent (false) increase in Down syndrome diagnoses from the initial screening. The data were the problem.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Michael Jordan is the Pehong Chen Distinguished Professor at the University of California, Berkeley. After earning a PhD in Cognitive Science from University of California, San Diego, in 1985, he was appointed as an assistant professor at MIT, being promoted to full professor in 1997, and in 1998 he moved to Berkeley. One area of his research is statistical machine learning. For instance, one particularly important paper is Blei, Ng, and Jordan (2003), which defined how text could be grouped together to define topics, and we cover this in Chapter 16.\n\n\nIt is not just the “science” bit that is hard, it is the “data” bit as well. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were improperly duplicated (Bandy and Vincent 2021). There is an entire field—linguistics—that specializes in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together people with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. This means that we must go out of our way to show respect for those who do not come from our own tradition, but who are as interested in a dataset as we are. Data science is multi-disciplinary and increasingly critical; hence it must reflect our world. There is a need for a diversity of backgrounds, of approaches, and of disciplines in data science.\nOur world is messy, and so are our data. To successfully tell stories with data you need to become comfortable with the fact that the process will be difficult. Hannah Fry, the British mathematician, describes spending six months rewriting code before it solved her problem (Thornhill 2021). You need to learn to stick with it. You also need to accept failure at times, and you do this by developing resilience and having intrinsic motivation. The world of data is about considering possibilities and probabilities, and learning to make trade-offs between them. There is hardly anything that we know for certain, and there is no perfect analysis.\nUltimately, we are all just telling stories with data, but these stories are increasingly among the most important in the world."
  },
  {
    "objectID": "01-introduction.html#exercises",
    "href": "01-introduction.html#exercises",
    "title": "1  Telling stories with data",
    "section": "1.6 Exercises",
    "text": "1.6 Exercises\n\nQuestions\n\nAccording to Register (2020) data decisions impact (pick one)?\n\nReal people.\nNo one.\nThose in the training set.\nThose in the test set.\n\nWhat is data science (in your own words)?\nAccording to Keyes (2019) what is data science (pick one)?\n\nThe inhumane reduction of humanity down to what can be counted.\nThe quantitative analysis of large amounts of data for the purpose of decision-making.\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from many structured and unstructured data.\n\nImagine that you have a job in which including race and/or sexuality as predictors improves the performance of your model. When deciding whether to include these in your analysis, what factors would you consider (in your own words)?\nAccording to Crawford (2021), as described in this chapter, which of the following forces shape our world, and hence our data (select all that apply)?\n\nPolitical.\nHistorical.\nCultural.\nSocial.\n\nWhy is ethics a key element of telling convincing stories (in your own words)?\nConsider the results of a survey that asked about gender. It finds the following counts: “man: 879”, “woman: 912”, “non-binary: 10” “prefer not to say: 3”, and “other: 1”. What is the appropriate way to consider “prefer not to say” (pick one)?\n\nDrop them.\nMerge it into “other”.\nInclude them.\nIt depends.\n\n\n\n\nTutorial\nThe purpose of this tutorial is to clarify in your mind the difficulty of measurement, even of seemingly simple things, and hence the likelihood of measurement issues in more complicated areas.\nPlease obtain some seeds for a fast-growing plant such as radishes, mustard greens, or arugula. Plant the seeds and measure how much soil you used. Water them and measure the water you used. Each day take a note of any changes. More generally, measure and record as much as you can. Note your thoughts about the difficulty of measurement. Eventually your seeds will sprout, and you should measure how they grow. We will return to use the data that you gathered.\n\n\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing ‘Documentation Debt’ in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” arXiv. https://doi.org/10.48550/arXiv.2105.05241.\n\n\nBaumer, Benjamin, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science With R. 2nd ed. Chapman; Hall/CRC. https://mdsr-book.github.io/mdsr2e/.\n\n\nBlei, David, Andrew Ng, and Michael Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022. https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf.\n\n\nBoland, Philip. 1984. “A Biographical Glimpse of William Sealy Gosset.” The American Statistician 38 (3): 179–83. https://doi.org/10.2307/2683648.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P. S. King.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic, Xiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big Surveys Significantly Overestimated US Vaccine Uptake.” Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nCook, Dianne, Nancy Reid, and Emi Tanaka. 2021. “The Foundation Is Available for Thinking about Data Visualization Inferentially.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.8453435d.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed, and Allison Jones-Farmer. 2021. “Explaining Predictive Model Performance: An Experimental Study of Data Preparation and Model Choice.” Big Data, October. https://doi.org/10.1089/big.2021.0067.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.” Journal of the Statistical Society of London, 181–217.\n\n\nFord, Paul. 2015. “What Is Code?” Bloomberg Businessweek, June. https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.\n\n\nForster, Edward Morgan. 1927. Aspects of the Novel. London: Edward Arnold.\n\n\nFoster, Gordon. 1968. “Computers, Statistics and Planning: Systems or Chaos?” Geary Lecture. https://www.esri.ie/system/files/publications/GLS2.pdf.\n\n\nFranklin, Laura. 2005. “Exploratory Experiments.” Philosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing Science and Engineering. 2nd ed. Stripe Press.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, and Stephanie Hicks. 2021. “Reproducibility Standards for Machine Learning in the Life Sciences.” Nature Methods 18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHullman, Jessica, and Andrew Gelman. 2021. “Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2013) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJordan, Michael. 2019. “Artificial Intelligence–The Revolution Hasn’t Happened Yet.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, Andrew Gelman, Yajun Jia, and Julien Teitler. 2022. “He, She, They: Using Sex and Gender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real Life. https://reallifemag.com/counting-the-countless/.\n\n\nLeek, Jeff, and Roger Peng. 2020. “Advanced Data Science 2020.” http://jtleek.com/ads2020/index.html.\n\n\nMacDorman, Marian, and Eugene Declercq. 2018. “The Failure of United States Maternal Mortality Reporting and Its Impact on Women’s Lives.” Birth 45 (2): 105–8. https://doi.org/1111/birt.12333.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nRegister, Yim. 2020. “Data Science Ethics in 6 Minutes.” YouTube, December. https://youtu.be/mA4gypAiRYU.\n\n\nRilke, Rainer Maria. (1929) 2014. Letters to a Young Poet. Penguin Classics.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah Fry.” Financial Times, July. https://www.ft.com/content/a5e33e5a-99b9-4bbc-948f-8a527c7675c3.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWorld Health Organization. 2019. “Trends in Maternal Mortality 2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the United Nations Population Division.” https://apps.who.int/iris/handle/10665/327596."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#hello-world",
    "href": "02-drinking_from_a_fire_hose.html#hello-world",
    "title": "2  Drinking from a fire hose",
    "section": "2.1 Hello, World!",
    "text": "2.1 Hello, World!\nThe way to start, is to start. In this chapter we go through three complete examples of the data science workflow advocated in this book. This means we:\n\\[\n\\mbox{Plan} \\rightarrow \\mbox{Simulate} \\rightarrow \\mbox{Acquire} \\rightarrow \\mbox{Explore} \\rightarrow \\mbox{Share}\n\\] If you are new to R, then some of the code may be a bit unfamiliar to you. If you are new to statistics, then some of the concepts may be unfamiliar. Do not worry. It will all soon become familiar.\nThe only way to learn how to tell stories, is to start telling stories yourself. This means that you should try to get these examples working. Do the sketches yourself, type everything out yourself (using Posit Cloud if you are new to R and do not have it locally installed), and execute it all. It is important to realize that it will be challenging at the start. This is normal.\n\nWhenever you’re learning a new tool, for a long time, you’re going to suck\\(\\dots\\) But the good news is that is typical; that’s something that happens to everyone, and it’s only temporary.\nHadley Wickham as quoted by Barrett (2021).\n\nYou will be guided thoroughly here. Hopefully by experiencing the excitement of telling stories with data, you will feel empowered to stick with it.\nThe first step of the workflow is to plan. We do this because we need to establish an end-point, even if we later need to update it as we learn more about the situation. We then simulate because this forces us into the details of our plan. In some projects, data acquisition may be as straight-forward as downloading a dataset, but in others the data acquisition may be much of the focus, for instance, if we conduct a survey. We explore the data using various quantitative methods to come to understand it. And finally, we share our understanding, in a way that is focused on the needs of our audience.\nTo get started, go to Posit Cloud and create an account; the free version is fine for now. We use that initially, rather than the desktop, so that getting started is the same for everyone, but to avoid having to pay you should change to a local installation later. Once you have an account and log in, it should look something like Figure 2.1 (a).\n\n\n\n\n\n\n\n(a) Opening Posit Cloud for the first time\n\n\n\n\n\n\n\n(b) Opening a new RStudio project\n\n\n\n\nFigure 2.1: Getting started with Posit Cloud and a new project\n\n\nYou will be in “Your Projects”. From here you should start a new project: “New Project” \\(\\rightarrow\\) “New RStudio Project” (Figure 2.1 (b)). You can give the project a name by clicking on “Untitled Project” and replacing it.\nWe will now go through three worked examples: Australian elections, Toronto shelter usage, and neonatal mortality. These examples build increasing complexity, but from the first one, we will be telling a story with data. While we briefly explain many aspects here, almost everything is explained in much more detail in the rest of the book."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#australian-elections",
    "href": "02-drinking_from_a_fire_hose.html#australian-elections",
    "title": "2  Drinking from a fire hose",
    "section": "2.2 Australian elections",
    "text": "2.2 Australian elections\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the lower house and that from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties and independents. In this example we will create a graph of the number of seats that each party won in the 2022 Federal Election.\n\n2.2.1 Plan\nFor this example, we need to plan two aspects. The first is what the dataset that we need will look like, and the second is what the final graph will look like.\nThe basic requirement for the dataset is that it has the name of the seat (sometimes called a “division” in Australia) and the party of the person elected. A quick sketch of the dataset that we would need is Figure 2.2 (a).\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset that could be useful for analyzing Australian elections\n\n\n\n\n\n\n\n(b) Quick sketch of a possible graph of the number of seats won by each party\n\n\n\n\nFigure 2.2: Sketches of a potential dataset and graph related to an Australian election\n\n\nWe also need to plan the graph that we are interested in. Given we want to display the number of seats that each party won, a quick sketch of what we might aim for is Figure 2.2 (b).\n\n\n2.2.2 Simulate\nWe now simulate some data, to bring some specificity to our sketches.\nTo get started, within Posit Cloud, make a new Quarto document: “File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “Quarto document\\(\\dots\\)”. Give it a title, such as “Exploring the 2022 Australian Election”, add your name as author, and unclick “Use visual markdown editor” (Figure 2.3 (a)). Leave the other options as their default, and then click “Create”.\n\n\n\n\n\n\n\n(a) Creating a new Quarto document\n\n\n\n\n\n\n\n(b) Installing rmarkdown if necessary\n\n\n\n\n\n\n\n\n\n(c) After initial setup and with a preamble\n\n\n\n\n\n\n\n(d) Highlighting the green arrow to run the chunk\n\n\n\n\n\n\n\n\n\n(e) Highlighting the cross to remove the messages\n\n\n\n\n\n\n\n(f) Highlighting the render button\n\n\n\n\nFigure 2.3: Getting started with a Quarto document\n\n\nYou may get a notification along the lines of “Package rmarkdown required\\(\\dots\\).” (Figure 2.3 (b)). If that happens, click “Install”. For this example, we will put everything into this one Quarto document. You should save it as “australian_elections.qmd”: “File” \\(\\rightarrow\\) “Save As\\(\\dots\\)”.\nRemove almost all the default content, and then beneath the heading material create a new R code chunk: “Code” \\(\\rightarrow\\) “Insert Chunk”. Then add preamble documentation that explains:\n\nthe purpose of the document;\nthe author and contact details;\nwhen the file was written or last updated; and\nprerequisites that the file relies on.\n\n\n#### Preamble ####\n# Purpose: Read in data from the 2022 Australian Election and make\n# a graph of the number of seats each party won.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 January 2023\n# Prerequisites: Know where to get Australian elections data.\n\nIn R, lines that start with “#” are comments. This means that they are not run as code by R, but are instead designed to be read by humans. Each line of this preamble should start with a “#”. Also make it clear that this is the preamble section by surrounding that with “####”. The result should look like Figure 2.3 (c).\nAfter this we need to setup the workspace. This involves installing and loading any packages that will be needed. A package only needs to be installed once for each computer, but needs to be loaded each time it is to be used. In this case we are going to use the tidyverse and janitor packages. They will need to be installed because this is the first time they are being used, and then each will need to be loaded.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nHadley Wickham is Chief Scientist at RStudio. After earning a PhD in Statistics from Iowa State University in 2008 he was appointed as an assistant professor at Rice University, and became Chief Scientist at RStudio, now Posit, in 2013. He developed the tidyverse collection of packages, and has published many books including R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund [2016] 2023) and Advanced R (Wickham 2019). He was awarded the COPSS Presidents’ Award in 2019.\n\n\nAn example of installing the packages follows. Run this code by clicking the small green arrow associated with the R code chunk (Figure 2.3 (d)).\n\n#### Workspace setup ####\ninstall.packages(\"tidyverse\")\ninstall.packages(\"janitor\")\n\nNow that the packages are installed, they need to be loaded. As that package installation step only needs to be done once per computer, that code can be commented out so that it is not accidentally run, or even just removed. Additionally, we can remove the message that printed when we installed the packages (Figure 2.3 (e)).\n\n#### Workspace setup ####\n# install.packages(\"tidyverse\")\n# install.packages(\"janitor\")\n\nlibrary(tidyverse)\nlibrary(janitor)\n\nWe can render the entire document by clicking “Render” (Figure 2.3 (f)). When you do this, you may be asked to install some packages. If that happens, then you should agree to this. This will result in a HTML document.\nFor an introduction to the packages that were just installed, each package contains a help file that provides information about them and their functions. It can be accessed by prepending a question mark to the package name and then running that code in the console. For instance ?tidyverse.\nTo simulate our data, we need to create a dataset with two variables: “Division” and “Party”, and some values for each. In the case of “Division” reasonable values would be a name of one of the 151 Australian divisions. In the case of “Party” reasonable values would be one of the following five: “Liberal”, “Labor”, “National”, “Green”, or “Other”. Again, this code can be run by clicking the small green arrow associated with the R code chunk.\n\nsimulated_data &lt;-\n  tibble(\n    # Use 1 through to 151 to represent each division\n    \"Division\" = 1:151,\n    # Randomly pick an option, with replacement, 151 times\n    \"Party\" = sample(\n      x = c(\"Liberal\", \"Labor\", \"National\", \"Green\", \"Other\"),\n      size = 151,\n      replace = TRUE\n    )\n  )\n\nsimulated_data\n\n# A tibble: 151 × 2\n   Division Party   \n      &lt;int&gt; &lt;chr&gt;   \n 1        1 Liberal \n 2        2 Other   \n 3        3 Labor   \n 4        4 Labor   \n 5        5 National\n 6        6 National\n 7        7 Green   \n 8        8 Other   \n 9        9 Labor   \n10       10 Labor   \n# ℹ 141 more rows\n\n\nAt a certain point, your code will not run and you will want to ask others for help. Do not take a screenshot of a small snippet of the code and expect that someone will be able to help based on that. They, almost surely, cannot. Instead, you need to provide them with your whole script in a way that they can run. We will explain what GitHub is more completely in Chapter 3, but for now, if you need help, then you should naively create a GitHub Gist which will enable you to share your code in a way that is more helpful than taking a screenshot. The first step is to create a free account on GitHub (Figure 2.4 (a)). Thinking about an appropriate username is important because this will become part of your professional profile. It would make sense to have a username that is professional, independent of any course, and ideally related to your real name. Then look for a “+” in the top right, and select “New gist” (Figure 2.4 (b)).\n\n\n\n\n\n\n\n(a) GitHub sign-up screen\n\n\n\n\n\n\n\n(b) New GitHub Gist\n\n\n\n\n\n\n\n\n\n(c) Create a public GitHub Gist to share code\n\n\n\n\nFigure 2.4: Creating a Gist to share code when asking for help\n\n\nFrom here you should add all the code to that Gist, not just the final bit that is giving an error. And give it a meaningful filename that includes “.R” at the end, for instance, “australian_elections.R”. In Figure 2.4 (c) it will turn out that we have incorrect capitalization, library(Tidyverse) instead of library(tidyverse).\nClick “Create public gist”. We can then share the URL to this Gist with whoever we are asking to help, explain what the problem is, and what we are trying to achieve. It will be easier for them to help, because all the code is available.\n\n\n2.2.3 Acquire\nNow we want to get the actual data. The data we need is from the Australian Electoral Commission (AEC), which is the non-partisan agency that organizes Australian federal elections. We can pass a page of their website to read_csv() from readr. We do not need to explicitly load readr because it is part of the tidyverse. The &lt;- or “assignment operator” allocates the output of read_csv() to an object called “raw_elections_data”.\n\n#### Read in the data ####\nraw_elections_data &lt;-\n  read_csv(\n    file = \n      \"https://results.aec.gov.au/27966/website/Downloads/HouseMembersElectedDownload-27966.csv\",\n    show_col_types = FALSE,\n    skip = 1\n  )\n\n# We have read the data from the AEC website. We may like to save\n# it in case something happens or they move it.\nwrite_csv(\n  x = raw_elections_data,\n  file = \"australian_voting.csv\"\n)\n\nWe can take a quick look at the dataset using head() which will show the first six rows, and tail() which will show the last six rows.\n\nhead(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm   Surname   PartyNm  PartyAb\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;  \n1        179 Adelaide   SA            36973 Steve     GEORGANAS Austral… ALP    \n2        197 Aston      VIC           36704 Alan      TUDGE     Liberal  LP     \n3        198 Ballarat   VIC           36409 Catherine KING      Austral… ALP    \n4        103 Banks      NSW           37018 David     COLEMAN   Liberal  LP     \n5        180 Barker     SA            37083 Tony      PASIN     Liberal  LP     \n6        104 Barton     NSW           36820 Linda     BURNEY    Austral… ALP    \n\ntail(raw_elections_data)\n\n# A tibble: 6 × 8\n  DivisionID DivisionNm StateAb CandidateID GivenNm    Surname  PartyNm  PartyAb\n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;  \n1        152 Wentworth  NSW           37451 Allegra    SPENDER  Indepen… IND    \n2        153 Werriwa    NSW           36810 Anne Maree STANLEY  Austral… ALP    \n3        150 Whitlam    NSW           36811 Stephen    JONES    Austral… ALP    \n4        178 Wide Bay   QLD           37506 Llew       O'BRIEN  Liberal… LNP    \n5        234 Wills      VIC           36452 Peter      KHALIL   Austral… ALP    \n6        316 Wright     QLD           37500 Scott      BUCHHOLZ Liberal… LNP    \n\n\nWe need to clean the data so that we can use it. We are trying to make it similar to the dataset that we thought we wanted in the planning stage. While it is fine to move away from the plan, this needs to be a deliberate, reasoned decision. After reading in the dataset that we saved, the first thing that we will do is adjust the names of the variables. We will do this using clean_names() from janitor.\n\n#### Basic cleaning ####\nraw_elections_data &lt;-\n  read_csv(\n    file = \"australian_voting.csv\",\n    show_col_types = FALSE\n  )\n\n\n# Make the names easier to type\ncleaned_elections_data &lt;-\n  clean_names(raw_elections_data)\n\n# Have a look at the first six rows\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 8\n  division_id division_nm state_ab candidate_id given_nm  surname   party_nm    \n        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       \n1         179 Adelaide    SA              36973 Steve     GEORGANAS Australian …\n2         197 Aston       VIC             36704 Alan      TUDGE     Liberal     \n3         198 Ballarat    VIC             36409 Catherine KING      Australian …\n4         103 Banks       NSW             37018 David     COLEMAN   Liberal     \n5         180 Barker      SA              37083 Tony      PASIN     Liberal     \n6         104 Barton      NSW             36820 Linda     BURNEY    Australian …\n# ℹ 1 more variable: party_ab &lt;chr&gt;\n\n\nThe names are faster to type because RStudio will auto-complete them. To do this, we begin typing the name of a variable and then use the “tab” key to complete it.\nThere are many variables in the dataset, and we are primarily interested in two: “division_nm” and “party_nm”. We can choose certain variables of interest with select() from dplyr which we loaded as part of the tidyverse. The “pipe operator”, |&gt;, pushes the output of one line to be the first input of the function on the next line.\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  select(\n    division_nm,\n    party_nm\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division_nm party_nm              \n  &lt;chr&gt;       &lt;chr&gt;                 \n1 Adelaide    Australian Labor Party\n2 Aston       Liberal               \n3 Ballarat    Australian Labor Party\n4 Banks       Liberal               \n5 Barker      Liberal               \n6 Barton      Australian Labor Party\n\n\nSome of the variable names are still not obvious because they are abbreviated. We can look at the names of the columns in this dataset with names(). And we can change the names using rename() from dplyr.\n\nnames(cleaned_elections_data)\n\n[1] \"division_nm\" \"party_nm\"   \n\n\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  rename(\n    division = division_nm,\n    elected_party = party_nm\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party         \n  &lt;chr&gt;    &lt;chr&gt;                 \n1 Adelaide Australian Labor Party\n2 Aston    Liberal               \n3 Ballarat Australian Labor Party\n4 Banks    Liberal               \n5 Barker   Liberal               \n6 Barton   Australian Labor Party\n\n\nWe could now look at the unique values in the “elected_party” column using unique().\n\ncleaned_elections_data$elected_party |&gt;\n  unique()\n\n[1] \"Australian Labor Party\"              \n[2] \"Liberal\"                             \n[3] \"Liberal National Party of Queensland\"\n[4] \"The Greens\"                          \n[5] \"The Nationals\"                       \n[6] \"Independent\"                         \n[7] \"Katter's Australian Party (KAP)\"     \n[8] \"Centre Alliance\"                     \n\n\nAs there is more detail in this than we wanted, we may want to simplify the party names to match what we simulated, using case_match() from dplyr.\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  mutate(\n    elected_party =\n      case_match(\n        elected_party,\n        \"Australian Labor Party\" ~ \"Labor\",\n        \"Liberal National Party of Queensland\" ~ \"Liberal\",\n        \"Liberal\" ~ \"Liberal\",\n        \"The Nationals\" ~ \"Nationals\",\n        \"The Greens\" ~ \"Greens\",\n        \"Independent\" ~ \"Other\",\n        \"Katter's Australian Party (KAP)\" ~ \"Other\",\n        \"Centre Alliance\" ~ \"Other\"\n      )\n  )\n\nhead(cleaned_elections_data)\n\n# A tibble: 6 × 2\n  division elected_party\n  &lt;chr&gt;    &lt;chr&gt;        \n1 Adelaide Labor        \n2 Aston    Liberal      \n3 Ballarat Labor        \n4 Banks    Liberal      \n5 Barker   Liberal      \n6 Barton   Labor        \n\n\nOur data now matches our plan (Figure 2.2 (a)). For every electoral division we have the party of the person that won it.\nHaving now nicely cleaned the dataset, we should save it, so that we can start with that cleaned dataset in the next stage. We should make sure to save it under a new file name so we are not replacing the raw data, and so that it is easy to identify the cleaned dataset later.\n\nwrite_csv(\n  x = cleaned_elections_data,\n  file = \"cleaned_elections_data.csv\"\n)\n\n\n\n2.2.4 Explore\nWe may like to explore the dataset that we created. One way to better understand a dataset is to make a graph. In particular, here we would like to build the graph that we planned in Figure 2.2 (b).\nFirst, we read in the dataset that we just created.\n\n#### Read in the data ####\ncleaned_elections_data &lt;-\n  read_csv(\n    file = \"cleaned_elections_data.csv\",\n    show_col_types = FALSE\n  )\n\nWe can get a quick count of how many seats each party won using count() from dplyr.\n\ncleaned_elections_data |&gt;\n  count(elected_party)\n\n# A tibble: 5 × 2\n  elected_party     n\n  &lt;chr&gt;         &lt;int&gt;\n1 Greens            4\n2 Labor            77\n3 Liberal          48\n4 Nationals        10\n5 Other            12\n\n\nTo build the graph that we are interested in, we use ggplot2 which is part of the tidyverse. The key aspect of this package is that we build graphs by adding layers using “+”, which we call the “add operator”. In particular we will create a bar chart using geom_bar() from ggplot2 (Figure 2.5 (a)).\n\ncleaned_elections_data |&gt;\n  ggplot(aes(x = elected_party)) + # aes abbreviates \"aesthetics\" \n  geom_bar()\n\ncleaned_elections_data |&gt;\n  ggplot(aes(x = elected_party)) +\n  geom_bar() +\n  theme_minimal() + # Make the theme neater\n  labs(x = \"Party\", y = \"Number of seats\") # Make labels more meaningful\n\n\n\n\n\n\n\n(a) Default options\n\n\n\n\n\n\n\n(b) Improved theme and labels\n\n\n\n\nFigure 2.5: Number of seats won, by political party, at the 2022 Australian Federal Election\n\n\n\nFigure 2.5 (a) accomplishes what we set out to do. But we can make it look a bit nicer by modifying the default options and improving the labels (Figure 2.5 (b)).\n\n\n2.2.5 Share\nTo this point we have downloaded some data, cleaned it, and made a graph. We would typically need to communicate what we have done at some length. In this case, we can write a few paragraphs about what we did, why we did it, and what we found to conclude our workflow. An example follows.\n\nAustralia is a parliamentary democracy with 151 seats in the House of Representatives, which is the house from which government is formed. There are two major parties—“Liberal” and “Labor”—two minor parties—“Nationals” and “Greens”—and many smaller parties. The 2022 Federal Election occurred on 21 May, and around 15 million votes were cast. We were interested in the number of seats that were won by each party.\nWe downloaded the results, on a seat-specific basis, from the Australian Electoral Commission website. We cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023) including the tidyverse (Wickham et al. 2019) and janitor (Firke 2023). We then created a graph of the number of seats that each political party won (Figure 2.5).\nWe found that the Labor Party won 77 seats, followed by the Liberal Party with 48 seats. The minor parties won the following number of seats: the Nationals won 10 seats and the Greens won 4 seats. Finally, there were 10 Independents elected as well as candidates from smaller parties.\nThe distribution of seats is skewed toward the two major parties which could reflect relatively stable preferences on the part of Australian voters, or possibly inertia due to the benefits of already being a major party such a national network or funding. A better understanding of the reasons for this distribution are of interest in future work. While the dataset consists of everyone who voted, it worth noting that in Australia some are systematically excluded from voting, and it is much more difficult for some to vote than others.\n\nOne aspect to be especially concerned with is making sure that this communication is focused on the needs of the audience and telling a story. Data journalism provides some excellent examples of how analysis needs to be tailored to the audience, for instance, Cardoso (2020) and Bronner (2020)."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#torontos-unhoused-population",
    "href": "02-drinking_from_a_fire_hose.html#torontos-unhoused-population",
    "title": "2  Drinking from a fire hose",
    "section": "2.3 Toronto’s unhoused population",
    "text": "2.3 Toronto’s unhoused population\nToronto has a large unhoused population (City of Toronto 2021). Freezing winters mean it is important there are enough places in shelters. In this example we will make a table of shelter usage in 2021 to compare average use in each month. Our expectation is that there is greater usage in the colder months, for instance, December, compared with warmer months, for instance, July.\n\n2.3.1 Plan\nThe dataset that we are interested in would need to have the date, the shelter, and the number of beds that were occupied that night. A quick sketch of a dataset that would work is Figure 2.6 (a). We are interested in creating a table that has the monthly average number of beds occupied each night. The table would probably look something like Figure 2.6 (b).\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset\n\n\n\n\n\n\n\n(b) Quick sketch of a table of the average number of beds occupied each month\n\n\n\n\nFigure 2.6: Sketches of a dataset and table related shelter usage in Toronto\n\n\n\n\n2.3.2 Simulate\nThe next step is to simulate some data that could resemble our dataset. Simulation provides us with an opportunity to think deeply about our data generating process.\nIn Posit Cloud make a new Quarto document, save it, and make a new R code chunk and add preamble documentation. Then install and/or load the packages that are needed. We will again use the tidyverse and janitor. As those were installed earlier, they do not need to be installed again. We will also use lubridate. That is part of the tidyverse and so does not need to be installed independently, but it does need to be loaded. We will also use opendatatoronto, and knitr and these will need to be installed and loaded.\n\n#### Preamble ####\n# Purpose: Get data on 2021 shelter usage and make table\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 July 2022\n# Prerequisites: -\n\n#### Workspace setup ####\ninstall.packages(\"opendatatoronto\")\ninstall.packages(\"knitr\")\n\nlibrary(knitr)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(opendatatoronto)\nlibrary(tidyverse)\n\nTo add a bit more detail to the earlier example, packages contain code that other people have written. There are a few common ones that you will see regularly in this book, especially the tidyverse. To use a package, we must first install it and then we need to load it. A package only needs to be installed once per computer but must be loaded every time. This means the packages that we installed earlier do not need to be reinstalled here.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Robert Gentleman is a co-creator of R. After earning a PhD in Statistics from the University of Washington in 1988, he moved to the University of Auckland. He then went onto various roles including at 23andMe and is now the Executive Director of the Center for Computational Biomedicine at Harvard Medical School.\n\n\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Ross Ihaka is a co-creator of R. He earned a PhD in Statistics from the University of California, Berkeley, in 1985. He wrote a dissertation titled “Ruaumoko”, which is the Māori god of earthquakes. He then moved to the University of Auckland where he remained for his entire career. He was awarded the Pickering Medal in 2008 by the Royal Society of New Zealand Te Apārangi.\n\n\nGiven that people donate their time to make R and the packages that we use, it is important to cite them. To get the information that is needed, we use citation(). When run without any arguments, that provides the citation information for R itself, and when run with an argument that is the name of a package, it provides the citation information for that package.\n\ncitation() # Get the citation information for R\n\nTo cite R in publications use:\n\n  R Core Team (2023). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2023},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\ncitation(\"ggplot2\") # Get citation information for a package\n\nTo cite ggplot2 in publications, please use\n\n  H. Wickham. ggplot2: Elegant Graphics for Data Analysis.\n  Springer-Verlag New York, 2016.\n\nA BibTeX entry for LaTeX users is\n\n  @Book{,\n    author = {Hadley Wickham},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    publisher = {Springer-Verlag New York},\n    year = {2016},\n    isbn = {978-3-319-24277-4},\n    url = {https://ggplot2.tidyverse.org},\n  }\n\n\nTurning to the simulation, we need three variables: “date”, “shelter”, and “occupancy”. This example will build on the earlier one by adding a seed using set.seed(). A seed enables us to always generate the same random data whenever we run the same code. Any integer can be used as the seed. In this case the seed will be 853. If you use that as your seed, then you should get the same random numbers as in this example. If you use a different seed, then you should expect different random numbers. Finally, we use rep() to repeat something a certain number of times. For instance, we repeat “Shelter 1” 365 times which accounts for about a year.\n\n#### Simulate ####\nset.seed(853)\n\nsimulated_occupancy_data &lt;-\n  tibble(\n    date = rep(x = as.Date(\"2021-01-01\") + c(0:364), times = 3),\n    # Based on Eddelbuettel: https://stackoverflow.com/a/21502386\n    shelter = c(\n      rep(x = \"Shelter 1\", times = 365),\n      rep(x = \"Shelter 2\", times = 365),\n      rep(x = \"Shelter 3\", times = 365)\n    ),\n    number_occupied =\n      rpois(\n        n = 365 * 3,\n        lambda = 30\n      ) # Draw 1,095 times from the Poisson distribution\n  )\n\nhead(simulated_occupancy_data)\n\n# A tibble: 6 × 3\n  date       shelter   number_occupied\n  &lt;date&gt;     &lt;chr&gt;               &lt;int&gt;\n1 2021-01-01 Shelter 1              28\n2 2021-01-02 Shelter 1              29\n3 2021-01-03 Shelter 1              35\n4 2021-01-04 Shelter 1              25\n5 2021-01-05 Shelter 1              21\n6 2021-01-06 Shelter 1              30\n\n\nIn this simulation we first create a list of all the dates in 2021. We repeat that list three times. We assume data for three shelters for every day of the year. To simulate the number of beds that are occupied each night, we draw from a Poisson distribution, assuming a mean number of 30 beds occupied per shelter, although this is just an arbitrary choice. By way of background, a Poisson distribution is often used when we have count data, and we return to it in Chapter 13.\n\n\n2.3.3 Acquire\nWe use data made available about Toronto shelter usage by the City of Toronto. Shelter usage is measured by a count made each night at 4 a.m. of the number of occupied beds. To access the data, we use opendatatoronto and then save our own copy.\n\n#### Acquire ####\ntoronto_shelters &lt;-\n  # Each package is associated with a unique id  found in the \"For \n  # Developers\" tab of the relevant page from Open Data Toronto\n  # https://open.toronto.ca/dataset/daily-shelter-overnight-service-occupancy-capacity/\n  list_package_resources(\"21c83b32-d5a8-4106-a54f-010dbe49f6f2\") |&gt;\n  # Within that package, we are interested in the 2021 dataset\n  filter(name == \n    \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\") |&gt;\n  # Having reduced the dataset to one row we can get the resource\n  get_resource()\n\nwrite_csv(\n  x = toronto_shelters,\n  file = \"toronto_shelters.csv\"\n)\n\nhead(toronto_shelters)\n\n\nhead(toronto_shelters)\n\n# A tibble: 6 × 32\n   X_id OCCUPANCY_DATE ORGANIZATION_ID ORGANIZATION_NAME        SHELTER_ID\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n1     1 21-01-01                    24 COSTI Immigrant Services         40\n2     2 21-01-01                    24 COSTI Immigrant Services         40\n3     3 21-01-01                    24 COSTI Immigrant Services         40\n4     4 21-01-01                    24 COSTI Immigrant Services         40\n5     5 21-01-01                    24 COSTI Immigrant Services         40\n6     6 21-01-01                    24 COSTI Immigrant Services         40\n# ℹ 27 more variables: SHELTER_GROUP &lt;chr&gt;, LOCATION_ID &lt;dbl&gt;,\n#   LOCATION_NAME &lt;chr&gt;, LOCATION_ADDRESS &lt;chr&gt;, LOCATION_POSTAL_CODE &lt;chr&gt;,\n#   LOCATION_CITY &lt;chr&gt;, LOCATION_PROVINCE &lt;chr&gt;, PROGRAM_ID &lt;dbl&gt;,\n#   PROGRAM_NAME &lt;chr&gt;, SECTOR &lt;chr&gt;, PROGRAM_MODEL &lt;chr&gt;,\n#   OVERNIGHT_SERVICE_TYPE &lt;chr&gt;, PROGRAM_AREA &lt;chr&gt;, SERVICE_USER_COUNT &lt;dbl&gt;,\n#   CAPACITY_TYPE &lt;chr&gt;, CAPACITY_ACTUAL_BED &lt;dbl&gt;, CAPACITY_FUNDING_BED &lt;dbl&gt;,\n#   OCCUPIED_BEDS &lt;dbl&gt;, UNOCCUPIED_BEDS &lt;dbl&gt;, UNAVAILABLE_BEDS &lt;dbl&gt;, …\n\n\nNot much needs to be done to this to make it similar to the dataset that we were interested in (Figure 2.6 (a)). We need to change the names to make them easier to type using clean_names(), and reduce the columns to only those that are relevant using select().\n\ntoronto_shelters_clean &lt;-\n  clean_names(toronto_shelters) |&gt;\n  mutate(occupancy_date = ymd(occupancy_date)) |&gt; \n  select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n# A tibble: 6 × 2\n  occupancy_date occupied_beds\n  &lt;date&gt;                 &lt;dbl&gt;\n1 2021-01-01                NA\n2 2021-01-01                NA\n3 2021-01-01                NA\n4 2021-01-01                NA\n5 2021-01-01                NA\n6 2021-01-01                 6\n\n\nAll that remains is to save the cleaned dataset.\n\nwrite_csv(\n  x = toronto_shelters_clean,\n  file = \"cleaned_toronto_shelters.csv\"\n)\n\n\n\n2.3.4 Explore\nFirst, we load the dataset that we just created.\n\n#### Explore ####\ntoronto_shelters_clean &lt;-\n  read_csv(\n    \"cleaned_toronto_shelters.csv\",\n    show_col_types = FALSE\n  )\n\nThe dataset contains daily records for each shelter. We are interested in understanding average usage for each month. To do this, we need to add a month column using month() from lubridate. By default, month() provides the number of the month, and so we include two arguments—“label” and “abbr”—to get the full name of the month. We remove rows that do not have any data for the number of beds using drop_na() from tidyr, which is part of the tidyverse. We will do this here unthinkingly because our focus is on getting started, but this is an important decision and we talk more about missing data in Chapter 6 and Chapter 11. We then create a summary statistic on the basis of monthly groups, using summarise() from dplyr. We use kable() from knitr to create Table 2.1.\n\ntoronto_shelters_clean |&gt;\n  mutate(occupancy_month = month(\n    occupancy_date,\n    label = TRUE,\n    abbr = FALSE\n  )) |&gt;\n  arrange(month(occupancy_date)) |&gt; \n  drop_na(occupied_beds) |&gt; \n  summarise(number_occupied = mean(occupied_beds),\n            .by = occupancy_month) |&gt;\n  kable()\n\n\n\nTable 2.1: Shelter usage in Toronto in 2021\n\n\noccupancy_month\nnumber_occupied\n\n\n\n\nJanuary\n28.55708\n\n\nFebruary\n27.73821\n\n\nMarch\n27.18521\n\n\nApril\n26.31561\n\n\nMay\n27.42596\n\n\nJune\n28.88300\n\n\nJuly\n29.67137\n\n\nAugust\n30.83975\n\n\nSeptember\n31.65405\n\n\nOctober\n32.32991\n\n\nNovember\n33.26980\n\n\nDecember\n33.52426\n\n\n\n\n\n\nAs with before, this looks fine, and achieves what we set out to do. But we can make some tweaks to the defaults to make it look even better (Table 2.2). In particular we make the column names easier to read, and only show an appropriate number of decimal places.\n\ntoronto_shelters_clean |&gt;\n  mutate(occupancy_month = month(\n    occupancy_date,\n    label = TRUE,\n    abbr = FALSE\n  )) |&gt;\n  arrange(month(occupancy_date)) |&gt; \n  drop_na(occupied_beds) |&gt;\n  summarise(number_occupied = mean(occupied_beds),\n            .by = occupancy_month) |&gt;\n  kable(\n    col.names = c(\"Month\", \"Average daily number of occupied beds\"),\n    digits = 1\n  )\n\n\n\nTable 2.2: Shelter usage in Toronto in 2021\n\n\nMonth\nAverage daily number of occupied beds\n\n\n\n\nJanuary\n28.6\n\n\nFebruary\n27.7\n\n\nMarch\n27.2\n\n\nApril\n26.3\n\n\nMay\n27.4\n\n\nJune\n28.9\n\n\nJuly\n29.7\n\n\nAugust\n30.8\n\n\nSeptember\n31.7\n\n\nOctober\n32.3\n\n\nNovember\n33.3\n\n\nDecember\n33.5\n\n\n\n\n\n\n\n\n2.3.5 Share\nWe need to write a few brief paragraphs about what we did, why we did it, and what we found to sum up our work. An example follows.\n\nToronto has a large unhoused population. Freezing winters mean it is critical there are enough places in shelters. We are interested to understand how usage of shelters changes in colder months, compared with warmer months.\nWe use data provided by the City of Toronto about Toronto shelter bed occupancy. Specifically, at 4 a.m. each night a count is made of the occupied beds. We are interested in averaging this over the month. We cleaned, tidied, and analyzed the dataset using the statistical programming language R (R Core Team 2023) as well as the tidyverse (Wickham 2017), janitor (Firke 2023), opendatatoronto (Gelfand 2022), lubridate (Grolemund and Wickham 2011), and knitr (Xie 2023). We then made a table of the average number of occupied beds each night for each month (Table 2.2).\nWe found that the daily average number of occupied beds was higher in December 2021 than July 2021, with 34 occupied beds in December, compared with 30 in July (Table 2.2). More generally, there was a steady increase in the daily average number of occupied beds between July and December, with a slight overall increase each month.\nThe dataset is on the basis of shelters, and so our results may be skewed by changes that are specific to especially large or small shelters. It may be that specific shelters are particularly attractive in colder months. Additionally, we were concerned with counts of the number of occupied beds, but if the supply of beds changes over the season, then an additional statistic of interest would be the proportion occupied.\n\nAlthough this example is only a few paragraphs, it could be reduced to form an abstract, or increased to form a full report, for instance, by expanding each paragraph into a section. The first paragraph is a general overview, the second focuses on the data, the third on the results, and the fourth is a discussion. Following the example of Hao (2019), that fourth paragraph is a good place to consider areas in which bias may have crept in."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#neonatal-mortality",
    "href": "02-drinking_from_a_fire_hose.html#neonatal-mortality",
    "title": "2  Drinking from a fire hose",
    "section": "2.4 Neonatal mortality",
    "text": "2.4 Neonatal mortality\nNeonatal mortality refers to a death that occurs within the first month of life. The neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births (UN IGME 2021). The Third Sustainable Development Goal (SDG) calls for a reduction in NMR to 12. In this example we will create a graph of the estimated NMR for the past 50 years for: Argentina, Australia, Canada, and Kenya.\n\n2.4.1 Plan\nFor this example, we need to think about what our dataset should look like, and what the graph should look like.\nThe dataset needs to have variables that specify the country and the year. It also needs to have a variable with the NMR estimate for that year for that country. Roughly, it should look like Figure 2.7 (a).\n\n\n\n\n\n\n\n(a) Quick sketch of a potentially useful NMR dataset\n\n\n\n\n\n\n\n(b) Quick sketch of a graph of NMR by country over time\n\n\n\n\nFigure 2.7: Sketches of a dataset and graph about the neonatal mortality rate (NMR)\n\n\nWe are interested to make a graph with year on the x-axis and estimated NMR on the y-axis. Each country should have its own series. A quick sketch of what we are looking for is Figure 2.7 (b).\n\n\n2.4.2 Simulate\nWe would like to simulate some data that aligns with our plan. In this case we will need three columns: country, year, and NMR.\nWithin Posit Cloud, make a new Quarto document and save it. Add preamble documentation and set up the workspace. We will use the tidyverse, janitor, and lubridate.\n\n#### Preamble ####\n# Purpose: Obtain and prepare data about neonatal mortality for\n# four countries for the past fifty years and create a graph.\n# Author: Rohan Alexander\n# Email: rohan.alexander@utoronto.ca\n# Date: 1 July 2022\n# Prerequisites: -\n\n#### Workspace setup ####\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(tidyverse)\n\nThe code contained in packages can change from time to time as the authors update it and release new versions. We can see which version of a package we are using with packageVersion(). For instance, we are using version 2.0.0 of the tidyverse and version 2.2.0 of janitor.\n\npackageVersion(\"tidyverse\")\n\n[1] '2.0.0'\n\npackageVersion(\"janitor\")\n\n[1] '2.2.0'\n\n\nTo update the version of all of the packages that we have installed, we use update.packages(). We can use tidyverse_update() to just install the tidyverse packages. This does not need to be run, say, every day, but from time to time it is worth updating packages. While many packages take care to ensure backward compatibility, at a certain point this does not become possible. Updating packages could result in old code needing to be rewritten. This is not a big deal when you are getting started and in any case there are tools aimed at loading particular versions that we cover in Chapter 3.\nReturning to the simulation, we repeat the name of each country 50 times with rep(), and enable the passing of 50 years. Finally, we draw from the uniform distribution with runif() to simulate an estimated NMR value for that year for that country.\n\n#### Simulate data ####\nset.seed(853)\n\nsimulated_nmr_data &lt;-\n  tibble(\n    country =\n      c(rep(\"Argentina\", 50), rep(\"Australia\", 50), \n        rep(\"Canada\", 50), rep(\"Kenya\", 50)),\n    year =\n      rep(c(1971:2020), 4),\n    nmr =\n      runif(n = 200, min = 0, max = 100)\n  )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nWhile this simulation works, it would be time consuming and error prone if we decided that instead of 50 years, we were interested in simulating, say, 60 years. One way to improve this code is to replace all instances of 50 with a variable.\n\n#### Simulate data ####\nset.seed(853)\n\nnumber_of_years &lt;- 50\n\nsimulated_nmr_data &lt;-\n  tibble(\n    country =\n      c(rep(\"Argentina\", number_of_years), rep(\"Australia\", number_of_years),\n        rep(\"Canada\", number_of_years), rep(\"Kenya\", number_of_years)),\n    year =\n      rep(c(1:number_of_years + 1970), 4),\n    nmr =\n      runif(n = number_of_years * 4, min = 0, max = 100)\n  )\n\nhead(simulated_nmr_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Argentina  1971 35.9 \n2 Argentina  1972 12.0 \n3 Argentina  1973 48.4 \n4 Argentina  1974 31.6 \n5 Argentina  1975  3.74\n6 Argentina  1976 40.4 \n\n\nThe result will be the same, but now if we want to change from 50 to 60 years, we only have to make the change in one place.\nWe can have confidence in this simulated dataset because it is relatively straight forward, and we wrote the code for it. But when we turn to the real dataset, it is more difficult to be sure that it is what it claims to be. Even if we trust the data, we need to be able to share that confidence with others. One way forward is to establish some tests of whether our data are as they should be. For instance, we expect:\n\nThat “country” is exclusively one of these four: “Argentina”, “Australia”, “Canada”, or “Kenya”.\nConversely, “country” contains all those four countries.\nThat “year” is no smaller than 1971 and no larger than 2020, and is an integer, not a letter or a number with decimal places.\nThat “nmr” is a value somewhere between 0 and 1,000, and is a number.\n\nWe can write a series of tests based on these features, that we expect the dataset to pass.\n\nsimulated_nmr_data$country |&gt;\n  unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\nsimulated_nmr_data$country |&gt;\n  unique() |&gt;\n  length() == 4\n\nsimulated_nmr_data$year |&gt; min() == 1971\nsimulated_nmr_data$year |&gt; max() == 2020\nsimulated_nmr_data$nmr |&gt; min() &gt;= 0\nsimulated_nmr_data$nmr |&gt; max() &lt;= 1000\nsimulated_nmr_data$nmr |&gt; class() == \"numeric\"\n\nHaving passed these tests, we can have confidence in the simulated dataset. More importantly, we can apply these tests to the real dataset. This enables us to have greater confidence in that dataset and to share that confidence with others.\n\n\n2.4.3 Acquire\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides NMR estimates that we can download and save.\n\n#### Acquire data ####\nraw_igme_data &lt;-\n  read_csv(\n    file =\n      \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE\n  )\n\nwrite_csv(x = raw_igme_data, file = \"igme.csv\")\n\nWith established data, such as this, it can be useful to read supporting material about the data. In this case, a codebook is available here. After this we can take a quick look at the dataset to get a better sense of it. We might be interested in what the dataset looks like with head() and tail(), and what the names of the columns are with names().\n\nhead(raw_igme_data)\n\n# A tibble: 6 × 29\n  `Geographic area` Indicator              Sex   `Wealth Quintile` `Series Name`\n  &lt;chr&gt;             &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;        \n1 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n2 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n3 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n4 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n5 Afghanistan       Neonatal mortality ra… Total Total             Multiple Ind…\n6 Afghanistan       Neonatal mortality ra… Total Total             Afghanistan …\n# ℹ 24 more variables: `Series Year` &lt;chr&gt;, `Regional group` &lt;chr&gt;,\n#   TIME_PERIOD &lt;chr&gt;, OBS_VALUE &lt;dbl&gt;, COUNTRY_NOTES &lt;chr&gt;, CONNECTION &lt;lgl&gt;,\n#   DEATH_CATEGORY &lt;lgl&gt;, CATEGORY &lt;chr&gt;, `Observation Status` &lt;chr&gt;,\n#   `Unit of measure` &lt;chr&gt;, `Series Category` &lt;chr&gt;, `Series Type` &lt;chr&gt;,\n#   STD_ERR &lt;dbl&gt;, REF_DATE &lt;dbl&gt;, `Age Group of Women` &lt;chr&gt;,\n#   `Time Since First Birth` &lt;chr&gt;, DEFINITION &lt;chr&gt;, INTERVAL &lt;dbl&gt;,\n#   `Series Method` &lt;chr&gt;, LOWER_BOUND &lt;dbl&gt;, UPPER_BOUND &lt;dbl&gt;, …\n\n\n\nnames(raw_igme_data)\n\n [1] \"Geographic area\"        \"Indicator\"              \"Sex\"                   \n [4] \"Wealth Quintile\"        \"Series Name\"            \"Series Year\"           \n [7] \"Regional group\"         \"TIME_PERIOD\"            \"OBS_VALUE\"             \n[10] \"COUNTRY_NOTES\"          \"CONNECTION\"             \"DEATH_CATEGORY\"        \n[13] \"CATEGORY\"               \"Observation Status\"     \"Unit of measure\"       \n[16] \"Series Category\"        \"Series Type\"            \"STD_ERR\"               \n[19] \"REF_DATE\"               \"Age Group of Women\"     \"Time Since First Birth\"\n[22] \"DEFINITION\"             \"INTERVAL\"               \"Series Method\"         \n[25] \"LOWER_BOUND\"            \"UPPER_BOUND\"            \"STATUS\"                \n[28] \"YEAR_TO_ACHIEVE\"        \"Model Used\"            \n\n\nWe would like to clean up the names and only keep the rows and columns that we are interested in. Based on our plan, we are interested in rows where “Sex” is “Total”, “Series Name” is “UN IGME estimate”, “Geographic area” is one of “Argentina”, “Australia”, “Canada”, and “Kenya”, and the “Indicator” is “Neonatal mortality rate”. After this we are interested in just a few columns: “geographic_area”, “time_period”, and “obs_value”.\n\ncleaned_igme_data &lt;-\n  clean_names(raw_igme_data) |&gt;\n  filter(\n    sex == \"Total\",\n    series_name == \"UN IGME estimate\",\n    geographic_area %in% c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\"),\n    indicator == \"Neonatal mortality rate\"\n    ) |&gt;\n  select(geographic_area, time_period, obs_value)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  geographic_area time_period obs_value\n  &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;\n1 Argentina       1970-06          24.9\n2 Argentina       1971-06          24.7\n3 Argentina       1972-06          24.6\n4 Argentina       1973-06          24.6\n5 Argentina       1974-06          24.5\n6 Argentina       1975-06          24.1\n\n\nWe need to fix two other aspects: the class of “time_period” is character when we need it to be a year, and the name of “obs_value” should be “nmr” to be more informative.\n\ncleaned_igme_data &lt;-\n  cleaned_igme_data |&gt;\n  mutate(\n    time_period = str_remove(time_period, \"-06\"),\n    time_period = as.integer(time_period)\n  ) |&gt;\n  filter(time_period &gt;= 1971) |&gt;\n  rename(nmr = obs_value, year = time_period, country = geographic_area)\n\nhead(cleaned_igme_data)\n\n# A tibble: 6 × 3\n  country    year   nmr\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Argentina  1971  24.7\n2 Argentina  1972  24.6\n3 Argentina  1973  24.6\n4 Argentina  1974  24.5\n5 Argentina  1975  24.1\n6 Argentina  1976  23.3\n\n\nFinally, we can check that our dataset passes the tests that we developed based on the simulated dataset.\n\ncleaned_igme_data$country |&gt;\n  unique() == c(\"Argentina\", \"Australia\", \"Canada\", \"Kenya\")\n\n[1] TRUE TRUE TRUE TRUE\n\ncleaned_igme_data$country |&gt;\n  unique() |&gt;\n  length() == 4\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; min() == 1971\n\n[1] TRUE\n\ncleaned_igme_data$year |&gt; max() == 2020\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; min() &gt;= 0\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; max() &lt;= 1000\n\n[1] TRUE\n\ncleaned_igme_data$nmr |&gt; class() == \"numeric\"\n\n[1] TRUE\n\n\nAll that remains is to save the nicely cleaned dataset.\n\nwrite_csv(x = cleaned_igme_data, file = \"cleaned_igme_data.csv\")\n\n\n\n2.4.4 Explore\nWe would like to make a graph of estimated NMR using the cleaned dataset. First, we read in the dataset.\n\n#### Explore ####\ncleaned_igme_data &lt;-\n  read_csv(\n    file = \"cleaned_igme_data.csv\",\n    show_col_types = FALSE\n  )\n\nWe can now make a graph of how NMR has changed over time and the differences between countries (Figure 2.8).\n\ncleaned_igme_data |&gt;\n  ggplot(aes(x = year, y = nmr, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Neonatal Mortality Rate (NMR)\", color = \"Country\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 2.8: Neonatal Mortality Rate (NMR), for Argentina, Australia, Canada, and Kenya (1971-2020)\n\n\n\n\n\n\n2.4.5 Share\nTo this point we downloaded some data, cleaned it, wrote some tests, and made a graph. We would typically need to communicate what we have done at some length. In this case, we will write a few paragraphs about what we did, why we did it, and what we found.\n\nNeonatal mortality refers to a death that occurs within the first month of life. In particular, the neonatal mortality rate (NMR) is the number of neonatal deaths per 1,000 live births. We obtain estimates for NMR for four countries—Argentina, Australia, Canada, and Kenya—over the past 50 years.\nThe UN Inter-agency Group for Child Mortality Estimation (IGME) provides estimates of the NMR at the website: https://childmortality.org/. We downloaded their estimates then cleaned and tidied the dataset using the statistical programming language R (R Core Team 2023).\nWe found considerable change in the estimated NMR over time and between the four countries of interest (Figure 2.8). We found that the 1970s tended to be associated with reductions in the estimated NMR. Australia and Canada were estimated to have a low NMR at that point and remained there through 2020, with further slight reductions. The estimates for Argentina and Kenya continued to have substantial reductions through 2020.\nOur results suggest considerable improvements in estimated NMR over time. NMR estimates are based on a statistical model and underlying data. The double burden of data is that often high-quality data are less easily available for groups, in this case countries, with worse outcomes. Our conclusions are subject to the model that underpins the estimates and the quality of the underlying data, and we did not independently verify either of these."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#concluding-remarks",
    "href": "02-drinking_from_a_fire_hose.html#concluding-remarks",
    "title": "2  Drinking from a fire hose",
    "section": "2.5 Concluding remarks",
    "text": "2.5 Concluding remarks\nWe have covered a lot of ground in this chapter, and it is normal to have not followed it all. The best way to proceed is to go through each of the three case studies in your own time. Type all the code out yourself, rather than copy-pasting, and run it bit by bit, even if you do not entirely understand what it is doing. Then try to add your own comments to it.\nIt is also the case that it is not necessary to fully understand everything in this chapter at this point. Some students find it best to continue going through the next few chapters of this book, and return to this one later."
  },
  {
    "objectID": "02-drinking_from_a_fire_hose.html#exercises",
    "href": "02-drinking_from_a_fire_hose.html#exercises",
    "title": "2  Drinking from a fire hose",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Every day for a year a person records whether they donated $1, $2, or $3. Please sketch what that dataset could look like, and then sketch a graph that you could build to show all observations.\n(Simulate I) Please further consider the scenario described. Which of the following could be used to simulate the situation (select all that apply)?\n\nrunif(n = 365, min = 1, max = 3) |&gt; floor()\nrunif(n = 365, min = 1, max = 4) |&gt; floor()\nsample(x = 1:3, size = 365, replace = TRUE)\nsample(x = 1:3, size = 365, replace = FALSE)\n\n(Simulate II) Please write three tests based on this simulation.\n(Acquire) Please identify one possible source of actual data about the amount of money that is donated to charity in a country that you are interested in.\n(Explore) Assume that the tidyverse is loaded and the dataset “donations” has the column “amount”. Which of the following would result in a bar chart (pick one)?\n\ndonations |&gt; geom_bar(aes(x = amount)) + ggplot()\namount |&gt; geom_bar(aes(x = donations)) + ggplot()\ndonations |&gt; ggplot(aes(x = amount)) + geom_bar()\namount |&gt; ggplot(aes(x = donations)) + geom_bar()\n\n(Communicate) Please write two paragraphs as if you had gathered data from the source you identified, and had built a graph. The exact details contained in the paragraphs do not have to be factual (i.e. you do not actually have to get the data nor create the graphs).\n\n\n\nQuestions\n\nFollowing Barrett (2021), please list four atomic habits, related to learning data science, that you could implement.\nWhat is not one of the four challenges for mitigating bias mentioned in Hao (2019) (pick one)?\n\nUnknown unknowns.\nImperfect processes.\nThe definitions of fairness.\nLack of social context.\nDisinterest given profit considerations.\n\nHow does Chambliss (1989) define “excellence” (pick one)?\n\nProlonged performance at world-class level.\nAll Olympic medal winners.\nConsistent superiority of performance.\nAll national-level athletes.\n\nThink about the following quote from Chambliss (1989, 81) and list three small skills or activities that could help you achieve excellence in data science.\n\n\nExcellence is mundane. Superlative performance is really a confluence of dozens of small skills or activities, each one learned or stumbled upon, which have been carefully drilled into habit and then are fitted together in a synthesized whole. There is nothing extraordinary or super-human in any one of those actions; only the fact that they are done consistently and correctly, and all together, produce excellence.\n\n\nWhat is the first sentence in the help file for the tidyverse?\nUse a help file to determine which of the following are arguments for read_csv() (select all that apply)?\n\n“all_cols”\n“file”\n“show_col_types”\n“number”\n\nWe used rpois() and runif() to draw from the Poisson and Uniform distributions, respectively. Which of the following can be used to draw from the Normal and Binomial distributions (select all that apply)?\n\nrnormal() and rbinom()\nrnorm() and rbinomial()\nrnormal() and rbinomial()\nrnorm() and rbinom()\n\nWhat is the result of sample(x = letters, size = 2) when the seed is set to “853”? What about when the seed is set to “1234” (pick one)?\n\n‘“i” “q”’ and ‘“p” “v”’\n‘“e” “l”’ and ‘“e” “r”’\n‘“i” “q”’ and ‘“e” “r”’\n‘“e” “l”’ and ‘“p” “v”’\n\nWhich function provides the recommended citation to cite R (pick one)?\n\ncite(\"R\").\ncite().\ncitation(\"R\").\ncitation().\n\nHow do we get the citation information for opendatatoronto (pick one)?\n\ncite()\ncitation()\ncite(\"opendatatoronto\")\ncitation(\"opendatatoronto\")\n\nWhich function is used to update packages (pick one)?\n\nupdate.packages()\nupgrade.packages()\nrevise.packages()\nrenovate.packages()\n\nWhat are some features that we might typically expect of a column that claimed to be a year (select all that apply)?\n\nThe class is “character”.\nThere are no negative numbers.\nThere are letters in the column.\nEach entry has four digits.\n\nPlease add a small mistake to the following code. Then add it to a GitHub Gist and submit the URL.\n\n\nmidwest |&gt;\n  ggplot(aes(x = poptotal, y = popdensity, color = state)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n\n\nWhy do we simulate a dataset (write at least three dot points)?\n\n\n\nTutorial\nThe purpose of this tutorial is to provide an opportunity to do a small self-contained project. We will redo the Australian Elections worked example, but for Canada.\nCanada is a parliamentary democracy with 338 seats in the House of Commons, which is the lower house and that from which government is formed. There are two major parties—“Liberal” and “Conservative”—three minor parties—“Bloc Québécois”, “New Democratic”, and “Green”—and many smaller parties and independents. In this example we will create a graph of the number of seats that each party won in the 2021 Federal Election.\nBegin by planning what the dataset that we need will look like, and what the final graph will look like. The basic requirement for the dataset is that it has the name of the seat (sometimes called a “riding” in Canada) and the party of the person elected.\nMake a quick sketch of the dataset that we would need. Then make a quick sketch of a graph that we might be interested in.\nPut together a Quarto document that simulates some data. Add preamble documentation, then load the packages that are needed: tidyverse, and janitor. Add numbers for the riding, then use sample() to randomly choose one of six options, with replacement, 338 times.\nNext we need to get the actual data, from Elections Canada, and the file that we need to download is here.\nClean the names, and then select the two columns that are of interest: “electoral_district_name_nom_de_circonscription”, and “elected_candidate_candidat_elu”. Finally, rename the columns to remove the French and simplify the names.\nThe column that we need is about the elected candidates. That has the surname of the elected candidate, followed by a comma, followed by their first name, followed by a space, followed by the name of the party in both English and French, separated by a slash. Break-up this column into its pieces using separate() from tidyr and then use select() to just keep party information.\n\ncleaned_elections_data &lt;-\n  cleaned_elections_data |&gt;\n  separate(\n    col = elected_candidate,\n    into = c(\"Other\", \"party\"),\n    sep = \"/\"\n  ) |&gt;\n  select(-Other)\n\nThen recode the party names from French to English to match what we simulated.\nAt this point we can make a nice graph of the number of ridings won by each party in the 2019 Canadian Federal Election.\n\n\n\n\nBarrett, Malcolm. 2021. Data Science as an Atomic Habit. https://malco.io/2021/01/04/data-science-as-an-atomic-habit/.\n\n\nBronner, Laura. 2020. “Why Statistics Don’t Capture the Full Extent of the Systemic Bias in Policing.” FiveThirtyEight, June. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\nCardoso, Tom. 2020. “Bias behind bars: A Globe investigation finds a prison system stacked against Black and Indigenous inmates.” The Globe and Mail, October. https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHao, Karen. 2019. “This is How AI Bias Really Happens—And Why It’s So Hard To Fix.” MIT Technology Review, February. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality, 2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. tidyverse: Easily Install and Load the “Tidyverse”. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC. https://adv-r.hadley.nz.\n\n\n———. 2022. stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "03-workflow.html#introduction",
    "href": "03-workflow.html#introduction",
    "title": "3  Reproducible workflows",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\n\nThe number one thing to keep in mind about machine learning is that performance is evaluated on samples from one dataset, but the model is used in production on samples that may not necessarily follow the same characteristics\\(\\dots\\) So when asking the question, “would you rather use a model that was evaluated as 90% accurate, or a human that was evaluated as 80% accurate”, the answer depends on whether your data is typical per the evaluation process. Humans are adaptable, models are not. If significant uncertainty is involved, go with the human. They may have inferior pattern recognition capabilities (versus models trained on enormous amounts of data), but they understand what they do, they can reason about it, and they can improvise when faced with novelty.\nFrançois Chollet, 20 February 2020.\n\nIf science is about systematically building and organizing knowledge in terms of testable explanations and predictions, then data science takes this and focuses on data. This means that building, organizing, and sharing knowledge is a critical aspect. Creating knowledge, once, in a way that only you can do it, does not meet this standard. Hence, there is a need for reproducible data science workflows.\nAlexander (2019) defines reproducible research as that which can be exactly redone, given all the materials used. This underscores the importance of providing the code, data, and environment. The minimum expectation is that another person is independently able to use your code, data, and environment to get your results, including figures and tables. Ironically, there are different definitions of reproducibility between disciplines. Barba (2018) surveys a variety of disciplines and concludes that the predominant language usage implies the following definitions:\n\nReproducible research is when “[a]uthors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.”\nA replication is a study “that arrives at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.”\n\nRegardless of what it is specifically called, Gelman (2016) identifies how large an issue the lack of it is in various social sciences. The problem with work that is not reproducible is that it does not contribute to our stock of knowledge about the world. This is wasteful and potentially even unethical. Since Gelman (2016), a great deal of work has been done in many social sciences and the situation has improved a little, but much work remains. That is also the case in the life sciences (Heil et al. 2021) and computer science (Pineau et al. 2021).\nSome of the examples that Gelman (2016) talks about are not that important in the scheme of things. But at the same time, we saw, and continue to see, similar approaches being used in areas with big impacts. For instance, many governments have created “nudge” units that implement public policy (Sunstein and Reisch 2017) even though there is evidence that some of the claims lack credibility (Maier et al. 2022; Szaszi et al. 2022). Governments are increasingly using algorithms that they do not make open (Chouldechova et al. 2018). And Herndon, Ash, and Pollin (2014) document how research in economics that was used by governments to justify austerity policies following the 2007–2008 financial crisis turned out to not be reproducible.\nAt a minimum, and with few exceptions, we must release our code, datasets, and environment. Without these, it is difficult to know what a finding speaks to (Miyakawa 2020). More banally, we also do not know if there are mistakes or aspects that were inadvertently overlooked (Merali 2010; Hillel 2017; Silver 2020). Increasingly, following Buckheit and Donoho (1995), we consider a paper to be an advertisement, and for the associated code, data, and environment to be the actual work. Steve Jobs, a co-founder of Apple, talked about how the best craftsmen ensure that even the aspects of their work that no one else will ever see are as well finished and high quality as the aspects that are public facing (Isaacson 2011). The same is true in data science, where often one of the distinguishing aspects of high-quality work is that the README and code comments are as polished as, say, the abstract of the associated paper.\nWorkflows exist within a cultural and social context, which imposes an additional ethical reason for the need for them to be reproducible. For instance, Wang and Kosinski (2018) train a neural network to distinguish between the faces of gay and heterosexual men. (Murphy (2017) provides a summary of the paper, the associated issues, and comments from its authors.) To do this, Wang and Kosinski (2018, 248) needed a dataset of photos of people that were “adult, Caucasian, fully visible, and of a gender that matched the one reported on the user’s profile”. They verified this using Amazon Mechanical Turk, an online platform that pays workers a small amount of money to complete specific tasks. The instructions provided to the Mechanical Turk workers for this task specify that Barack Obama, the 44th US President, who had a white mother and a black father, should be classified as “Black”; and that Latino is an ethnicity, rather than a race (Mattson 2017). The classification task may seem objective, but, perhaps unthinkingly, echoes the views of Americans with a certain class and background.\nThis is just one specific concern about one part of the Wang and Kosinski (2018) workflow. Broader concerns are raised by others including Gelman, Mattson, and Simpson (2018). The main issue is that statistical models are specific to the data on which they were trained. And the only reason that we can identify likely issues in the model of Wang and Kosinski (2018) is because, despite not releasing the specific dataset that they used, they were nonetheless open about their procedure. For our work to be credible, it needs to be reproducible by others.\nSome of the steps that we can take to make our work more reproducible include:\n\nEnsure the entire workflow is documented. This may involve addressing questions such as:\n\nHow was the original, unedited dataset obtained and is access likely to be persistent and available to others?\nWhat specific steps are being taken to transform the original, unedited data into the data that were analyzed, and how can this be made available to others?\nWhat analysis has been done, and how clearly can this be shared?\nHow has the final paper or report been built and to what extent can others follow that process themselves?\n\nNot worrying about perfect reproducibility initially, but instead focusing on trying to improve with each successive project. For instance, each of the following requirements are increasingly more onerous and there is no need to be concerned about not being able to do the last, until you can do the first:\n\nCan you run your entire workflow again?\nCan another person run your entire workflow again?\nCan “future-you” run your entire workflow again?\nCan “future-another-person” run your entire workflow again?\n\nIncluding a detailed discussion about the limitations of the dataset and the approach in the final paper or report.\n\nThe workflow that we advocate in this book is:\n\\[\n\\mbox{Plan}\\rightarrow\\mbox{Simulate}\\rightarrow\\mbox{Acquire}\\rightarrow\\mbox{Explore}\\rightarrow\\mbox{Share}\n\\]\nBut it can be alternatively considered as: “Think an awful lot, mostly read and write, sometimes code”.\nThere are various tools that we can use at the different stages that will improve the reproducibility of this workflow. This includes Quarto, R Projects, and Git and GitHub."
  },
  {
    "objectID": "03-workflow.html#quarto",
    "href": "03-workflow.html#quarto",
    "title": "3  Reproducible workflows",
    "section": "3.2 Quarto",
    "text": "3.2 Quarto\n\n3.2.1 Getting started\nQuarto integrates code and natural language in a way that is called “literate programming” (Knuth 1984). It is the successor to R Markdown, which was a variant of Markdown specifically designed to allow R code chunks to be included. Quarto uses a mark-up language similar to HyperText Markup Language (HTML) or LaTeX, in comparison to a “What You See Is What You Get” (WYSIWYG) language, such as Microsoft Word. This means that all the aspects are consistent, for instance, all top-level headings will look the same. But it means that we must designate or “mark up” how we would like certain aspects to appear. And it is only when we render the document that we get to see what it looks like. A visual editor option can also be used, and this hides the need for the user to do this mark-up themselves.\nWhile it makes sense to use Quarto going forward, there are many resources written for and in R Markdown. For this reason we provide R Markdown equivalents in Online Appendix C.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nFernando Pérez is an associate professor of statistics at the University of California, Berkeley and a Faculty Scientist, Data Science and Technology Division, at Lawrence Berkeley National Laboratory. He earned a PhD in particle physics from the University of Colorado, Boulder. During his PhD he created iPython, which enables Python to be used interactively, and now underpins Project Jupyter, which inspired similar notebook approaches such as R Markdown and now Quarto. Somers (2018) describes how open-source notebook approaches create virtuous feedback loops that result in dramatically improved scientific computing. And Romer (2018) aligns the features of open-source approaches, such as Jupyter, with the features that enable scientific consensus and progress. In 2017 Pérez was awarded the Association for Computing Machinery (ACM) Software System Award.\n\n\nOne advantage of literate programming is that we get a “live” document in which code executes and then forms part of the document. Another advantage of Quarto is that similar code can compile into a variety of documents, including HTML and PDFs. Quarto also has default options for including a title, author, and date. One disadvantage is that it can take a while for a document to compile because the code needs to run.\nWe need to download Quarto from here. (Skip this step if you are using Posit Cloud because it is already installed.) We can then create a new Quarto document within RStudio: “File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “Quarto Document\\(\\dots\\)”.\nAfter opening a new Quarto document and selecting “Source” view, you will see the default top matter, contained within a pair of three dashes, as well as some examples of text showing a few of the markdown essential commands and R chunks, each of which are discussed further in the following sections.\n\n\n3.2.2 Top matter\nTop matter consists of defining aspects such as the title, author, and date. It is contained within three dashes at the top of a Quarto document. For instance, the following would specify a title, a date that automatically updated to the date the document was rendered, and an author.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: html\n---\nAn abstract is a short summary of the paper, and we could add that to the top matter.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: html\n---\nBy default, Quarto will create an HTML document, but we can change the output format to produce a PDF. This uses LaTeX in the background and requires the installation of supporting packages. To do this install tinytex. But as it is used in the background we should not need to load it.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nabstract: \"This is my abstract.\"\nformat: pdf\n---\nWe can include references by specifying a BibTeX file in the top matter and then calling it within the text, as needed.\n---\ntitle: \"My document\"\nauthor: \"Rohan Alexander\"\ndate: format(Sys.time(), \"%d %B %Y\")\nformat: pdf\nabstract: \"This is my abstract.\"\nbibliography: bibliography.bib\n---\nWe would need to make a separate file called “bibliography.bib” and save it next to the Quarto file. In the BibTeX file we need an entry for the item that is to be referenced. For instance, the citation for R can be obtained with citation() and this can be added to the “bibliography.bib” file. The citation for a package can be found by including the package name, for instance citation(\"tidyverse\"), and again adding the output to the “.bib” file. It can be helpful to use Google Scholar or doi2bib to get citations for books or articles.\nWe need to create a unique key that we use to refer to this item in the text. This can be anything, provided it is unique, but meaningful ones can be easier to remember, for instance “citeR”.\n@Manual{citeR,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2021},\n    url = {https://www.R-project.org/},\n  }\n@book{tellingstories,\n    title = {Telling Stories with Data},\n    author = {Rohan Alexander},\n    year = {2023},\n    publisher = {Chapman and Hall/CRC},\n    url = {https://tellingstorieswithdata.com}\n  }\nTo cite R in the Quarto document we then include @citeR, which would put brackets around the year: R Core Team (2023), or [@citeR], which would put brackets around the whole thing: (R Core Team 2023).\nThe reference list at the end of the paper is automatically built based on calling the BibTeX file and including references in the paper. At the end of the Quarto document, include a heading “# References” and the actual citations will be included after that. When the Quarto file is rendered, Quarto sees these in the content, goes to the BibTeX file to get the reference details that it needs, builds the reference list, and then adds it at the end of the rendered document.\n\n\n3.2.3 Essential commands\nQuarto uses a variation of Markdown as its underlying syntax. Essential Markdown commands include those for emphasis, headers, lists, links, and images. A reminder of these is included in RStudio: “Help” \\(\\rightarrow\\) “Markdown Quick Reference”. It is your choice as to whether you want to use the visual or source editor. But either way, it is good to understand these essentials because it will not always be possible to use a visual editor (for instance if you are quickly looking at a Quarto document in GitHub). As you get more experience it can be useful to use a text editor such as Sublime Text, or an alternative Integrated Development Environment such as VS Code.\n\nEmphasis: *italic*, **bold**\nHeaders (these go on their own line with a blank line before and after):\n\n         # First level header\n         \n         ## Second level header\n         \n         ### Third level header\n\nUnordered list, with sub-lists:\n\n    * Item 1\n    * Item 2\n        + Item 2a\n        + Item 2b\n\nOrdered list, with sub-lists:\n\n    1. Item 1\n    2. Item 2\n    3. Item 3\n        + Item 3a\n        + Item 3b\n\nURLs can be added: [this book](https://www.tellingstorieswithdata.com) results in this book.\nA paragraph is created by leaving a blank line.\n\nA paragraph about an idea, nicely spaced from the following paragraph.\n\nA paragraph about another idea, again spaced from the earlier paragraph.\nOnce we have added some aspects, then we may want to see the actual document. To build the document click “Render”.\n\n\n3.2.4 R chunks\nWe can include code for R and many other languages in code chunks within a Quarto document. When we render the document the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell Quarto that this is an R chunk. Anything inside this chunk will be considered R code and run as such. We use data from Kleiber and Zeileis (2008) who provide the R package AER to accompany their book Applied Econometrics with R. We could load the tidyverse and install and load AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\nThe output of that code is Figure 3.1.\n\n\n\n\n\nFigure 3.1: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\nThere are various evaluation options that are available in chunks. We include these, each on a new line, by opening the line with the chunk-specific comment delimiter “#|” and then the option. Helpful options include:\n\necho: This controls whether the code itself is included in the document. For instance, #| echo: false would mean the code will be run and its output will show, but the code itself would not be included in the document.\ninclude: This controls whether the output of the code is included in the document. For instance, #| include: false would run the code, but would not result in any output, and the code itself would not be included in the document.\neval: This controls whether the code should be included in the document. For instance, #| eval: false would mean that the code is not run, and hence there would not be any output to include, but the code itself would be included in the document.\nwarning: This controls whether warnings should be included in the document. For instance, #| warning: false would mean that warnings are not included.\nmessage: This controls whether messages should be included in the document. For instance, #| message: false would mean that messages are not included in the document.\n\nFor instance, we could include the output, but not the code, and suppress any warnings.\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\nLeave a blank line on either side of an R chunk, otherwise it may not run properly. And use lower case for logical values, i.e. “false” not “FALSE”.\nMost people did not visit a doctor in the past week.\n\n```{r}\n#| echo: false\n#| warning: false\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\")\n```\n\nThere were some people that visited a doctor once, and then...\nThe Quarto document itself must load any datasets that are needed. It is not enough that they are in the environment. This is because the Quarto document evaluates the code in the document when it is rendered, not necessarily the environment.\nOften when writing code, we may want to make the same change across multiple lines or change all instances of a particular thing. We achieve this with multiple cursors. If we want a cursor across multiple, consecutive lines, then hold “option” on Mac or “Alt” on PC, while you drag your cursor over the relevant lines. If you want to select all instances of something, then highlight one instance, say a variable name, then use Find/Replace (Command + F on Mac or CTRL + F on PC) and select “All”. This will then enable a cursor at all the other instances.\n\n\n3.2.5 Equations\nWe can include equations by using LaTeX, which is based on the programming language TeX. We invoke math mode in LaTeX by using two dollar signs as opening and closing tags. Then whatever is inside is evaluated as LaTeX mark-up. For instance we can produce the compound interest formula with:\n$$\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n$$\n\\[\nA = P\\left(1+\\frac{r}{n}\\right)^{nt}\n\\]\nLaTeX is a comprehensive mark-up language but we will mostly just use it to specify the model of interest. We include some examples here that contain the critical aspects we will draw on starting in Chapter 12.\n$$\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n$$\n\\[\ny_i|\\mu_i, \\sigma \\sim \\mbox{Normal}(\\mu_i, \\sigma)\n\\]\nUnderscores are used to get subscripts: y_i for \\(y_i\\). And we can get a subscript of more than one item by surrounding it with curly braces: y_{i,c} for \\(y_{i,c}\\). In this case we wanted math mode within the line, and so we surround these with only one dollar sign as opening and closing tags.\nGreek letters are typically preceded by a backslash. Common Greek letters include: \\alpha for \\(\\alpha\\), \\beta for \\(\\beta\\), \\delta for \\(\\delta\\), \\epsilon for \\(\\epsilon\\), \\gamma for \\(\\gamma\\), \\lambda for \\(\\lambda\\), \\mu for \\(\\mu\\), \\phi for \\(\\phi\\), \\pi for \\(\\pi\\), \\Pi for \\(\\Pi\\), \\rho for \\(\\rho\\), \\sigma for \\(\\sigma\\), \\Sigma for \\(\\Sigma\\), \\tau for \\(\\tau\\), and \\theta for \\(\\theta\\).\nLaTeX math mode assumes letters are variables and so makes them italic, but sometimes we want a word to appear in normal font because it is not a variable, such as “Normal”. In that case we surround it with \\mbox{}, for instance \\mbox{Normal} for \\(\\mbox{Normal}\\).\nWe line up equations across multiple lines using \\begin{aligned} and \\end{aligned}. Then the item that is to be lined up is noted by an ampersand. The following is a model that we will estimate in Chapter 15.\n$$\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]\nFinally, certain functions are built into LaTeX. For instance, we can appropriately typeset “log” with \\log.\n\n\n3.2.6 Cross-references\nIt can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, consider the following code.\n\n```{r}\n#| label: fig-uniquename\n#| fig-cap: Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\n#| warning: false\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\n\n\n\n\nFigure 3.3: Number of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\nThen (@fig-uniquename) would produce: (Figure 3.3) as the name of the R chunk is fig-uniquename. We need to add “fig” to the start of the chunk name so that Quarto knows that this is a figure. We then include a “fig-cap:” in the R chunk that specifies a caption.\nWe can add #| layout-ncol: 2 in an R chunk within a Quarto document to have two graphs appear side by side (Figure 3.4). Here Figure 3.4 (a) uses the minimal theme, and Figure 3.4 (b) uses the classic theme. These both cross-reference the same label #| label: fig-doctorgraphsidebyside in the R chunk, with an additional option added in the R chunk of #| fig-subcap: [\"Number of illnesses\",\"Number of visits to the doctor\"] which provides the sub-captions. The addition of a letter in-text is accomplished by adding “-1” and “-2” to the end of the label when it is used in-text: (@fig-doctorgraphsidebyside), @fig-doctorgraphsidebyside-1, and @fig-doctorgraphsidebyside-2 for (Figure 3.4), Figure 3.4 (a), and Figure 3.4 (b), respectively.\n\n```{r}\n#| eval: true\n#| warning: false\n#| label: fig-doctorgraphsidebyside\n#| fig-cap: \"Two variants of graphs\"\n#| fig-subcap: [\"Illnesses\",\"Visits to the doctor\"]\n#| layout-ncol: 2\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\") +\n  theme_minimal()\n\nDoctorVisits |&gt;\n  ggplot(aes(x = visits)) +\n  geom_histogram(stat = \"count\") +\n  theme_classic()\n```\n\n\n\n\n\n\n\n(a) Illnesses\n\n\n\n\n\n\n\n(b) Visits to the doctor\n\n\n\n\nFigure 3.4: Two variants of graphs\n\n\n\nWe can take a similar approach to cross-reference tables. For instance, (@tbl-docvisittable) will produce: (Table 3.1). In this case we specify “tbl” at the start of the label so that Quarto knows that it is a table. And we specify a caption for the table with “tbl-cap:”.\n\n```{r}\n#| label: tbl-docvisittable\n#| tbl-cap: \"Distribution of the number of doctor visits\"\n\nDoctorVisits |&gt;\n  count(visits) |&gt;\n  kable()\n```\n\n\n\nTable 3.1: Distribution of the number of doctor visits\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1\n\n\n\n\n\n\nFinally, we can also cross-reference equations. To that we need to add a tag such as {#eq-macroidentity} which we then reference.\n$$\nY = C + I + G + (X - M)\n$$ {#eq-gdpidentity}\nFor instance, we then use @eq-gdpidentity to produce Equation 3.1\n\\[\nY = C + I + G + (X - M)\n\\tag{3.1}\\]\nLabels should be relatively simple when using cross-references. In general, try to keep the names simple but unique, avoid punctuation, and stick to letters and hyphens. Try not to use underscores, because they can cause an error."
  },
  {
    "objectID": "03-workflow.html#r-projects-and-file-structure",
    "href": "03-workflow.html#r-projects-and-file-structure",
    "title": "3  Reproducible workflows",
    "section": "3.3 R Projects and file structure",
    "text": "3.3 R Projects and file structure\nProjects are widely used in software development and exist to keep all the files (data, analysis, report, etc) associated with a particular project together and related to each other. (This use of “project” in a software development sense, is distinct to a “project”, in the project management sense.) An R Project can be created in RStudio. Click “File” \\(\\rightarrow\\) “New Project”, then select “Empty project”, name the R Project and decide where to save it. For instance, a R Project focused on maternal mortality may be called “maternalmortality”. The use of R Projects enables “reliable, polite behavior across different computers or users and over time” (Bryan and Hester 2020). This is because they remove the context of that folder from its broader existence; files exist in relation to the base of the R Project, not the base of the computer.\nOnce a project has been created, a new file with the extension “.RProj” will appear in that folder. An example of a folder with an R Project, a Quarto document, and an appropriate file structure is available here. That can be downloaded: “Code” \\(\\rightarrow\\) “Download ZIP”.\nThe main advantage of using an R Project is that we can reference files within it in a self-contained way. That means when others want to reproduce our work, they will not need to change all the file references and structure as everything is referenced in relation to the “.Rproj” file. For instance, instead of reading a CSV from, say, \"~/Documents/projects/book/data/\" you can read it from book/data/. It may be that someone else does not have a projects folder, and so the former would not work for them, while the latter would.\nThe use of projects is required to meet the minimal level of reproducibility expected of credible work. The use of functions such as setwd(), and computer-specific file paths, bind work to a specific computer in a way that is not appropriate.\nThere are a variety of ways to set up a folder. A variant of Wilson et al. (2017) that is often useful when you are getting started is shown in the example file structure linked above.\nexample_project/\n├── .gitignore\n├── LICENSE.md\n├── README.md\n├── example_project.Rproj\n├── inputs\n│   ├── data\n│   │   ├── unedited_data.csv\n│   │   └── ...\n│   ├── literature\n│   │   ├── alexander-tellingstorieswithdata.pdf\n│   │   ├── gelman-xboxpaper.pdf\n│   │   └── ...\n├── outputs\n│   ├── README.md\n│   ├── data\n│   │   ├── analysis_data.csv\n│   │   └── ...\n│   ├── paper\n│   │   ├── paper.pdf\n│   │   ├── paper.qmd\n│   │   ├── references.bib\n│   │   └── ...\n│   └── ...\n├── scripts\n│   ├── 00-simulate_data.R\n│   ├── 01-download_data.R\n│   ├── 02-data_cleaning.R\n│   ├── 03-test_data.R\n│   └── ...\n└── ...\nHere we have an inputs folder that contains original, unedited data that should not be written over (Wilson et al. 2017) and literature related to the project. An outputs folder contains data that we create using R, as well as the paper that we are writing. And a scripts folder is what modifies the unedited data and saves it into outputs. We will do most of our work in “scripts”, and the Quarto file for the paper in outputs. Useful other aspects include a README.md which will specify overview details about the project, and a LICENSE. An example of what to put in the README is here. Another helpful variant of this project skeleton is provided by Mineault and The Good Research Code Handbook Community (2021)."
  },
  {
    "objectID": "03-workflow.html#version-control",
    "href": "03-workflow.html#version-control",
    "title": "3  Reproducible workflows",
    "section": "3.4 Version control",
    "text": "3.4 Version control\nIn this book we implement version control through a combination of Git and GitHub. There are a variety of reasons for this including:\n\nenhancing the reproducibility of work by making it easier to share code and data;\nmaking it easier to share work;\nimproving workflow by encouraging systematic approaches; and\nmaking it easier to work in teams.\n\nGit is a version control system with a fascinating history (Brown 2018). The way one often starts doing version control is to have various copies of the one file: “first_go.R”, “first_go-fixed.R”, “first_go-fixed-with-mons-edits.R”. But this soon becomes cumbersome. One often soon turns to dates, for instance: “2022-01-01-analysis.R”, “2022-01-02-analysis.R”, “2022-01-03-analysis.R”, etc. While this keeps a record, it can be difficult to search when we need to go back, because it is hard to remember the date some change was made. In any case, it quickly gets unwieldy for a project that is being regularly worked on.\nInstead of this, we use Git so that we can have one version of the file. Git keeps a record of the changes to that file, and a snapshot of that file at a given point in time. We determine when Git takes that snapshot. We additionally include a message saying what changed between this snapshot and the last. In that way, there is only ever one version of the file, and the history can be more easily searched.\nOne complication is that Git was designed for teams of software developers. As such, while it works, it can be a little ungainly for non-developers. Nonetheless Git has been usefully adapted for data science, even when the only collaborator one may have is one’s future self (Bryan 2018a).\nGitHub, GitLab, and various other companies offer easier-to-use services that build on Git. While there are tradeoffs, we introduce GitHub here because it is the predominant platform (Eghbal 2020, 21). Git and GitHub are built into Posit Cloud, which provides a nice option if you have issues with local installation. One of the initial challenging aspects of Git is the terminology. Folders are called “repos”. Creating a snapshot is called a “commit”. One gets used to it eventually, but feeling confused initially is normal. Bryan (2020) is especially useful for setting up and using Git and GitHub.\n\n3.4.1 Git\nWe first need to check whether Git is installed. Open RStudio, go to the Terminal, type the following, and then enter/return.\n\ngit --version\n\nIf you get a version number, then you are done (Figure 3.5 (a)).\n\n\n\n\n\n\n\n(a) Using Terminal to check whether Git is installed in RStudio\n\n\n\n\n\n\n\n\n\n(b) Adding a username and email address to Git in RStudio\n\n\n\n\nFigure 3.5: An overview of the steps involved in setting up Git\n\n\nGit is pre-installed in Posit Cloud, it should be pre-installed on Mac, and it may be pre-installed on Windows. If you do not get a version number in response, then you need to install it. To do that you should follow the instructions specific to your operating system in Bryan (2020, chap. 5).\nAfter Git is installed we need to tell it a username and email. We need to do this because Git adds this information whenever we take a snapshot, or to use Git’s language, whenever we make a commit.\nAgain, within the Terminal, type the following, replacing the details with yours, and then press “enter/return” after each line.\n\ngit config --global user.name \"Rohan Alexander\"\ngit config --global user.email \"rohan.alexander@utoronto.ca\"\ngit config --global --list\n\nWhen this set-up has been done properly, the values that you entered for “user.name” and “user.email” will be returned after the last line (Figure 3.5 (b)).\nThese details—username and email address—will be public. There are various ways to hide the email address if necessary, and GitHub provides instructions about this. Bryan (2020, chap. 7) provides more detailed instructions about this step, and a trouble-shooting guide.\n\n\n3.4.2 GitHub\nNow that Git is set up, we need to set up GitHub. We created a GitHub account in Chapter 2, which we use again here. After being signed in at github.com we first need to make a new folder, which is called a “repo” in Git. Look for a “+” in the top right, and then select “New Repository” (Figure 3.6 (a)).\n\n\n\n\n\n\n\n(a) Start process of creating a new repository\n\n\n\n\n\n\n\n(b) Copy the URL of the new repository\n\n\n\n\n\n\n\n\n\n(c) Adding the project to Posit Cloud\n\n\n\n\n\n\n\n(d) Creating a PAT\n\n\n\n\n\n\n\n\n\n(e) Adding files to be committed\n\n\n\n\n\n\n\n(f) Making a commit\n\n\n\n\nFigure 3.6: An overview of the steps involved in setting up GitHub\n\n\nAt this point we can add a sensible name for the repo. Leave it as “public” for now, because it can always be deleted later. And check the box to “Initialize this repository with a README”. Change “Add .gitignore” to R. After that, click “Create repository”.\nThis will take us to a screen that is fairly empty, but the details that we need—a URL—are in the green “Clone or Download” button, which we can copy by clicking the clipboard (Figure 3.6 (b)).\nNow returning to RStudio, in Posit Cloud, we create a new project using “New Project from Git Repository”. It will ask for the URL that we just copied (Figure 3.6 (c)). If you are using a local computer, then this step is accomplished through the menu: “File” \\(\\rightarrow\\) “New Project…” \\(\\rightarrow\\) “Version Control” \\(\\rightarrow\\) “Git”, then paste in the URL, give the folder a meaningful name, check “Open in new session”, then click “Create Project”.\nAt this point, a new folder has been created that we can use. We will want to be able to push it back to GitHub, and for that we will need to use a Personal Access Token (PAT) to link our RStudio Workspace with our GitHub account. We use usethis and gitcreds to enable this. These are, respectively, a package that automates repetitive tasks, and a package that authenticates with GitHub. To create a PAT, while signed into GitHub in the browser, and after installing and loading usethis run create_github_token() in your R session. GitHub will open in the browser with various options filled out (Figure 3.6 (d)). It can be useful to give the PAT an informative name by replacing “Note”, for instance “PAT for RStudio”, then click “Generate token”.\nWe only have one chance to copy this token, and if we make a mistake then we will need to generate a new one. Do not include the PAT in any R script or Quarto document. Instead, after installing and loading gitcreds, run gitcreds_set(), which will then prompt you to add your PAT in the console.\nTo use GitHub for a project that we are actively working on we follow this procedure:\n\nThe first thing to do is almost always to get any changes with “pull”. To do this, open the Git pane in RStudio, and click the blue down arrow. This gets any changes to the folder, as it is on GitHub, into our own version of the folder.\nWe can then make our changes to our copy of the folder. For instance, we could update the README, and then save it as normal.\nOnce this is done, we need to add, commit, and push. In the Git pane in RStudio, select the files to be added. This adds them to the staging area. Then click “Commit” (Figure 3.6 (e)). A new window will open. Add an informative message about the change that was made, and then click “Commit” in that new window (Figure 3.6 (f)). Finally, click “Push” to send the changes to GitHub.\n\nThere are a few common pain-points when it comes to Git and GitHub. We recommend committing and pushing regularly, especially when you are new to version control. This increases the number of snapshots that you could come back to if needed. All commits should have an informative commit message. If you are new to version control, then the expectation of a good commit message is that it contains a short summary of the change, followed by a blank line, and then an explanation of the change including what the change is, and why it is being made. For instance, if your commit adds graphs to a paper, then a commit message could be:\nAdd graphs\n\nGraphs of unemployment and inflation added into Data section.\nThere is some evidence of a relationship between overall quality and commit behavior (Sprint and Conci 2019). As you get more experience ideally the commit messages will act as a kind of journal of the project. But the main thing is to commit regularly.\nGit and GitHub were designed for software developers, rather than data scientists. GitHub limits the size of the files it will consider to 100MB, and even 50MB can prompt a warning. Data science projects regularly involve datasets that are larger than this. In Chapter 10 we discuss the use of data deposits, which can be especially useful when a project is completed, but when we are actively working on a project it can be useful to ignore large data files, at least as far as Git and GitHub are concerned. We do this using a “.gitignore” file, in which we list all of the files that we do not want to track using Git. The example folder contains a “.gitignore” file. And it can be helpful to run git_vaccinate() from usethis, which will add a variety of files to a global “.gitignore” file in case you forget to do it on a project basis. Mac users will find it useful that this will cause “.DS_Store” files to be ignored.\nWe used the Git pane in RStudio which removed the need to use the Terminal, but it did not remove the need to go to GitHub and set up a new project. Having set up Git and GitHub, we can further improve this aspect of our workflow with usethis.\nFirst check that Git is set up with git_sitrep() from usethis. This should print information about the username and email. We can use use_git_config() to update these details if needed.\n\nuse_git_config(\n  user.name = \"Rohan Alexander\",\n  user.email = \"rohan.alexander@utoronto.ca\"\n)\n\nRather than starting a new project in GitHub, and then adding it locally, we can now use use_git() to initiate it and commit the files. Having committed, we can use use_github() to push to GitHub, which will create the folder on GitHub as well.\nIt is normal to be intimidated by Git and GitHub. Many data scientists only know a little about how to use it, and that is okay. Try to push regularly so that you have a recent snapshot in case you need it."
  },
  {
    "objectID": "03-workflow.html#using-r-in-practice",
    "href": "03-workflow.html#using-r-in-practice",
    "title": "3  Reproducible workflows",
    "section": "3.5 Using R in practice",
    "text": "3.5 Using R in practice\n\n3.5.1 Dealing with errors\n\nWhen you are programming, eventually your code will break, when I say eventually, I mean like probably 10 or 20 times a day.\nGelfand (2021)\n\nEveryone who uses R, or any programming language for that matter, has trouble find them at some point. This is normal. Programming is hard. At some point code will not run or will throw an error. This happens to everyone. It is common to get frustrated, but to move forward we develop strategies to work through the issues:\n\nIf you are getting an error message, then sometimes it will be useful. Try to read it carefully to see if there is anything of use in it.\nTry to search for the error message. It can be useful to include “tidyverse” or “in R” in the search to help make the results more appropriate. Sometimes Stack Overflow results can be useful.\nLook at the help file for the function by putting “?” before the function, for instance, ?pivot_wider(). A common issue is to use a slightly incorrect argument name or format, such as accidentally including a string instead of an object name.\nLook at where the error is happening and remove or comment out code until the error is resolved, and then slowly add code back again.\nCheck the class of the object with class(), for instance, class(data_set$data_column). Ensure that it is what is expected.\nRestart R: “Session” \\(\\rightarrow\\) “Restart R and Clear Output”. Then load everything again.\nRestart your computer.\nSearch for what you are trying to do, rather than the error, being sure to include “tidyverse” or “in R” in the search to help make the results more appropriate. For instance, “save PDF of graph in R using ggplot”. Sometimes there are relevant blog posts or Stack Overflow answers that will help.\nMake a small, self-contained, reproducible example “reprex” to see if the issue can be isolated and to enable others to help.\n\nMore generally, while this is not always possible, it is almost always helpful to take a break and come back the next day.\n\n\n3.5.2 Reproducible examples\n\nNo one can advise or help you—no one. There is only one thing you should do. Go into yourself.\nRilke ([1929] 2014)\n\nAsking for help is a skill like any other. We get better at it with practice. It is important to try not to say “this doesn’t work”, “I tried everything”, “your code does not work”, or “here is the error message, what do I do?”. In general, it is not possible to help based on these comments, because there are too many possible issues. You need to make it easy for others to help you. This involves a few steps.\n\nProvide a small, self-contained example of your data, and code, and detail what is going wrong.\nDocument what you have tried so far, including which Stack Overflow and Posit Forum posts you looked at, and why they are not what you are after.\nBe clear about the outcome that you would like.\n\nBegin by creating a minimal REPRoducible EXample—a “reprex”. This is code that contains what is needed to reproduce the error, but only what is needed. This means that the code is likely a smaller, simpler version that nonetheless reproduces the error.\nSometimes this process enables one to solve the problem. If it does not, then it gives someone else a fighting chance of being able to help. There is almost no chance that you have got a problem that someone has not addressed before. It is more likely that the main difficulty is trying to communicate what you want to do and what is happening, in a way that allows others to recognize both. Developing tenacity is important.\nTo develop reproducible examples, reprex is especially useful. After installing it we:\n\nLoad the reprex package: library(reprex).\nHighlight and copy the code that is giving issues.\nRun reprex() in the console.\n\nIf the code is self-contained, then it will preview in the viewer. If it is not, then it will error, and you should rewrite the code so that it is self-contained.\nIf you need data to reproduce the error, then you should use data that is built into R. There are a large number of datasets that are built into R and can be seen using library(help = \"datasets\"). But if possible, you should use a common option such as mtcars or faithful. Combining a reprex with a GitHub Gist that was introduced in Chapter 2 increases the chances that someone is able to help you.\n\n\n3.5.3 Mentality\n\n(Y)ou are a real, valid, competent user and programmer no matter what IDE you develop in or what tools you use to make your work work for you\n(L)et’s break down the gates, there’s enough room for everyone\nSharla Gelfand, 10 March 2020.\n\nIf you write code, then you are a programmer, regardless of how you do it, what you are using it for, or who you are. But there are a few traits that one tends to notice great programmers have in common.\n\nFocused: Often having an aim to “learn R” or similar tends to be problematic, because there is no real end point to that. It tends to be more efficient to have smaller, more specific goals, such as “make a histogram about the 2022 Australian Election with ggplot2”. This is something that can be focused on and achieved in a few hours. The issue with goals that are more nebulous, such as “I want to learn R”, is that it is easier to get lost on tangents and more difficult to get help. This can be demoralizing and lead to people quitting too early.\nCurious: It is almost always useful to “have a go”; that is, if you are not sure, then just try it. In general, the worst that happens is that you waste your time. You can rarely break something irreparably. For instance, if you want to know what happens if you pass a vector instead of a dataframe to ggplot() then try it.\nPragmatic: At the same time, it can be useful to stick within reasonable bounds, and make one small change each time. For instance, say you want to run some regressions, and are curious about the possibility of using rstanarm instead of lm(). A pragmatic way to proceed is to use one aspect from rstanarm initially and then make another change next time.\nTenacious: Again, this is a balancing act. Unexpected problems and issues arise with every project. On the one hand, persevering despite these is a good tendency. But on the other hand, sometimes one does need to be prepared to give up on something if it does not seem like a break through is possible. Mentors can be useful as they tend to be a better judge of what is reasonable.\nPlanned: It is almost always useful to excessively plan what you are going to do. For instance, you may want to make a histogram of some data. You should plan the steps that are needed and even sketch out how each step might be implemented. For instance, the first step is to get the data. What packages might be useful? Where might the data be? What is the back-up plan if the data do not exist there?\nDone is better than perfect: We all have various perfectionist tendencies, but it can be useful to initially try to turn them off to a certain extent. Initially just worry about writing code that works. You can always come back and improve aspects of it. But it is important to actually ship. Ugly code that gets the job done is better than beautiful code that is never finished.\n\n\n\n3.5.4 Code comments and style\nCode must be commented. Comments should focus on why certain code was written and to a lesser extent, why a common alternative was not selected. Indeed, it can be a good idea to write the comments before you write the code, explaining what you want to do and why, and then returning to write the code (Fowler and Beck 2018, 59).\nThere is no one way to write code, especially in R. However, there are some general guidelines that will make it easier for you even if you are just working on your own. Most projects will evolve over time, and one purpose of code comments is to enable future-you to retrace what was done and why certain decisions were made (Bowers and Voors 2016).\nComments in R scripts can be added by including the # symbol. (The behavior of # is different for lines inside an R chunk in a Quarto document where it acts as a comment, compared with lines outside an R chunk where it sets heading levels.) We do not have to put a comment at the start of the line, it can be midway through. In general, you do not need to comment what every aspect of your code is doing but you should comment parts that are not obvious. For instance, if we read in some value then we may like to comment where it is coming from.\nYou should try to comment why you are doing something (Wickham 2021). What are you trying to achieve? You must comment to explain weird things. Like if you are removing some specific row, say row 27, then why are you removing that row? It may seem obvious in the moment, but future-you will not remember.\nYou should break your code into sections. For instance, setting up the workspace, reading in datasets, manipulating and cleaning the datasets, analyzing the datasets, and finally producing tables and figures. Each of these should be separated with comments explaining what is going on, and sometimes into separate files, depending on the length.\nAdditionally, at the top of each file it is important to note basic information, such as the purpose of the file, and prerequisites or dependencies, the date, the author and contact information, and finally any red flags or todos.\nYour R scripts should have a preamble and a clear demarcation of sections.\n#### Preamble ####\n# Purpose: Brief sentence about what this script does\n# Author: Your name\n# Date: The date it was written\n# Contact: Add your email\n# License: Think about how your code may be used\n# Pre-requisites: \n# - Maybe you need some data or some other script to have been run?\n\n\n#### Workspace setup ####\n# do not keep install.packages lines; comment out if need be\n# Load packages\nlibrary(tidyverse)\n\n# Read in the unedited data. \nraw_data &lt;- read_csv(\"inputs/data/unedited_data.csv\")\n\n\n#### Next section ####\n...\n\nFinally, try not to rely on a user commenting and uncommenting code, or any other manual step, such as directory specification, for code to work. This will preclude the use of automated code checking and testing.\nThis all takes time. As a rough rule of thumb, you should expect to spend at least as much time commenting and improving your code as you spent writing it. Some examples of nicely commented code include Dolatsara et al. (2021) and Burton, Cruz, and Hahn (2021).\n\n\n3.5.5 Tests\nTests should be written throughout the code, and you need to write them as we go, not all at the end. This will slow you down. But it will help you to think, and to fix mistakes, which will make your code better and lead to better overall productivity. Code without tests should be viewed with suspicion. There is room for improvement when it comes to testing practices in R packages (Vidoni 2021), let alone R code more generally.\nThe need for other people, and ideally, automated processes, to run tests on code is one reason that we emphasize reproducibility. That is also why we emphasize smaller aspects such as not hardcoding file-paths, using projects, and not having spaces in file names.\nIt is difficult to define a complete and general suite of tests, but broadly we want to test:\n\nboundary conditions,\nclasses,\nmissing data,\nthe number of observations and variables,\nduplicates, and\nregression results.\n\nWe do all this initially on our simulated data and then move to the real data. It is possible to write an infinite number of tests but a smaller number of high-quality tests is better than many thoughtless tests.\nOne type of test is an “assertion”. Assertions are written throughout the code to check whether something is true and stop the code from running if not (Irving et al. 2021, 272). For instance, you might assert that a variable should be numeric. If it was tested against this assertion and found to be a character, then the test would fail and the script would stop running. Assertion tests in data science will typically be used in data cleaning and preparation scripts. We have more to say about these in Chapter 9. Unit tests check some complete aspect of code (Irving et al. 2021, 274). We will consider them more in Chapter 12 when we consider modeling."
  },
  {
    "objectID": "03-workflow.html#efficiency",
    "href": "03-workflow.html#efficiency",
    "title": "3  Reproducible workflows",
    "section": "3.6 Efficiency",
    "text": "3.6 Efficiency\nGenerally in this book we are, and will continue to be, concerned with just getting something done. Not necessarily getting it done in the best or most efficient way, because to a large extent, being worried about that is a waste of time. For the most part one is better off just pushing things into the cloud, letting them run for a reasonable time, and using that time to worry about other aspects of the pipeline. But that eventually becomes unfeasible. At a certain point, and this differs depending on context, efficiency becomes important. Eventually ugly or slow code, and dogmatic insistence on a particular way of doing things, have an effect. And it is at that point that one needs to be open to new approaches to ensure efficiency. There is rarely a most common area for obvious performance gains. Instead, it is important to develop the ability to measure, evaluate, and think.\nOne of the best ways to improve the efficiency of our code is preparing it in such a way that we can bring in a second pair of eyes. To make the most of their time, it is important that our code easy to read. So we start with “code linting” and “styling”. This does not speed up our code, per se, but instead makes it more efficient when another person comes to it, or we revisit it. This enables formal code review and refactoring, which is where we rewrite code to make it better, while not changing what it does (it does the same thing, but in a different way). We then turn to measurement of run time, and introduce parallel processing, where we allow our computer to run code for multiple processes at the same time\n\n3.6.1 Sharing a code environment\nWe have discussed at length the need to share code, and we have put forward an approach to this using GitHub. And in Chapter 10, we will discuss sharing data. But, there is another requirement to enable other people to run our code. In Chapter 2 we discussed how R itself, as well as R packages update from time to time, as new functionality is developed, errors fixed, and other general improvements made. Online Appendix A describes how one advantage of the tidyverse is that it can update faster than base R, because it is more specific. But this could mean that even if we were to share all the code and data that we use, it is possible that the software versions that have become available would cause errors.\nThe solution to this is to detail the environment that was used. There are a large number of ways to do this, and they can add complexity. We just focus on documenting the version of R and R packages that were used, and making it easier for others to install that exact version. We use renv to do this.\nOnce renv is installed and loaded, we use init() to get the infrastructure set-up that we will need. We are going to create a file that will record the packages and versions used. We then use snapshot() to actually document what we are using. This creates a “lockfile” that records the information.\nIf we want to see which packages we are using in the R Project, then we can use dependencies(). Doing this for the example folder indicates that the following packages are used: rmarkdown, bookdown, knitr, rmarkdown, bookdown, knitr, palmerpenguins, tidyverse, renv, haven, readr, and tidyverse.\nWe could open the lockfile file—“renv.lock”—to see the exact versions if we wanted. The lockfile also documents all the other packages that were installed and where they were downloaded from. Someone coming to this project from outside could then use restore() which would install the exact version of the packages that we used.\n\n\n3.6.2 Code linting and styling\nBeing fast is valuable but it is mostly about being able to iterate fast, not necessarily having code that runs fast. Backus (1981, 26) describes how even in 1954 a programmer cost at least as much as a computer, and these days additional computational power is usually much cheaper than a programmer. Performant code is important, but it is also important to use other people’s time efficiently. Code is rarely only written once. Instead we typically have to come back to it, even if to just fix mistakes, and this means that code must be able to be read by humans (Matsumoto 2007, 478). If this is not done then there will be an efficiency cost.\nLinting and styling is the process of checking code, mostly for stylistic issues, and re-arranging code to make it easier to read. (There is another aspect of linting, which is dealing with programming errors, such as forgetting a closing bracket, but here we focus on stylistic issues.) Often the best efficiency gain comes from making it easier for others to read our code, even if this is just ourselves returning to the code after a break. Jane Street, a US proprietary trading firm, places a very strong focus on ensuring their code is readable, as a core part of risk mitigation (Minsky 2011). While we may not all have billions of dollars under the potentially mercurial management of code, we all would likely prefer that our code does not produce errors.\nWe use lint() from lintr to lint our code. For instance, consider the following R code (saved as “linting_example.R”).\n\nSIMULATED_DATA &lt;-\n  tibble(\n    division = c(1:150, 151),\n    party = sample(\n      x = c(\"Liberal\"),\n      size = 151,\n      replace = T\n    )\n  )\n\n\nlint(filename = \"linting_example.R\")\n\nThe result is that the file “linting_example.R” is opened and the issues that lint() found are printed in “Markers” (Figure 3.7). It is then up to you to deal with the issues.\n\n\n\nFigure 3.7: Linting results from example R code\n\n\nMaking the recommended changes results in code that is more readable, and consistent with best practice, as defined by Wickham (2021).\n\nsimulated_data &lt;-\n  tibble(\n    division = c(1:150, 151),\n    party = sample(\n      x = c(\"Liberal\"),\n      size = 151,\n      replace = TRUE\n    )\n  )\n\nAt first it may seem that some aspects that the linter is identifying, like trailing whitespace and only using double quotes are small and inconsequential. But they distract from being able to fix bigger issues. Further, if we are not able to get small things right, then how could anyone trust that we could get the big things right? Therefore, it is important to have dealt with all the small aspects that a linter identifies.\nIn addition to lintr we also use styler. This will automatically adjust style issues, in contrast to the linter, which gave a list of issues to look at. To run this we use style_file().\n\nstyle_file(path = \"linting_example.R\")\n\nThis will automatically make changes, such as spacing and indentation. As such this should be done regularly, rather than only once at the end of a project, so as to be able to review the changes and make sure no errors have been introduced.\n\n\n3.6.3 Code review\nHaving dealt with all of these aspects of style, we can turn to code review. This is the process of having another person go through and critique the code. Code review is a critical part of writing code, and Irving et al. (2021, 465) describe it as “the most effective way to find bugs”. It is especially helpful, although quite daunting, when learning to code because getting feedback is a great way to improve.\nGo out of your way to be polite and collegial when reviewing another person’s code. Small aspects to do with style, things like spacing and separation, should have been taken care of by a linter and styler, but if not, then make a general recommendation about that. Most of your time as a code reviewer in data science should be spent on aspects such as:\n\nIs there an informative README and how could it be improved?\nAre the file names and variable names consistent, informative, and meaningful?\nDo the comments allow you to understand why something is being done?\nAre the tests both appropriate and sufficient? Are there edge cases or corner solutions that are not considered? Similarly, are there unnecessary tests that could be removed?\nAre there magic numbers that could be changed to variables and explained?\nIs there duplicated code that could be changed?\nAre there any outstanding warnings that should be addressed?\nAre there any especially large functions or pipes that could be separated into smaller ones?\nIs the structure of the project appropriate?\nCan we change any of the code to data (Irving et al. 2021, 462)?\n\nFor instance, consider some code that looked for the names of prime ministers and presidents. When we first wrote this code we likely added the relevant names directly into the code. But as part of code review, we might instead recommend that this be changed. We might recommend creating a small dataset of relevant names, and then re-writing the code to have it look up that dataset.\nCode review ensures that the code can be understood by at least one other person. This is a critical part of building knowledge about the world. At Google, code review is not primarily about finding defects, although that may happen, but is instead about ensuring readability and maintainability as well as education (Sadowski et al. 2018). This is also the case at Jane Street where they use code review to catch bugs, share institutional knowledge, assist with training, and oblige staff to write code that can be read (Minsky 2015).\nFinally, code review does not have to, and should not, be an onerous days-consuming process of reading all the code. The best code review is a quick review of just one file, focused on suggesting changes to just a handful of lines. Indeed, it may be better to have a review done by a small team of people rather than one individual. Do not review too much code at any one time. At most a few hundred lines, which should take around an hour, because any more than that has been found to be associated with reduced efficacy (Cohen, Teleki, and Brown 2006, 79).\n\n\n3.6.4 Code refactoring\nTo refactor code means to rewrite it so that the new code achieves the same outcome as the old code, but the new code does it better. For instance, Chawla (2020) discuss how the code underpinning an important UK Covid model was initially written by epidemiologists, and months later clarified and cleaned up by a team from the Royal Society, Microsoft, and GitHub. This was valuable because it provided more confidence in the model, even though both versions produced the same outputs, given the same inputs.\nWe typically refer to code refactoring in relation to code that someone else wrote. (Although it may be that we actually wrote the code, and it was just that it was some time ago.) When we start to refactor code, we want to make sure that the rewritten code achieves the same outcomes as the original code. This means that we need a suite of appropriate tests written that we can depend on. If these do not exist, then we may need to create them.\nWe rewrite code to make it easier for others to understand, which in turn allows more confidence in our conclusions. But before we can do that, we need to understand what the existing code is doing. One way to get started is to go through the code and add extensive comments. These comments are different to normal comments. They are our active process of trying to understand what is each code chunk trying to do and how could this be improved.\nRefactoring code is an opportunity to ensure that it satisfies best practice. Trisovic et al. (2022) details some core recommendations based on examining 9,000 R scripts including:\n\nRemove setwd() and any absolute paths, and ensure that only relative paths, in relation to the “.Rproj” file, are used.\nEnsure there is a clear order of execution. We have recommended using numbers in filenames to achieve this initially, but eventually more sophisticated approaches, such as targets (Landau 2021), could be used instead.\nEnsure that code can run on a different computer.\n\nFor instance, consider the following code:\n\nsetwd(\"/Users/rohanalexander/Documents/telling_stories\")\n\nlibrary(tidyverse)\n\nd = read_csv(\"cars.csv\")\n\nmtcars =\n  mtcars |&gt; \n  mutate(K_P_L = mpg / 2.352)\n\nlibrary(datasauRus)\n\ndatasaurus_dozen\n\nWe could change that, starting by creating an R Project which enables us to remove setwd(), grouping all the library() calls at the top, using “&lt;-” instead of “=”, and being consistent with variable names:\n\nlibrary(tidyverse)\nlibrary(datasauRus)\n\ncars_data &lt;- read_csv(\"cars.csv\")\n\nmpg_to_kpl_conversion_factor &lt;- 2.352\n\nmtcars &lt;-\n  mtcars |&gt; \n  mutate(kpl = mpg / mpg_to_kpl_conversion_factor)\n\n\n\n3.6.5 Parallel processing\nSometimes code is slow because the computer needs to do the same thing many times. We may be able to take advantage of this and enable these jobs to be done at the same time using parallel processing. This will be especially useful starting from Chapter 12 for modeling.\nAfter installing and loading tictoc we can use tic() and toc() to time various aspects of our code. This is useful with parallel processing, but also more generally, to help us find out where the largest delays are.\n\ntic(\"First bit of code\")\nprint(\"Fast code\")\n\n[1] \"Fast code\"\n\ntoc()\n\nFirst bit of code: 0.003 sec elapsed\n\ntic(\"Second bit of code\")\nSys.sleep(3)\nprint(\"Slow code\")\n\n[1] \"Slow code\"\n\ntoc()\n\nSecond bit of code: 3.007 sec elapsed\n\n\nAnd so we know that there is something slowing down the code. (In this artificial case it is Sys.sleep() causing a delay of three seconds.)\nWe could use parallel which is part of base R to run functions in parallel. We could also use future which brings additional features. After installing and loading future we use plan() to specify whether we want to run things sequentially (“sequential”) or in parallel (“multisession”). We then wrap what we want this applied to within future().\nTo see this in action we will create a dataset and then implement a function on a row-wise basis.\n\nsimulated_data &lt;-\n  tibble(\n    random_draws = runif(n = 1000000, min = 0, max = 1000) |&gt; round(),\n    more_random_draws = runif(n = 1000000, min = 0, max = 1000) |&gt; round()\n  )\n\nplan(sequential)\n\ntic()\nsimulated_data &lt;-\n  simulated_data |&gt;\n  rowwise() |&gt;\n  mutate(which_is_smaller =\n           min(c(random_draws,\n                 more_random_draws)))\ntoc()\n\nplan(multisession)\n\ntic()\nsimulated_data &lt;-\n  future(simulated_data |&gt;\n           rowwise() |&gt;\n           mutate(which_is_smaller =\n                    min(c(\n                      random_draws,\n                      more_random_draws\n                    ))))\ntoc()\n\nThe sequential approach takes about 5 seconds, while the multisession approach takes about 0.3 seconds."
  },
  {
    "objectID": "03-workflow.html#concluding-remarks",
    "href": "03-workflow.html#concluding-remarks",
    "title": "3  Reproducible workflows",
    "section": "3.7 Concluding remarks",
    "text": "3.7 Concluding remarks\nIn this chapter we have covered a lot of ground and it is normal to be overwhelmed. Come back to the Quarto section as needed. Many people are confused by Git and Github and just know enough to get by. And while there was a lot of material in efficiency, the most important aspect of performant code is making it easier for another person to read it, even if that person is just yourself returning after a break."
  },
  {
    "objectID": "03-workflow.html#exercises",
    "href": "03-workflow.html#exercises",
    "title": "3  Reproducible workflows",
    "section": "3.8 Exercises",
    "text": "3.8 Exercises\n\nScales\n\n(Plan) Consider the following scenario: In a certain country there are only ever four parties that could win a seat in parliament. Whichever candidate has a plurality of votes in the area associated with a given seat wins that seat. The parliament is made up of 175 total seats. An analyst is interested in the number of votes for each party by seat. Please sketch what that dataset could look like, and then sketch a graph that you could build to show all observations.\n(Simulate I) Please further consider the scenario described, and decide which of the following could be used to simulate the situation (select all that apply)?\n\ntibble(seat = rep(1:175, each = 4), party = rep(x = 1:4, times = 175), votes = runif(n = 175 * 4, min = 0, max = 1000) |&gt; floor())\ntibble(seat = rep(1:175, each = 4), party = sample(x = 1:4, size = 175, replace = TRUE), votes = runif(n = 175 * 4, min = 0, max = 1000) |&gt; floor())\ntibble(seat = rep(1:175, each = 4), party = rep(x = 1:4, times = 175), votes = sample(x = 1:1000, size = 175 * 4))\ntibble(seat = rep(1:175, each = 4), party = sample(x = 1:4, size = 175, replace = TRUE), votes = sample(x = 1:1000, size = 175 * 4))\n\n(Simulate II) Please write three tests based on this simulation.\n(Acquire) Please identify one possible source of actual data about voting in a country of interest to you.\n(Explore) Assume that the tidyverse is loaded and the dataset “election_results” has the columns “seat”, “party”, and “votes”, as in the earlier question. Which of the following would result in a count of the number of seats won by each party (pick one)?\n\nelection_results |&gt; slice_max(votes, n = 1, by = seat) |&gt; count(party)\nelection_results |&gt; slice_max(votes, n = 1, by = party) |&gt; count(seat)\nelection_results |&gt; slice_max(votes, n = 1, by = party) |&gt; count(party)\nelection_results |&gt; slice_max(votes, n = 1, by = seat) |&gt; count(seat)\n\n(Communicate) Please write two paragraphs as if you had gathered data from that source and built a graph. The exact details contained in the paragraphs do not have to be factual (i.e. you do not actually have to get the data nor create the graphs).\n\n\n\nQuestions\n\nAccording to Alexander (2019) research is reproducible if (pick one)?\n\nIt is published in peer-reviewed journals.\nAll of the materials used in the study are provided.\nIt can be reproduced exactly without the authors providing materials.\nIt can be reproduced exactly, given all the materials used in the study.\n\nWhich of the following are components of the project layout recommended by Wilson et al. (2017) (select all that apply)?\n\nrequirements.txt\ndoc\ndata\nLICENSE\nCITATION\nREADME\nsrc\nresults\n\nBased on Alexander (2021) please write a paragraph about some of the barriers you overcame, or still face, with regard to sharing code that you wrote.\nAccording to Wickham (2021) for naming files, how would the files “00_get_data.R” and “get data.R” be classified (pick one)?\n\nbad; bad.\ngood; bad.\nbad; good.\ngood; good.\n\nWhich of the following would result in bold text in Quarto (pick one)?\n\n**bold**\n##bold##\n*bold*\n#bold#\n\nWhich option would hide the warnings in a Quarto R chunk (pick one)?\n\necho: false\neval: false\nwarning: false\nmessage: false\n\nWhich options would run the R code chunk and display the results, but not show the code in a Quarto R chunk (pick one)?\n\necho: false\ninclude: false\neval: false\nwarning: false\nmessage: false\n\nWhy are R Projects important (select all that apply)?\n\nThey help with reproducibility.\nThey make it easier to share code.\nThey make your workspace more organized.\n\nAssuming the packages and datasets have been loaded, what is the mistake in this code: DoctorVisits |&gt; filter(visits) (pick one)?\n\nvisits\nDoctorVisits\nfilter\n|&gt;\n\nWhat is a reprex and why is it important to be able to make one (select all that apply)?\n\nA reproducible example that enables your error to be reproduced.\nA reproducible example that helps others help you.\nA reproducible example during the construction of which you may solve your own problem.\nA reproducible example that demonstrates you have actually tried to help yourself.\n\nAccording to Gelfand (2021), what is the key part of “If you need help getting unstuck, the first step is to create a reprex, or reproducible example. The goal of a reprex is to package your problematic code in such a way that other people can run it and feel your pain. Then, hopefully, they can provide a solution and put you out of your misery.” (pick one)?\n\npackage your problematic code\nother people can run it and feel your pain\nthe first step is to create a reprex\nthey can provide a solution and put you out of your misery\n\n\n\nThe following code produces an error. Please use reprex to build a reproducible example that you could use to get help with it, and submit the reprex using a GitHub Gist. You should simplify many aspects including reducing the number of packages, changing the dataset, and simplifying the filter() and mutate().\n\n\nlibrary(tidyverse)\n\noecd_gdp &lt;-\n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nhead(oecd_gdp)\n\nlibrary(forcats)\nlibrary(dplyr)\n\noecd_gdp_most_recent &lt;-\n  oecd_gdp |&gt;\n  filter(\n    TIME == \"2021-Q3\",\n    SUBJECT == \"TOT\",\n    LOCATION %in% c(\n      \"AUS\", \"CAN\", \"CHL\", \"DEU\", \"GBR\",\n      \"IDN\", \"ESP\", \"NZL\", \"USA\", \"ZAF\"\n    ),\n    MEASURE == \"PC_CHGPY\"\n  ) |&gt;\n  mutate(\n    european = if_else(\n      LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n      \"European\",\n      \"Not european\"\n    ),\n    hemisphere = if_else(\n      LOCATION %in%\n        c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n      \"Northern Hemisphere\",\n      \"Southern Hemisphere\"\n    ),\n  )\n\nlibrary(ggplot)\nlibrary(patchwork)\n\noecd_gdp_most_recent |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value)) |&gt;\n  geom_bar(stat = \"identity\")\n\n\n\nTutorial\nCode review is an important part of working as a professional (Sadowski et al. 2018). Please put together a small Quarto file that downloads a dataset using opendatatoronto, cleans it, and makes a graph. Then exchange it with someone else. Following the advice of Google (2022), please provide them with a review of their code. That should be at least two pages of single-spaced content. Submit the review as a PDF.\n\n\nPaper\nAt about this point the Donaldson Paper from Online Appendix D would be appropriate.\n\n\n\n\nAlexander, Monica. 2019. “Reproducibility in Demographic Research.” https://www.monicaalexander.com/posts/2019-10-20-reproducibility/.\n\n\n———. 2021. “Overcoming Barriers to Sharing Code.” YouTube, February. https://youtu.be/yvM2C6aZ94k.\n\n\nBackus, John. 1981. “The History of FORTRAN I, II, and III.” In History of Programming Languages, edited by Richard Wexelblat, 25–74. Academic Press.\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible Research.” https://arxiv.org/abs/1802.03311.\n\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and Distributed Processing in R using Futures.” The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBowers, Jake, and Maarten Voors. 2016. “How to Improve Your Relationship with Your Future Self.” Revista de Ciencia Polı́tica 36 (3): 829–48. https://doi.org/10.4067/S0718-090X2016000300011.\n\n\nBrown, Zack. 2018. “A Git Origin Story.” Linux Journal, July. https://www.linuxjournal.com/content/git-origin-story.\n\n\nBryan, Jenny. 2018a. “Excuse Me, Do You Have a Moment to Talk about Version Control?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\n———. 2018b. “Code Smells and Feels.” YouTube, July. https://youtu.be/7oyiPBjLAWY.\n\n\n———. 2020. Happy Git and GitHub for the useR. https://happygitwithr.com.\n\n\nBryan, Jenny, and Jim Hester. 2020. What They Forgot to Teach You About R. https://rstats.wtf/index.html.\n\n\nBryan, Jenny, Jim Hester, David Robinson, Hadley Wickham, and Christophe Dervieux. 2022. reprex: Prepare Reproducible Example Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBuckheit, Jonathan, and David Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer. https://doi.org/10.1007/978-1-4612-2544-7_5.\n\n\nBurton, Jason, Nicole Cruz, and Ulrike Hahn. 2021. “Reconsidering Evidence of Moral Contagion in Online Social Networks.” Nature Human Behaviour 5 (12): 1629–35. https://doi.org/10.1038/s41562-021-01133-5.\n\n\nChawla, Dalmeet Singh. 2020. “Critiqued Coronavirus Simulation Gets Thumbs up from Code-Checking Efforts.” Nature 582: 323–24. https://doi.org/10.1038/d41586-020-01685-y.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted Decision Making in Child Maltreatment Hotline Screening Decisions.” In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, edited by Sorelle Friedler and Christo Wilson, 81:134–48. Proceedings of Machine Learning Research. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nCohen, Jason, Steven Teleki, and Eric Brown. 2006. Best Kept Secrets of Peer Code Review. Smart Bear Incorporated.\n\n\nCsárdi, Gábor. 2022. gitcreds: Query “git” Credentials from “R”. https://CRAN.R-project.org/package=gitcreds.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed, and Allison Jones-Farmer. 2021. “Explaining Predictive Model Performance: An Experimental Study of Data Preparation and Model Choice.” Big Data, October. https://doi.org/10.1089/big.2021.0067.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance of Open Source Software. California: Stripe Press.\n\n\nFowler, Martin, and Kent Beck. 2018. Refactoring: Improving the Design of Existing Code. 2nd ed. New York: Addison-Wesley Professional.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... Please.” YouTube, February. https://youtu.be/G5Nm-GpmrLw.\n\n\nGelman, Andrew. 2016. “What has happened down here is the winds have changed,” September. https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\nGelman, Andrew, Greggor Mattson, and Daniel Simpson. 2018. “Gaydar and the Fallacy of Decontextualized Measurement.” Sociological Science 5 (12): 270–80. https://doi.org/10.15195/v5.a12.\n\n\nGoogle. 2022. “What to Look for in a Code Review.” Google Engineering Practices Documentation. https://google.github.io/eng-practices/review/reviewer/looking-for.html.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, and Stephanie Hicks. 2021. “Reproducibility Standards for Machine Learning in the Life Sciences.” Nature Methods 18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” Cambridge Journal of Economics 38 (2): 257–79. https://doi.org/10.1093/cje/bet075.\n\n\nHester, Jim, Florent Angly, Russ Hyde, Michael Chirico, Kun Ren, Alexander Rosenstock, and Indrajeet Patil. 2022. lintr: A “Linter” for R Code. https://CRAN.R-project.org/package=lintr.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte Wickham, and Greg Wilson. 2021. Research Software Engineering with Python. Chapman; Hall/CRC.\n\n\nIsaacson, Walter. 2011. Steve Jobs. 1st ed. Simon & Schuster.\n\n\nIzrailev, Sergei. 2022. tictoc: Functions for Timing R Scripts, as Well as Implementations of “Stack” and “List” Structures. https://CRAN.R-project.org/package=tictoc.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, Donald. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLandau, William Michael. 2021. “The targets R Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nMaier, Maximilian, František Bartoš, Tom Stanley, David Shanks, Adam Harris, and Eric-Jan Wagenmakers. 2022. “No Evidence for Nudging After Adjusting for Publication Bias.” Proceedings of the National Academy of Sciences 119 (31): e2200300119. https://doi.org/10.1073/pnas.2200300119.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as an Essay.” In Beautiful Code, edited by Andy Oram and Greg Wilson, 477–81. O’Reilly.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers Gayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.” Nature 467 (7317): 775–77. https://doi.org/10.1038/467775a.\n\n\nMineault, Patrick, and The Good Research Code Handbook Community. 2021. “The Good Research Code Handbook.” https://doi.org/10.5281/zenodo.5796873.\n\n\nMinsky, Yaron. 2011. “OCaml for the masses.” Communications of the ACM 54 (11): 53–58. https://doi.org/10.1145/2018396.2018413.\n\n\n———. 2015. “Automated Trading and OCaml with Yaron Minsky.” Hackers — Software Engineering Daily, November. https://softwareengineeringdaily.com/2015/11/09/automated-trading-and-ocaml-with-yaron-minsky/.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another Possible Source of the Reproducibility Crisis.” Molecular Brain 13 (1): 1–6. https://doi.org/10.1186/s13041-020-0552-2.\n\n\nMüller, Kirill, and Lorenz Walthert. 2022. styler: Non-Invasive Pretty Printing of R Code. https://CRAN.R-project.org/package=styler.\n\n\nMurphy, Heather. 2017. “Why Stanford Researchers Tried to Create a ‘Gaydar’ Machine.” The New York Times, October. https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research (a Report from the NeurIPS 2019 Reproducibility Program).” Journal of Machine Learning Research 22 (164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRilke, Rainer Maria. (1929) 2014. Letters to a Young Poet. Penguin Classics.\n\n\nRomer, Paul. 2018. “Jupyter, Mathematica, and the Future of the Research Paper,” April. https://paulromer.net/jupyter-mathematica-and-the-future-of-the-research-paper/.\n\n\nSadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and Alberto Bacchelli. 2018. “Modern Code Review: A Case Study at Google.” In Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, 181–90. ICSE-SEIP ’18. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3183519.3183525.\n\n\nSilver, Nate. 2020. “We Fixed an Issue with How Our Primary Forecast Was Calculating Candidates’ Demographic Strengths.” FiveThirtyEight, February. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSomers, James. 2018. “The Scientific Paper Is Obsolete.” The Atlantic, April. https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining GitHub Classroom Commit Behavior in Elective and Introductory Computer Science Courses.” Journal of Computing Sciences in Colleges 35 (1): 76–84.\n\n\nSunstein, Cass, and Lucia Reisch. 2017. The Economics of Nudge. Routledge.\n\n\nSzaszi, Barnabas, Anthony Higney, Aaron Charlton, Andrew Gelman, Ignazio Ziano, Balazs Aczel, Daniel Goldstein, David Yeager, and Elizabeth Tipton. 2022. “No Reason to Expect Large and Consistent Effects of Nudge Interventions.” Proceedings of the National Academy of Sciences 119 (31): e2200732119. https://doi.org/10.1073/pnas.2200732119.\n\n\nTrisovic, Ana, Matthew Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.\n\n\nUshey, Kevin. 2022. renv: Project Environments. https://CRAN.R-project.org/package=renv.\n\n\nVidoni, Melina. 2021. “Evaluating Unit Testing Practices in R Packages.” In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 1523–34. https://doi.org/10.1109/ICSE43902.2021.00136.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWickham, Hadley. 2021. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2019. “TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "04-writing_research.html#introduction",
    "href": "04-writing_research.html#introduction",
    "title": "4  Writing research",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\n\nIf you want to be a writer, you must do two things above all others: read a lot and write a lot. There’s no way around these two things that I’m aware of, no shortcut.\nS. King (2000, 145)\n\nWe predominately tell stories with data by writing them down. Writing allows us to communicate efficiently. It is also a way to work out what we believe and allows us to get feedback on our ideas. Effective papers are tightly written and well-organized, which makes their story flow well. Proper sentence structure, spelling, vocabulary, and grammar are important because they remove distractions and enable each aspect of the story to be clearly articulated.\nThis chapter is about writing. By the end of it, you will have a better idea of how to write short, detailed, quantitative papers that communicate what you want them to, and do not waste the reader’s time. We write for the reader, not for ourselves. Specifically, we write to be useful to the reader. This means clearly communicating something new, true, and important (Graham 2020). That said, the greatest benefit of writing nonetheless often accrues to the writer, even when we write for our audience. This is because the process of writing is a way to work out what we think and how we came to believe it.\nAspects of this chapter can feel a little like a list. It may be that you go through those aspects quickly initially, and then return to them as needed."
  },
  {
    "objectID": "04-writing_research.html#writing",
    "href": "04-writing_research.html#writing",
    "title": "4  Writing research",
    "section": "4.2 Writing",
    "text": "4.2 Writing\n\nThe way to do a piece of writing is three or four times over, never once. For me, the hardest part comes first, getting something—anything—out in front of me. Sometimes in a nervous frenzy I just fling words as if I were flinging mud at a wall. Blurt out, heave out, babble out something—anything—as a first draft.\nMcPhee (2017, 159)\n\nThe process of writing is a process of rewriting. The critical task is to get to a first draft as quickly as possible. Until that complete first draft exists, it is useful to try to not to delete, or even revise, anything that was written, regardless of how bad it may seem. Just write. (This advice is directed at less-experienced writers. As you get more experience, you may find that your approach changes.)\nOne of the most intimidating stages is a blank page, and we deal with this by immediately adding headings such as: “Introduction”, “Data”, “Model”, “Results”, and “Discussion”. And then adding fields in the top matter for the various bits and pieces that are needed, such as “title”, “date”, “author”, and “abstract”. This creates a generic outline, which will play the role of mise en place for the paper. By way of background, mise en place is a preparatory phase in a professional kitchen when ingredients are sorted, prepared, and arranged for easy access. This ensures that everything that is needed is available without unnecessary delay. Putting together an outline plays the same role when writing quantitative papers, and is akin to placing on the counter, the ingredients that we will use to prepare dinner (McPhee 2017).\nHaving established this generic outline, we need to develop an understanding of what we are exploring through thinking deeply about our research question. In theory, we develop a research question, answer it, and then do all the writing; but that rarely actually happens (Franklin 2005). Instead, we typically have some idea of the question and the shape of an answer, and these become less vague as we write. This is because it is through the process of writing that we refine our thinking (S. King 2000, 131). Having put down some thoughts about the research question, we can start to add dot points in each of the sections, adding sub-sections with informative sub-headings as needed. We then go back and expand those dot points into paragraphs. While we do this our thinking is influenced by a web of other researchers, but also other aspects such as our circumstances and environment (Latour 1996).\nWhile writing the first draft you should ignore the feeling that you are not good enough, or that it is impossible. Just write. You need words on paper, even if they are bad, and the first draft is when you accomplish this. Remove distractions and focus on writing. Perfectionism is the enemy, and should be set aside. Sometimes this can be accomplished by getting up very early to write, by creating a deadline, or forming a writing group. Creating a sense of urgency can be useful and one option is to not bother with adding proper citations as you go, which could slow you down, and instead just add something like “[TODO: CITE R HERE]”. Do similar with graphs and tables. That is, include textual descriptions such as “[TODO: ADD GRAPH THAT SHOWS EACH COUNTRY OVER TIME HERE]” instead of actual graphs and tables. Focus on adding content, even if it is bad. When this is all done, a first draft exists.\nThis first draft will be poorly written and far from great. But it is by writing a bad first draft that you can get to a good second draft, a great third draft, and eventually excellence (Lamott 1994, 20). That first draft will be too long, it will not make sense, it will contain claims that cannot be supported, and some claims that should not be. If you are not embarrassed by your first draft, then you have not written it quickly enough.\nUse the “delete” key extensively, as well as “cut” and “paste”, to turn that first draft into a second. Print the draft and using a red pen to move or remove words, sentences, and entire paragraphs, is especially helpful. The process of going from a first draft to a second draft is best done in one sitting, to help with the flow and consistency of the story. One aspect of this first rewrite is enhancing the story that we want to tell. Another aspect is taking out everything that is not the story (S. King 2000, 57).\nIt can be painful to remove work that seems good even if it does not quite fit into what the draft is becoming. One way to make this less painful is to make a temporary document, perhaps named “debris.qmd”, to save these unwanted paragraphs instead of immediately deleting them. Another strategy is to comment out the paragraphs. That way you can still look at the raw file and notice aspects that could be useful.\nAs you go through what was written in each of the sections try to bring some sense to it with special consideration to how it supports the story that is developing. This revision process is the essence of writing (McPhee 2017, 160). You should also fix the references, and add the real graphs and tables. As part of this rewriting process, the paper’s central message tends to develop, and the answers to the research questions tend to become clearer. At this point, aspects such as the introduction can be returned to and, finally, the abstract. Typos and other issues affect the credibility of the work. So these should be fixed as part of the second draft.\nAt this point the draft is starting to become sensible. The job is to now make it brilliant. Print it and again go through it on paper. Try to remove everything that does not contribute to the story. At about this stage, you may start to get too close to the paper. This is a great opportunity to give it to someone else for their comments. Ask for feedback about what is weak about the story. After addressing these, it can be helpful to go through the paper once more, this time reading it aloud. A paper is never “done” and it is more that at a certain point you either run out of time or become sick of the sight of it."
  },
  {
    "objectID": "04-writing_research.html#asking-questions",
    "href": "04-writing_research.html#asking-questions",
    "title": "4  Writing research",
    "section": "4.3 Asking questions",
    "text": "4.3 Asking questions\nBoth qualitative and quantitative approaches have their place. In this book we focus on quantitative approaches. Nonetheless qualitative research is important, and often the most interesting work has a little of both. When conducting quantitative analysis, we are subject to issues such as data quality, measurement, and relevance. We are often especially interested in trying to tease out causality. Regardless, we are trying to learn something about the world. Our research questions need to take this all into account.\nBroadly, and at the risk of over-simplification, there are two ways to go about research:\n\ndata-first; or\nquestion-first.\n\nBut it is not a binary, and often research proceeds by iterating between data and questions, organized around a research puzzle (Gustafsson and Hagström 2017). Light, Singer, and Willett (1990, 39) describe this approach as a spiral of \\(\\mbox{theory}\\rightarrow\\mbox{data}\\rightarrow\\mbox{theory}\\rightarrow\\mbox{data}\\), etc. For instance, a question-first approach could be theory-driven or data-driven, as could a data-first approach. An alternative framing is to compare an inductive, or specific-to-general, approach with a deductive, or general-to-specific, approach to research.\nConsider two examples:\n\nMok et al. (2022) examine eight billion unique listening events from 100,000 Spotify users to understand how users explore content. They find a clear relationship between age and behavior, with younger users exploring unknown content less than older users, despite having more diverse consumption. While it is clear that research questions around discovery and exploration drive this paper, it would not have been possible without access to this dataset. There likely would have been an iterative process where potential research questions and potential datasets were considered, before the ultimate match.\nThink of wanting to explore the neonatal mortality rate (NMR), which was introduced in Chapter 2. One might be interested in what NMR could look like in Sub-Saharan Africa in 20 years. This would be question-first. But within this, there could be: theory-driven aspects, such as what do we expect based on biological relationships with other quantities; or data-driven aspects such as collecting as much data as possible to make forecasts. An alternative, purely data-driven approach would be having access to the NMR and then working out what is possible.\n\n\n4.3.1 Data-first\nWhen being data-first, the main issue is working out the questions that can be reasonably answered with the available data. When deciding what these are, it is useful to consider:\n\nTheory: Is there a reasonable expectation that there is something causal that could be determined? For instance, Mark Christensen used to joke that if the question involved charting the stock market, then it might be better to hark back to The Odyssey and read bull entrails on a fire, because at least that way you would have something to eat at the end of the day. Questions usually need to have some plausible theoretical underpinning to help avoid spurious relationships. One way to develop theory, given data, is to consider “of what is this an instance?” (Rosenau 1999, 7). Following that approach, one tries to generalize beyond the specific setting. For instance, thinking of some particular civil war as an instance of all civil wars. The benefit of this is it focuses attention on the general attributes needed for building theory.\nImportance: There are plenty of trivial questions that can be answered, but it is important to not waste our time or that of the reader. Having an important question can also help with motivation when we find ourselves in, say, the fourth straight week of cleaning data and debugging code. In industry it can also make it easier to attract talented employees and funding. That said, a balance is needed; the question needs to have a decent chance of being answered. Attacking a generation-defining question might be best broken into chunks.\nAvailability: Is there a reasonable expectation of additional data being available in the future? This could allow us to answer related questions and turn one paper into a research agenda.\nIteration: Is this something that could be run multiple times, or is it a once-off analysis? If it is the former, then it becomes possible to start answering specific research questions and then iterate. But if we can only get access to the data once then we need to think about broader questions.\n\nThere is a saying, sometimes attributed to Xiao-Li Meng, that all of statistics is a missing data problem. And so paradoxically, another way to ask data-first questions is to think about the data we do not have. For instance, returning to the neonatal and maternal mortality examples discussed earlier one problem is that we do not have complete cause of death data. If we did, then we could count the number of relevant deaths. (Castro et al. (2023) remind us that this simplistic hypothetical would be complicated in reality because there are sometimes causes of death that are not independent of other causes.) Having established some missing data problem, we can take a data-driven approach. We look at the data we do have, and then ask research questions that speak to the extent that we can use that to approximate our hypothetical dataset.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nXiao-Li Meng is the Whipple V. N. Jones Professor of Statistics at Harvard University. After earning a PhD in Statistics from Harvard University in 1990 he was appointed as an assistant professor at the University of Chicago where he was promoted to professor in 2000. He moved to Harvard in 2001, serving as chair of the statistics department between 2004 and 2012. He has published on a wide range of topics including missing data—Meng (1994) and Meng (2012)—and data quality—Meng (2018). He was awarded the COPSS Presidents’ Award in 2001.\n\n\nOne way that some researchers are data-first is that they develop a particular expertise in the data of some geographical or historical circumstance. For instance, they may be especially knowledgeable about, say, the present-day United Kingdom, or late nineteenth century Japan. They then look at the questions that other researchers are asking in other circumstances, and bring their data to that question. For instance, it is common to see a particular question initially asked for the United States, and then a host of researchers answer that same question for the United Kingdom, Canada, Australia, and many other countries.\nThere are a number of negatives to data-first research, including the fact that it can be especially uncertain. It can also struggle for external validity because there is always a worry about a selection effect.\nA variant of data-driven research is model-driven research. Here a researcher becomes an expert on some particular statistical approach and then applies that approach to appropriate contexts.\n\n\n4.3.2 Question-first\nWhen trying to be question-first, there is the inverse issue of being concerned about data availability. The “FINER framework” is used in medicine to help guide the development of research questions. It recommends asking questions that are: Feasible, Interesting, Novel, Ethical, and Relevant (Hulley et al. 2007). Farrugia et al. (2010) build on FINER with PICOT, which recommends additional considerations: Population, Intervention, Comparison group, Outcome of interest, and Time.\nIt can feel overwhelming trying to write out a question. One way to go about it is to ask a very specific question. Another is to decide whether we are interested in descriptive, predictive, inferential, or causal analysis. These then lead to different types of questions. For instance:\n\ndescriptive analysis: “What does \\(x\\) look like?”;\npredictive analysis: “What will happen to \\(x\\)?”;\ninferential: “How can we explain \\(x\\)?”; and\ncausal: “What impact does \\(x\\) have on \\(y\\)?”.\n\nEach of these have a role to play. Since the credibility revolution (Angrist and Pischke 2010), causal questions answered with a particular approach have been predominant. This has brought some benefit, but not without cost. Descriptive analysis can be just as, indeed sometimes more, illuminating, and is critical (Sen 1980). The nature of the question being asked matters less than being genuinely interested in answering it.\nTime will often be constrained, possibly in an interesting way and this can guide the specifics of the research question. If we are interested in the effect of a celebrity’s announcements on the stock market, then that can be done by looking at stock prices before and after the announcement. But what if we are interested in the effect of a cancer drug on long term outcomes? If the effect takes 20 years, then we must either wait a while, or we need to look at people who were treated twenty years ago. We then have selection effects and different circumstances compared to if we were to administer the drug today. Often the only reasonable thing to do is to build a statistical model, but that brings other issues."
  },
  {
    "objectID": "04-writing_research.html#answering-questions",
    "href": "04-writing_research.html#answering-questions",
    "title": "4  Writing research",
    "section": "4.4 Answering questions",
    "text": "4.4 Answering questions\nThe creation of a counterfactual is often crucial when answering questions. A counterfactual is an if-then statement in which the “if” is false. Consider the example of Humpty Dumpty in Through the Looking-Glass by Lewis Carroll:\n\n“What tremendously easy riddles you ask!” Humpty Dumpty growled out. “Of course I don’t think so! Why, if ever I did fall off—which there’s no chance of—but if I did—” Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. “If I did fall,” he went on, “The King has promised me—with his very own mouth-to-to-” “To send all his horses and all his men,” Alice interrupted, rather unwisely.\nCarroll (1871)\n\nHumpty is satisfied with what would happen if he were to fall off, even though he is convinced that this would never happen. It is this comparison group that often determines the answer to a question. For instance, in Chapter 14 we consider the effect of VO2 max on a cyclist’s chance of winning a race. If we compare over the general population then it is an important variable. But if we only compare over well-trained athletes, then it is less important, because of selection.\nTwo aspects of the data to be especially aware of when deciding on a research question are selection bias and measurement bias.\nSelection bias occurs when the results depend on who is in the sample. One of the pernicious aspects of selection bias is that we need to know about its existence in order to do anything about it. But many default diagnostics will not identify selection bias. In A/B testing, which we discuss in Chapter 8, A/A testing is a slight variant where we create groups and compare them before imposing a treatment (hence the A/A nomenclature). This effort to check whether the groups are initially the same, can help to identify selection bias. More generally, comparing the properties of the sample, such as age-group, gender, and education, with characteristics of the population can assist as well. But the fundamental problem with selection bias and observational data is that we know people about whom we have data are different in at least one way to those about whom we do not! But we do not know in what other ways they may be different.\nSelection bias can pervade many aspects of our analysis. Even a sample that is initially representative may become biased over time. For instance, survey panels, that we discuss in Chapter 6, need to be updated from time to time because the people who do not get anything out of it stop responding.\nAnother bias to be aware of is measurement bias, which occurs when the results are affected by how the data were collected. A common example of this is if we were to ask respondents their income, then we may get different answers in-person compared with an online survey.\nWe will typically be interested in using data to answer our question and it is important that we are clear about specifics. For instance, we might be interested in the effect of smoking on life expectancy. In that case, there is some true effect, which we can never know, and that true effect is called the “estimand” (Little and Lewis 2021). Defining the estimand at some point in the paper, ideally in the introduction, is critical (Lundberg, Johnson, and Stewart 2021). This is because it is easy to slightly change some specific aspect of the analysis plan and end up accidentally estimating something different (Kahan et al. 2022). We are looking for a clear description of what the effect represents (Kahan et al. 2023). An “estimator” is a process by which we use the data that we have available to generate an “estimate” of the “estimand”. Efron and Morris (1977) provide a discussion of estimators and related concerns.\nBueno de Mesquita and Fowler (2021, 94) describe the relationship between an estimate and an estimand as:\n\\[\n\\mbox{Estimate = Estimand + Bias + Noise}\n\\]\nBias refers to issues with an estimator systematically providing estimates that are different from the estimand, while noise refers to non-systematic differences. For instance, consider a standard Normal distribution. We might be interested in understanding the average, which would be our estimand. We know (in a way that we can never with real data) that the estimand is zero. Let us draw ten times from that distribution. One estimator we could use to produce an estimate is: sum the draws and divide by the number of draws. Another is to order the draws and find the middle observation. To be more specific, we will simulate this situation (Table 4.1).\n\nset.seed(853)\n\ntibble(\n  num_draws = c(\n    rep(10, times = 10),\n    rep(100, times = 100),\n    rep(1000, times = 1000),\n    rep(10000, times = 10000)\n  ),\n  draw = rnorm(\n    n = length(num_draws),\n    mean = 0,\n    sd = 1)\n  ) |&gt; \n  summarise(\n    estimator_one = sum(draw) / unique(num_draws),\n    estimator_two = sort(draw)[round(unique(num_draws) / 2, 0)],\n    .by = num_draws\n  ) |&gt;\n  kable(\n    col.names = c(\"Number of draws\", \"Estimator one\", \"Estimator two\"),\n    digits = 2,\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 4.1: Comparing two estimators of the average of random draws as the number of draws increases\n\n\nNumber of draws\nEstimator one\nEstimator two\n\n\n\n\n10\n-0.58\n-0.82\n\n\n100\n-0.06\n-0.07\n\n\n1,000\n0.06\n0.04\n\n\n10,000\n-0.01\n-0.01\n\n\n\n\n\n\nAs the number of draws increases, the effect of noise is removed, and our estimates illustrate the bias of our estimators. In this example, we know what the truth is, but when considering real data it can be more difficult to know what to do. Hence the importance of being clear about what the estimand is, before turning to generating estimates."
  },
  {
    "objectID": "04-writing_research.html#components-of-a-paper",
    "href": "04-writing_research.html#components-of-a-paper",
    "title": "4  Writing research",
    "section": "4.5 Components of a paper",
    "text": "4.5 Components of a paper\n\nI had not indeed published anything before I commenced The Professor, but in many a crude effort, destroyed almost as soon as composed, I had got over any such taste as I might once have had for ornamented and redundant composition, and come to prefer what was plain and homely.\nThe Professor (Brontë 1857)\n\nWe discuss the following components: title, abstract, introduction, data, results, discussion, figures, tables, equations, and technical terms.1 Throughout the paper try to be as brief and specific as possible. Most readers will not get past the title. Almost no one will read more than the abstract. Section and sub-section headings, as well as graph and table captions should work on their own, without the surrounding text, because that type of skimming is how many people read papers (Keshav 2007).\n\n4.5.1 Title\nA title is the first opportunity that we have to engage our reader in our story. Ideally, we are able to tell our reader exactly what we found. Effective titles are critical because otherwise papers could be ignored by readers. While a title does not have to be “cute”, it does need to be meaningful. This means it needs to make the story clear.\nOne example of a title that is good enough is “On the 2016 Brexit referendum”. This title is useful because the reader knows what the paper is about. But it is not particularly informative or enticing. A slightly better title could be “On the Vote Leave outcome in the 2016 Brexit referendum”. This variant adds informative specificity. We argue the best title would be something like “Vote Leave outperforms in rural areas in the 2016 Brexit referendum: Evidence from a Bayesian hierarchical model”. Here the reader knows the approach of the paper and also the main take-away.\nWe will consider a few examples of particularly effective titles. Hug et al. (2019) use “National, regional, and global levels and trends in neonatal mortality between 1990 and 2017, with scenario-based projections to 2030: a systematic analysis”. Here it is clear what the paper is about and the methods that are used. R. Alexander and Alexander (2021) use “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018”. The main finding is, along with a good deal of information about what the content will be, clear from the title. M. Alexander, Kiang, and Barbieri (2018) use “Trends in Black and White Opioid Mortality in the United States, 1979–2015”; Frei and Welsh (2022) use “How the closure of a US tax loophole may affect investor portfolios”. Possibly one of the best titles ever is Bickel, Hammel, and O’Connell (1975) “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation”, which we return to in Chapter 14.\nA title is often among the last aspects of a paper to be finalized. While getting through the first draft, we typically use a working title that gets the job done. We then refine it over the course of redrafting. The title needs to reflect the final story of the paper, and this is not usually something that we know at the start. We must strike a balance between getting our reader interested enough to read the paper, and conveying enough of the content so as to be useful (Hayot 2014). Two excellent examples are The History of England from the Accession of James the Second by Thomas Babington Macaulay, and A History of the English-Speaking Peoples by Winston Churchill. Both are clear about what the content is, and, for their target audience, spark interest.\nOne specific approach is the form: “Exciting content: Specific content”, for instance, “Returning to their roots: Examining the performance of Vote Leave in the 2016 Brexit referendum”. Kennedy and Gelman (2021) provide a particularly nice example of this approach with “Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample”, as does Craiu (2019) with “The Hiring Gambit: In Search of the Twofer Data Scientist”. A close variant of this is “A question? And an approach”. For instance, Cahill, Weinberger, and Alkema (2020) with “What increase in modern contraceptive use is needed in FP2020 countries to reach 75% demand satisfied by 2030? An assessment using the Accelerated Transition Method and Family Planning Estimation Model”. As you gain experience with this variant, it becomes possible to know when it is appropriate to drop the answer part yet remain effective, such as Briggs (2021) with “Why Does Aid Not Target the Poorest?”. Another specific approach is “Specific content then broad content” or the inverse. For instance, “Rurality, elites, and support for Vote Leave in the 2016 Brexit referendum” or “Support for Vote Leave in the 2016 Brexit referendum, rurality and elites”. This approach is used by Tolley and Paquet (2021) with “Gender, municipal party politics, and Montreal’s first woman mayor”.\n\n\n4.5.2 Abstract\nFor a ten-to-fifteen-page paper, a good abstract is a three-to-five sentence paragraph. For a longer paper the abstract can be slightly longer. The abstract needs to specify the story of the paper. It must also convey what was done and why it matters. To do so, an abstract typically touches on the context of the work, its objectives, approach, and findings.\nMore specifically, a good recipe for an abstract is: first sentence: specify the general area of the paper and encourage the reader; second sentence: specify the dataset and methods at a general level; third sentence: specify the headline result; and a fourth sentence about implications.\nWe see this pattern in a variety of abstracts. For instance, Tolley and Paquet (2021) draw in the reader with their first sentence by mentioning the election of the first woman mayor in 400 years. The second sentence is clear about what is done in the paper. The third sentence tells the reader how it is done i.e. a survey, and the fourth sentence adds some detail. The fifth and final sentence makes the main take-away clear.\n\nIn 2017, Montreal elected Valérie Plante, the first woman mayor in the city’s 400-year history. Using this election as a case study, we show how gender did and did not influence the outcome. A survey of Montreal electors suggests that gender was not a salient factor in vote choice. Although gender did not matter much for voters, it did shape the organization of the campaign and party. We argue that Plante’s victory can be explained in part by a strategy that showcased a less leader-centric party and a degendered campaign that helped counteract stereotypes about women’s unsuitability for positions of political leadership.\n\nSimilarly, Beauregard and Sheppard (2021) make the broader environment clear within the first two sentences, and the specific contribution of this paper to that environment. The third and fourth sentences make the data source and main findings clear. The fifth and sixth sentences add specificity that would be of interest to likely readers of this abstract i.e. academic political scientists. In the final sentence, the position of the authors is made clear.\n\nPrevious research on support for gender quotas focuses on attitudes toward gender equality and government intervention as explanations. We argue the role of attitudes toward women in understanding support for policies aiming to increase the presence of women in politics is ambivalent—both hostile and benevolent forms of sexism contribute in understanding support, albeit in different ways. Using original data from a survey conducted on a probability-based sample of Australian respondents, our findings demonstrate that hostile sexists are more likely to oppose increasing of women’s presence in politics through the adoption of gender quotas. Benevolent sexists, on the other hand, are more likely to support these policies than respondents exhibiting low levels of benevolent sexism. We argue this is because benevolent sexism holds that women are pure and need protection; they do not have what it takes to succeed in politics without the assistance of quotas. Finally, we show that while women are more likely to support quotas, ambivalent sexism has the same relationship with support among both women and men. These findings suggest that aggregate levels of public support for gender quotas do not necessarily represent greater acceptance of gender equality generally.\n\nAnother excellent example of an abstract is Sides, Vavreck, and Warshaw (2021). In just five sentences, they make it clear what they do, how they do it, what they find, and why it is important.\n\nWe provide a comprehensive assessment of the influence of television advertising on United States election outcomes from 2000–2018. We expand on previous research by including presidential, Senate, House, gubernatorial, Attorney General, and state Treasurer elections and using both difference-in-differences and border-discontinuity research designs to help identify the causal effect of advertising. We find that televised broadcast campaign advertising matters up and down the ballot, but it has much larger effects in down-ballot elections than in presidential elections. Using survey and voter registration data from multiple election cycles, we also show that the primary mechanism for ad effects is persuasion, not the mobilization of partisans. Our results have implications for the study of campaigns and elections as well as voter decision making and information processing.\n\nKasy and Teytelboym (2023) provide an excellent example of a more statistical abstract. They clearly identify what they do and why it is important.\n\nWe consider an experimental setting in which a matching of resources to participants has to be chosen repeatedly and returns from the individual chosen matches are unknown but can be learned. Our setting covers two-sided and one-sided matching with (potentially complex) capacity constraints, such as refugee resettlement, social housing allocation, and foster care. We propose a variant of the Thompson sampling algorithm to solve such adaptive combinatorial allocation problems. We give a tight, prior-independent, finite-sample bound on the expected regret for this algorithm. Although the number of allocations grows exponentially in the number of matches, our bound does not. In simulations based on refugee resettlement data using a Bayesian hierarchical model, we find that the algorithm achieves half of the employment gains (relative to the status quo) that could be obtained in an optimal matching based on perfect knowledge of employment probabilities.\n\nFinally, Briggs (2021) begins with a claim that seems unquestionably true. In the second sentence he then says that it is false! The third sentence specifies the extent of this claim, and the fourth sentence details how he comes to this position, before providing more detail. The final two sentences speak broader implications and importance.\n\nForeign-aid projects typically have local effects, so they need to be placed close to the poor if they are to reduce poverty. I show that, conditional on local population levels, World Bank (WB) project aid targets richer parts of countries. This relationship holds over time and across world regions. I test five donor-side explanations for pro-rich targeting using a pre-registered conjoint experiment on WB Task Team Leaders (TTLs). TTLs perceive aid-receiving governments as most interested in targeting aid politically and controlling implementation. They also believe that aid works better in poorer or more remote areas, but that implementation in these areas is uniquely difficult. These results speak to debates in distributive politics, international bargaining over aid, and principal-agent issues in international organizations. The results also suggest that tweaks to WB incentive structures to make ease of project implementation less important may encourage aid to flow to poorer parts of countries.\n\nNature, a scientific journal, provides a guide for constructing an abstract. They recommend a structure that results in an abstract of six parts and adds up to around 200 words:\n\nAn introductory sentence that is comprehensible to a wide audience.\nA more detailed background sentence that is relevant to likely readers.\nA sentence that states the general problem.\nSentences that summarize and then explain the main results.\nA sentence about general context.\nAnd finally, a sentence about the broader perspective.\n\nThe first sentence of an abstract should not be vacuous. Assuming the reader continued past the title, this first sentence is the next opportunity that we have to implore them to keep reading our paper. And then the second sentence of the abstract, and so on. Work and re-work the abstract until it is so good that you would be fine if that was the only thing that was read; because that will often be the case.\n\n\n4.5.3 Introduction\nAn introduction needs to be self-contained and convey everything that a reader needs to know. We are not writing a mystery story. Instead, we want to give away the most important points in the introduction. For a ten-to-fifteen-page paper, an introduction may be two or three paragraphs of main content. Hayot (2014, 90) says the goal of an introduction is to engage the reader, locate them in some discipline and background, and then tell them what happens in the rest of the paper. It should be completely reader-focused.\nThe introduction should set the scene and give the reader some background. For instance, we typically start a little broader. This provides some context to the paper. We then describe how the paper fits into that context, and give some high-level results, especially focused on the one key result that is the main part of the story. We provide more detail here than we provided in the abstract, but not the full extent. And we broadly discuss next steps in a sentence or two. Finally, we finish the introduction with an additional short final paragraph that highlights the structure of the paper.\nAs an example (with made-up details):\n\nThe UK Conservative Party has always done well in rural electorates. And the 2016 Brexit vote was no different with a significant difference in support between rural and urban areas. But even by the standard of rural support for conservative issues, support for “Vote Leave” was unusually strong with “Vote Leave” being most heavily supported in the East Midlands and the East of England, while the strongest support for “Remain” was in Greater London.\nIn this paper we look at why the performance of “Vote Leave” in the 2016 Brexit referendum was so correlated with rurality. We construct a model in which support for “Vote Leave” at a voting area level is explained by the number of farms in the area, the average internet connectivity, and the median age. We find that as the median age of an area increases, the likelihood that an area supported “Vote Leave” decreases by 14 percentage points. Future work could look at the effect of having a Conservative MP which would allow a more nuanced understanding of these effects.\nThe remainder of this paper is structured as follows: Section 2 discusses the data, Section 3 discusses the model, Section 4 presents the results, and finally Section 5 discusses our findings and some weaknesses.\n\nThe introduction needs to be self-contained and tell the reader almost everything that they need to know. A reader should be able to only read the introduction and have an accurate picture of all the major aspects of the whole paper. It would be rare to include graphs or tables in the introduction. An introduction should close by telegraphing the structure of the paper.\n\n\n4.5.4 Data\nRobert Caro, Lyndon Johnson’s biographer, describes the importance of conveying “a sense of place” when writing a biography (Caro 2019, 141). He defines this as “the physical setting in which a book’s action is occurring: to see it clearly enough, in sufficient detail, so that he feels as if he himself were present while the action is occurring.” He provides the following example:\n\nWhen Rebekah walked out the front door of that little house, there was nothing—a roadrunner streaking behind some rocks with something long and wet dangling from his beak, perhaps, or a rabbit disappearing around a bush so fast that all she really saw was the flash of a white tail—but otherwise nothing. There was no movement except for the ripple of the leaves in the scattered trees, no sound except for the constant whisper of the wind\\(\\dots\\) If Rebekah climbed, almost in desperation, the hill in the back of the house, what she saw from its crest was more hills, an endless vista of hills, hills on which there was visible not a single house\\(\\dots\\) hills on which nothing moved, empty hills with, above them, empty sky; a hawk circling silently overhead was an event. But most of all, there was nothing human, no one to talk to.\nCaro (2019, 146)\n\nHow thoroughly we can imagine the circumstances of Johnson’s mother, Rebekah Baines Johnson. When writing our papers, we need to achieve that same sense of place, for our data, as Caro provides for the Hill County. We do this by being as explicit as possible. We typically have a whole section about it and this is designed to show the reader, as closely as possible, the actual data that underpin our story.\nWhen writing the data section, we are beginning our answer to the critical question about our claim, which is, how is it possible to know this? (McPhee 2017, 78). An excellent example of a data section is provided by Doll and Hill (1950). They are interested in the effect of smoking between control and treatment groups. After clearly describing their dataset they use tables to display relevant cross-tabs and graphs to contrast groups.\nIn the data section we need to thoroughly discuss the variables in the dataset that we are using. If there are other datasets that could have been used, but were not, then this should be mentioned and the choice justified. If variables were constructed or combined, then this process and motivation should be explained.\nWe want the reader to understand what the data that underpin the results look like. This means that we should graph the data that are used in our analysis, or as close to them as possible. And we should also include tables of summary statistics. If the dataset was created from some other source, then it can also help to include an example of that original source. For instance, if the dataset was created from survey responses then the underlying survey questions should be included in an appendix.\nSome judgment is required when it comes to the figures and tables in the data section. The reader should have the opportunity to understand the details, but it may be that some are better placed in an appendix. Figures and tables are a critical aspect of convincing people of a story. In a graph we can show the data and then let the reader decide for themselves. And using a table, we can summarize a dataset. At the very least, every variable should be shown in a graph and summarized in a table. If there are too many, then some of these could be relegated to an appendix, with the critical relationships shown in the main body. Figures and tables should be numbered and then cross-referenced in the text, for instance, “Figure 1 shows\\(\\dots\\)”, “Table 1 describes\\(\\dots\\)”. For every graph and table there should be accompanying text that describes their main aspects, and adds additional detail.\nWe discuss the components of graphs and tables, including titles and labels, in Chapter 5. But here we will discuss captions, as they are between the text and the graph or table. Captions need to be informative and self-contained. Borkin et al. (2015) use eye-tracking to understand how visualizations are recognized and recalled. They find that captions need to make the central message of the figure clear, and that there should be redundancy. As Cleveland ([1985] 1994, 57) says, the “interplay between graph, caption, and text is a delicate one”, however the reader should be able to read only the caption and understand what the graph or table shows. A caption that is two lines long is not necessarily inappropriate. And all aspects of the graph or table should be explained. For instance, consider Figure 4.1 (a) and Figure 4.1 (b), both from Bowley (1901, 151). They are clear, and self-contained.\n\n\n\n\n\n\n\n(a) Example of a well-captioned figure\n\n\n\n\n\n\n\n(b) Example of a well-captioned table\n\n\n\n\nFigure 4.1: Examples of a graph and table from Bowley (1901)\n\n\nThe choice between a table and a graph comes down to how much information is to be conveyed. In general, if there is specific information that should be considered, such as a summary statistic, then a table is a good option. If we are interested in the reader making comparisons and understanding trends, then a graph is a good option (Gelman, Pasarica, and Dodhia 2002).\n\n\n4.5.5 Model\nWe often build a statistical model that we will use to explore the data, and it is normal to have a specific section about this. At a minimum you should specify the equations that describe the model being used and explain their components with plain language and cross-references.\nThe model section typically begins with the model being written out, explained, and justified. Depending on the expected reader, some background may be needed. After specifying the model with appropriate mathematical notation and cross-referencing it, the components of the model should then be defined and explained. Try to define each aspect of the notation. This helps convince the reader that the model was well-chosen and enhances the credibility of the paper. The model’s variables should correspond to those that were discussed in the data section, making a clear link between the two sections.\nThere should be some discussion of how features enter the model and why. Some examples could include:\n\nWhy use age rather than age-groups?\nWhy does state/province have a levels effect?\nWhy is gender a categorical variable? In general, we are trying to convey a sense that this is the appropriate model for the situation. We want the reader to understand how the aspects that were discussed in the data section assert themselves in the modeling decisions that were made.\n\nThe model section should close with some discussion of the assumptions that underpin the model. It should also have a brief discussion of alternative models or variants. You want the strengths and weaknesses to be clear and for the reader to know why this particular model was chosen.\nAt some point in this section, it is usually appropriate to specify the software that was used to run the model, and to provide some evidence of thought about the circumstances in which the model may not be appropriate. That second point would typically be expanded on in the discussion section. And there should be evidence of model validation and checking, model convergence, and/or diagnostic issues. Again, there is a balance needed here, and some of this content may be more appropriately placed in appendices.\nWhen technical terms are used, they should be briefly explained in plain language for readers who might not be familiar with it. For instance, M. Alexander (2019) integrates an explanation of the Gini coefficient that brings the reader along.\n\nTo look at the concentration of baby names, let’s calculate the Gini coefficient for each country, sex and year. The Gini coefficient measures dispersion or inequality among values of a frequency distribution. It can take any value between 0 and 1. In the case of income distributions, a Gini coefficient of 1 would mean one person has all the income. In this case, a Gini coefficient of 1 would mean that all babies have the same name. In contrast, a Gini coefficient of 0 would mean names are evenly distributed across all babies.\n\nThere may be papers that do not include a statistical model. In that case, this “Model” section should be replaced by a broader “Methodology” section. It might describe the simulation that was conducted, or contain more general details about the approach.\n\n\n4.5.6 Results\nTwo excellent examples of results sections are provided by Kharecha and Hansen (2013) and Kiang et al. (2021). In the results section, we want to communicate the outcomes of the analysis in a clear way and without too much focus on the discussion of implications. The results section likely requires summary statistics, tables, and graphs. Each of those aspects should be cross-referenced and have text associated with them that details what is seen in each figure. This section should relay results; that is, we are interested in what the results are, rather than what they mean.\nThis section would also typically include tables of graphs of coefficient estimates based on the modeling. Various features of the estimates should be discussed, and differences between the models explained. It may be that different subsets of the data are considered separately. Again, all graphs and tables need to have text in plain language accompany them. A rough guide is that the amount of text should be at least equal to the amount of space taken up by the tables and graphs. For instance, if a full page is used to display a table of coefficient estimates, then that should be cross-referenced and accompanied by about a full page of text about that table.\n\n\n4.5.7 Discussion\nA discussion section may be the final section of a paper and would typically have four or five sub-sections.\nThe discussion section would typically begin with a sub-section that comprises a brief summary of what was done in the paper. This would be followed by two or three sub-sections that are devoted to the key things that we learn about the world from this paper. These sub-sections are the main opportunity to justify or detail the implications of the story being told in the paper. Typically, these sub-sections do not see newly introduced graphs or tables, but are instead focused on what we learn from those that were introduced in earlier sections. It may be that some of the results are discussed in relation to what others have found, and differences could be attempted to be reconciled here.\nFollowing these sub-sections of what we learn about the world, we would typically have a sub-section focused on some of the weaknesses of what was done. This could concern aspects such as the data that were used, the approach, and the model. In the case of the model we are especially concerned with those aspects that might affect the findings. This can be especially difficult in the case of machine learning models and Smith et al. (2022) provide guidance for aspects to consider. And the final sub-section is typically a few paragraphs that specify what is left to learn, and how future work could proceed.\nIn general, we would expect this section to take at least 25 per cent of the total paper. This means that in an eight-page paper we would expect at least two pages of discussion.\n\n\n4.5.8 Brevity, typos, and grammar\nBrevity is important. This is partly because we write for the reader, and the reader has other priorities. But it is also because as the writer it forces us to consider what our most important points are, how we can best support them, and where our arguments are weakest. Jean Chrétien, is a former Canadian prime minister. In Chrétien (2007, 105) he wrote that he used to ask “\\(\\dots\\)the officials to summarize their documents in two or three pages and attach the rest of the materials as background information. I soon discovered that this was a problem only for those who didn’t really know what they were talking about” .\nThis experience is not unique to Canada and it is not new. In Hughes and Rutter (2016) Oliver Letwin, the former British cabinet member, describes there being “a huge amount of terrible guff, at huge, colossal, humongous length coming from some departments” and how he asked “for them to be one quarter of the length”. He found that the departments were able to accommodate this request without losing anything important. Winston Churchill asked for brevity during the Second World War, saying “the discipline of setting out the real points concisely will prove an aid to clearer thinking.” The letter from Szilard and Einstein to FDR that was the catalyst for the Manhattan Project was only two pages!\nZinsser (1976) goes further and describes “the secret of good writing” being “to strip every sentence to its cleanest components.” Every sentence should be simplified to its essence. And every word that does not contribute should be removed.\nUnnecessary words, typos, and grammatical issues should be removed from papers. These mistakes affect the credibility of claims. If the reader cannot trust you to use a spell-checker, then why should they trust you to use logistic regression? RStudio has a spell-checker built in, but Microsoft Word and Google Docs are useful additional checks. Copy from the Quarto document and paste into Word, then look for the red and green lines, and fix them in the Quarto document.\nWe are not worried about the n-th degree of grammatical content. Instead, we are interested in grammar and sentence structure that occurs in conversational language use (S. King 2000, 118). The way to develop comfort is by reading a lot and asking others to also read your work. Another useful tactic is to read your writing aloud, which can be useful for detecting odd sentences based on how they sound. One small aspect to check that will regularly come up is that any number from one to ten should be written as words, while 11 and over should be written as numbers.\n\n\n4.5.9 Rules\nA variety of authors have established rules for writing. This famously includes those of Orwell (1946) which were reimagined by The Economist (2013). A further reimagining of rules for writing, focused on telling stories with data, could be:\n\nFocus on the reader and their needs. Everything else is commentary.\nEstablish a structure and then rely on that to tell the story.\nWrite a first draft as quickly as possible.\nRewrite that draft extensively.\nBe concise and direct. Remove as many words as possible.\nUse words precisely. For instance, stock prices rise or fall, rather than improve or worsen.\nUse short sentences where possible.\nAvoid jargon.\nWrite as though your work will be on the front page of a newspaper.\nNever claim novelty or that you are the “first to study X”—there is always someone else who got there first.\n\nFiske and Kuriwaki (2021) have a list of rules for scientific papers and the appendix of Pineau et al. (2021) provides a checklist for machine learning papers. But perhaps the last word should be from Savage and Yeh (2019):\n\n[T]ry to write the best version of your paper: the one that you like. You can’t please an anonymous reader, but you should be able to please yourself. Your paper—you hope—is for posterity.\nSavage and Yeh (2019, 442)"
  },
  {
    "objectID": "04-writing_research.html#exercises",
    "href": "04-writing_research.html#exercises",
    "title": "4  Writing research",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\nScales\n\n(Plan) Consider the following scenario: A child and their parent watch street cars from their apartment window. Every hour, for eight hours, they record the number of streetcars that go past. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include three tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe one possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched using the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nAccording to Chapter 2 of Zinsser (1976), what is the secret to good writing (pick one)?\n\nCorrect sentence structure and grammar.\nThe use of long words, adverbs, and passive voice.\nThorough planning.\nStrip every sentence to its cleanest components.\n\nAccording to Chapter 2 of Zinsser (1976), what must a writer constantly ask (pick one)?\n\nWhat am I trying to say?\nWho am I writing for?\nHow can this be rewritten?\nWhy does this matter?\n\nWhich two repeated words, for instance in Chapter 3, characterize the advice of Zinsser (1976) (pick one)?\n\nRewrite, rewrite.\nRemove, remove.\nSimplify, simplify.\nLess, less.\n\nAccording to G. King (2006), what is the key task of subheadings (pick one)?\n\nEnable a reader who randomly falls asleep but keeps turning pages to know where they are.\nBe broad and sweeping so that a reader is impressed by the importance of the paper.\nUse acronyms to integrate the paper into the literature.\n\nAccording to G. King (2006), if our standard error was 0.05 then which of the following specificities for a coefficient would be silly (select all that apply)?\n\n2.7182818\n2.718282\n2.72\n2.7\n2.7183\n2.718\n3\n2.71828\n\nWhat is a key aspect of the re-drafting process (select all that apply)?\n\nGoing through it with a red pen to remove unneeded words.\nPrinting the paper and reading a physical copy.\nCutting and pasting to enhance flow.\nReading it aloud.\nExchanging it with others.\n\nWhat are three features of a good research question (write a paragraph or two)?\nIn your own words, what is a counterfactual (include examples and references and write at least three paragraphs)?\nWhat is an estimate (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe object of inquiry.\nA result given a particular dataset and approach.\n\nWhat is an estimator (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe object of inquiry.\nA result given a particular dataset and approach.\n\nWhat is an estimand (pick one)?\n\nA rule for calculating an estimate of a given quantity based on observed data.\nThe object of inquiry.\nA result given a particular dataset and approach.\n\nWhich of the following is the best title (pick one)?\n\n“Problem Set 2”\n“Standard errors”\n“Standard errors of estimates from small samples”\n\nPlease write a new title for Fourcade and Healy (2017).\nUsing only the 1,000 most popular words in the English language, according to the XKCD Simple Writer, rewrite the abstract of Chambliss (1989) so that it retains its original meaning.\n\n\n\nTutorial\nCaro (2019, xii) writes at least 1,000 words almost every day. In this tutorial you will write every day for a week. Please pick one of the papers specified in the prerequisites and complete the following tasks:\n\nDay 1: Transcribe, by writing each word yourself, the entire introduction.\nDay 2: Rewrite the introduction so that it is five lines (or 10 per cent, whichever is less) shorter.\nDay 3: Transcribe, by writing each word yourself, the abstract.\nDay 4: Rewrite a new, four-sentence, abstract for the paper.\nDay 5: Write a second version of your new abstract using only the 1,000 most popular words in the English language as defined here.\nDay 6: Detail three points about the way the paper is written that you like\nDay 7: Detail one point about the way the paper is written that you do not like.\n\nPlease use Quarto to produce a single PDF for the whole week. Make use of headings and sub-headings to structure your submission. Submit the PDF.\n\n\n\n\nAlexander, Monica. 2019. “The Concentration and Uniqueness of Baby Names in Australia and the US,” January. https://www.monicaalexander.com/posts/2019-20-01-babynames/.\n\n\nAlexander, Monica, Mathew Kiang, and Magali Barbieri. 2018. “Trends in Black and White Opioid Mortality in the United States, 1979–2015.” Epidemiology 29 (5): 707–15. https://doi.org/10.1097/EDE.0000000000000858.\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament Between 1901 and 2018.” https://doi.org/10.48550/arXiv.2111.09299.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2010. “The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con Out of Econometrics.” Journal of Economic Perspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nBarron, Alexander, Jenny Huang, Rebecca Spang, and Simon DeDeo. 2018. “Individuals, Institutions, and Innovation in the Debates of the French Revolution.” Proceedings of the National Academy of Sciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBeauregard, Katrine, and Jill Sheppard. 2021. “Antiwomen but Proquota: Disaggregating Sexism and Support for Gender Quota Policies.” Political Psychology 42 (2): 219–37. https://doi.org/10.1111/pops.12696.\n\n\nBickel, Peter, Eugene Hammel, and William O’Connell. 1975. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring Bias Is Harder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary to Expectation.” Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBirkmeyer, John, Jonathan Finks, Amanda O’Reilly, Mary Oerline, Arthur Carlin, Andre Nunn, Justin Dimick, Mousumi Banerjee, and Nancy Birkmeyer. 2013. “Surgical Skill and Complication Rates After Bariatric Surgery.” New England Journal of Medicine 369 (15): 1434–42. https://doi.org/10.1056/nejmsa1300625.\n\n\nBland, Martin, and Douglas Altman. 1986. “Statistical Methods for Assessing Agreement Between Two Methods of Clinical Measurement.” The Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nBorkin, Michelle, Zoya Bylinskii, Nam Wook Kim, Constance May Bainbridge, Chelsea Yeh, Daniel Borkin, Hanspeter Pfister, and Aude Oliva. 2015. “Beyond Memorability: Visualization Recognition and Recall.” IEEE Transactions on Visualization and Computer Graphics 22 (1): 519–28. https://doi.org/10.1109/TVCG.2015.2467732.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P. S. King.\n\n\nBriggs, Ryan. 2021. “Why Does Aid Not Target the Poorest?” International Studies Quarterly 65 (3): 739–52. https://doi.org/10.1093/isq/sqab035.\n\n\nBronner, Laura. 2021. “Quantitative Editing.” YouTube, June. https://youtu.be/LI5m9RzJgWc.\n\n\nBrontë, Charlotte. 1857. The Professor. https://www.gutenberg.org/files/1028/1028-h/1028-h.htm.\n\n\nBueno de Mesquita, Ethan, and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. New Jersey: Princeton University Press.\n\n\nCahill, Niamh, Michelle Weinberger, and Leontine Alkema. 2020. “What Increase in Modern Contraceptive Use Is Needed in FP2020 Countries to Reach 75% Demand Satisfied by 2030? An Assessment Using the Accelerated Transition Method and Family Planning Estimation Model.” Gates Open Research 4. https://doi.org/10.12688/gatesopenres.13125.1.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan. https://www.gutenberg.org/files/12/12-h/12-h.htm.\n\n\nCastro, Marcia, Susie Gurzenda, Cassio Turra, Sun Kim, Theresa Andrasfay, and Noreen Goldman. 2023. “Research Note: COVID-19 Is Not an Independent Cause of Death.” Demography, February. https://doi.org/10.1215/00703370-10575276.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. 1st ed. Toronto: Knopf Canada.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data. 2nd ed. New Jersey: Hobart Press.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nDoll, Richard, and Bradford Hill. 1950. “Smoking and Carcinoma of the Lung.” British Medical Journal 2 (4682): 739–48. https://doi.org/10.1136/bmj.2.4682.739.\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in Statistics.” Scientific American 236 (May): 119–27. https://doi.org/10.1038/scientificamerican0577-119.\n\n\nFarrugia, Patricia, Bradley Petrisor, Forough Farrokhyar, and Mohit Bhandari. 2010. “Research Questions, Hypotheses and Objectives.” Canadian Journal of Surgery 53 (4): 278.\n\n\nFiske, Susan, and Shiro Kuriwaki. 2021. “Words to the Wise on Writing Scientific Papers,” November. https://doi.org/10.31234/osf.io/n32qw.\n\n\nFourcade, Marion, and Kieran Healy. 2017. “Seeing Like a Market.” Socio-Economic Review 15 (1): 9–29. https://doi.org/10.1093/ser/mww033.\n\n\nFranklin, Laura. 2005. “Exploratory Experiments.” Philosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nFrei, Christoph, and Liam Welsh. 2022. “How the Closure of a U.S. Tax Loophole May Affect Investor Portfolios.” Journal of Risk and Financial Management 15 (5): 209. https://doi.org/10.3390/jrfm15050209.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s Practice What We Preach: Turning Tables into Graphs.” The American Statistician 56 (2): 121–30. https://doi.org/10.1198/000313002317572790.\n\n\nGraham, Paul. 2020. “How to Write Usefully,” February. http://paulgraham.com/useful.html.\n\n\nGustafsson, Karl, and Linus Hagström. 2017. “What Is the Point? Teaching Graduate Students How to Construct Political Science Research Puzzles.” European Political Science 17 (4): 634–48. https://doi.org/10.1057/s41304-017-0130-y.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. New York: Columbia University Press.\n\n\nHug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN Inter-agency Group for Child. 2019. “National, Regional, and Global Levels and Trends in Neonatal Mortality Between 1990 and 2017, with Scenario-Based Projections to 2030: A Systematic Analysis.” Lancet Global Health 7 (6): e710–20. https://doi.org/10.1016/S2214-109X(19)30163-9.\n\n\nHughes, Nicola, and Jill Rutter. 2016. “Ministers Reflect: Interview with Oliver Letwin,” December. https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/.\n\n\nHulley, Stephen, Steven Cummings, Warren Browner, Deborah Grady, and Thomas Newman. 2007. Designing Clinical Research. 3rd ed. Lippincott Williams & Wilkins.\n\n\nJoyner, Michael. 1991. “Modeling: Optimal Marathon Performance on the Basis of Physiological Factors.” Journal of Applied Physiology 70 (2): 683–87. https://doi.org/10.1152/jappl.1991.70.2.683.\n\n\nKahan, Brennan, Suzie Cro, Fan Li, and Michael Harhay. 2023. “Eliminating Ambiguous Treatment Effects Using Estimands.” American Journal of Epidemiology, February. https://doi.org/10.1093/aje/kwad036.\n\n\nKahan, Brennan, Fan Li, Andrew Copas, and Michael Harhay. 2022. “Estimands in Cluster-Randomized Trials: Choosing Analyses That Answer the Right Question.” International Journal of Epidemiology, July. https://doi.org/10.1093/ije/dyac131.\n\n\nKasy, Maximilian, and Alexander Teytelboym. 2023. “Matching with Semi-Bandits.” The Econometrics Journal 26 (1): 45–66. https://doi.org/10.1093/ectj/utac021.\n\n\nKennedy, Lauren, and Andrew Gelman. 2021. “Know Your Population and Know Your Model: Using Model-Based Regression and Poststratification to Generalize Findings Beyond the Observed Sample.” Psychological Methods 26 (5): 547–58. https://doi.org/10.1037/met0000362.\n\n\nKeshav, Srinivasan. 2007. “How to Read a Paper.” ACM SIGCOMM Computer Communication Review 37 (3): 83–84. https://doi.org/10.1145/1273445.1273458.\n\n\nKharecha, Pushker, and James Hansen. 2013. “Prevented Mortality and Greenhouse Gas Emissions from Historical and Projected Nuclear Power.” Environmental Science & Technology 47 (9): 4889–95. https://doi.org/10.1021/es3051197.\n\n\nKiang, Mathew, Alexander Tsai, Monica Alexander, David Rehkopf, and Sanjay Basu. 2021. “Racial/Ethnic Disparities in Opioid-Related Mortality in the USA, 1999–2019: The Extreme Case of Washington DC.” Journal of Urban Health 98 (5): 589–95. https://doi.org/10.1007/s11524-021-00573-8.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS: Political Science & Politics 39 (1): 119–25. https://doi.org/10.1017/S1049096506060252.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nKoenker, Roger, and Achim Zeileis. 2009. “On Reproducible Econometric Research.” Journal of Applied Econometrics 24 (5): 833–47. https://doi.org/10.1002/jae.1083.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and Life. Anchor Books.\n\n\nLatour, Bruno. 1996. “On Actor-Network Theory: A Few Clarifications.” Soziale Welt 47 (4): 369–81. http://www.jstor.org/stable/40878163.\n\n\nLight, Richard, Judith Singer, and John Willett. 1990. By Design: Planning Research on Higher Education. 1st ed. Cambridge: Harvard University Press.\n\n\nLittle, Roderick, and Roger Lewis. 2021. “Estimands, Estimators, and Estimates.” JAMA 326 (10): 967. https://doi.org/10.1001/jama.2021.2886.\n\n\nLucas, Robert. 1978. “Asset Prices in an Exchange Economy.” Econometrica 46 (6): 1429–45. https://doi.org/10.2307/1913837.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 86 (3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus; Giroux.\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with Uncongenial Sources of Input.” Statistical Science 9 (4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\n———. 2012. “You Want Me to Analyze Data i Don’t Have? Are You Insane?” Shanghai Archives of Psychiatry 24 (5): 297–301. https://doi.org/10.3969/j.issn.1002-0829.2012.05.011.\n\n\n———. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nMok, Lillio, Samuel Way, Lucas Maystre, and Ashton Anderson. 2022. “The Dynamics of Exploration on Spotify.” In Proceedings of the International AAAI Conference on Web and Social Media, 16:663–74. https://doi.org/10.1609/icwsm.v16i1.19324.\n\n\nOrwell, George. 1946. Politics and the English Language. https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. “Improving Reproducibility in Machine Learning Research (a Report from the NeurIPS 2019 Reproducibility Program).” Journal of Machine Learning Research 22 (164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nRosenau, James N. 1999. “A Transformed Observer in a Transforming World.” Studia Diplomatica 52 (1/2): 5–14. http://www.jstor.org/stable/44838096.\n\n\nSamuel, Arthur. 1959. “Some Studies in Machine Learning Using the Game of Checkers.” IBM Journal of Research and Development 3 (3): 210–29. https://doi.org/10.1147/rd.33.0210.\n\n\nSavage, Van, and Pamela Yeh. 2019. “Novelist Cormac McCarthy’s Tips on How to Write a Great Science Paper.” Nature 574 (7778): 441–42. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nSen, Amartya. 1980. “Description as Choice.” Oxford Economic Papers 32 (3): 353–69. https://doi.org/10.1093/oxfordjournals.oep.a041484.\n\n\nSides, John, Lynn Vavreck, and Christopher Warshaw. 2021. “The Effect of Television Advertising in United States Elections.” American Political Science Review, 1–17. https://doi.org/10.1017/s000305542100112x.\n\n\nSmith, Jessie, Saleema Amershi, Solon Barocas, Hanna Wallach, and Jennifer Wortman Vaughan. 2022. “REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research.” 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22). https://doi.org/10.1145/3531146.3533122.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nThe Economist. 2013. “Johnson: Those Six Little Rules: George Orwell on Writing,” July. https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules.\n\n\nTolley, Erin, and Mireille Paquet. 2021. “Gender, Municipal Party Politics, and Montreal’s First Woman Mayor.” Canadian Journal of Urban Research 30 (1): 40–52. https://cjur.uwinnipeg.ca/index.php/cjur/article/view/323.\n\n\nWardrop, Robert. 1995. “Simpson’s Paradox and the Hot Hand in Basketball.” The American Statistician 49 (1): 24–28. https://doi.org/10.2307/2684806.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nZinsser, William. 1976. On Writing Well. New York: HarperCollins."
  },
  {
    "objectID": "04-writing_research.html#footnotes",
    "href": "04-writing_research.html#footnotes",
    "title": "4  Writing research",
    "section": "",
    "text": "While there is sometimes a need for a separate literature review section, another approach is to discuss relevant literature throughout the paper as appropriate. For instance, when there is literature relevant to the data then it should be discussed in this section, while literature relevant to the model, results, or discussion should be mentioned as appropriate in those sections.↩︎"
  },
  {
    "objectID": "05-static_communication.html#introduction",
    "href": "05-static_communication.html#introduction",
    "title": "5  Static communication",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nWhen telling stories with data, we would like the data to do much of the work of convincing our reader. The paper is the medium, and the data are the message. To that end, we want to show our reader the data that allowed us to come to our understanding of the story. We use graphs, tables, and maps to help achieve this.\nTry to show the observations that underpin our analysis. For instance, if your dataset consists of 2,500 responses to a survey, then at some point in the paper you should have a plot/s that contains each of the 2,500 observations, for every variable of interest. To do this we build graphs using ggplot2 which is part of the core tidyverse and so does not have to be installed or loaded separately. In this chapter we go through a variety of different options including bar charts, scatterplots, line plots, and histograms.\nIn contrast to the role of graphs, which is to show each observation, the role of tables is typically to show an extract of the dataset or to convey various summary statistics, or regression results. We will build tables primarily using knitr. Later we will use modelsummary to build tables related to regression output.\nFinally, we cover maps as a variant of graphs that are used to show a particular type of data. We will build static maps using ggmap after having obtained geocoded data using tidygeocoder."
  },
  {
    "objectID": "05-static_communication.html#graphs",
    "href": "05-static_communication.html#graphs",
    "title": "5  Static communication",
    "section": "5.2 Graphs",
    "text": "5.2 Graphs\n\nA world turning to a saner and richer civilization will be a world turning to charts.\nKarsten (1923, 684)\n\nGraphs are a critical aspect of compelling data stories. They allow us to see both broad patterns and details (Cleveland [1985] 1994, 5). Graphs enable a familiarity with our data that is hard to get from any other method. Every variable of interest should be graphed.\nThe most important objective of a graph is to convey as much of the actual data, and its context, as possible. In a way, graphing is an information encoding process where we construct a deliberate representation to convey information to our audience. The audience must decode that representation. The success of our graph depends on how much information is lost in this process so the decoding is a critical aspect (Cleveland [1985] 1994, 221). This means that we must focus on creating effective graphs that are suitable for our specific audience.\nTo see why graphing the actual data is important, after installing and loading datasauRus consider the datasaurus_dozen dataset.\n\ndatasaurus_dozen\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# ℹ 1,836 more rows\n\n\nThe dataset consists of values for “x” and “y”, which should be plotted on the x-axis and y-axis, respectively. There are 13 different values in the variable “dataset” including: “dino”, “star”, “away”, and “bullseye”. We focus on those four and generate summary statistics for each (Table 5.1).\n\n# Based on: https://juliasilge.com/blog/datasaurus-multiclass/\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  summarise(across(c(x, y), list(mean = mean, sd = sd)),\n            .by = dataset) |&gt;\n  kable(col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n        booktabs = TRUE, digits = 1)\n\n\n\nTable 5.1: Mean and standard deviation for four datasauRus datasets\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\ndino\n54.3\n16.8\n47.8\n26.9\n\n\naway\n54.3\n16.8\n47.8\n26.9\n\n\nstar\n54.3\n16.8\n47.8\n26.9\n\n\nbullseye\n54.3\n16.8\n47.8\n26.9\n\n\n\n\n\n\nNotice that the summary statistics are similar (Table 5.1). Despite this it turns out that the different datasets are actually very different beasts. This becomes clear when we plot the data (Figure 5.1).\n\ndatasaurus_dozen |&gt;\n  filter(dataset %in% c(\"dino\", \"star\", \"away\", \"bullseye\")) |&gt;\n  ggplot(aes(x = x, y = y, colour = dataset)) +\n  geom_point() +\n  theme_minimal() +\n  facet_wrap(vars(dataset), nrow = 2, ncol = 2) +\n  labs(color = \"Dataset\")\n\n\n\n\nFigure 5.1: Graph of four datasauRus datasets\n\n\n\n\nWe get a similar lesson—always plot your data—from “Anscombe’s Quartet”, created by the twentieth century statistician Frank Anscombe. The key takeaway is that it is important to plot the actual data and not rely solely on summary statistics.\n\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n\n\nAnscombe’s Quartet consists of eleven observations for four different datasets, with x and y values for each observation. We need to manipulate this dataset with pivot_longer() to get it into the “tidy” format discussed in Online Appendix A.\n\n# From: https://www.njtierney.com/post/2020/06/01/tidy-anscombe/\n# And the pivot_longer() vignette.\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(\n    everything(),\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )\n\nWe can first create summary statistics (Table 5.2) and then plot the data (Figure 5.2). This again illustrates the importance of graphing the actual data, rather than relying on summary statistics.\n\ntidy_anscombe |&gt;\n  summarise(\n    across(c(x, y), list(mean = mean, sd = sd)),\n    .by = set\n    ) |&gt;\n  kable(\n    col.names = c(\"Dataset\", \"x mean\", \"x sd\", \"y mean\", \"y sd\"),\n    digits = 1, booktabs = TRUE\n  )\n\n\n\nTable 5.2: Mean and standard deviation for Anscombe’s quartet\n\n\nDataset\nx mean\nx sd\ny mean\ny sd\n\n\n\n\n1\n9\n3.3\n7.5\n2\n\n\n2\n9\n3.3\n7.5\n2\n\n\n3\n9\n3.3\n7.5\n2\n\n\n4\n9\n3.3\n7.5\n2\n\n\n\n\n\n\n\ntidy_anscombe |&gt;\n  ggplot(aes(x = x, y = y, colour = set)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  facet_wrap(vars(set), nrow = 2, ncol = 2) +\n  labs(colour = \"Dataset\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5.2: Recreation of Anscombe’s Quartet\n\n\n\n\n\n5.2.1 Bar charts\nWe typically use a bar chart when we have a categorical variable that we want to focus on. We saw an example of this in Chapter 2 when we constructed a graph of the number of occupied beds. The geometric object—a “geom”—that we primarily use is geom_bar(), but there are many variants to cater for specific situations. To illustrate the use of bar charts, we use a dataset from the 1997-2001 British Election Panel Study that was put together by Fox and Andersen (2006) and made available with BEPS, after installing and loading carData.\n\nbeps &lt;- \n  BEPS |&gt; \n  as_tibble() |&gt; \n  clean_names() |&gt; \n  select(age, vote, gender, political_knowledge)\n\nThe dataset consists of which party the respondent supports, along with various demographic, economic, and political variables. In particular, we have the age of the respondent. We begin by creating age-groups from the ages, and making a bar chart showing the frequency of each age-group using geom_bar() (Figure 5.3 (a)).\n\nbeps &lt;-\n  beps |&gt;\n  mutate(\n    age_group =\n      case_when(\n        age &lt; 35 ~ \"&lt;35\",\n        age &lt; 50 ~ \"35-49\",\n        age &lt; 65 ~ \"50-64\",\n        age &lt; 80 ~ \"65-79\",\n        age &lt; 100 ~ \"80-99\"\n      ),\n    age_group = \n      factor(age_group, levels = c(\"&lt;35\", \"35-49\", \"50-64\", \"65-79\", \"80-99\"))\n  )\n\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\nbeps |&gt; \n  count(age_group) |&gt; \n  ggplot(mapping = aes(x = age_group, y = n)) +\n  geom_col() +\n  theme_minimal() +\n  labs(x = \"Age group\", y = \"Number of observations\")\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n(b) Using count() and geom_col()\n\n\n\n\nFigure 5.3: Distribution of age-groups in the 1997-2001 British Election Panel Study\n\n\n\nThe default axis label used by ggplot2 is the name of the relevant variable, so it is often useful to add more detail. We do this using labs() by specifying a variable and a name. In the case of Figure 5.3 (a) we have specified labels for the x-axis and y-axis.\nBy default, geom_bar() creates a count of the number of times each age-group appears in the dataset. It does this because the default statistical transformation—a “stat”—for geom_bar() is “count”, which saves us from having to create that statistic ourselves. But if we had already constructed a count (for instance, with beps |&gt; count(age_group)), then we could specify a variable for the y-axis and then use geom_col() (Figure 5.3 (b)).\nWe may also like to consider various groupings of the data to get a different insight. For instance, we can use color to look at which party the respondent supports, by age-group (Figure 5.4 (a)).\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar(position = \"dodge2\") +\n  labs(x = \"Age group\", y = \"Number of observations\", fill = \"Vote\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) Using geom_bar()\n\n\n\n\n\n\n\n(b) Using geom_bar() with dodge2\n\n\n\n\nFigure 5.4: Distribution of age-group, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\nBy default, these different groups are stacked, but they can be placed side by side with position = \"dodge2\" (Figure 5.4 (b)). (Using “dodge2” rather than “dodge” adds a little space between the bars.)\n\n5.2.1.1 Themes\nAt this point, we may like to address the general look of the graph. There are various themes that are built into ggplot2. These include: theme_bw(), theme_classic(), theme_dark(), and theme_minimal(). A full list is available in the ggplot2 cheat sheet. We can use these themes by adding them as a layer (Figure 5.5). We could also install more themes from other packages, including ggthemes (Arnold 2021), and hrbrthemes (Rudis 2020). We could even build our own!\n\ntheme_bw &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_bw()\n\ntheme_classic &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\ntheme_dark &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_dark()\n\ntheme_minimal &lt;-\n  beps |&gt;\n  ggplot(mapping = aes(x = age_group)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()\n\n(theme_bw + theme_classic) / (theme_dark + theme_minimal)\n\n\n\n\nFigure 5.5: Distribution of age-groups, and vote preference, in the 1997-2001 British Election Panel Study, illustrating different themes and the use of patchwork\n\n\n\n\nIn Figure 5.5 we use patchwork to bring together multiple graphs. To do this, after installing and loading the package, we assign the graph to a variable. We then use “+” to signal which should be next to each other, “/” to signal which should be on top, and use brackets to indicate precedence\n\n\n5.2.1.2 Facets\nWe use facets to show variation, based on one or more variables (Wilkinson 2005, 219). Facets are especially useful when we have already used color to highlight variation in some other variable. For instance, we may be interested to explain vote, by age and gender (Figure 5.6). We rotate the x-axis with guides(x = guide_axis(angle = 90)) to avoid overlapping. We also change the position of the legend with theme(legend.position = \"bottom\").\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5.6: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nWe could change facet_wrap() to wrap vertically instead of horizontally with dir = \"v\". Alternatively, we could specify a few rows, say nrow = 2, or a number of columns, say ncol = 2.\nBy default, both facets will have the same x-axis and y-axis. We could enable both facets to have different scales with scales = \"free\", or just the x-axis with scales = \"free_x\", or just the y-axis with scales = \"free_y\" (Figure 5.7).\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Gender\"\n  ) +\n  facet_wrap(vars(vote), scales = \"free\") +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5.7: Distribution of age-group by gender, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nFinally, we can change the labels of the facets using labeller() (Figure 5.8).\n\nnew_labels &lt;- \n  c(\"0\" = \"No knowledge\", \"1\" = \"Low knowledge\",\n    \"2\" = \"Moderate knowledge\", \"3\" = \"High knowledge\")\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(\n    vars(political_knowledge),\n    scales = \"free\",\n    labeller = labeller(political_knowledge = new_labels)\n  ) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5.8: Distribution of age-group by political knowledge, and vote preference, in the 1997-2001 British Election Panel Study\n\n\n\n\nWe now have three ways to combine multiple graphs: sub-figures, facets, and patchwork. They are useful in different circumstances:\n\nsub-figures—which we covered in Chapter 3—for when we are considering different variables;\nfacets for when we are considering a categorical variable; and\npatchwork for when we are interested in bringing together entirely different graphs.\n\n\n\n5.2.1.3 Colors\nWe now turn to the colors used in the graph. There are a variety of different ways to change the colors. The many palettes available from RColorBrewer (Neuwirth 2022) can be specified using scale_fill_brewer(). In the case of viridis (Garnier et al. 2021) we can specify the palettes using scale_fill_viridis_d(). Additionally, viridis is particularly focused on color-blind palettes (Figure 5.9). Neither RColorBrewer nor viridis need to be explicitly installed or loaded because ggplot2, which is part of the tidyverse, takes care of that for us.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nThe name of the “brewer” palette refers to Cindy Brewer (Miller 2014). After earning a PhD in Geography from Michigan State University in 1991, she joined San Diego State University as an assistant professor, moving to Pennsylvania State University in 1994, where she was promoted to full professor in 2007. One of her best-known books is Designing Better Maps: A Guide for GIS Users (Brewer 2015). In 2019 she became only the ninth person to have been awarded the O. M. Miller Cartographic Medal since it was established in 1968.\n\n\n\n# Panel (a)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Blues\")\n\n# Panel (b)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n# Panel (c)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d()\n\n# Panel (d)\nbeps |&gt;\n  ggplot(mapping = aes(x = age_group, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age-group\", y = \"Number\", fill = \"Voted for\") +\n  theme(legend.position = \"bottom\") +\n  scale_fill_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\nFigure 5.9: Distribution of age-group and vote preference, in the 1997-2001 British Election Panel Study, illustrating different colors\n\n\n\nIn addition to using pre-built palettes, we could build our own palette. That said, color is something to be considered with care. It should be used to increase the amount of information that is communicated (Cleveland [1985] 1994). Color should not be added to graphs unnecessarily—that is to say, it should play some role. Typically, that role is to distinguish different groups, which implies making the colors dissimilar. Color may also be appropriate if there is some relationship between the color and the variable. For instance, if making a graph of the price of mangoes and raspberries, then it could help the reader decode the information if the colors were yellow and red, respectively (Franconeri et al. 2021, 121).\n\n\n\n5.2.2 Scatterplots\nWe are often interested in the relationship between two numeric or continuous variables. We can use scatterplots to show this. A scatterplot may not always be the best choice, but it is rarely a bad one (Weissgerber et al. 2015). Some consider it the most versatile and useful graph option (Friendly and Wainer 2021, 121). To illustrate scatterplots, we install and load WDI and then use that to download some economic indicators from the World Bank. In particular, we use WDIsearch() to find the unique key that we need to pass to WDI() to facilitate the download.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nFrom OECD (2014, 15) Gross Domestic Product (GDP) “combines in a single figure, and with no double counting, all the output (or production) carried out by all the firms, non-profit institutions, government bodies and households in a given country during a given period, regardless of the type of goods and services produced, provided that the production takes place within the country’s economic territory.” The modern concept was developed by the twentieth century economist Simon Kuznets and is widely used and reported. There is a certain comfort in having a definitive and concrete single number to describe something as complicated as the economic activity of a country. It is useful and informative that we have such summary statistics. But as with any summary statistic, its strength is also its weakness. A single number necessarily loses information about constituent components, and disaggregated differences can be important (Moyer and Dunn 2020). It highlights short term economic progress over longer term improvements. And “the quantitative definiteness of the estimates makes it easy to forget their dependence upon imperfect data and the consequently wide margins of possible error to which both totals and components are liable” (Kuznets, Epstein, and Jenks 1941, xxvi). Summary measures of economic performance shows only one side of a country’s economy. While there are many strengths there are also well-known areas where GDP is weak.\n\n\n\nWDIsearch(\"gdp growth\")\nWDIsearch(\"inflation\")\nWDIsearch(\"population, total\")\nWDIsearch(\"Unemployment, total\")\n\n\nworld_bank_data &lt;-\n  WDI(\n    indicator =\n      c(\"FP.CPI.TOTL.ZG\", \"NY.GDP.MKTP.KD.ZG\", \"SP.POP.TOTL\",\"SL.UEM.TOTL.NE.ZS\"),\n    country = c(\"AU\", \"ET\", \"IN\", \"US\")\n  )\n\nWe may like to change the variable names to be more meaningful, and only keep those that we need.\n\nworld_bank_data &lt;-\n  world_bank_data |&gt;\n  rename(\n    inflation = FP.CPI.TOTL.ZG,\n    gdp_growth = NY.GDP.MKTP.KD.ZG,\n    population = SP.POP.TOTL,\n    unem_rate = SL.UEM.TOTL.NE.ZS\n  ) |&gt;\n  select(country, year, inflation, gdp_growth, population, unem_rate)\n\nhead(world_bank_data)\n\n# A tibble: 6 × 6\n  country    year inflation gdp_growth population unem_rate\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Australia  1960     3.73       NA      10276477        NA\n2 Australia  1961     2.29        2.48   10483000        NA\n3 Australia  1962    -0.319       1.29   10742000        NA\n4 Australia  1963     0.641       6.22   10950000        NA\n5 Australia  1964     2.87        6.98   11167000        NA\n6 Australia  1965     3.41        5.98   11388000        NA\n\n\nTo get started we can use geom_point() to make a scatterplot showing GDP growth and inflation, by country (Figure 5.10 (a)).\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point()\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n(a) Default settings\n\n\n\n\n\n\n\n(b) With the addition of a theme and labels\n\n\n\n\nFigure 5.10: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nAs with bar charts, we can change the theme, and update the labels (Figure 5.10 (b)).\nFor scatterplots we use “color” instead of “fill”, as we did for bar charts, because they use dots rather than bars. This also then slightly affects how we change the palette (Figure 5.11). That said, with particular types of dots, for instance shape = 21, it is possible to have both fill and color aesthetics.\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Blues\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d()\n\n# Panel (d)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"GDP growth\",  y = \"Inflation\", color = \"Country\") +\n  theme(legend.position = \"bottom\") +\n  scale_colour_viridis_d(option = \"magma\")\n\n\n\n\n\n\n\n(a) Brewer palette ‘Blues’\n\n\n\n\n\n\n\n(b) Brewer palette ‘Set1’\n\n\n\n\n\n\n\n\n\n(c) Viridis palette default\n\n\n\n\n\n\n\n(d) Viridis palette ‘magma’\n\n\n\n\nFigure 5.11: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nThe points of a scatterplot sometimes overlap. We can address this situation in a variety of ways (Figure 5.12):\n\nAdding a degree of transparency to our dots with “alpha” (Figure 5.12 (a)). The value for “alpha” can vary between 0, which is fully transparent, and 1, which is completely opaque.\nAdding a small amount of noise, which slightly moves the points, using geom_jitter() (Figure 5.12 (b)). By default, the movement is uniform in both directions, but we can specify which direction movement occurs with “width” or “height”. The decision between these two options turns on the degree to which accuracy matters, and the number of points: it is often useful to use geom_jitter() when you want to highlight the relative density of points and not necessarily the exact value of individual points. When using geom_jitter() it is a good idea to set a seed, as introduced in Chapter 2, for reproducibility.\n\n\nset.seed(853)\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country )) +\n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter(width = 1, height = 1) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n(a) Changing the alpha setting\n\n\n\n\n\n\n\n(b) Using jitter\n\n\n\n\nFigure 5.12: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\nWe often use scatterplots to illustrate a relationship between two continuous variables. It can be useful to add a “summary” line using geom_smooth() (Figure 5.13). We can specify the relationship using “method”, change the color with “color”, and add or remove standard errors with “se”. A commonly used “method” is lm, which computes and plots a simple linear regression line similar to using the lm() function. Using geom_smooth() adds a layer to the graph, and so it inherits aesthetics from ggplot(). For instance, that is why we have one line for each country in Figure 5.13 (a) and Figure 5.13 (b). We could overwrite that by specifying a particular color (Figure 5.13 (c)). There are situation where other types of fitted lines such as splines might be preferred.\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth() +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, y = inflation, color = country)) +\n  geom_jitter() +\n  geom_smooth(method = lm, color = \"black\", se = FALSE) +\n  theme_minimal() +\n  labs(x = \"GDP growth\", y = \"Inflation\", color = \"Country\")\n\n\n\n\n\n\n\n(a) Default line of best fit\n\n\n\n\n\n\n\n(b) Specifying a linear relationship\n\n\n\n\n\n\n\n\n\n(c) Specifying only one color\n\n\n\n\nFigure 5.13: Relationship between inflation and GDP growth for Australia, Ethiopia, India, and the United States\n\n\n\n\n\n5.2.3 Line plots\nWe can use a line plot when we have variables that should be joined together, for instance, an economic time series. We will continue with the dataset from the World Bank and focus on GDP growth in the United States using geom_line() (Figure 5.14 (a)). The source of the data can be added to the graph using “caption” within labs().\n\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = year, y = gdp_growth)) +\n  geom_step() +\n  theme_minimal() +\n  labs(x = \"Year\",y = \"GDP growth\", caption = \"Data source: World Bank.\")\n\n\n\n\n\n\n\n(a) Using a line plot\n\n\n\n\n\n\n\n(b) Using a stairstep line plot\n\n\n\n\nFigure 5.14: United States GDP growth (1961-2020)\n\n\n\nWe can use geom_step(), a slight variant of geom_line(), to focus attention on the change from year to year (Figure 5.14 (b)).\nThe Phillips curve is the name given to plot of the relationship between unemployment and inflation over time. An inverse relationship is sometimes found in the data, for instance in the United Kingdom between 1861 and 1957 (Phillips 1958). We have a variety of ways to investigate this relationship in our data, including:\n\nAdding a second line to our graph. For instance, we could add inflation (Figure 5.15 (a)). This requires us to use pivot_longer(), which is discussed in Online Appendix A, to ensure that the data are in a tidy format.\nUsing geom_path() to link values in the order they appear in the dataset. In Figure 5.15 (b) we show a Phillips curve for the United States between 1960 and 2020. Figure 5.15 (b) does not appear to show any clear relationship between unemployment and inflation.\n\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  select(-population, -gdp_growth) |&gt;\n  pivot_longer(\n    cols = c(\"inflation\", \"unem_rate\"),\n    names_to = \"series\",\n    values_to = \"value\"\n  ) |&gt;\n  ggplot(mapping = aes(x = year, y = value, color = series)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    x = \"Year\", y = \"Value\", color = \"Economic indicator\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Inflation\", \"Unemployment\")) +\n  theme(legend.position = \"bottom\")\n\nworld_bank_data |&gt;\n  filter(country == \"United States\") |&gt;\n  ggplot(mapping = aes(x = unem_rate, y = inflation)) +\n  geom_path() +\n  theme_minimal() +\n  labs(\n    x = \"Unemployment rate\", y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\n\n\n\n(a) Comparing the two time series over time\n\n\n\n\n\n\n\n(b) Plotting the two time series against each other\n\n\n\n\nFigure 5.15: Unemployment and inflation for the United States (1960-2020)\n\n\n\n\n\n5.2.4 Histograms\nA histogram is useful to show the shape of the distribution of a continuous variable. The full range of the data values is split into intervals called “bins” and the histogram counts how many observations fall into which bin. In Figure 5.16 we examine the distribution of GDP in Ethiopia.\n\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\nFigure 5.16: Distribution of GDP growth in Ethiopia (1960-2020)\n\n\n\n\nThe key component that determines the shape of a histogram is the number of bins. This can be specified in one of two ways (Figure 5.17):\n\nspecifying the number of “bins” to include; or\nspecifying their “binwidth”.\n\n\n# Panel (a)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (b)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(bins = 20) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 2) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n# Panel (d)\nworld_bank_data |&gt;\n  filter(country == \"Ethiopia\") |&gt;\n  ggplot(aes(x = gdp_growth)) +\n  geom_histogram(binwidth = 5) +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\",\n    y = \"Number of occurrences\"\n  )\n\n\n\n\n\n\n\n(a) Five bins\n\n\n\n\n\n\n\n(b) 20 bins\n\n\n\n\n\n\n\n\n\n(c) Binwidth of two\n\n\n\n\n\n\n\n(d) Binwidth of five\n\n\n\n\nFigure 5.17: Distribution of GDP growth in Ethiopia (1960-2020)\n\n\n\nHistograms can be thought of as locally averaging data, and the number of bins affects how much of this occurs. When there are only two bins then there is a lot of smoothing, but we lose a lot of accuracy. Too few bins results in more bias, while too many bins results in more variance (Wasserman 2005, 303). Our decision as to the number of bins, or their width, is concerned with trying to balance bias and variance. This will depend on a variety of concerns including the subject matter and the goal (Cleveland [1985] 1994, 135). This is one of the reasons that Denby and Mallows (2009) consider histograms to be especially valuable as exploratory tools.\nFinally, while we can use “fill” to distinguish between different types of observations, it can get quite messy. It is usually better to:\n\ntrace the outline of the distribution with geom_freqpoly() (Figure 5.18 (a))\nbuild stack of dots with geom_dotplot() (Figure 5.18 (b)); or\nadd transparency, especially if the differences are more stark (Figure 5.18 (c)).\n\n\n# Panel (a)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, color = country)) +\n  geom_freqpoly() +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (b)\nworld_bank_data |&gt;\n  ggplot(aes(x = gdp_growth, group = country, fill = country)) +\n  geom_dotplot(method = \"histodot\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n# Panel (c)\nworld_bank_data |&gt;\n  filter(country %in% c(\"India\", \"United States\")) |&gt;\n  ggplot(mapping = aes(x = gdp_growth, fill = country)) +\n  geom_histogram(alpha = 0.5, position = \"identity\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Number of occurrences\",\n    fill = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n(a) Tracing the outline\n\n\n\n\n\n\n\n(b) Using dots\n\n\n\n\n\n\n\n\n\n(c) Adding transparency\n\n\n\n\nFigure 5.18: Distribution of GDP growth across various countries (1960-2020)\n\n\n\nAn interesting alternative to a histogram is the empirical cumulative distribution function (ECDF). The choice between this and a histogram is tends to be audience-specific. It may not appropriate for less-sophisticated audiences, but if the audience is quantitatively comfortable, then it can be a great choice because it does less smoothing than a histogram. We can build an ECDF with stat_ecdf(). For instance, Figure 5.19 shows an ECDF equivalent to Figure 5.16.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = gdp_growth, color = country)) +\n  stat_ecdf(geom = \"point\") +\n  theme_minimal() +\n  labs(\n    x = \"GDP growth\", y = \"Proportion\", color = \"Country\",\n    caption = \"Data source: World Bank.\"\n  ) + \n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 5.19: Distribution of GDP growth in four countries (1960-2020)\n\n\n\n\n\n\n5.2.5 Boxplots\nA boxplot typically shows five aspects: 1) the median, 2) the 25th, and 3) 75th percentiles. The fourth and fifth elements differ depending on specifics. One option is the minimum and maximum values. Another option is to determine the difference between the 75th and 25th percentiles, which is the interquartile range. The fourth and fifth elements are then \\(1.5\\times\\mbox{IQR}\\) from the 25th and 75th percentiles. That latter approach is used, by default, in geom_boxplot from ggplot2. Spear (1952, 166) introduced the notion of a chart that focused on the range and various summary statistics including the median and the range, while Tukey (1977) focused on which summary statistics and popularized it (Wickham and Stryjewski 2011).\nOne reason for using graphs is that they help us understand and embrace how complex our data are, rather than trying to hide and smooth it away (Armstrong 2022). One appropriate use case for boxplots is to compare the summary statistics of many variables at once, such as in Bethlehem et al. (2022). But boxplots alone are rarely the best choice because they hide the distribution of data, rather than show it. The same boxplot can apply to very different distributions. To see this, consider some simulated data from the beta distribution of two types. The first contains draws from two beta distributions: one that is right skewed and another that is left skewed. The second contains draws from a beta distribution with no skew, noting that \\(\\mbox{Beta}(1, 1)\\) is equivalent to \\(\\mbox{Uniform}(0, 1)\\).\n\nset.seed(853)\n\nnumber_of_draws &lt;- 10000\n\nboth_left_and_right_skew &lt;-\n  c(\n    rbeta(number_of_draws / 2, 5, 2),\n    rbeta(number_of_draws / 2, 2, 5)\n  )\n\nno_skew &lt;-\n  rbeta(number_of_draws, 1, 1)\n\nbeta_distributions &lt;-\n  tibble(\n    observation = c(both_left_and_right_skew, no_skew),\n    source = c(\n      rep(\"Left and right skew\", number_of_draws),\n      rep(\"No skew\", number_of_draws)\n    )\n  )\n\nWe can first compare the boxplots of the two series (Figure 5.20 (a)). But if we plot the actual data then we can see how different they are (Figure 5.20 (b)).\n\nbeta_distributions |&gt;\n  ggplot(aes(x = source, y = observation)) +\n  geom_boxplot() +\n  theme_classic()\n\nbeta_distributions |&gt;\n  ggplot(aes(x = observation, color = source)) +\n  geom_freqpoly(binwidth = 0.05) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) Illustrated with a boxplot\n\n\n\n\n\n\n\n(b) Actual data\n\n\n\n\nFigure 5.20: Data drawn from beta distributions with different parameters\n\n\n\nOne way forward, if a boxplot is to be used, is to include the actual data as a layer on top of the boxplot. For instance, in Figure 5.21 we show the distribution of inflation across the four countries. The reason that this works well is that it shows the actual observations, as well as the summary statistics.\n\nworld_bank_data |&gt;\n  ggplot(mapping = aes(x = country, y = inflation)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.3, width = 0.15, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Country\",\n    y = \"Inflation\",\n    caption = \"Data source: World Bank.\"\n  )\n\n\n\n\nFigure 5.21: Distribution of inflation data for four countries (1960-2020)"
  },
  {
    "objectID": "05-static_communication.html#tables",
    "href": "05-static_communication.html#tables",
    "title": "5  Static communication",
    "section": "5.3 Tables",
    "text": "5.3 Tables\nTables are an important part of telling a compelling story. Tables can communicate less information than a graph, but they do so at a high fidelity. They are especially useful to highlight a few specific values (Andersen and Armstrong 2021). In this book, we primarily use tables in three ways:\n\nTo show an extract of the dataset.\nTo communicate summary statistics.\nTo display regression results.\n\n\n5.3.1 Showing part of a dataset\nWe illustrate showing part of a dataset using kable() from knitr. We use the World Bank dataset that we downloaded earlier and focus on inflation, GDP growth, and population as unemployment data are not available for every year for every country.\n\nworld_bank_data &lt;- \n  world_bank_data |&gt; \n  select(-unem_rate)\n\nTo begin, after installing and loading knitr, we can display the first ten rows with the default kable() settings.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable()\n\n\n\n\ncountry\nyear\ninflation\ngdp_growth\npopulation\n\n\n\n\nAustralia\n1960\n3.7288136\nNA\n10276477\n\n\nAustralia\n1961\n2.2875817\n2.482656\n10483000\n\n\nAustralia\n1962\n-0.3194888\n1.294611\n10742000\n\n\nAustralia\n1963\n0.6410256\n6.216107\n10950000\n\n\nAustralia\n1964\n2.8662420\n6.980061\n11167000\n\n\nAustralia\n1965\n3.4055728\n5.980438\n11388000\n\n\nAustralia\n1966\n3.2934132\n2.379040\n11651000\n\n\nAustralia\n1967\n3.4782609\n6.304945\n11799000\n\n\nAustralia\n1968\n2.5210084\n5.094034\n12009000\n\n\nAustralia\n1969\n3.2786885\n7.045584\n12263000\n\n\n\n\n\nTo be able to cross-reference a table in the text, we need to add a table caption and label to the R chunk as shown in Section 3.2.6 of Chapter 3. We can also make the column names more informative with “col.names” and specify the number of digits to be displayed (Table 5.3).\n\n```{r}\n#| label: tbl-gdpfirst\n#| message: false\n#| tbl-cap: \"A dataset of economic indicators for four countries\"\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1\n  )\n```\n\n\n\nTable 5.3: A dataset of economic indicators for four countries\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000\n\n\n\n\n\n\n\n\n5.3.2 Improving the formatting\nWhen producing PDFs, the “booktabs” option makes a host of small changes to the default display and results in tables that look better (Table 5.4). (This should not have an effect for HTML output.) By default a small space will be added every five lines. We can additionally specify “linesep” to stop that.\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\"\n  )\n\n\n\nTable 5.4: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10276477\n\n\nAustralia\n1961\n2.3\n2.5\n10483000\n\n\nAustralia\n1962\n-0.3\n1.3\n10742000\n\n\nAustralia\n1963\n0.6\n6.2\n10950000\n\n\nAustralia\n1964\n2.9\n7.0\n11167000\n\n\nAustralia\n1965\n3.4\n6.0\n11388000\n\n\nAustralia\n1966\n3.3\n2.4\n11651000\n\n\nAustralia\n1967\n3.5\n6.3\n11799000\n\n\nAustralia\n1968\n2.5\n5.1\n12009000\n\n\nAustralia\n1969\n3.3\n7.0\n12263000\n\n\n\n\n\n\nWe can specify the alignment of the columns using a character vector of “l” (left), “c” (center), and “r” (right) (Table 5.5). Additionally, we can change the formatting. For instance, we could specify groupings for numbers that are at least 1,000 using format.args = list(big.mark = \",\").\n\nworld_bank_data |&gt;\n  slice(1:10) |&gt;\n  mutate(year = as.factor(year)) |&gt;\n  kable(\n    col.names = c(\"Country\", \"Year\", \"Inflation\", \"GDP growth\", \"Population\"),\n    digits = 1,\n    booktabs = TRUE,\n    linesep = \"\",\n    align = c(\"l\", \"l\", \"c\", \"c\", \"r\", \"r\"),\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 5.5: First ten rows of a dataset of economic indicators for Australia, Ethiopia, India, and the United States\n\n\nCountry\nYear\nInflation\nGDP growth\nPopulation\n\n\n\n\nAustralia\n1960\n3.7\nNA\n10,276,477\n\n\nAustralia\n1961\n2.3\n2.5\n10,483,000\n\n\nAustralia\n1962\n-0.3\n1.3\n10,742,000\n\n\nAustralia\n1963\n0.6\n6.2\n10,950,000\n\n\nAustralia\n1964\n2.9\n7.0\n11,167,000\n\n\nAustralia\n1965\n3.4\n6.0\n11,388,000\n\n\nAustralia\n1966\n3.3\n2.4\n11,651,000\n\n\nAustralia\n1967\n3.5\n6.3\n11,799,000\n\n\nAustralia\n1968\n2.5\n5.1\n12,009,000\n\n\nAustralia\n1969\n3.3\n7.0\n12,263,000\n\n\n\n\n\n\n\n\n5.3.3 Communicating summary statistics\nAfter installing and loading modelsummary we can use datasummary_skim() to create tables of summary statistics from our dataset.\nWe can use this to get a table such as Table 5.6. That might be useful for exploratory data analysis, which we cover in Chapter 11. (Here we remove population to save space and do not include a histogram of each variable.)\n\nworld_bank_data |&gt;\n  select(-population) |&gt; \n  datasummary_skim(histogram = FALSE)\n\n\n\n\n\nTable 5.6:  Summary of economic indicator variables for four countries \n  \n    \n    \n       \n      Unique (#)\n      Missing (%)\n      Mean\n      SD\n      Min\n      Median\n      Max\n    \n  \n  \n    year\n62\n0\n1990.5\n17.9\n1960.0\n1990.5\n2021.0\n    inflation\n243\n2\n6.1\n6.5\n-9.8\n4.3\n44.4\n    gdp_growth\n224\n10\n4.2\n3.7\n-11.1\n3.9\n13.9\n  \n  \n  \n\n\n\n\n\nBy default, datasummary_skim() summarizes the numeric variables, but we can ask for the categorical variables (Table 5.7). Additionally we can add cross-references in the same way as kable(), that is, include a “tbl-cap” entry and then cross-reference the name of the R chunk.\n\nworld_bank_data |&gt;\n  datasummary_skim(type = \"categorical\")\n\n\n\n\n\nTable 5.7:  Summary of categorical economic indicator variables for four\ncountries \n  \n    \n    \n      country\n      N\n      %\n    \n  \n  \n    Australia\n62\n25.0\n    Ethiopia\n62\n25.0\n    India\n62\n25.0\n    United States\n62\n25.0\n  \n  \n  \n\n\n\n\n\nWe can create a table that shows the correlation between variables using datasummary_correlation() (Table 5.8).\n\nworld_bank_data |&gt;\n  datasummary_correlation()\n\n\n\n\n\nTable 5.8:  Correlation between the economic indicator variables for four\ncountries (Australia, Ethiopia, India, and the United States) \n  \n    \n    \n       \n      year\n      inflation\n      gdp_growth\n      population\n    \n  \n  \n    year\n1\n.\n.\n.\n    inflation\n.03\n1\n.\n.\n    gdp_growth\n.11\n.01\n1\n.\n    population\n.25\n.06\n.16\n1\n  \n  \n  \n\n\n\n\n\nWe typically need a table of descriptive statistics that we could add to our paper (Table 5.9). This contrasts with Table 5.7 which would likely not be included in the main section of a paper, and is more to help us understand the data. We can add a note about the source of the data using notes.\n\ndatasummary_balance(\n  formula = ~country,\n  data = world_bank_data |&gt; \n    filter(country %in% c(\"Australia\", \"Ethiopia\")),\n  dinm = FALSE,\n  notes = \"Data source: World Bank.\"\n)\n\n\n\n\n\nTable 5.9:  Descriptive statistics for the inflation and GDP dataset \n  \n    \n    \n       \n      \n        Australia (N=62)    \n      \n      \n        Ethiopia (N=62)    \n      \n    \n    \n      Mean\n      Std. Dev.\n      Mean \n      Std. Dev. \n    \n  \n  \n    year\n1990.5\n18.0\n1990.5\n18.0\n    inflation\n4.7\n3.8\n9.1\n10.6\n    gdp_growth\n3.4\n1.8\n5.9\n6.4\n    population\n17351313.1\n4407899.0\n57185292.0\n29328845.8\n  \n  \n    \n      Data source: World Bank.\n    \n  \n  \n\n\n\n\n\n\n\n5.3.4 Display regression results\nWe can report regression results using modelsummary() from modelsummary. For instance, we could display the estimates from a few different models (Table 5.10).\n\nfirst_model &lt;- lm(\n  formula = gdp_growth ~ inflation,\n  data = world_bank_data\n)\n\nsecond_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country,\n  data = world_bank_data\n)\n\nthird_model &lt;- lm(\n  formula = gdp_growth ~ inflation + country + population,\n  data = world_bank_data\n)\n\nmodelsummary(list(first_model, second_model, third_model))\n\n\n\n\n\nTable 5.10:  Explaining GDP as a function of inflation \n  \n    \n    \n       \n      (1)\n      (2)\n      (3)\n    \n  \n  \n    (Intercept)\n4.147\n3.676\n3.611\n    \n(0.343)\n(0.484)\n(0.482)\n    inflation\n0.006\n-0.068\n-0.065\n    \n(0.039)\n(0.040)\n(0.039)\n    countryEthiopia\n\n2.896\n2.716\n    \n\n(0.740)\n(0.740)\n    countryIndia\n\n1.916\n-0.730\n    \n\n(0.642)\n(1.465)\n    countryUnited States\n\n-0.436\n-1.145\n    \n\n(0.633)\n(0.722)\n    population\n\n\n0.000\n    \n\n\n(0.000)\n    Num.Obs.\n223\n223\n223\n    R2\n0.000\n0.111\n0.127\n    R2 Adj.\n-0.004\n0.095\n0.107\n    AIC\n1217.7\n1197.5\n1195.4\n    BIC\n1227.9\n1217.9\n1219.3\n    Log.Lik.\n-605.861\n-592.752\n-590.704\n    F\n0.024\n6.806\n\n    RMSE\n3.66\n3.45\n3.42\n  \n  \n  \n\n\n\n\n\nThe number of significant digits can be adjusted with “fmt” (Table 5.11). To help establish credibility you should generally not add as many significant digits as possible (Howes 2022). Instead, you should think carefully about the data-generating process and adjust based on that.\n\nmodelsummary(\n  list(first_model, second_model, third_model),\n  fmt = 1\n)\n\n\n\n\n\nTable 5.11:  Three models of GDP as a function of inflation \n  \n    \n    \n       \n      (1)\n      (2)\n      (3)\n    \n  \n  \n    (Intercept)\n4.1\n3.7\n3.6\n    \n(0.3)\n(0.5)\n(0.5)\n    inflation\n0.0\n-0.1\n-0.1\n    \n(0.0)\n(0.0)\n(0.0)\n    countryEthiopia\n\n2.9\n2.7\n    \n\n(0.7)\n(0.7)\n    countryIndia\n\n1.9\n-0.7\n    \n\n(0.6)\n(1.5)\n    countryUnited States\n\n-0.4\n-1.1\n    \n\n(0.6)\n(0.7)\n    population\n\n\n0.0\n    \n\n\n(0.0)\n    Num.Obs.\n223\n223\n223\n    R2\n0.000\n0.111\n0.127\n    R2 Adj.\n-0.004\n0.095\n0.107\n    AIC\n1217.7\n1197.5\n1195.4\n    BIC\n1227.9\n1217.9\n1219.3\n    Log.Lik.\n-605.861\n-592.752\n-590.704\n    F\n0.024\n6.806\n\n    RMSE\n3.66\n3.45\n3.42"
  },
  {
    "objectID": "05-static_communication.html#maps",
    "href": "05-static_communication.html#maps",
    "title": "5  Static communication",
    "section": "5.4 Maps",
    "text": "5.4 Maps\nIn many ways maps can be thought of as another type of graph, where the x-axis is latitude, the y-axis is longitude, and there is some outline or background image. It is possible that they are the oldest and best understood type of chart (Karsten 1923, 1). We can generate a map in a straight-forward manner. That said, it is not to be taken lightly; things quickly get complicated!\nThe first step is to get some data. There is some geographic data built into ggplot2 that we can access with map_data(). There are additional variables in the world.cities dataset from maps.\n\nfrance &lt;- map_data(map = \"france\")\n\nhead(france)\n\n      long      lat group order region subregion\n1 2.557093 51.09752     1     1   Nord      &lt;NA&gt;\n2 2.579995 51.00298     1     2   Nord      &lt;NA&gt;\n3 2.609101 50.98545     1     3   Nord      &lt;NA&gt;\n4 2.630782 50.95073     1     4   Nord      &lt;NA&gt;\n5 2.625894 50.94116     1     5   Nord      &lt;NA&gt;\n6 2.597699 50.91967     1     6   Nord      &lt;NA&gt;\n\nfrench_cities &lt;-\n  world.cities |&gt;\n  filter(country.etc == \"France\")\n\nhead(french_cities)\n\n             name country.etc    pop   lat long capital\n1       Abbeville      France  26656 50.12 1.83       0\n2         Acheres      France  23219 48.97 2.06       0\n3            Agde      France  23477 43.33 3.46       0\n4            Agen      France  34742 44.20 0.62       0\n5 Aire-sur-la-Lys      France  10470 50.64 2.39       0\n6 Aix-en-Provence      France 148622 43.53 5.44       0\n\n\nUsing that information you can create a map of France that shows the larger cities (Figure 5.22). Use geom_polygon() from ggplot2 to draw shapes by connecting points within groups. And coord_map() adjusts for the fact that we are making a 2D map to represent a world that is 3D.\n\nggplot() +\n  geom_polygon(\n    data = france,\n    aes(x = long, y = lat, group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  coord_map() +\n  geom_point(\n    aes(x = french_cities$long, y = french_cities$lat),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  theme_minimal() +\n  labs(x = \"Longitude\", y = \"Latitude\")\n\n\n\n\nFigure 5.22: Map of France showing the largest cities\n\n\n\n\nAs is often the case with R, there are many ways to get started creating static maps. We have seen how they can be built using only ggplot2, but ggmap brings additional functionality.\nThere are two essential components to a map:\n\na border or background image (sometimes called a tile); and\nsomething of interest within that border, or on top of that tile.\n\nIn ggmap, we use an open-source option for our tile, Stamen Maps. And we use plot points based on latitude and longitude.\n\n5.4.1 Australian polling places\nIn Australia, people have to go to “booths” in order to vote. Because the booths have coordinates (latitude and longitude), we can plot them. One reason we may like to do that is to notice spatial voting patterns.\nTo get started we need to get a tile. We are going to use ggmap to get a tile from Stamen Maps, which builds on OpenStreetMap. The main argument to this function is to specify a bounding box. A bounding box is the coordinates of the edges that you are interested in. This requires two latitudes and two longitudes.\nIt can be useful to use Google Maps, or other mapping platform, to find the coordinate values that you need. In this case we have provided it with coordinates such that it will be centered around Australia’s capital Canberra.\n\nbbox &lt;- c(left = 148.95, bottom = -35.5, right = 149.3, top = -35.1)\n\nOnce you have defined the bounding box, the function get_stamenmap() will get the tiles in that area (Figure 5.23). The number of tiles that it needs depends on the zoom, and the type of tiles that it gets depends on the type of map. We have used “toner-lite”, which is black and white, but there are others including: “terrain”, “toner”, and “toner-lines”. We pass the tiles to ggmap() which will plot it. An internet connection is needed for this to work as get_stamenmap() downloads the tiles.\n\ncanberra_stamen_map &lt;- get_stamenmap(bbox, zoom = 11, maptype = \"toner-lite\")\n\nggmap(canberra_stamen_map)\n\n\n\n\nFigure 5.23: Map of Canberra, Australia\n\n\n\n\nOnce we have a map then we can use ggmap() to plot it. Now we want to get some data that we plot on top of our tiles. We will plot the location of the polling place based on its “division”. This is available from the Australian Electoral Commission (AEC).\n\nbooths &lt;-\n  read_csv(\n    \"https://results.aec.gov.au/24310/Website/Downloads/GeneralPollingPlacesDownload-24310.csv\",\n    skip = 1,\n    guess_max = 10000\n  )\n\nThis dataset is for the whole of Australia, but as we are only plotting the area around Canberra, we will filter the data to only booths with a geography close to Canberra.\n\nbooths_reduced &lt;-\n  booths |&gt;\n  filter(State == \"ACT\") |&gt;\n  select(PollingPlaceID, DivisionNm, Latitude, Longitude) |&gt;\n  filter(!is.na(Longitude)) |&gt; # Remove rows without geography\n  filter(Longitude &lt; 165) # Remove Norfolk Island\n\nNow we can use ggmap in the same way as before to plot our underlying tiles, and then build on that using geom_point() to add our points of interest.\n\nggmap(canberra_stamen_map, extent = \"normal\", maprange = FALSE) +\n  geom_point(data = booths_reduced,\n             aes(x = Longitude, y = Latitude, colour = DivisionNm),\n             alpha = 0.7) +\n  scale_color_brewer(name = \"2019 Division\", palette = \"Set1\") +\n  coord_map(\n    projection = \"mercator\",\n    xlim = c(attr(map, \"bb\")$ll.lon, attr(map, \"bb\")$ur.lon),\n    ylim = c(attr(map, \"bb\")$ll.lat, attr(map, \"bb\")$ur.lat)\n  ) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\nFigure 5.24: Map of Canberra, Australia, with polling places\n\n\n\n\nWe may like to save the map so that we do not have to create it every time, and we can do that in the same way as any other graph, using ggsave().\n\nggsave(\"map.pdf\", width = 20, height = 10, units = \"cm\")\n\nFinally, the reason that we used Stamen Maps and OpenStreetMap is because they are open source, but we could have also used Google Maps. This requires you to first register a credit card with Google, and specify a key, but with low usage the service should be free. Using Google Maps—by using get_googlemap() within ggmap—brings some advantages over get_stamenmap(). For instance it will attempt to find a place name rather than needing to specify a bounding box.\n\n\n5.4.2 United States military bases\nTo see another example of a static map we will plot some United States military bases after installing and loading troopdata. We can access data about United States overseas military bases back to the start of the Cold War using get_basedata().\n\nbases &lt;- get_basedata()\n\nhead(bases)\n\n# A tibble: 6 × 9\n  countryname ccode iso3c basename            lat   lon  base lilypad fundedsite\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   700 AFG   Bagram AB          34.9  69.3     1       0          0\n2 Afghanistan   700 AFG   Kandahar Airfield  31.5  65.8     1       0          0\n3 Afghanistan   700 AFG   Mazar-e-Sharif     36.7  67.2     1       0          0\n4 Afghanistan   700 AFG   Gardez             33.6  69.2     1       0          0\n5 Afghanistan   700 AFG   Kabul              34.5  69.2     1       0          0\n6 Afghanistan   700 AFG   Herat              34.3  62.2     1       0          0\n\n\nWe will look at the locations of United States military bases in Germany, Japan, and Australia. The troopdata dataset already has the latitude and longitude of each base, and we will use that as our item of interest. The first step is to define a bounding box for each country.\n\n# Use: https://data.humdata.org/dataset/bounding-boxes-for-countries\nbbox_germany &lt;- c(left = 5.867, bottom = 45.967, right = 15.033, top = 55.133)\n\nbbox_japan &lt;- c(left = 127, bottom = 30, right = 146, top = 45)\n\nbbox_australia &lt;- c(left = 112.467, bottom = -45, right = 155, top = -9.133)\n\nThen we need to get the tiles using get_stamenmap() from ggmap.\n\ngerman_stamen_map &lt;-get_stamenmap(bbox_germany, zoom = 6, maptype = \"toner-lite\")\n\njapan_stamen_map &lt;- get_stamenmap(bbox_japan, zoom = 6, maptype = \"toner-lite\")\n\naus_stamen_map &lt;- get_stamenmap(bbox_australia, zoom = 5, maptype = \"toner-lite\")\n\nAnd finally, we can bring it all together with maps showing United States military bases in Germany (Figure 5.25 (a)), Japan (Figure 5.25 (b)), and Australia (Figure 5.25 (c)).\n\nggmap(german_stamen_map) +\n  geom_point(data = bases, aes(x = lon, y = lat)) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\nggmap(japan_stamen_map) +\n  geom_point(data = bases, aes(x = lon, y = lat)) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\nggmap(aus_stamen_map) +\n  geom_point(data = bases, aes(x = lon, y = lat)) +\n  labs(x = \"Longitude\",\n       y = \"Latitude\") +\n  theme_minimal()\n\n\n\n\n\n\n\n(a) Germany\n\n\n\n\n\n\n\n(b) Japan\n\n\n\n\n\n\n\n\n\n(c) Australia\n\n\n\n\nFigure 5.25: Map of United States military bases in various parts of the world\n\n\n\n\n\n5.4.3 Geocoding\nSo far we have assumed that we already have geocoded data. This means that we have latitude and longitude coordinates for each place. But sometimes we only have place names, such as “Sydney, Australia”, “Toronto, Canada”, “Accra, Ghana”, and “Guayaquil, Ecuador”. Before we can plot them, we need to get the latitude and longitude coordinates for each case. The process of going from names to coordinates is called geocoding.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nWhile you almost surely know where you live, it can be surprisingly difficult to specifically define the boundaries of many places. And this is made especially difficult when different levels of government have different definitions. Bronner (2021) illustrates this in the case of Atlanta, Georgia, where there are (at least) three official different definitions:\n\nthe metropolitan statistical area;\nthe urbanized area; and\nthe census place.\n\nWhich definition is used can have a substantial effect on the analysis, or even the data that are available, even though they are all “Atlanta”.\n\n\nThere are a range of options to geocode data in R, but tidygeocoder is especially useful. We first need a dataframe of locations.\n\nplace_names &lt;-\n  tibble(\n    city = c(\"Sydney\", \"Toronto\", \"Accra\", \"Guayaquil\"),\n    country = c(\"Australia\", \"Canada\", \"Ghana\", \"Ecuador\")\n  )\n\nplace_names\n\n# A tibble: 4 × 2\n  city      country  \n  &lt;chr&gt;     &lt;chr&gt;    \n1 Sydney    Australia\n2 Toronto   Canada   \n3 Accra     Ghana    \n4 Guayaquil Ecuador  \n\n\n\nplace_names &lt;-\n  geo(\n    city = place_names$city,\n    country = place_names$country,\n    method = \"osm\"\n  )\n\nplace_names\n\n# A tibble: 4 × 4\n  city      country      lat    long\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 Sydney    Australia -33.9  151.   \n2 Toronto   Canada     43.7  -79.4  \n3 Accra     Ghana       5.56  -0.201\n4 Guayaquil Ecuador    -2.19 -79.9  \n\n\nAnd we can now plot and label these cities (Figure 5.26).\n\nworld &lt;- map_data(map = \"world\")\n\nggplot() +\n  geom_polygon(\n    data = world,\n    aes(x = long, y = lat, group = group),\n    fill = \"white\",\n    colour = \"grey\"\n  ) +\n  geom_point(\n    aes(x = place_names$long, y = place_names$lat),\n    color = \"black\") +\n  geom_text(\n    aes(x = place_names$long, y = place_names$lat, label = place_names$city),\n    nudge_y = -5) +\n  theme_minimal() +\n  labs(x = \"Longitude\",\n       y = \"Latitude\")\n\n\n\n\nFigure 5.26: Map of Accra, Sydney, Toronto, and Guayaquil after geocoding to obtain their locations"
  },
  {
    "objectID": "05-static_communication.html#concluding-remarks",
    "href": "05-static_communication.html#concluding-remarks",
    "title": "5  Static communication",
    "section": "5.5 Concluding remarks",
    "text": "5.5 Concluding remarks\nIn this chapter we have covered a lot of ground, focused on communicating data. We spent a lot of time on graphs, because of their ability to convey a large amount of information in an efficient way. We then turned to tables because of how they can specifically convey information. Finally, we discussed maps, which allow us to display geographic information. The most important task is to show the observations to the full extent possible."
  },
  {
    "objectID": "05-static_communication.html#exercises",
    "href": "05-static_communication.html#exercises",
    "title": "5  Static communication",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Three friends—Edward, Hugo, and Lucy—each measure the height of 20 of their friends. Each of the three use a slightly different approach to measurement and so make slightly different errors. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation with every variable independent of each other. Please include three tests based on the simulated data.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched using the data that you simulated.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nAssume the tidyverse and datasauRus are installed and loaded. What would be the outcome of the following code datasaurus_dozen |&gt; filter(dataset == \"v_lines\") |&gt; ggplot(aes(x=x, y=y)) + geom_point()?\n\nFour vertical lines\nFive vertical lines\nThree vertical lines\nTwo vertical lines\n\nWhich theme does not have solid lines along the x and y axes (pick one)?\n\ntheme_minimal()\ntheme_classic()\ntheme_bw()\n\nAssume the tidyverse and the beps dataset as generated in this chapter have been installed and loaded. Which argument should be added to geom_bar() in the following code to make the bars for the different parties be next to each other rather than on top of each other beps |&gt; ggplot(mapping = aes(x = age, fill = vote)) + geom_bar()?\n\nposition = \"side_by_side\"\nposition = \"dodge2\"\nposition = \"adjacent\"\nposition = \"closest\"\n\nIn the code below, what should be added to labs() to change the text of the legend?\n\ncolor = \"Voted for\"\nlegend = \"Voted for\"\nscale = \"Voted for\"\nfill = \"Voted for\"\n\n\n\nbeps |&gt;\n  ggplot(mapping = aes(x = age, fill = vote)) +\n  geom_bar() +\n  theme_minimal() +\n  labs(x = \"Age of respondent\", y = \"Number of respondents\")\n\n\nBased on the help file for scale_colour_brewer() which palette diverges?\n\n“Accent”\n“RdBu”\n“GnBu”\n“Set1”\n\nWhich geom should be used to make a scatter plot?\n\ngeom_smooth()\ngeom_point()\ngeom_bar()\ngeom_dotplot()\n\nWhich of these would result in the largest number of bins?\n\ngeom_histogram(binwidth = 5)\ngeom_histogram(binwidth = 2)\n\nSuppose there is a dataset that contains the heights of 100 birds, each from one of three different species. If we are interested in understanding the distribution of these heights, then in a paragraph or two, please explain which type of graph should be used and why.\nWould this code data |&gt; ggplot(aes(x = col_one)) |&gt; geom_point() if we assume the dataset and columns exist (pick one)?\n\nYes\nNo\n\nWhich of the following, if any, are elements of the layered grammar of graphics of Wickham (2010) (select all that apply)?\n\nA default dataset and set of mappings from variables to aesthetics.\nOne or more layers, with each layer having one geometric object, one statistical transformation, one position adjustment, and optionally, one dataset and set of aesthetic mappings.\nColors that enable the reader to understand the main point.\nA coordinate system.\nThe facet specification.\nOne scale for each aesthetic mapping used.\n\nWhich function from modelsummary could we use to create a table of descriptive statistics?\n\ndatasummary_descriptive()\ndatasummary_skim()\ndatasummary_crosstab()\ndatasummary_balance()\n\n\n\n\nTutorial\nPlease create a graph using ggplot2 and a map using ggmap and add explanatory text to accompany both. Be sure to include cross-references and captions, etc. Each of these should take about pages.\nThen, with regard the graph you created, please reflect on Vanderplas, Cook, and Hofmann (2020). Add a few paragraphs about the different options that you considered to make the graph more effective.\nAnd finally, with regard to the map that you created, please reflect on the following quote from Heather Krause, founder of We All Count: “maps only show people who aren’t invisible to the makers” as well as Chapter 3 from D’Ignazio and Klein (2020) and add a few paragraphs related to this.\nUse Quarto, and include an appropriate title, author, date, and citations. Submit a PDF.\n\n\nPaper\nAt about this point the Mawson Paper from Online Appendix D would be appropriate.\n\n\n\n\nAndersen, Robert, and David Armstrong. 2021. Presenting Statistical Results Effectively. London: Sage.\n\n\nArel-Bundock, Vincent. 2021. WDI: World Development Indicators and Other World Bank Data. https://CRAN.R-project.org/package=WDI.\n\n\n———. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nArmstrong, Zan. 2022. “Stop Aggregating Away the Signal in Your Data.” The Overflow, March. https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/.\n\n\nArnold, Jeffrey. 2021. ggthemes: Extra Themes, Scales and Geoms for “ggplot2”. https://CRAN.R-project.org/package=ggthemes.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, and Alex Deckmyn. 2022. maps: Draw Geographical Maps. https://CRAN.R-project.org/package=maps.\n\n\nBethlehem, R. A. I., J. Seidlitz, S. R. White, J. W. Vogel, K. M. Anderson, C. Adamson, S. Adler, et al. 2022. “Brain Charts for the Human Lifespan.” Nature 604 (7906): 525–33. https://doi.org/10.1038/s41586-022-04554-y.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS Users. 2nd ed.\n\n\nBronner, Laura. 2021. “Quantitative Editing.” YouTube, June. https://youtu.be/LI5m9RzJgWc.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “tidygeocoder: Geocoding Made Easy.” Zenodo. https://doi.org/10.5281/zenodo.3981510.\n\n\nChase, William. 2020. “The Glamour of Graphics.” RStudio Conference, January. https://posit.co/resources/videos/the-glamour-of-graphics/.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data. 2nd ed. New Jersey: Hobart Press.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDenby, Lorraine, and Colin Mallows. 2009. “Variations on the Histogram.” Journal of Computational and Graphical Statistics 18 (1): 21–31. https://doi.org/10.1198/jcgs.2009.0002.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFlynn, Michael. 2022. troopdata: Tools for Analyzing Cross-National Military Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nFox, John, and Robert Andersen. 2006. “Effect Displays for Multinomial and Proportional-Odds Logit Models.” Sociological Methodology 36 (1): 225–55. https://doi.org/10.1111/j.1467-9531.2006.00180.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2022. carData: Companion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData.\n\n\nFranconeri, Steven, Lace Padilla, Priti Shah, Jeffrey Zacks, and Jessica Hullman. 2021. “The Science of Visual Data Communication: What Works.” Psychological Science in the Public Interest 22 (3): 110–61. https://doi.org/10.1177/15291006211051956.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data Visualization and Graphic Communication. 1st ed. Massachusetts: Harvard University Press.\n\n\nFunkhouser, Gray. 1937. “Historical Development of the Graphical Representation of Statistical Data.” Osiris 3: 269–404. https://doi.org/10.1086/368480.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Camargo, Marco Sciaini, and Cédric Scherer. 2021. viridis – Colorblind-Friendly Color Maps for R. https://doi.org/10.5281/zenodo.4679424.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton University Press. https://socviz.co.\n\n\nHowes, Adam. 2022. “Representing Uncertainty Using Significant Figures,” April. https://athowes.github.io/posts/2022-04-24-representing-uncertainty-using-significant-figures/.\n\n\nKahle, David, and Hadley Wickham. 2013. “ggmap: Spatial Visualization with ggplot2.” The R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKarsten, Karl. 1923. Charts and Graphs. New York: Prentice-Hall.\n\n\nKuznets, Simon, Lillian Epstein, and Elizabeth Jenks. 1941. National Income and Its Composition, 1919-1938. National Bureau of Economic Research.\n\n\nMcIlroy, Doug, Ray Brownrigg, Thomas Minka, and Roger Bivand. 2023. mapproj: Map Projections. https://CRAN.R-project.org/package=mapproj.\n\n\nMiller, Greg. 2014. “The Cartographer Who’s Transforming Map Design.” Wired, October. https://www.wired.com/2014/10/cindy-brewer-map-design/.\n\n\nMoyer, Brian, and Abe Dunn. 2020. “Measuring the Gross Domestic Product (GDP): The Ultimate Data Science Project.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.414caadb.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In Understanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPhillips, Alban. 1958. “The Relation Between Unemployment and the Rate of Change of Money Wage Rates in the United Kingdom, 1861-1957.” Economica 25 (100): 283–99. https://doi.org/10.1111/j.1468-0335.1958.tb00003.x.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRudis, Bob. 2020. hrbrthemes: Additional Themes, Theme Components and Utilities for “ggplot2”. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nSpear, Mary Eleanor. 1952. Charting Statistics. https://archive.org/details/ChartingStatistics_201801/.\n\n\nTukey, John. 1977. Exploratory Data Analysis.\n\n\nVanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing Statistical Charts: What Makes a Good Graph?” Annual Review of Statistics and Its Application 7: 61–88. https://doi.org/10.1146/annurev-statistics-031219-041252.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWeissgerber, Tracey, Natasa Milic, Stacey Winham, and Vesna Garovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.” PLoS Biology 13 (4): e1002128. https://doi.org/10.1371/journal.pbio.1002128.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, and Lisa Stryjewski. 2011. “40 Years of Boxplots,” November. https://vita.had.co.nz/papers/boxplots.pdf.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "06-farm.html#introduction",
    "href": "06-farm.html#introduction",
    "title": "6  Farm data",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nAs we think of our world, and telling stories about it, one of the most difficult aspects is to reduce the beautiful complexity of it into a dataset that we can use. We need to know what we give up when we do this. And be deliberate and thoughtful as we proceed. Some datasets are so large that one specific data point does not matter—it could be swapped for another without any effect (Crawford 2021, 94). But this is not always reasonable: how different would your life be if you had a different mother?\nWe are often interested in understanding the implications of some dataset; making forecasts based on it or using that dataset to make claims about broader phenomena. Regardless of how we turn our world into data, we will usually only ever have a sample of the data that we need. Statistics provides formal approaches that we use to keep these issues front of mind and understand the implications. But it does not provide definitive guidance about broader issues, such as considering who profits from the data that were collected, and whose power it reflects.\nIn this chapter we first discuss measurement, and some of the concerns that it brings. We then turn to censuses, in which we typically try to obtain data about an entire population. We also discuss other government official statistics, and long-standing surveys. We describe datasets of this type as “farmed data”.  Farmed datasets are typically well put together, thoroughly documented, and the work of collecting, preparing, and cleaning these datasets is mostly done for us. They are also, usually, conducted on a known release cycle. For instance, many countries release unemployment and inflation datasets monthly, GDP quarterly, and a census every five to ten years.\nWe then introduce statistical notions around sampling to provide a foundation that we will continually return to. Over the past one hundred years or so, statisticians have developed considerable sophistication for thinking about samples, and dealt with many controversies (Brewer 2013). In this chapter we consider probability and non-probability sampling and introduce certain key terminology and concepts.\nThis chapter is about data that are made available for us. Data are not neutral. For instance, archivists are now careful to consider archives not only as a source of fact, but also as part of the production of fact which occurred within a particular context especially constructed by the state (Stoler 2002). Thinking clearly about who is included in the dataset, and who is systematically excluded, is critical. Understanding, capturing, classifying, and naming data is an exercise in building a world and reflects power (Crawford 2021, 121), be that social, historical, financial, or legal.\nFor instance, we can consider the role of sex and gender in survey research. Sex is based on biological attributes and is assigned at birth, while gender is socially constructed and has both biological and cultural aspects (Lips 2020, 7). We may be interested in the relationship between gender, rather than sex, and some outcome. But the move toward a nuanced concept of gender in official statistics has only happened recently. Surveys that insist on a binary gender variable that is the same as sex, will not reflect those respondents who do not identify as such. Kennedy et al. (2022) provide a variety of aspects to consider when deciding what to do with gender responses, including: ethics, accuracy, practicality, and flexibility. But there is no universal best solution. Ensuring respect for the survey respondent should be the highest priority (Kennedy et al. 2022, 16).\nWhy do we even need classifications and groupings if it causes such concerns? Scott (1998) positions much of this as an outcome of the state, for its own purposes, wanting to make society legible and considers this a defining feature of modern states. For instance, Scott (1998) sees the use of surnames as arising because of the state’s desire for legible lists to use for taxation, property ownership, conscription, and censuses. The state’s desire for legibility also required imposing consistency on measurement. The modern form of metrology, which is “the study of how measurements are made, and how data are compared” (Plant and Hanisch 2020), began in the French Revolution when various measurements were standardized. This later further developed as part of Napoleonic state building (Scott 1998, 30). Prévost and Beaud (2015, 154) describe the essence of the change as one where knowledge went from being “singular, local, idiosyncratic\\(\\dots\\) and often couched in literary form” to generalized, standardized, and numeric. That all said, it would be difficult to collect data without categorizable, measurable scales. A further concern is reification, where we forget that these measures must be constructed.\nAll datasets have shortcomings. In this chapter we develop comfort with “farmed data”. We use that term to refer to a dataset that has been developed specifically for the purpose of being used as data.\nEven though these farmed datasets are put together for us to use, and can generally be easily obtained, it is nonetheless especially important for us to develop a thorough understanding of their construction. James Mill, the nineteenth century Scottish author, famously wrote The History of British India without having set foot in the country. He claimed:\n\nWhatever is worth seeing or hearing in India, can be expressed in writing. As soon as every thing of importance is expressed in writing, a man who is duly qualified may attain more knowledge of India, in one year, in his closet in England, than he could obtain during the course of the longest life, by the use of his eyes and his ears in India.\nMill (1817, xv)\n\nIt may seem remarkable that he was considered an expert and his views had influence. Yet today, many will, say, use inflation statistics without ever having tried to track a few prices, use the responses from political surveys without themselves ever having asked a respondent a question, or use ImageNet without the experience of hand-labeling some of the images. We should always throw ourselves into the details of the data."
  },
  {
    "objectID": "06-farm.html#measurement",
    "href": "06-farm.html#measurement",
    "title": "6  Farm data",
    "section": "6.2 Measurement",
    "text": "6.2 Measurement\nMeasurement is an old concern. Even Aristotle distinguished between quantities and qualities (Tal 2020). Measurement, and especially, the comparison of measurements, underpins all quantitative analysis. But deciding what to measure, and how to do it, is challenging.\nMeasurement is trickier than it seems. For instance, in music, David Peterson, Professor of Political Science, Iowa State University, make it clear how difficult it is to define a one-hit wonder. A surprising number of artists that may immediately come to mind, turn out to have at least one or two other songs that did reasonably well in terms of making it onto charts (Molanphy 2012). Should an analysis of all one-term governments include those that did not make it through a full term? How about those that only lasted a month, or even a week? How do we even begin to measure the extent of government transfers when so much of these are in-kind benefits (Garfinkel, Rainwater, and Smeeding 2006)? How can we measure how well represented a person is in a democracy despite that being the fundamental concern (Achen 1978)? And why should the standard definition used by the World Health Organization (WHO) of pregnancy-related and maternal deaths only include those that occur within 42 days of delivery, termination, or abortion when this has a substantial effect on the estimate (Gazeley et al. 2022)?\nPhilosophy brings more nuance and depth to their definitions of measurement (Tal 2020), but the International Organization Of Legal Metrology (2007, 44) define measurement as the “process of experimentally obtaining one or more quantity values that can reasonably be attributed to a quantity”, where a quantity is a “number and reference together”. It implies “comparison of quantities, including counting of entities”, and “presupposes a description of the quantity commensurate with the intended use of a measurement result, a measurement procedure, and a calibrated measuring system\\(\\dots\\)”. This definition of measurement makes clear that we have a variety of concerns including instrumentation and units, and that we are interested in measurements that are valid and reliable.\nInstrumentation refers to what we use to conduct the measurement. Thorough consideration of instrumentation is important because it determines what we can measure. For instance, Morange (2016, 63) describes how the invention of microscopes in the sixteenth century led to the observation of capillaries by Marcello Malpighi in 1661, cells by Robert Hooke in 1665, and bacteria by Antonie van Leeuwenhoek in 1677 (Lane 2015). And consider the measurement of time. Again we see the interaction between instrumentation and measurement. With a sundial it was difficult to be much more specific about elapsed time than an hour or so. But the gradual development of more accurate instruments of timekeeping would eventually enable some sports to differentiate competitors to the thousandth of the second, and through GPS, allow navigation that is accurate to within meters.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nKnowing the time is a critical measurement. For instance, Formula 1 times laps to the thousandth of a second. And Michael Phelps, an American swimmer, won a gold medal at the Beijing Olympics by only one-hundredth of a second. Timing allows us to distinguish between outcomes even when the event does not happen concurrently. For instance, think back to the discussion of swimmers by Chambliss (1989) and how this would be impossible without knowing how long each event took each swimmer. Timing is also critical in finance where we need market participants to agree on whether an asset is available for sale. But the answer to “What time is it?” can be difficult to answer. The time, according to some individual, can be set to different sources, and so will differ depending on who you ask. Since the 1970s the definitive answer has been to use atomic time. A cesium second is defined by “9192 631 770 cycles of the hyperfine transition frequency in the ground state of cesium 133” (Levine, Tavella, and Milton 2022, 4). But the problem of clock synchronization—how to have all the non-atomic clocks match atomic time and each other—remains. Hopper (2022) provides an overview of how the Network Time Protocol (NTP) of Mills (1991) enables clock synchronization and some of the difficulties that exist for computer networks to discover atomic time. Another measure of time is astronomical time, which is based on the rotation of the Earth. But because the Earth spins inconsistently and other issues, adjustments have been made to ensure atomic and astronomical time match. This has resulted in the inclusions of positive leap seconds, and the possibility of a negative leap second, which have created problems (Levine, Tavella, and Milton 2022). As a result, at some point in the future astronomical and atomic time will be allowed to diverge (Gibney 2022; Mitchell 2022b).\n\n\nA common instrument of measurement is a survey, and we discuss these further in Chapter 8. Another commonly-used instrument is sensors. For instance, climate scientists may be interested in temperature, humidity, or pressure. Much analysis of animal movement, such as Leos-Barajas et al. (2016), uses accelerometers. Sensors placed on satellites may be particularly concerned with images, and such data are available from the Landsat Program. Physicists are very concerned with measurement, and can be constrained not only by their instrumentation, but also storage capacity. For instance, the ATLAS detector at CERN is focused on the collision of particles, but not all of the measurements can be saved because that would result in 80TB per second (Colombo et al. 2016)! And in the case of A/B testing, which we discuss in Chapter 8, extensive use is made of cookies, beacons, system settings, and behavioral patterns. Another aspect of instrumentation is delivery. For instance, if using surveys, then should they be mailed or online? Should they be filled out by the respondent or by an enumerator?\nThe definition of measurement, provided by metrology, makes it clear that the second fundamental concern is a reference, which we refer to as units. The choice of units is related to both the research question of interest and available instrumentation. For instance, in the Tutorial in Chapter 1 we were concerned with measuring the growth of plants. This would not be well served by using kilometers or miles as a unit. If we were using a ruler, then we may be able to measure millimeters, but with calipers, we might be able to consider tens of micrometers.\n\n6.2.1 Properties of measurements\nValid measurements are those where the quantity that we are measuring is related to the estimand and research question of interest. It speaks to appropriateness. Recall, from Chapter 4, that an estimand is the actual effect, such as the (unknowable) actual effect of smoking on life expectancy. It can be useful to think about estimands as what is actually of interest. This means that we need to ensure that we are measuring relevant aspects of an individual. For instance, the number of cigarettes that they smoked, and the number of years they lived, rather than, say, their opinion about smoking.\nFor some units, such as a meter or a second, there is a clear definition. And when that definition evolves it is widely agreed on (Mitchell 2022a). But for other aspects that we may wish to measure it is less clear and so the validity of the measurement becomes critical. At one point in the fourteenth century attempts were made to measure grace and virtue (Crosby 1997, 14)! More recently, we try to measure intelligence or even the quality of a university. That is not to say there are not people with more or less grace, virtue, and intelligence than others, and there are certainly better and worse universities. But the measurement of these is difficult.\nThe U.S. News and World Report tries to quantify university quality based on aspects such as class size, number of faculty with a PhD, and number of full-time faculty. But an issue with such constructed measures, especially in social settings, is that it changes the incentives of those being measured. For instance, Columbia University increased from 18th in 1988 to 2nd in 2022. But Michael Thaddeus, Professor of Mathematics, Columbia University, showed how there was a difference, in Columbia’s favor, between what Columbia reported to U.S. News and World Report and what was available through other sources (Hartocollis 2022).\nSuch concerns are of special importance in psychology because there is no clear measure of many fundamental concepts. Fried, Flake, and Robinaugh (2022) review the measurement of depression and find many concerns including a lack of validity and reliability. This is not to say that we should not try to measure such things, but we should ensure transparency about measurement decisions. For instance, Flake and Fried (2020) recommend answering various clarifying questions whenever measurements have to be constructed. These include questioning the underlying construct of interest, the decision process that led to the measure, what alternatives were considered, the quantification process, and the scale. These questions are especially important when the measure is being constructed for a particular purpose, rather than being adopted from elsewhere. This is because of the concern that the measure will be constructed in a way that provides a pre-ordained outcome.\nReliability draws on the part of the definition of measurement that reads “process of experimentally obtaining\\(\\dots\\)”. It implies some degree of consistency and means that multiple measurements of one particular aspect, at one particular time, should be essentially the same. If two enumerators count the number of shops on a street, then we would hope that their counts are the same. And if they were different then we would hope we could understand the reason for the difference. For instance, perhaps one enumerator misunderstood the instructions and incorrectly counted only shops that were open. To consider another example, demographers are often concerned with the migration of people between countries, and economists are often concerned with international trade. It is concerning the number of times that the in-migration or imports data of Country A from Country B do not match the out-migration or exports data of Country B to Country A.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nIt is common for the pilot of a plane to announce the altitude to their passengers. But the notion and measurement of altitude is deceptively complicated, and underscores the fact that measurement occurs within a broader context (Vanhoenacker 2015). For instance, if we are interested in how many meters there are between the plane and the ground, then should we measure the difference between the ground and where the pilot is sitting, which would be most relevant for the announcement, or to the bottom of the wheels, which would be most relevant for landing? What happens if we go over a mountain? Even if the plane has not descended, such a measure—the number of meters between the plane and the ground—would claim a reduction in altitude and make it hard to vertically separate multiple planes. We may be interested in a comparison to sea level. But sea level changes because of the tide, and is different at different locations. As such, a common measure of altitude is flight level, which is determined by the amount of air pressure. And because air pressure is affected by weather, season, and location, the one flight level may be associated with very different numbers of meters to the ground over the course of a flight. The measures of altitude used by planes serve their purpose of enabling relatively safe air travel.\n\n\n\n\n6.2.2 Measurement error\nMeasurement error is the difference between the value we observe and the actual value. Sometimes it is possible to verify certain responses. If the difference is consistent between the responses that we can verify and those that we cannot, then we are able to estimate the extent of overall measurement error. For instance, Sakshaug, Yan, and Tourangeau (2010) considered a survey of university alumni and compared replies about a respondent’s grades with their university record. They find that the mode of the survey—telephone interview conducted by a human, telephone interview conducted by a computer, or an internet survey—affected the extent of the measurement error.\nSuch error can be particularly pervasive when an enumerator fills out the survey form on behalf of the respondent. This is especially of concern around race. For instance, Davis (1997, 177) describes how Black people in the United States may limit the extent to which they describe their political and racial belief to white interviewers.\nAnother example is censored data, which is when we have some partial knowledge of the actual value. Right-censored data is when we know that the actual value is above some observed value, but we do not know by how much. For instance, immediately following the Chernobyl disaster in 1986, the only available instruments to measure radiation had a certain maximum limit. While the radiation was measured as being at that (maximum) level, the implication was that the actual value was much higher.\nRight-censored data are often seen in medical studies. For instance, say some experiment is conducted, and then patients are followed for ten years. At the end of that ten-year period all we know is whether a patient lived at least ten years, not the exact length of their life. Left-censored data is the opposite situation. For instance, consider a thermometer that only went down to freezing. Even when the actual temperature was less, the thermometer would still register that as freezing.\nA slight variation of censored data is winsorizing data. This occurs when we observe the actual value, but we change it to a less extreme one. For instance, if we were considering age then we may change the age of anyone older than 100 to be 100. We may do this if we are worried that values that were too large would have too significant of an effect.\nTruncated data is a slightly different situation in which we do not even record those values. For instance, consider a situation in which we were interested in the relationship between a child’s age and height. Our first question might be “what is your age?” and if it turns out the respondent is an adult, then we would not continue to ask height. Truncated data are especially closely related to selection bias. For instance, consider a student who drops a course—their opinion is not measured on course evaluations.\nTo illustrate the difference between these concepts, consider a situation in which the actual distribution of newborn baby weight has a normal distribution, centered around 3.5kg. Imagine there is some defect with the scale, such that any value less than or equal to 2.75kg is assigned 2.75kg. And imagine there is some rule such that any baby expected to weigh more than 4.25kg is transferred to a different hospital to be born. These three scenarios are illustrated in Figure 6.1. We may also be interested in considering the mean weight, which highlights the bias (Table 6.1).\n\nset.seed(853)\n\nnewborn_weight &lt;-\n  tibble(\n    weight = rep(\n      x = rnorm(n = 1000, mean = 3.5, sd = 0.5), \n      times = 3),\n    measurement = rep(\n      x = c(\"Actual\", \"Censored\", \"Truncated\"),\n      each = 1000)\n    )\n\nnewborn_weight &lt;-\n  newborn_weight |&gt;\n  mutate(\n    weight = case_when(\n      weight &lt;= 2.75 & measurement == \"Censored\" ~ 2.75,\n      weight &gt;= 4.25 & measurement == \"Truncated\" ~ NA_real_,\n      TRUE ~ weight\n    )\n  )\n\nnewborn_weight |&gt;\n  ggplot(aes(x = weight)) +\n  geom_histogram(bins = 50) +\n  facet_wrap(vars(measurement)) +\n  theme_minimal()\n\n\n\n\nFigure 6.1: Comparison of actual weights with censored and truncated weights\n\n\n\n\n\nnewborn_weight |&gt;\n  summarise(mean = mean(weight, na.rm = TRUE),\n            .by = measurement) |&gt;\n  kable(\n    col.names = c(\"Measurement\", \"Mean\"),\n    digits = 3\n  )\n\n\n\nTable 6.1: Comparing the means of the different scenarios identifies the bias\n\n\nMeasurement\nMean\n\n\n\n\nActual\n3.521\n\n\nCensored\n3.530\n\n\nTruncated\n3.455\n\n\n\n\n\n\n\n\n6.2.3 Missing data\nRegardless of how good our data acquisition process is, there will be missing data. That is, observations that we know we do not have. But a variable must be measured, or at least thought about and considered, in order to be missing. With insufficient consideration, there is the danger of missing data that we do not even know are missing because the variables were never considered. They are missing in a “dog that did not bark” sense. This is why it is so important to think about the situation, sketch and simulate, and work with subject-matter experts.\nNon-response could be considered a variant of measurement error whereby we observe a null, even though there should be an actual value. But it is usually considered in its own right. And there are different extents of non-response: from refusing to even respond to the survey, through to just missing one question. Non-response is a key issue, especially with non-probability samples, because there is usually good reason to consider that people who do not respond are systematically different to those who do. And this serves to limit the extent to which the survey can be used to speak to more than just survey respondents. Gelman et al. (2016) go so far as to say that much of the changes in public opinion that are reported in the lead-up to an election are not people changing their mind, but differential non-response. That is, individual choosing whether to respond to a survey at all depending on the circumstances, not just choosing which survey response to choose. The use of pre-notification and reminders may help address non-response in some circumstances (Koitsalu et al. 2018; Frandell et al. 2021).\nData might be missing because a respondent did not want to respond to one particular question, a particular collection of related questions, or the entire survey, although these are not mutually exclusive nor collectively exhaustive (Newman 2014). In an ideal situation data are Missing Completely At Random (MCAR). This rarely occurs, but if it does, then inference should still be reflective of the broader population. It is more likely that data are Missing At Random (MAR) or Missing Not At Random (MNAR). The extent to which we must worry about that differs. For instance, if we are interested in the effect of gender on political support, then it may be that men are less likely to respond to surveys, but this is not related to who they will support. If that differential response is only due to being a man, and not related to political support, then we may be able to continue, provided we include gender in the regression, or are able to post-stratify based on gender. That said, the likelihood of this independence holding is low, and it is more likely, as in Gelman et al. (2016), that there is a relationship between responding to the survey and political support. In that more likely case, we may have a more significant issue. One approach would be to consider additional explanatory variables. It is tempting to drop incomplete cases, but this may further bias the sample, and requires justification and the support of simulation. Data imputation could be considered, but again may bias the sample. Ideally we could rethink, and improve, the data collection process.\nWe return to missing data in Chapter 11."
  },
  {
    "objectID": "06-farm.html#censuses-and-other-government-data",
    "href": "06-farm.html#censuses-and-other-government-data",
    "title": "6  Farm data",
    "section": "6.3 Censuses and other government data",
    "text": "6.3 Censuses and other government data\nThere are a variety of sources of data that have been produced for the purposes of being used as datasets. One thinks here especially of censuses of population. Whitby (2020, 30–31) describes how the earliest censuses for which we have written record are from China’s Yellow River Valley. One motivation for censuses was taxation, and Jones (1953) describes census records from the late third or early fourth century A.D. which enabled a new system of taxation. Detailed records, such as censuses, have also been abused. For instance, Luebke and Milton (1994, 25) set out how the Nazis used censuses and police registration datasets to “locate groups eventually slated for deportation and death”. And Bowen (2022, 17) describes how the United States Census Bureau provided information that contributed to the internship of Japanese Americans. President Clinton apologized for this in the 1990s.\nAnother source of data deliberately put together to be a dataset include official statistics like surveys of economic conditions such as unemployment, inflation, and GDP. Interestingly, Rockoff (2019) describes how these economic statistics were not actually developed by the federal government, even though governments typically eventually took over that role. Censuses and other government-run surveys have the power, and financial resources, of the state behind them, which enables them to be thorough in a way that other datasets cannot be. For instance, the 2020 United States Census is estimated to have cost US$15.6 billion (Hawes 2020). But this similarly brings a specific perspective. Census data, like all data, are not unimpeachable. Common errors include under- and over-enumeration, as well as misreporting (Steckel 1991). There are various measures and approaches used to assess quality (Statistics Canada 2017).\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nCensuses of population are critical, but not unimpeachable. Anderson and Fienberg (1999) describe how the history of the census in the United States is one of undercount, and that even George Washington complained about this in the 1790s. The extent of the undercount was estimated due to the Selective Service registration system used for conscription in World War II. Those records were compared with census records, and it was found that there were about half a million more men recorded for conscription purposes than in the census. This was race-specific, with an average undercount of around 3 per cent, but an undercount of Black men of draft age of around 13 per cent (Anderson and Fienberg 1999, 29). This became a political issue in the 1960s, and race and ethnicity related questions were of special concern in the 1990s. Nobles (2002, 47) discusses how counting by race first requires that race exists, but that this may be biologically difficult to establish. Despite how fundamental race is to the United States census it is not something that is “fixed” and “objective” but instead has influences from class, social, legal, structural, and political aspects (Nobles 2002, 48).\n\n\nAnother similarly large and established source of data are from long-running large surveys. These are conducted on a regular basis, and while not usually directly conducted by the government, they are usually funded, one way or another, by the government. For instance, here we often think of electoral surveys, such as the Canadian Election Study, which has run in association with every federal election since 1965, and similarly the British Election Study which has been associated with every general election since 1964.\nMore recently there has been a large push toward open data in government. The underlying principle—that the government should make available the data that it has—is undeniable. But the term has become a little contentious because of how it has occurred in practice. Governments only provide data that they want to provide. We may even sometimes see manipulation of data to suit a government’s narrative (Kalgin 2014; Zhang et al. 2019; Berdine, Geloso, and Powell 2018). One way to get data that the government has, but does not necessarily want to provide, is to submit a Freedom of Information (FOI) request (Walby and Luscombe 2019). For instance, Cardoso (2020) use data from FOI to find evidence of systematic racism in the Canadian prison system.\nWhile farmed datasets have always been useful, they were developed for a time when much analysis was conducted without the use of programming languages. Many R packages have been developed to make it easier to get these datasets into R. Here we cover a few that are especially useful.\n\n6.3.1 Canada\nThe first census in Canada was conducted in 1666. This was also the first modern census where every individual was recorded by name, although it does not include Aboriginal peoples (Godfrey 1918, 179). There were 3,215 inhabitants that were counted, and the census asked about age, sex, marital status, and occupation (Statistics Canada 2017). In association with Canadian Confederation, in 1867 a decennial census was required so that political representatives could be allocated for the new Parliament. Regular censuses have occurred since then.\nWe can explore some data on languages spoken in Canada from the 2016 Census using canlang. This package is not on CRAN, but can be installed from GitHub with: install.packages(\"devtools\") then devtools::install_github(\"ttimbers/canlang\").\nAfter loading canlang we can use the can_lang dataset. This provides the number of Canadians who use each of 214 languages.\n\ncan_lang\n\n# A tibble: 214 × 6\n   category          language mother_tongue most_at_home most_at_work lang_known\n   &lt;chr&gt;             &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Aboriginal langu… Aborigi…           590          235           30        665\n 2 Non-Official & N… Afrikaa…         10260         4785           85      23415\n 3 Non-Official & N… Afro-As…          1150          445           10       2775\n 4 Non-Official & N… Akan (T…         13460         5985           25      22150\n 5 Non-Official & N… Albanian         26895        13135          345      31930\n 6 Aboriginal langu… Algonqu…            45           10            0        120\n 7 Aboriginal langu… Algonqu…          1260          370           40       2480\n 8 Non-Official & N… America…          2685         3020         1145      21930\n 9 Non-Official & N… Amharic          22465        12785          200      33670\n10 Non-Official & N… Arabic          419890       223535         5585     629055\n# ℹ 204 more rows\n\n\nWe can quickly see the top-ten most common languages to have as a mother tongue.\n\ncan_lang |&gt;\n  slice_max(mother_tongue, n = 10) |&gt;\n  select(language, mother_tongue)\n\n# A tibble: 10 × 2\n   language                     mother_tongue\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 English                           19460850\n 2 French                             7166700\n 3 Mandarin                            592040\n 4 Cantonese                           565270\n 5 Punjabi (Panjabi)                   501680\n 6 Spanish                             458850\n 7 Tagalog (Pilipino, Filipino)        431385\n 8 Arabic                              419890\n 9 German                              384040\n10 Italian                             375635\n\n\nWe could combine two datasets: region_lang and region_data, to see if the five most common languages differ between the largest region, Toronto, and the smallest, Belleville.\n\nregion_lang |&gt;\n  left_join(region_data, by = \"region\") |&gt;\n  slice_max(c(population)) |&gt;\n  slice_max(mother_tongue, n = 5) |&gt;\n  select(region, language, mother_tongue, population) |&gt;\n  mutate(prop = mother_tongue / population)\n\n# A tibble: 5 × 5\n  region  language          mother_tongue population   prop\n  &lt;chr&gt;   &lt;chr&gt;                     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Toronto English                 3061820    5928040 0.516 \n2 Toronto Cantonese                247710    5928040 0.0418\n3 Toronto Mandarin                 227085    5928040 0.0383\n4 Toronto Punjabi (Panjabi)        171225    5928040 0.0289\n5 Toronto Italian                  151415    5928040 0.0255\n\nregion_lang |&gt;\n  left_join(region_data, by = \"region\") |&gt;\n  slice_min(c(population)) |&gt;\n  slice_max(mother_tongue, n = 5) |&gt;\n  select(region, language, mother_tongue, population) |&gt;\n  mutate(prop = mother_tongue / population)\n\n# A tibble: 5 × 5\n  region     language mother_tongue population    prop\n  &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 Belleville English          93655     103472 0.905  \n2 Belleville French            2675     103472 0.0259 \n3 Belleville German             635     103472 0.00614\n4 Belleville Dutch              600     103472 0.00580\n5 Belleville Spanish            350     103472 0.00338\n\n\nWe can see a considerable difference between the proportions, with a little over 50 per cent of those in Toronto having English as their mother tongue, compared with around 90 per cent of those in Belleville.\nIn general, data from Canadian censuses are not as easily available through the relevant government agency as in other countries, although the Integrated Public Use Microdata Series (IPUMS), which we discuss later, provides access to some. Statistics Canada, which is the government agency that is responsible for the census and other official statistics, freely provides an “Individuals File” from the 2016 census as a Public Use Microdata File (PUMF), but only in response to request. And while it is a 2.7 per cent sample from the 2016 census, this PUMF provides limited detail.\nAnother way to access data from the Canadian census is to use cancensus. It requires an API key, which can be requested by creating an account and then going to “edit profile”. The package has a helper function that makes it easier to add the API key to an “.Renviron” file, which we will explain in more detail in Chapter 7.\nAfter installing and loading cancensus we can use get_census() to get census data. We need to specify a census of interest, and a variety of other arguments. For instance, we could get data from the 2016 census about Ontario, which is the largest Canadian province by population.\n\nset_api_key(\"ADD_YOUR_API_KEY_HERE\", install = TRUE)\n\nontario_population &lt;-\n  get_census(\n    dataset = \"CA16\",\n    level = \"Regions\",\n    vectors = \"v_CA16_1\",\n    regions = list(PR = c(\"35\"))\n  )\n\nontario_population\n\n\n\n# A tibble: 1 × 9\n  GeoUID Type  `Region Name` `Area (sq km)` Population Dwellings Households\n  &lt;chr&gt;  &lt;fct&gt; &lt;fct&gt;                  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 35     PR    Ontario              986722.   13448494   5598391    5169174\n# ℹ 2 more variables: C_UID &lt;chr&gt;, `v_CA16_1: Age Stats` &lt;dbl&gt;\n\n\nData for censuses since 1996 are available, and list_census_datasets() provides the metadata that we need to provide to get_census() to access these. Data are available based on a variety of regions, and list_census_regions() provides the metadata that we need. Finally, list_census_vectors() provides the metadata about the variables that are available.\n\n\n6.3.2 United States\n\n6.3.2.1 Census\nThe requirement for a census is included in the United States Constitution, although births and deaths were legally required to be registered in what became Massachusetts as early as 1639 (Gutman 1958). After installing and loading it we can use tidycensus to get started with access to United States census data. As with cancensus, we first need to obtain an API key from the Census Bureau API and store it locally using a helper function.\nHaving set that up, we can use get_decennial() to obtain data on variables of interest. As an example, we could gather data about the average household size in 2010 overall, and by owner or renter, for certain states (Figure 6.2).\n\ncensus_api_key(\"ADD_YOUR_API_KEY_HERE\")\n\nus_ave_household_size_2010 &lt;-\n  get_decennial(\n    geography = \"state\",\n    variables = c(\"H012001\", \"H012002\", \"H012003\"),\n    year = 2010\n  )\n\nus_ave_household_size_2010 |&gt;\n  filter(NAME %in% c(\"District of Columbia\", \"Utah\", \"Massachusetts\")) |&gt;\n  ggplot(aes(y = NAME, x = value, color = variable)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Average household size\", y = \"State\", color = \"Household type\"\n    ) +\n  scale_color_brewer(\n    palette = \"Set1\", labels = c(\"Total\", \"Owner occupied\", \"Renter occupied\")\n    )\n\n\n\n\n\n\nFigure 6.2: Comparing average household size in DC, Utah, and Massachusetts, by household type\n\n\n\n\nWalker (2022) provides further detail about analyzing United States census data with R.\n\n\n6.3.2.2 American Community Survey\nThe United States is in the enviable situation where there is usually a better approach than using the census and there is a better way than having to use government statistical agency websites. IPUMS provides access to a wide range of datasets, including international census microdata. In the specific case of the United States, the American Community Survey (ACS) is a survey whose content is comparable to the questions asked on many censuses, but it is available on an annual basis, compared with a census which could be quite out-of-date by the time the data are available. It ends up with millions of responses each year. Although the ACS is smaller than a census, the advantage is that it is available on a more timely basis. We access the ACS through IPUMS.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nSteven Ruggles is Regents Professor of History and Population Studies at the University of Minnesota and is in charge of IPUMS. After earning a PhD in historical demography from the University of Pennsylvania in 1984, he was appointed as an assistant professor at the University of Minnesota, and promoted to full professor in 1995. The initial IPUMS data release was in 1993 (Sobek and Ruggles 1999). Since then it has grown and now includes social and economic data from many countries. Ruggles was awarded a MacArthur Foundation Fellowship in 2022.\n\n\nGo to IPUMS, then “IPUMS USA”, and click “Get Data”. We are interested in a sample, so go to “SELECT SAMPLE”. Un-select “Default sample from each year” and instead select “2019 ACS” and then “SUBMIT SAMPLE SELECTIONS” (Figure 6.3 (a)).\n\n\n\n\n\n\n\n(a) Selecting a sample from IPUMS USA and specifying interest in the 2019 ACS\n\n\n\n\n\n\n\n(b) Specifying that we are interested in the state\n\n\n\n\n\n\n\n\n\n(c) Adding STATEICP to the cart\n\n\n\n\n\n\n\n(d) Beginning the checkout process\n\n\n\n\n\n\n\n\n\n(e) Specifying that we are interested in .dta files\n\n\n\n\n\n\n\n(f) Reducing the sample size from three million responses to half a million\n\n\n\n\nFigure 6.3: An overview of the steps involved in getting data from IPUMS\n\n\nWe might be interested in data based on state. We would begin by looking at “HOUSEHOLD” variables and selecting “GEOGRAPHIC” (Figure 6.3 (b)). We add “STATEICP” to our “cart” by clicking the plus, which will then turn into a tick (Figure 6.3 (c)). We might then be interested in data on a “PERSON” basis, for instance, “DEMOGRAPHIC” variables such as “AGE”, which we should add to our cart. We also want “SEX” and “EDUC” (both are in “PERSON”).\nWhen we are done, we can click “VIEW CART”, and then click “CREATE DATA EXTRACT” (Figure 6.3 (d)). At this point there are two aspects that we likely want to change:\n\nChange the “DATA FORMAT” from “.dat” to “.dta” (Figure 6.3 (e)).\nCustomize the sample size as we likely do not need three million responses, and could just change it to, say, 500,000 (Figure 6.3 (f)).\n\nBriefly check the dimensions of the request. It should not be much more than around 40MB. If it is then check whether there are variables accidentally selected that are not needed or further reduce the number of observations.\nFinally, we want to include a descriptive name for the extract, for instance, “2023-05-15: State, age, sex, education”, which specifies the date we made the extract and what is in the extract. After that we can click “SUBMIT EXTRACT”.\nWe will be asked to log in or create an account, and after doing that will be able to submit the request. IPUMS will email when the extract is available, after which we can download it and read it into R in the usual way. We assume the dataset has been saved locally as “usa_00015.dta” (your dataset may have a slightly different filename).\nIt is critical that we cite this dataset when we use it. For instance we can use the following BibTeX entry for Ruggles et al. (2021).\n@misc{ipumsusa,\n  author       = {Ruggles,  Steven and Flood,  Sarah and Foster,  Sophia and Goeken,  Ronald and Pacas,  Jose and Schouweiler,  Megan and Sobek,  Matthew},\n  year         = 2021,\n  title        = {IPUMS USA: Version 11.0},\n  publisher    = {Minneapolis,  MN: IPUMS},\n  doi          = {10.18128/d010.v11.0},\n  url          = {https://usa.ipums.org},\n  language     = {en},\n}\nWe will briefly tidy and prepare this dataset because we will use it in Chapter 15. Our code is based on Mitrovski, Yang, and Wankiewicz (2020).\n\nipums_extract &lt;- read_dta(\"usa_00015.dta\")\n\nipums_extract &lt;- \n  ipums_extract |&gt;\n  select(stateicp, sex, age, educd) |&gt;\n  to_factor()\n\nipums_extract\n\n\n\n# A tibble: 500,221 × 4\n   stateicp sex    age   educd                                       \n * &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;                                       \n 1 alabama  male   77    grade 9                                     \n 2 alabama  male   62    1 or more years of college credit, no degree\n 3 alabama  male   25    ged or alternative credential               \n 4 alabama  female 20    1 or more years of college credit, no degree\n 5 alabama  male   37    1 or more years of college credit, no degree\n 6 alabama  female 19    regular high school diploma                 \n 7 alabama  female 67    regular high school diploma                 \n 8 alabama  female 20    1 or more years of college credit, no degree\n 9 alabama  male   66    grade 8                                     \n10 alabama  male   58    regular high school diploma                 \n# ℹ 500,211 more rows\n\n\n\ncleaned_ipums &lt;-\n  ipums_extract |&gt;\n  mutate(age = as.numeric(age)) |&gt;\n  filter(age &gt;= 18) |&gt;\n  rename(gender = sex) |&gt;\n  mutate(\n    age_group = case_when(\n      age &lt;= 29 ~ \"18-29\",\n      age &lt;= 44 ~ \"30-44\",\n      age &lt;= 59 ~ \"45-59\",\n      age &gt;= 60 ~ \"60+\",\n      TRUE ~ \"Trouble\"\n    ),\n    education_level = case_when(\n      educd %in% c(\n        \"nursery school, preschool\", \"kindergarten\", \"grade 1\",\n        \"grade 2\", \"grade 3\", \"grade 4\", \"grade 5\", \"grade 6\",\n        \"grade 7\", \"grade 8\", \"grade 9\", \"grade 10\", \"grade 11\",\n        \"12th grade, no diploma\", \"regular high school diploma\",\n        \"ged or alternative credential\", \"no schooling completed\"\n      ) ~ \"High school or less\",\n      educd %in% c(\n        \"some college, but less than 1 year\",\n        \"1 or more years of college credit, no degree\"\n      ) ~ \"Some post sec\",\n      educd  %in% c(\"associate's degree, type not specified\",\n                    \"bachelor's degree\") ~ \"Post sec +\",\n      educd %in% c(\n        \"master's degree\",\n        \"professional degree beyond a bachelor's degree\",\n        \"doctoral degree\"\n      ) ~ \"Grad degree\",\n      TRUE ~ \"Trouble\"\n    )\n  ) |&gt;\n  select(gender, age_group, education_level, stateicp) |&gt;\n  mutate(across(c(\n    gender, stateicp, education_level, age_group),\n    as_factor)) |&gt;\n  mutate(age_group =\n           factor(age_group, levels = c(\"18-29\", \"30-44\", \"45-59\", \"60+\")))\n\ncleaned_ipums\n\n# A tibble: 407,354 × 4\n   gender age_group education_level     stateicp\n   &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;               &lt;fct&gt;   \n 1 male   60+       High school or less alabama \n 2 male   60+       Some post sec       alabama \n 3 male   18-29     High school or less alabama \n 4 female 18-29     Some post sec       alabama \n 5 male   30-44     Some post sec       alabama \n 6 female 18-29     High school or less alabama \n 7 female 60+       High school or less alabama \n 8 female 18-29     Some post sec       alabama \n 9 male   60+       High school or less alabama \n10 male   45-59     High school or less alabama \n# ℹ 407,344 more rows\n\n\nWe will draw on this dataset in Chapter 15, so we will save it.\n\nwrite_csv(x = cleaned_ipums,\n          file = \"cleaned_ipums.csv\")\n\nWe can also have a look at some of the variables (Figure 6.4).\n\ncleaned_ipums |&gt;\n  ggplot(mapping = aes(x = age_group, fill = gender)) +\n  geom_bar(position = \"dodge2\") +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Education\"\n  ) +\n  facet_wrap(vars(education_level)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nFigure 6.4: Examining counts of the IPUMS ACS sample, by age, gender and education\n\n\n\n\nFull count data—that is the entire census—are available through IPUMS for United States censuses conducted between 1850 and 1940, with the exception of 1890. Most of the 1890 census records were destroyed due to a fire in 1921. One per cent samples are available for all censuses through to 1990. ACS data are available from 2000."
  },
  {
    "objectID": "06-farm.html#sampling-essentials",
    "href": "06-farm.html#sampling-essentials",
    "title": "6  Farm data",
    "section": "6.4 Sampling essentials",
    "text": "6.4 Sampling essentials\nStatistics is at the heart of telling stories with data because it is almost never possible to get all the data that we would like. Statisticians have spent considerable time and effort thinking about the properties that various samples of data will have and how they enable us to speak to implications for the broader population.\nLet us say that we have some data. For instance, a particular toddler goes to sleep at 6:00pm every night. We might be interested to know whether that bedtime is common among all toddlers, or if we have an unusual toddler. If we only had one toddler then our ability to use their bedtime to speak about all toddlers would be limited.\nOne approach would be to talk to friends who also have toddlers. And then talk to friends-of-friends. How many friends, and friends-of-friends, do we have to ask before we can begin to feel comfortable that we can speak about some underlying truth of toddler bedtime?\nWu and Thompson (2020, 3) describe statistics as “the science of how to collect and analyze data and draw statements and conclusions about unknown populations.” Here “population” is used in a statistical sense and refers to some infinite group that we can never know exactly, but that we can use the probability distributions of random variables to describe the characteristics of. We discuss probability distributions in more detail in Chapter 12. Fisher ([1925] 1928, 41) goes further and says:\n\n[t]he idea of an infinite population distributed in a frequency distribution in respect of one or more characters is fundamental to all statistical work. From a limited experience,\\(\\dots\\) we may obtain some idea of the infinite hypothetical population from which our sample is drawn, and so of the probable nature of future samples to which our conclusions are to be applied.\n\nAnother way to say this is that statistics involves getting some data and trying to say something sensible based on it even though we can never have all of the data.\nThree pieces of critical terminology are:\n\n“Target population”: The collection of all items about which we would like to speak.\n“Sampling frame”: A list of all the items from the target population that we could get data about.\n“Sample”: The items from the sampling frame that we get data about.\n\nA target population is a finite set of labelled items, of size \\(N\\). For instance, in theory we could add a label to all the books in the world: “Book 1”, “Book 2”, “Book 3”, \\(\\dots\\), “Book \\(N\\)”. There is a difference between the use of the term population here, and that of everyday usage. For instance, one sometimes hears those who work with census data say that they do not need to worry about sampling because they have the whole population of the country. This is a conflation of the terms, as what they have is the sample gathered by the census of the population of a country. While the goal of a census is to get every unit—and if this was achieved then sampling error would be less of an issue—there would still be many other issues. Even if a census was done perfectly and we got data about every unit in the target population, there are still issues, for instance due to measurement error, and it being a sample at a particular time. Groves and Lyberg (2010) provide a discussion of the evolution of total survey error.\nIn the same way that we saw how difficult it can be to define what to measure, it can be difficult to define a target population. For instance, say we have been asked to find out about the consumption habits of university students. How can we define that target population? If someone is a student, but also works full time, then are they in the population? What about mature-aged students, who might have different responsibilities? Some aspects that we might be interested in are formally defined to an extent that is not always commonly realized. For instance, whether an area is classified as urban or rural is often formally defined by a country’s statistical agency. But other aspects are less clear. Gelman, Hill, and Vehtari (2020, 24) discuss the difficulty of how we might classify someone as a “smoker”. If a 15-year-old has had 100 cigarettes over their lifetime, then we need to treat them differently than if they have had none. But if a 90-year-old has had 100 cigarettes over their lifetime, then are they likely different to a 90-year-old who has had none? At what age, and number of cigarettes, do these answers change?\nConsider if we want to speak to the titles of all the books ever written. Our target population is all books ever written. But it is almost impossible for us to imagine that we could get information about the title of a book that was written in the nineteenth century, but that the author locked in their desk and never told anyone about. One sampling frame could be all books in the Library of Congress Online Catalog, another could be the 25 million books that were digitized by Google (Somers 2017). Our sample may be the tens of thousands of books that are available through Project Gutenberg, which we will use in later chapters.\nTo consider another example, consider wanting to speak of the attitudes of all Brazilians who live in Germany. The target population is all Brazilians who live in Germany. One possible source of information would be Facebook and so in that case, the sampling frame might be all Brazilians who live in Germany who have Facebook. And then our sample might be all Brazilians who live in Germany who have Facebook who we can gather data about. The target population and the sampling frame will be different because not all Brazilians who live in Germany will have Facebook. And the sampling frame will be different to the sample because we will likely not be able to gather data about all Brazilians who live in Germany and have Facebook.\n\n6.4.1 Sampling in Dublin and Reading\nTo be clearer, we consider two examples: a 1798 count of the number of inhabitants of Dublin, Ireland (Whitelaw 1805), and a 1912 count of working-class households in Reading, England (Bowley 1913).\n\n6.4.1.1 Survey of Dublin in 1798\nIn 1798 the Reverend James Whitelaw conducted a survey of Dublin, Ireland, to count its population. Whitelaw (1805) describes how population estimates at the time varied considerably. For instance, the estimated size of London at the time ranged from 128,570 to 300,000 people. Whitelaw expected that the Lord Mayor of Dublin could compel the person in charge of each house to affix a list of the inhabitants of that house to the door, and then Whitelaw could simply use this.\nInstead, he found that the lists were “frequently illegible, and generally short of the actual number by a third, or even one-half”. And so instead he recruited assistants, and they went door-to-door making their own counts. The resulting estimates are particularly informative (Figure 6.5). The total population of Dublin in 1798 was estimated at 182,370.\n\n\n\nFigure 6.5: Extract of the results that Whitelaw found in 1798\n\n\nOne aspect worth noticing is that Whitelaw includes information about class. It is difficult to know how that was determined, but it played a large role in the data collection. Whitelaw describes how the houses of “the middle and upper classes always contained some individual who was competent to the task [of making a list]”. But that “among the lower class, which forms the great mass of the population of this city, the case was very different”. It is difficult to see how Whitelaw could have known that without going into the houses of both upper and lower classes. But it is also difficult to imagine Whitelaw going into the houses of the upper class and counting their number. It may be that different approaches were needed.\nWhitelaw attempted to construct a full sample of the inhabitants of Dublin without using much in the way of statistical machinery to guide his choices. We will now consider a second example, conducted in 1912, where they were able to start to use sampling approaches that we still use today.\n\n\n6.4.1.2 Survey of working-class households in Reading in 1912\nA little over one hundred years after Whitelaw (1805), Bowley (1913) was interested in counting the number of working-class households in Reading, England. Bowley selected the sample using the following procedure (Bowley 1913, 672):\n\nOne building in ten was marked throughout the local directory in alphabetical order of streets, making about 1,950 in all. Of those about 300 were marked as shops, factories, institutions and non-residential buildings, and about 300 were found to be indexed among Principal Residents, and were so marked. The remaining 1,350 were working-class houses\\(\\dots\\) [I]t was decided to take only one house in 20, rejecting the incomplete information as to the intermediate tenths. The visitors were instructed never to substitute another house for that marked, however difficult it proved to get information, or whatever the type of house.\n\nBowley (1913) says that they were able to obtain information about 622 working-class households. For instance, they were able to estimate how much rent was paid each week (Figure 6.6).\n\n\n\nFigure 6.6: Extract of the results that Bowley found about rent paid by the working-class in Reading, England\n\n\nThen, having judged from the census that there were about 18,000 households in Reading, Bowley (1913) applied a multiplier of 21 to the sample, resulting in estimates for Reading overall. The key aspect that ensures the resulting estimates are reasonable is that the sampling was done in a random way. This is why Bowley (1913) was so insistent that the visitors go to the actual house that was selected, and not substitute it for another.\n\n\n\n6.4.2 Probabilistic sampling\nHaving identified a target population and a sampling frame, we need to distinguish between probability and non-probability sampling:\n\n“Probability sampling”: Every unit in the sampling frame has some known chance of being sampled and the specific sample is obtained randomly based on these chances. The chance of being sampled does not necessarily need to be same for each unit.\n“Non-probability sampling”: Units from the sampling frame are sampled based on convenience, quotas, judgement, or other non-random processes.\n\nOften the difference between probability and non-probability sampling is one of degree. For instance, we usually cannot forcibly obtain data, and so there is almost always an aspect of volunteering on the part of a respondent. Even when there are penalties for not providing data, such as the case for completing a census form in many countries, it is difficult for even a government to force people to fill it out completely or truthfully—famously in the 2001 New Zealand census more than one per cent of the population listed their religion as “Jedi” (Taylor 2015). The most important aspect to be clear about with probability sampling is the role of uncertainty. This allows us to make claims about the population, based on our sample, with known amounts of error. The trade-off is that probability sampling is often expensive and difficult.\nWe will consider four types of probability sampling:\n\nsimple random;\nsystematic;\nstratified; and\ncluster.\n\nTo add some more specificity to our discussion, in a way that is also used by Lohr ([1999] 2022, 27), it may help to consider the numbers one to 100 as our target population. With simple random sampling, every unit has the same chance of being included. In this case let us say it is 20 per cent. That means we would expect to have around 20 units in our sample, or around one in five compared with our target population.\n\nset.seed(853)\n\nillustrative_sampling &lt;- tibble(\n  unit = 1:100,\n  simple_random_sampling =\n    sample(x = c(\"In\", \"Out\"), \n           size = 100, \n           replace = TRUE, \n           prob = c(0.2, 0.8))\n  )\n\nillustrative_sampling |&gt;\n  count(simple_random_sampling)\n\n# A tibble: 2 × 2\n  simple_random_sampling     n\n  &lt;chr&gt;                  &lt;int&gt;\n1 In                        14\n2 Out                       86\n\n\nWith systematic sampling, as was used by Bowley (1913), we proceed by selecting some value, and we then sample every fifth unit to obtain a 20 per cent sample. To begin, we randomly pick a starting point from units one to five, say three. And so sampling every fifth unit would mean looking at the third, the eighth, the thirteenth, and so on.\n\nset.seed(853)\n\nstarting_point &lt;- sample(x = c(1:5), size = 1)\n\nillustrative_sampling &lt;-\n  illustrative_sampling |&gt;\n  mutate(\n    systematic_sampling =\n      if_else(unit %in% seq.int(from = starting_point, to = 100, by = 5), \n              \"In\", \n              \"Out\"\n              )\n    )\n\nillustrative_sampling |&gt;\n  count(systematic_sampling)\n\n# A tibble: 2 × 2\n  systematic_sampling     n\n  &lt;chr&gt;               &lt;int&gt;\n1 In                     20\n2 Out                    80\n\n\nWhen we consider our population, it will typically have some grouping. This may be as straight forward as a country having states, provinces, counties, or statistical districts; a university having faculties and departments; and humans having age-groups. A stratified structure is one in which we can divide the population into mutually exclusive, and collectively exhaustive, sub-populations called “strata”.\nWe use stratification to help with the efficiency of sampling or with the balance of the survey. For instance, the population of the United States is around 335 million, with around 40 million people in California and around half a million people in Wyoming. Even a survey of 10,000 responses would only expect to have 15 responses from Wyoming, which could make inference about Wyoming difficult. We could use stratification to ensure there are, say, 200 responses from each state. We could use random sampling within each state to select the person about whom data will be gathered.\nIn our case, we will stratify our illustration by considering that our strata are the tens, that is, one to ten is one stratum, 11 to 20 is another, and so on. We will use simple random sampling within these strata to select two units from each.\n\nset.seed(853)\n\npicked_in_strata &lt;-\n  illustrative_sampling |&gt;\n  mutate(strata = (unit - 1) %/% 10) |&gt;\n  slice_sample(n = 2, by = strata) |&gt;\n  pull(unit)\n\nillustrative_sampling &lt;-\n  illustrative_sampling |&gt;\n  mutate(stratified_sampling = \n           if_else(unit %in% picked_in_strata, \"In\", \"Out\"))\n\nillustrative_sampling |&gt;\n  count(stratified_sampling)\n\n# A tibble: 2 × 2\n  stratified_sampling     n\n  &lt;chr&gt;               &lt;int&gt;\n1 In                     20\n2 Out                    80\n\n\nAnd finally, we can also take advantage of some clusters that may exist in our dataset. Like strata, clusters are collectively exhaustive and mutually exclusive. Our examples from earlier of states, departments, and age-groups remain valid as clusters. However, it is our intention toward these groups that is different. Specifically, with cluster sampling, we do not intend to collect data from every cluster, whereas with stratified sampling we do. With stratified sampling we look at every stratum and conduct simple random sampling within each strata to select the sample. With cluster sampling we select clusters of interest. We can then either sample every unit in those selected clusters or use simple random sampling, within the selected clusters, to select units. That all said, this difference can become less clear in practice, especially after the fact. Rose et al. (2006) gather mortality data for North Darfur, Sudan, in 2005. They find that both cluster and systematic sampling provide similar results, and they point out that systematic sampling requires less training of the survey teams. In general, cluster sampling can be cheaper because of the focus on geographically close locations.\nIn our case, we will cluster our illustration again based on the tens. We will use simple random sampling to select two clusters for which we will use the entire cluster.\n\nset.seed(853)\n\npicked_clusters &lt;-\n  sample(x = c(0:9), size = 2)\n\nillustrative_sampling &lt;-\n  illustrative_sampling |&gt;\n  mutate(\n    cluster = (unit - 1) %/% 10,\n    cluster_sampling = if_else(cluster %in% picked_clusters, \"In\", \"Out\")\n    ) |&gt;\n  select(-cluster)\n\nillustrative_sampling |&gt;\n  count(cluster_sampling)\n\n# A tibble: 2 × 2\n  cluster_sampling     n\n  &lt;chr&gt;            &lt;int&gt;\n1 In                  20\n2 Out                 80\n\n\nAt this point we can illustrate the differences between our approaches (Figure 6.7). We could also consider it visually, by pretending that we randomly sample using the different methods from different parts of the world (Figure 6.8). \n\nnew_labels &lt;- c(\n  simple_random_sampling = \"Simple random sampling\",\n  systematic_sampling = \"Systematic sampling\",\n  stratified_sampling = \"Stratified sampling\",\n  cluster_sampling = \"Cluster sampling\"\n)\n\nillustrative_sampling_long &lt;-\n  illustrative_sampling |&gt;\n  pivot_longer(\n    cols = names(new_labels), names_to = \"sampling_method\",\n    values_to = \"in_sample\"\n    ) |&gt;\n  mutate(sampling_method = \n           factor(sampling_method,levels = names(new_labels)))\n\nillustrative_sampling_long |&gt;\n  filter(in_sample == \"In\") |&gt;\n  ggplot(aes(x = unit, y = in_sample)) +\n  geom_point() +\n  facet_wrap(vars(sampling_method), dir = \"v\", ncol = 1, \n             labeller = labeller(sampling_method = new_labels)\n             ) +\n  theme_minimal() +\n  labs(x = \"Unit\", y = \"Is included in sample\") +\n  theme(axis.text.y = element_blank())\n\n\n\n\nFigure 6.7: Illustrative example of simple random sampling, systematic sampling, stratified sampling, and cluster sampling over the numbers from 1 to 100\n\n\n\n\n\n\n\n\n\n\n\n(a) The world\n\n\n\n\n\n\n\n(b) Systematic sampling\n\n\n\n\n\n\n\n\n\n(c) Stratified sampling\n\n\n\n\n\n\n\n(d) Cluster sampling\n\n\n\n\nFigure 6.8: Illustrative example of simple random sampling, systematic sampling, stratified sampling, and cluster sampling across different parts of the world\n\n\nFigure 6.7 and Figure 6.8 illustrate the trade-offs between the different methods, and the ways in which they will be differently appropriate. For instance, we see that systematic sampling provides a useful picture of the world in Figure 6.8, but if we were interested only in, say, only land, we would still be left with many samples that were not informative. Stratified sampling and cluster sampling enable us to focus on aspects of interest, but at the cost of a more holistic picture.\nA good way to appreciate the differences between these approaches is to consider them in practice. Au (2022) provides a number of examples. One in particular is in the context of counting raptors where Fuller and Mosher (1987) compares simple random sampling, stratified sampling, systematic sampling and cluster sampling, as well as additional considerations.\n\n6.4.2.1 Inference for probability samples\nHaving established our sample, we typically want to use it to make claims about the population. Neyman (1934, 561) goes further and says that “\\(\\dots\\)the problem of the representative method is par excellence the problem of statistical estimation. We are interested in characteristics of a certain population, say \\(\\pi\\), which it is either impossible or at least very difficult to study in detail, and we try to estimate these characteristics basing our judgment on the sample.”\nIn particular, we would typically be interested to estimate a population mean and variance. We introduced the idea of estimators, estimands, and estimates in Chapter 4. We can construct an estimator to estimate the population mean and variance. For instance, if we were using simple random sampling with a sample of size \\(n\\), then the sample mean and variance (which we return to in Chapter 12) could be constructed to produce estimates of the population mean and variance:\n\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\frac{1}{n} \\times \\sum_{i = 1}^{n}x_i\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n-1} \\times \\sum_{i = 1}^{n}\\left(x_i - \\hat{\\mu}\\right)^2\n\\end{aligned}\n\\]\nWe can use the approaches that we have used so far to simulate various types of survey designs. There are also packages that can help, including DeclareDesign (Blair et al. 2019) and survey (Lumley 2020).\nScaling up estimates can be used when we are interested in using a count from our sample to imply some total count for the target population. We saw this in Bowley (1913) where the ratio of the number of households in the sample, compared with the number of households known from the census, was 21 and this information was used to scale up the sample.\nTo consider an example, perhaps we were interested in the sum of the numbers from one to 100. Returning to our example illustrating different ways to sample from these number, we know that our samples are of size 20, and so need to be scaled up five times (Table 6.2).\n\n\n\n\nTable 6.2: Sum of the numbers in each sample, and implied sum of population\n\n\nSampling method\nSum of sample\nImplied population sum\n\n\n\n\nSystematic sampling\n970\n4,850\n\n\nStratified sampling\n979\n4,895\n\n\nCluster sampling\n910\n4,550\n\n\nSimple random sampling\n840\n4,200\n\n\n\n\n\n\nThe actual sum of the population is 5,050.1 While the specifics are unique to this sample, our estimate of the population sum, based on the scaling, are revealing. The closest is stratified sample, closely followed by systematic sampling. Cluster sampling is a little over 10 per cent off, while simple random sampling is a little further away. To get close to the true sum, it is important that our sampling method gets as many of the higher values as possible. And so stratified and systematic sampling, both of which ensured that we had outcomes from the larger numbers did particularly well. The performance of cluster and simple random sampling would depend on the particular clusters, and units, selected. In this case, stratified and systematic sampling ensured that our estimate of the sum of the population would not be too far away from the actual population sum. Here, we might think of implications for the construction and evaluation of measures, such as GDP and other constructions that are summed, and the effect on the total of the different strata based on their size.\nThis approach has a long history. For instance, Stigler (1986, 163) describes how by 1826 Adolphe Quetelet, the nineteenth century astronomer, had become involved in the statistical bureau, which was planning for a census. Quetelet argued that births and deaths were well known, but migration was not. He proposed an approach based on counts in specific geographies, which could then be scaled up to the whole country. The criticism of the plan focused on the difficulty of selecting appropriate geographies, which we saw also in our example of cluster sampling. The criticism was reasonable, and even today, some 200 years later, something that we should keep front of mind, (Stigler 1986):\n\nHe [Quetelet] was acutely aware of the infinite number of factors that could affect the quantities he wished to measure, and he lacked the information that could tell him which were indeed important. He\\(\\dots\\) was reluctant to group together as homogenous, data that he had reason to believe was not\\(\\dots\\) To be aware of a myriad of potentially important factors, without knowing which are truly important and how their effect may be felt, is often to fear the worst\\(\\dots\\) He [Quetelet] could not bring himself to treat large regions as homogeneous, [and so] he could not think of a single rate as applying to a large area.\n\nWe are able to do this scaling up when we know the population total, but if we do not know that, or we have concerns around the precision of that approach then we may use a ratio estimator.\nRatio estimators were used in 1802 by Pierre-Simon Laplace to estimate the total population of France, based on the ratio of the number of registered births, which was known throughout the country, to the number of inhabitants, which was only know for certain communes. He calculated this ratio for the three communes, and then scaled it, based on knowing the number of births across the whole country to produce an estimate of the population of France (Lohr [1999] 2022).\n\n\n\n\n\n\n\n\nA ratio estimator of some population parameter is the ratio of two means. For instance, imagine that we knew the total number of hours that a toddler slept for a 30-day period, and we want to know how many hours the parents slept over that same period. We may have some information on the number of hours that a toddler sleeps overnight, \\(x\\), and the number of hours their parents sleep overnight, \\(y\\), over a 30-day period.\n\nset.seed(853)\n\nsleep &lt;-\n  tibble(\n    toddler_sleep = sample(x = c(2:14), size = 30, replace = TRUE),\n    difference = sample(x = c(0:3), size = 30, replace = TRUE),\n    parent_sleep = toddler_sleep - difference\n  ) |&gt;\n  select(toddler_sleep, parent_sleep, difference)\n\nsleep\n\n# A tibble: 30 × 3\n   toddler_sleep parent_sleep difference\n           &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1            10            9          1\n 2            11           11          0\n 3            14           12          2\n 4             2            0          2\n 5             6            5          1\n 6            14           12          2\n 7             3            3          0\n 8             5            3          2\n 9             4            1          3\n10             4            3          1\n# ℹ 20 more rows\n\n\nThe average of each is:\n\nsleep |&gt;\n  summarise(\n    toddler_sleep_average = mean(toddler_sleep),\n    parent_sleep_average = mean(parent_sleep)\n  ) |&gt;\n  kable(\n    col.names = c(\"Toddler sleep average\", \"Parent sleep average\"),\n    format.args = list(big.mark = \",\"),\n    digits = 2\n  )\n\n\n\n\nToddler sleep average\nParent sleep average\n\n\n\n\n6.17\n4.9\n\n\n\n\n\nThe ratio of the proportion of sleep that a parent gets compared with their toddler is:\n\\[\\hat{B} = \\frac{\\bar{y}}{\\bar{x}} = \\frac{4.9}{6.16} \\approx 0.8.\\]\nGiven the toddler slept 185 hours over that 30-day period, our estimate of the number of hours that the parents slept is \\(185 \\times 0.8 = 148\\). This turns out to be almost exactly right, as the sum is 147. In this example, the estimate was not needed because we were able to sum the data, but say some other set of parents only recorded the number of hours that their toddler slept, not how long they slept, then we could use this to estimate how much they had slept.\nOne variant of the ratio estimator that is commonly used is capture and recapture, which is one of the crown jewels of data gathering. It is commonly used in ecology where we know we can never gather data about all animals. Instead, a sample is captured, marked, and released. The researchers return after some time to capture another sample. Assuming enough time passes that the initially captured animals had time to integrate back into the population, but not so much time has passed that there are insurmountable concerns around births, deaths, and migration, then we can use these values to estimate a population size. The key is what proportion in this second sample have been recaptured. This proportion can be used to estimate the size of the whole population. Interestingly, in the 1990s there was substantial debate about whether to use a capture-recapture model to adjust the 1990 US census due to concerns about methodology. The combination of Breiman (1994) and Gleick (1990) provides an overview of the concerns at the time, those of censuses more generally, and helpful background on capture and recapture methods. More recently we have seen capture and recapture combined with web scraping, which we consider in Chapter 7, for the construction of survey frames (Hyman, Sartore, and Young 2021).\n\n\n\n6.4.3 Non-probability samples\nWhile acknowledging that it is a spectrum, much of statistics was developed based on probability sampling. But a considerable amount of modern sampling is done using non-probability sampling. One approach is to use social media and other advertisements to recruit a panel of respondents, possibly in exchange for compensation. This panel is then the group that is sent various surveys as necessary. But think for a moment about the implications of this. For instance, what type of people are likely to respond to such an advertisement? Is the richest person in the world likely to respond? Are especially young or especially old people likely to respond? In some cases, it is possible to do a census. Governments typically do one every five to ten years. But there is a reason that it is generally governments that do them—they are expensive, time-consuming, and surprisingly, they are sometimes not as accurate as we may hope because of how general they need to be.\nNon-probability samples have an important role to play because they are typically cheaper and quicker to obtain than probability samples. Beaumont (2020) describes a variety of factors in favor of non-probability samples including declining response rates to probability samples, and increased demand for real-time statistics. Further, as we have discussed, the difference between probability and non-probability samples is sometimes one of degree, rather than dichotomy. Non-probability samples are legitimate and appropriate for some tasks provided one is clear about the trade-offs and ensures transparency (Baker et al. 2013). Low response rates mean that true probability samples are rare, and so grappling with the implications of non-probability sampling is important.\nConvenience sampling involves gathering data from a sample that is easy to access. For instance, one often asks one’s friends and family to fill out a survey as a way of testing it before wide-scale distribution. If we were to analyze such a sample, then we would likely be using convenience sampling.\nThe main concern with convenience sampling is whether it is able to speak to the broader population. There are also tricky ethical considerations, and typically a lack of anonymity which may further bias the results. On the other hand, it can be useful to cheaply get a quick sense of a situation.\nQuota sampling occurs when we have strata, but we do not use random sampling within those strata to select the unit. For instance, if we again stratified the United States based on state, but then instead of ensuring that everyone in Wyoming had the chance to be chosen for that stratum, just picked people at Jackson Hole. There are some advantages to this approach, especially in terms of speed and cost, but the resulting sample may be biased in various ways. That is not to say they are without merit. For instance, the Bank of Canada runs a non-probability survey focused on the method of payment for goods and services. They use quota sampling, and various adjustment methods. This use of non-probability sampling enables them to deliberately focus on hard-to-reach aspects of the population (H. Chen, Felt, and Henry 2018).\nAs the saying goes, birds of a feather flock together. And we can take advantage of that in our sampling. Although Handcock and Gile (2011) describe various uses before this, and it is notoriously difficult to define attribution in multidisciplinary work, snowball sampling is nicely defined by Goodman (1961). Following Goodman (1961), to conduct snowball sampling, we first draw a random sample from the sampling frame. Each of these is asked to name \\(k\\) others also in the sample population, but not in that initial draw, and these form the “first stage”. Each individual in the first stage is then similarly asked to name \\(k\\) others who are also in the sample population, but again not in the random draw or the first stage, and these form the “second stage”. We need to have specified the number of stages, \\(s\\), and also \\(k\\) ahead of time.\nRespondent-driven sampling was developed by Heckathorn (1997) to focus on hidden populations, which are those where:\n\nthere is no sampling frame; and\nbeing known to be in the sampling population could have a negative effect.\n\nFor instance, we could imagine various countries in which it would be difficult to sample from, say, the gay population or those who have had abortions. Respondent-driven sampling differs from snowball sampling in two ways:\n\nIn addition to compensation for their own response, as is the case with snowball sampling, respondent-driven sampling typically also involves compensation for recruiting others.\nRespondents are not asked to provide information about others to the investigator, but instead recruit them into the study. Selection into the sample occurs not from sampling frame, but instead from the networks of those already in the sample (Salganik and Heckathorn 2004)."
  },
  {
    "objectID": "06-farm.html#exercises",
    "href": "06-farm.html#exercises",
    "title": "6  Farm data",
    "section": "6.5 Exercises",
    "text": "6.5 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Every day for a year two people—Mark and Lauren—record the amount of snow that fell that day in the two different states they are from. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation with every variable independent of each other. Please include five tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched using the data that you simulated. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nIn at least two paragraphs, and using your own words, please define measurement error and provide an example from your own experience.\nImagine you take a job at a bank and they already have a dataset for you to use. What are some questions that you should explore when deciding whether that data will be useful to you?\nWith reference to W. Chen et al. (2019) and Martı́nez (2022), to what extent do you think we can trust government statistics? Please write at least a page and compare at least two governments in your answer.\nThe 2021 census in Canada asked, firstly, “What was this person’s sex at birth? Sex refers to sex assigned at birth. Male/Female”, and then “What is this person’s gender? Refers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. Male/Female/Or please specify this person’s gender (space for a typed or handwritten answer)”. With reference to Statistics Canada (2020), please discuss the extent to which you think this is an appropriate way for the census to have proceeded. You are welcome to discuss the case of a different country if you are more familiar with that.\nHow do Kennedy et al. (2022) define ethics (pick one)?\n\nRespecting the perspectives and dignity of individual survey respondents.\nGenerating estimates of the general population and for subpopulations of interest.\nUsing more complicated procedures only when they serve some useful function.\n\nWith reference to Beaumont (2020), do you think that probability surveys will disappear, and why or why not (please write a paragraph or two)?\nPlease use IPUMS to access the 2020 ACS. Making use of the codebook, how many respondents were there in California (STATEICP) that had a Doctoral degree as their highest educational attainment (EDUC) (pick one)?\n\n4,684\n5,765\n2,007\n732\n\nPlease use IPUMS to access the 1940 1% sample. Making use of the codebook, how many respondents were there in California (STATEICP) with 5+ years of college as their highest educational attainment (EDUC) (pick one)?\n\n1,789\n1,056\n532\n904\n\nPlease name some reasons why you may wish to use cluster sampling (select all that apply)?\n\nBalance in responses.\nAdministrative convenience.\nEfficiency in terms of money.\nUnderlying systematic concerns.\nEstimation of sub-populations.\n\nWrite R code that considers the numbers 1 to 100, and estimates the mean, based on a cluster sample of 20 numbers. Re-run this code one hundred times, noting the estimate of the mean each time, and then plot the histogram. What do you notice about the graph? Add a paragraph of explanation and discussion.\n\n\n\nTutorial\nPick one of the following options. Use Quarto, and include an appropriate title, author, date, and citations. Submit a PDF. Please write at least two pages.\n\nWith reference to Dean (2022), please discuss the difference between probability and non-probability sampling.\nWith reference to Daston (2000), please discuss whether GDP and counts of population are invented or discovered?\nWith reference to Meng (2018), please discuss the claim: “When you have one million responses, you do not need to worry about randomization”.\nWith reference to Gargiulo (2022), please discuss challenges of measurement in the real world.\n\n\n\n\n\nAchen, Christopher. 1978. “Measuring Representation.” American Journal of Political Science 22 (3): 475–510. https://doi.org/10.2307/2110458.\n\n\nAnderson, Margo, and Stephen Fienberg. 1999. Who Counts?: The Politics of Census-Taking in Contemporary America. Russell Sage Foundation. http://www.jstor.org/stable/10.7758/9781610440059.\n\n\nAu, Randy. 2022. “Celebrating Everyone Counting Things,” February. https://counting.substack.com/p/celebrating-everyone-counting-things.\n\n\nBaker, Reg, Michael Brick, Nancy Bates, Mike Battaglia, Mick Couper, Jill Dever, Krista Gile, and Roger Tourangeau. 2013. “Summary Report of the AAPOR Task Force on Non-Probability Sampling.” Journal of Survey Statistics and Methodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBeaumont, Jean-Francois. 2020. “Are Probability Surveys Bound to Disappear for the Production of Official Statistics?” Survey Methodology 46 (1): 1–29.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, and Alex Deckmyn. 2022. maps: Draw Geographical Maps. https://CRAN.R-project.org/package=maps.\n\n\nBerdine, Gilbert, Vincent Geloso, and Benjamin Powell. 2018. “Cuban Infant Mortality and Longevity: Health Care or Repression?” Health Policy and Planning 33 (6): 755–57. https://doi.org/10.1093/heapol/czy033.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys. 2019. “Declaring and Diagnosing Research Designs.” American Political Science Review 113 (3): 838–59. https://doi.org/10.1017/S0003055419000194.\n\n\nBowen, Claire McKay. 2022. Protecting Your Privacy in a Data-Driven World. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003122043.\n\n\nBowley, Arthur Lyon. 1913. “Working-Class Households in Reading.” Journal of the Royal Statistical Society 76 (7): 672–701. https://doi.org/10.2307/2339708.\n\n\nBreiman, Leo. 1994. “The 1991 Census Adjustment: Undercount or Bad Data?” Statistical Science 9 (4). https://doi.org/10.1214/ss/1177010259.\n\n\nBrewer, Ken. 2013. “Three Controversies in the History of Survey Sampling.” Survey Methodology 39 (2): 249–63.\n\n\nCardoso, Tom. 2020. “Bias behind bars: A Globe investigation finds a prison system stacked against Black and Indigenous inmates.” The Globe and Mail, October. https://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An Ethnographic Report on Stratification and Olympic Swimmers.” Sociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nChen, Heng, Marie-Hélène Felt, and Christopher Henry. 2018. “2017 Methods-of-Payment Survey: Sample Calibration and Variance Estimation.” Bank of Canada. https://doi.org/10.34989/tr-114.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. “A Forensic Examination of China’s National Accounts.” Brookings Papers on Economic Activity, 77–127. https://www.jstor.org/stable/26798817.\n\n\nColombo, Tommaso, Holger Fröning, Pedro Javier Garcı̀a, and Wainer Vandelli. 2016. “Optimizing the Data-Collection Time of a Large-Scale Data-Acquisition System Through a Simulation Framework.” The Journal of Supercomputing 72 (12): 4546–72. https://doi.org/10.1007/s11227-016-1764-1.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nCrosby, Alfred. 1997. The Measure of Reality: Quantification in Western Europe, 1250-1600. Cambridge: Cambridge University Press.\n\n\nDaston, Lorraine. 2000. “Why Statistics Tend Not Only to Describe the World but to Change It.” London Review of Books 22 (8). https://www.lrb.co.uk/the-paper/v22/n08/lorraine-daston/why-statistics-tend-not-only-to-describe-the-world-but-to-change-it.\n\n\nDavis, Darren. 1997. “Nonrandom Measurement Error and Race of Interviewer Effects Among African Americans.” The Public Opinion Quarterly 61 (1): 183–207. https://doi.org/10.1086/297792.\n\n\nDean, Natalie. 2022. “Tracking COVID-19 Infections: Time for Change.” Nature 602 (7896): 185. https://doi.org/10.1038/d41586-022-00336-8.\n\n\nFisher, Ronald. (1925) 1928. Statistical Methods for Research Workers. 2nd ed. London: Oliver; Boyd.\n\n\nFlake, Jessica, and Eiko Fried. 2020. “Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them.” Advances in Methods and Practices in Psychological Science 3 (4): 456–65. https://doi.org/10.1177/2515245920952393.\n\n\nFrandell, Ashlee, Mary Feeney, Timothy Johnson, Eric Welch, Lesley Michalegko, and Heyjie Jung. 2021. “The Effects of Electronic Alert Letters for Internet Surveys of Academic Scientists.” Scientometrics 126 (8): 7167–81. https://doi.org/10.1007/s11192-021-04029-3.\n\n\nFried, Eiko, Jessica Flake, and Donald Robinaugh. 2022. “Revisiting the Theoretical and Methodological Foundations of Depression Measurement.” Nature Reviews Psychology 1 (6): 358–68. https://doi.org/10.1038/s44159-022-00050-2.\n\n\nFuller, Mark, and James Mosher. 1987. “Raptor Survey Techniques.” In Raptor Management Techniques Manual, edited by Beth Pendleton, Brian Millsap, Keith Cline, and David Bird, 37–65. National Wildlife Federation. https://www.sandiegocounty.gov/content/dam/sdc/pds/ceqa/JVR/AdminRecord/IncorporatedByReference/Appendices/Appendix-D---Biological-Resources-Report/Fuller%20and%20Mosher%201987.pdf.\n\n\nGarfinkel, Irwin, Lee Rainwater, and Timothy Smeeding. 2006. “A Re-Examination of Welfare States and Inequality in Rich Nations: How in-Kind Transfers and Indirect Taxes Change the Story.” Journal of Policy Analysis and Management 25 (4): 897–919. https://doi.org/10.1002/pam.20213.\n\n\nGargiulo, Maria. 2022. “Statistical Biases, Measurement Challenges, and Recommendations for Studying Patterns of Femicide in Conflict.” Peace Review 34 (2): 163–76. https://doi.org/10.1080/10402659.2022.2049002.\n\n\nGazeley, Ursula, Georges Reniers, Hallie Eilerts-Spinelli, Julio Romero Prieto, Momodou Jasseh, Sammy Khagayi, and Veronique Filippi. 2022. “Women’s Risk of Death Beyond 42 Days Post Partum: A Pooled Analysis of Longitudinal Health and Demographic Surveillance System Data in Sub-Saharan Africa.” The Lancet Global Health 10 (11): e1582–89. https://doi.org/10.1016/s2214-109x(22)00339-4.\n\n\nGelman, Andrew, Sharad Goel, Douglas Rivers, and David Rothschild. 2016. “The Mythical Swing Voter.” Quarterly Journal of Political Science 11 (1): 103–30. https://doi.org/10.1561/100.00015031.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGibney, Elizabeth. 2022. “The leap second’s time is up: world votes to stop pausing clocks.” Nature 612 (7938): 18–18. https://doi.org/10.1038/d41586-022-03783-5.\n\n\nGleick, James. 1990. “The Census: Why We Can’t Count.” The New York Times, July. https://www.nytimes.com/1990/07/15/magazine/the-census-why-we-can-t-count.html.\n\n\nGodfrey, Ernest. 1918. “History and Development of Statistics in Canada.” In The History of Statistics–Their Development and Progress in Many Countries. New York: Macmillan, edited by John Koren, 179–98. Macmillan Company of New York.\n\n\nGoodman, Leo. 1961. “Snowball Sampling.” The Annals of Mathematical Statistics 32 (1): 148–70. https://doi.org/10.1214/aoms/1177705148.\n\n\nGroves, Robert, and Lars Lyberg. 2010. “Total Survey Error: Past, Present, and Future.” Public Opinion Quarterly 74 (5): 849–79. https://doi.org/10.1093/poq/nfq065.\n\n\nGutman, Robert. 1958. “Birth and Death Registration in Massachusetts: II. The Inauguration of a Modern System, 1800-1849.” The Milbank Memorial Fund Quarterly 36 (4): 373–402.\n\n\nHandcock, Mark, and Krista Gile. 2011. “Comment: On the Concept of Snowball Sampling.” Sociological Methodology 41 (1): 367–71. https://doi.org/10.1111/j.1467-9531.2011.01243.x.\n\n\nHartocollis, Anemona. 2022. “U.S. News Ranked Columbia No. 2, but a Math Professor Has His Doubts.” The New York Times, March. https://www.nytimes.com/2022/03/17/us/columbia-university-rank.html.\n\n\nHawes, Michael. 2020. “Implementing Differential Privacy: Seven Lessons From the 2020 United States Census.” Harvard Data Science Review 2 (2). https://doi.org/10.1162/99608f92.353c6f99.\n\n\nHeckathorn, Douglas. 1997. “Respondent-Driven Sampling: A New Approach to the Study of Hidden Populations.” Social Problems 44 (2): 174–99. https://doi.org/10.2307/3096941.\n\n\nHopper, Nate. 2022. “The Thorny Problem of Keeping the Internet’s Time.” The New Yorker, September. https://www.newyorker.com/tech/annals-of-technology/the-thorny-problem-of-keeping-the-internets-time.\n\n\nHyman, Michael, Luca Sartore, and Linda J Young. 2021. “Capture-Recapture Estimation of Characteristics of U.S. Local Food Farms Using a Web-Scraped List Frame.” Journal of Survey Statistics and Methodology 10 (4): 979–1004. https://doi.org/10.1093/jssam/smab008.\n\n\nInternational Organization Of Legal Metrology. 2007. International Vocabulary of Metrology – Basic and General Concepts and Associated Terms. 3rd ed. https://www.oiml.org/en/files/pdf%5Fv/v002-200-e07.pdf.\n\n\nJones, Arnold. 1953. “Census Records of the Later Roman Empire.” The Journal of Roman Studies 43: 49–64. https://doi.org/10.2307/297781.\n\n\nKalgin, Alexander. 2014. “Implementation of Performance Management in Regional Government in Russia: Evidence of Data Manipulation.” Public Management Review 18 (1): 110–38. https://doi.org/10.1080/14719037.2014.965271.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, Andrew Gelman, Yajun Jia, and Julien Teitler. 2022. “He, She, They: Using Sex and Gender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKoitsalu, Marie, Martin Eklund, Jan Adolfsson, Henrik Grönberg, and Yvonne Brandberg. 2018. “Effects of Pre-Notification, Invitation Length, Questionnaire Length and Reminder on Participation Rate: A Quasi-Randomised Controlled Trial.” BMC Medical Research Methodology 18 (3): 1–5. https://doi.org/10.1186/s12874-017-0467-5.\n\n\nLane, Nick. 2015. “The Unseen World: Reflections on Leeuwenhoek (1677) ‘Concerning Little Animals’.” Philosophical Transactions of the Royal Society B: Biological Sciences 370 (1666): 20140344. https://doi.org/10.1098/rstb.2014.0344.\n\n\nLeos-Barajas, Vianey, Theoni Photopoulou, Roland Langrock, Toby Patterson, Yuuki Watanabe, Megan Murgatroyd, and Yannis Papastamatiou. 2016. “Analysis of Animal Accelerometer Data Using Hidden Markov Models.” Methods in Ecology and Evolution 8 (2): 161–73. https://doi.org/10.1111/2041-210x.12657.\n\n\nLevine, Judah, Patrizia Tavella, and Martin Milton. 2022. “Towards a Consensus on a Continuous Coordinated Universal Time.” Metrologia 60 (1): 014001. https://doi.org/10.1088/1681-7575/ac9da5.\n\n\nLips, Hilary. 2020. Sex and Gender: An Introduction. 7th ed. Illinois: Waveland Press.\n\n\nLohr, Sharon. (1999) 2022. Sampling: Design and Analysis. 3rd ed. Chapman; Hall/CRC.\n\n\nLuebke, David Martin, and Sybil Milton. 1994. “Locating the Victim: An Overview of Census-Taking, Tabulation Technology, and Persecution in Nazi Germany.” IEEE Annals of the History of Computing 16 (3): 25–39. https://doi.org/10.1109/MAHC.1994.298418.\n\n\nLumley, Thomas. 2020. “survey: analysis of complex survey samples.” https://cran.r-project.org/web/packages/survey/index.html.\n\n\nMartı́nez, Luis. 2022. “How Much Should We Trust the Dictator’s GDP Growth Estimates?” Journal of Political Economy 130 (10): 2731–69. https://doi.org/10.1086/720458.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nMill, James. 1817. The History of British India. 1st ed. https://books.google.ca/books?id=Orw_AAAAcAAJ.\n\n\nMills, David L. 1991. “Internet Time Synchronization: The Network Time Protocol.” IEEE Transactions on Communications 39 (10): 1482–93.\n\n\nMitchell, Alanna. 2022a. “Get Ready for the New, Improved Second.” The New York Times, April. https://www.nytimes.com/2022/04/25/science/time-second-measurement.html.\n\n\n———. 2022b. “Time Has Run Out for the Leap Second.” The New York Times, November. https://www.nytimes.com/2022/11/14/science/time-leap-second.html.\n\n\nMitrovski, Alen, Xiaoyan Yang, and Matthew Wankiewicz. 2020. “Joe Biden Projected to Win Popular Vote in 2020 US Election.” https://github.com/matthewwankiewicz/US_election_forecast.\n\n\nMolanphy, Chris. 2012. “100 & Single: Three Rules to Define the Term ‘One-Hit Wonder’ in 2012.” The Village Voice, September. https://www.villagevoice.com/2012/09/10/100-single-three-rules-to-define-the-term-one-hit-wonder-in-2012/.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nNewman, Daniel. 2014. “Missing Data: Five Practical Guidelines.” Organizational Research Methods 17 (4): 372–411. https://doi.org/10.1177/1094428114548590.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection.” Journal of the Royal Statistical Society 97 (4): 558–625. https://doi.org/10.2307/2342192.\n\n\nNobles, Melissa. 2002. “Racial Categorization and Censuses.” In Census and Identity: The Politics of Race, Ethnicity, and Language in National Censuses, edited by David Kertzer and Dominique Arel, 43–70. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511606045.003.\n\n\nPlant, Anne, and Robert Hanisch. 2020. “Reproducibility in Science: A Metrology Perspective.” Harvard Data Science Review 2 (4). https://doi.org/10.1162/99608f92.eb6ddee4.\n\n\nPrévost, Jean-Guy, and Jean-Pierre Beaud. 2015. Statistics, Public Debate and the State, 1800–1945: A Social, Political and Intellectual History of Numbers. Routledge.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRegister, Yim. 2020. “Introduction to Sampling and Randomization.” YouTube, November. https://youtu.be/U272FFxG8LE.\n\n\nRockoff, Hugh. 2019. “On the Controversies Behind the Origins of the Federal Economic Statistics.” Journal of Economic Perspectives 33 (1): 147–64. https://doi.org/10.1257/jep.33.1.147.\n\n\nRose, Angela, Rebecca Grais, Denis Coulombier, and Helga Ritter. 2006. “A Comparison of Cluster and Systematic Sampling Methods for Measuring Crude Mortality.” Bulletin of the World Health Organization 84: 290–96. https://doi.org/10.2471/blt.05.029181.\n\n\nRuggles, Steven, Sarah Flood, Sophia Foster, Ronald Goeken, Jose Pacas, Megan Schouweiler, and Matthew Sobek. 2021. “IPUMS USA: Version 11.0.” Minneapolis, MN: IPUMS. https://doi.org/10.18128/d010.v11.0.\n\n\nSakshaug, Joseph, Ting Yan, and Roger Tourangeau. 2010. “Nonresponse Error, Measurement Error, and Mode of Data Collection: Tradeoffs in a Multi-Mode Survey of Sensitive and Non-Sensitive Items.” Public Opinion Quarterly 74 (5): 907–33. https://doi.org/10.1093/poq/nfq057.\n\n\nSalganik, Matthew, and Douglas Heckathorn. 2004. “Sampling and Estimation in Hidden Populations Using Respondent-Driven Sampling.” Sociological Methodology 34 (1): 193–240. https://doi.org/10.1111/j.0081-1750.2004.00152.x.\n\n\nScott, James. 1998. Seeing Like a State. Yale University Press.\n\n\nSobek, Matthew, and Steven Ruggles. 1999. “The IPUMS Project: An Update.” Historical Methods: A Journal of Quantitative and Interdisciplinary History 32 (3): 102–10. https://doi.org/10.1080/01615449909598930.\n\n\nSomers, James. 2017. “Torching the Modern-Day Library of Alexandria.” The Atlantic, April. https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/.\n\n\nStatistics Canada. 2017. “Guide to the Census of Population, 2016.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2016/ref/98-304/98-304-x2016001-eng.pdf.\n\n\n———. 2020. “Sex at Birth and Gender: Technical Report on Changes for the 2021 Census.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-20-0002/982000022020002-eng.pdf.\n\n\nSteckel, Richard. 1991. “The Quality of Census Data for Historical Inquiry: A Research Agenda.” Social Science History 15 (4): 579–99. https://doi.org/10.2307/1171470.\n\n\nStigler, Stephen. 1986. The History of Statistics. Massachusetts: Belknap Harvard.\n\n\nStoler, Ann Laura. 2002. “Colonial Archives and the Arts of Governance.” Archival Science 2 (March): 87–109. https://doi.org/10.1007/bf02435632.\n\n\nTal, Eran. 2020. “Measurement in Science.” In The Stanford Encyclopedia of Philosophy, edited by Edward Zalta, Fall 2020. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/; Metaphysics Research Lab, Stanford University.\n\n\nTaylor, Adam. 2015. “New Zealand Says No to Jedis.” The Washington Post, September. https://www.washingtonpost.com/news/worldviews/wp/2015/09/29/new-zealand-says-no-to-jedis/.\n\n\nTimbers, Tiffany. 2020. canlang: Canadian Census language data. https://ttimbers.github.io/canlang/.\n\n\nVanhoenacker, Mark. 2015. Skyfaring: A Journey with a Pilot. 1st ed. Alfred A. Knopf.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021. cancensus: R package to access, retrieve, and work with Canadian Census data and geography. https://mountainmath.github.io/cancensus/.\n\n\nWalby, Kevin, and Alex Luscombe. 2019. Freedom of Information and Social Science Research Design. Routledge.\n\n\nWalker, Kyle. 2022. Analyzing US Census Data. Chapman; Hall/CRC. https://walker-data.com/census-r/index.html.\n\n\nWalker, Kyle, and Matt Herman. 2022. tidycensus: Load US Census Boundary and Attribute Data as “tidyverse” and “sf”-Ready Data Frames. https://CRAN.R-project.org/package=tidycensus.\n\n\nWhitby, Andrew. 2020. The Sum of the People. New York: Basic Books.\n\n\nWhitelaw, James. 1805. An Essay on the Population of Dublin. Being the Result of an Actual Survey Taken in 1798, with Great Care and Precision, and Arranged in a Manner Entirely New. Graisberry; Campbell.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWu, Changbao, and Mary Thompson. 2020. Sampling Theory and Practice. Springer.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nZhang, Ping, XunPeng Shi, YongPing Sun, Jingbo Cui, and Shuai Shao. 2019. “Have China’s provinces achieved their targets of energy intensity reduction? Reassessment based on nighttime lighting data.” Energy Policy 128 (May): 276–83. https://doi.org/10.1016/j.enpol.2019.01.014."
  },
  {
    "objectID": "06-farm.html#footnotes",
    "href": "06-farm.html#footnotes",
    "title": "6  Farm data",
    "section": "",
    "text": "We can obtain this using a trick attributed to Leonhard Euler, the eighteenth century mathematician, who noticed that the sum of one to any number can be quickly obtained by finding the middle number and then multiplying that by one plus the number. In this case, we have \\(50 \\times 101\\). Alternatively we can use R: sum(1:100).↩︎"
  },
  {
    "objectID": "07-gather.html#introduction",
    "href": "07-gather.html#introduction",
    "title": "7  Gather data",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nIn this chapter we consider data that we must gather ourselves. This means that although the observations exist, we must parse, pull, clean, and prepare them to get the dataset that we will consider. In contrast to farmed data, discussed in Chapter 6, often these observations are not being made available for the purpose of analysis. This means that we need to be especially concerned with documentation, inclusion and exclusion decisions, missing data, and ethical behavior.\nAs an example of such a dataset, consider Cummins (2022) who create a dataset using individual-level probate records from England between 1892 and 1992. They find that about one-third of the inheritance of “elites” is concealed. Similarly, Taflaga and Kerby (2019) construct a systematic dataset of job responsibilities based on Australian Ministerial telephone directories. They find substantial differences by gender. Neither wills nor telephone directories were created for the purpose of being included in a dataset. But with a respectful approach they enable insights that we could not get by other means. We term this “data gathering”—the data exist but we need to get them.\nDecisions need to be made at the start of a project about the values we want the project to have. For instance, Saulnier et al. (2022) value transparency, reproducibility, fairness, being self-critical, and giving credit. How might that affect the project? Valuing “giving credit” might mean being especially zealous about attribution and licensing. In the case of gathered data we should give special thought to this as the original, unedited data may not be ours.\nThe results of a data science workflow cannot be better than their underlying data (Bailey 2008). Even the most-sophisticated statistical analysis will struggle to adjust for poorly-gathered data. This means when working in a team, data gathering should be overseen and at least partially conducted by senior members of the team. And when working by yourself, try to give special consideration and care to this stage.\nIn this chapter we go through a variety of approaches for gathering data. We begin with the use of APIs and semi-structured data, such as JSON and XML. Using an API is typically a situation in which the data provider has specified the conditions under which they are comfortable providing access. An API allows us to write code to gather data. This is valuable because it can be efficient and scales well. Developing comfort with gathering data through APIs enables access to exciting datasets. For instance, Wong (2020) use the Facebook Political Ad API to gather 218,100 of the Trump 2020 campaign ads to better understand the campaign.\nWe then turn to web scraping, which we may want to use when there are data available on a website. As these data have typically not been put together for the purposes of being a dataset, it is especially important to have deliberate and definite values for the project. Scraping is a critical part of data gathering because there are many data sources where the priorities of the data provider mean they have not implemented an API. For instance, considerable use of web scraping was critical for creating COVID-19 dashboards in the early days of the pandemic (Eisenstein 2022).\nFinally, we consider gathering data from PDFs. This enables the construction of interesting datasets, especially those contained in government reports and old books. Indeed, while freedom of information legislation exists in many countries and require the government to make data available, these all too often result in spreadsheets being shared as PDFs, even when they were a CSV to begin with.\nGathering data can require more of us than using farmed data, but it allows us to explore datasets and answer questions that we could not otherwise. Some of the most exciting work in the world uses gathered data, but it is especially important that we approach it with respect."
  },
  {
    "objectID": "07-gather.html#apis",
    "href": "07-gather.html#apis",
    "title": "7  Gather data",
    "section": "7.2 APIs",
    "text": "7.2 APIs\nIn everyday language, and for our purposes, an Application Programming Interface (API) is a situation in which someone has set up specific files on their computer such that we can follow their instructions to get them. For instance, when we use a gif on Slack, one way it could work in the background is that Slack asks Giphy’s server for the appropriate gif, Giphy’s server gives that gif to Slack, and then Slack inserts it into the chat. The way in which Slack and Giphy interact is determined by Giphy’s API. More strictly, an API is an application that runs on a server that we access using the HTTP protocol.\nHere we focus on using APIs for gathering data. In that context an API is a website that is set up for another computer to be able to access it, rather than a person. For instance, we could go to Google Maps. And we could then scroll and click and drag to center the map on, say, Canberra, Australia. Or we could paste this link into the browser. By pasting that link, rather than navigating, we have mimicked how we will use an API: provide a URL and be given something back. In this case the result should be a map like Figure 7.1.\n\n\n\nFigure 7.1: Example of an API response from Google Maps, as of 12 February 2023\n\n\nThe advantage of using an API is that the data provider usually specifies the data that they are willing to provide, and the terms under which they will provide it. These terms may include aspects such as rate limits (i.e. how often we can ask for data), and what we can do with the data, for instance, we might not be allowed to use it for commercial purposes, or to republish it. As the API is being provided specifically for us to use it, it is less likely to be subject to unexpected changes or legal issues. Because of this it is clear that when an API is available, we should try to use it rather than web scraping.\nWe will now go through a few case studies of using APIs. In the first we deal directly with an API using httr. And then we access data from Spotify using spotifyr.\n\n7.2.1 arXiv, NASA, and Dataverse\nAfter installing and loading httr we use GET() to obtain data from an API directly. This will try to get some specific data and the main argument is “url”. This is similar to the Google Maps example in Figure 7.1 where the specific information that we were interested in was a map.\n\n7.2.1.1 arXiv\nIn this case study we will use an API provided by arXiv. arXiv is an online repository for academic papers before they go through peer review. These papers are typically referred to as “pre-prints”. We use GET() to ask arXiv to obtain some information about a pre-print by providing a URL.\n\narxiv &lt;- GET(\"http://export.arxiv.org/api/query?id_list=2111.09299\")\n\nstatus_code(arxiv)\n\n[1] 200\n\n\nWe can use status_code() to check our response. For instance, 200 means a success, while 400 means we received an error from the server. Assuming we received something back from the server, we can use content() to display it. In this case we have received XML formatted data. XML is a markup language where entries are identified by tags, which can be nested within other tags. After installing and loading xml2 we can read XML using read_xml(). XML is a semi-formatted structure, and it can be useful to start by having a look at it using html_structure().\n\ncontent(arxiv) |&gt;\n  read_xml() |&gt;\n  html_structure()\n\n&lt;feed [xmlns]&gt;\n  &lt;link [href, rel, type]&gt;\n  &lt;title [type]&gt;\n    {text}\n  &lt;id&gt;\n    {text}\n  &lt;updated&gt;\n    {text}\n  &lt;totalResults [xmlns:opensearch]&gt;\n    {text}\n  &lt;startIndex [xmlns:opensearch]&gt;\n    {text}\n  &lt;itemsPerPage [xmlns:opensearch]&gt;\n    {text}\n  &lt;entry&gt;\n    &lt;id&gt;\n      {text}\n    &lt;updated&gt;\n      {text}\n    &lt;published&gt;\n      {text}\n    &lt;title&gt;\n      {text}\n    &lt;summary&gt;\n      {text}\n    &lt;author&gt;\n      &lt;name&gt;\n        {text}\n    &lt;author&gt;\n      &lt;name&gt;\n        {text}\n    &lt;comment [xmlns:arxiv]&gt;\n      {text}\n    &lt;link [href, rel, type]&gt;\n    &lt;link [title, href, rel, type]&gt;\n    &lt;primary_category [term, scheme, xmlns:arxiv]&gt;\n    &lt;category [term, scheme]&gt;\n\n\nWe might like to create a dataset based on extracting various aspects of this XML tree. For instance, we might look at “entry”, which is the eighth item, and in particular obtain the “title” and the “URL”, which are the fourth and ninth items, respectively, within “entry”.\n\ndata_from_arxiv &lt;-\n  tibble(\n    title = content(arxiv) |&gt;\n      read_xml() |&gt;\n      xml_child(search = 8) |&gt;\n      xml_child(search = 4) |&gt;\n      xml_text(),\n    link = content(arxiv) |&gt;\n      read_xml() |&gt;\n      xml_child(search = 8) |&gt;\n      xml_child(search = 9) |&gt;\n      xml_attr(\"href\")\n  )\ndata_from_arxiv\n\n# A tibble: 1 × 2\n  title                                                                    link \n  &lt;chr&gt;                                                                    &lt;chr&gt;\n1 \"The Increased Effect of Elections and Changing Prime Ministers on Topi… http…\n\n\n\n\n7.2.1.2 NASA Astronomy Picture of the Day\nTo consider another example, each day, NASA provides the Astronomy Picture of the Day (APOD) through its APOD API. We can use GET() to obtain the URL for the photo on particular dates and then display it.\n\nNASA_APOD_20190719 &lt;-\n  GET(\"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&date=2019-07-19\")\n\nExamining the returned data using content(), we can see that we are provided with various fields, such as date, title, explanation, and a URL.\n\n# APOD July 19, 2019\ncontent(NASA_APOD_20190719)$date\n\n[1] \"2019-07-19\"\n\ncontent(NASA_APOD_20190719)$title\n\n[1] \"Tranquility Base Panorama\"\n\ncontent(NASA_APOD_20190719)$explanation\n\n[1] \"On July 20, 1969 the Apollo 11 lunar module Eagle safely touched down on the Moon. It landed near the southwestern corner of the Moon's Mare Tranquillitatis at a landing site dubbed Tranquility Base. This panoramic view of Tranquility Base was constructed from the historic photos taken from the lunar surface. On the far left astronaut Neil Armstrong casts a long shadow with Sun is at his back and the Eagle resting about 60 meters away ( AS11-40-5961). He stands near the rim of 30 meter-diameter Little West crater seen here to the right ( AS11-40-5954). Also visible in the foreground is the top of the camera intended for taking stereo close-ups of the lunar surface.\"\n\ncontent(NASA_APOD_20190719)$url\n\n[1] \"https://apod.nasa.gov/apod/image/1907/apollo11TranquilitybasePan600h.jpg\"\n\n\nWe can provide that URL to include_graphics() from knitr to display it (Figure 7.3).\n\n\n\n\n\nFigure 7.2: Tranquility Base Panorama (Image Credit: Neil Armstrong, Apollo 11, NASA)\n\n\nFigure 7.3: Images obtained from the NASA APOD API\n\n\n\n\n7.2.1.3 Dataverse\nFinally, another common API response in semi-structured form is JSON. JSON is a human-readable way to store data that can be parsed by machines. In contrast to, say, a CSV, where we are used to rows and columns, JSON uses key-value pairs.\n\n{\n  \"firstName\": \"Rohan\",\n  \"lastName\": \"Alexander\",\n  \"age\": 36,\n  \"favFoods\": {\n    \"first\": \"Pizza\",\n    \"second\": \"Bagels\",\n    \"third\": null\n  }\n}\n\n\nWe can parse JSON with jsonlite. To consider a specific example, we use a “Dataverse” which is a web application that makes it easier to share datasets. We can use an API to query a demonstration dataverse. For instance, we might be interested in datasets related to politics.\n\npolitics_datasets &lt;-\n  fromJSON(\"https://demo.dataverse.org/api/search?q=politics\")\n\npolitics_datasets\n\n$status\n[1] \"OK\"\n\n$data\n$data$q\n[1] \"politics\"\n\n$data$total_count\n[1] 9\n\n$data$start\n[1] 0\n\n$data$spelling_alternatives\nnamed list()\n\n$data$items\n                                                                                               name\n1                                                                              CAP - United Kingdom\n2                                                                           China Archive Dataverse\n3 Policy Dynamics and Government Attention over Welfare Policies: An Analysis of the Brazilian Case\n4                                                              Aspetti della vita quotidiana (2016)\n5                                                              Aspetti della vita quotidiana (2020)\n6                                                                                   CAP - Australia\n7                                                                                       table_1.tab\n8                                                                      cora-efc2000-E-2000-4_F1.tab\n9                                                                                        survey.tab\n       type                                                    url\n1 dataverse            https://demo.dataverse.org/dataverse/CAP_UK\n2 dataverse     https://demo.dataverse.org/dataverse/china-archive\n3   dataset                    https://doi.org/10.70122/FK2/ZR9M6C\n4   dataset                    https://doi.org/10.70122/FK2/ZKZPSJ\n5   dataset                    https://doi.org/10.70122/FK2/NJ77JM\n6 dataverse     https://demo.dataverse.org/dataverse/CAP_Australia\n7      file https://demo.dataverse.org/api/access/datafile/2029365\n8      file https://demo.dataverse.org/api/access/datafile/2069367\n9      file https://demo.dataverse.org/api/access/datafile/2077715\n     identifier\n1        CAP_UK\n2 china-archive\n3          &lt;NA&gt;\n4          &lt;NA&gt;\n5          &lt;NA&gt;\n6 CAP_Australia\n7          &lt;NA&gt;\n8          &lt;NA&gt;\n9          &lt;NA&gt;\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     description\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The UK Policy Agendas Project seeks to develop systematic measures of the policy agenda of British government and politics over time. It applies the policy content coding system of the original Policy Agendas Project in the United States, founded by Frank Baumgartner and Bryan Jones, with the aim of creating a consistent record of the issues that are attended to at different points in time, across many of the main venues of British public policy and politics – namely in parliament, the media and public opinion. The reliability of these measures of policy attention are ensured through adherence to clearly defined coding rules and standards, which give us confidence that changes in the priorities of government can be tracked consistently over time and in different arenas of politics. Location: University of Edinburgh; University of Southampton Downloadable Data Series: 12 Time Span: 1910-2015 Total Observations: 125,539\n2 Introduction The China Archive is a data archive dedicated to support of scholarly, empirical research by anthropologists, economists, historians, political scientists, sociologists, and others in the fields of business, agriculture, and engineering. The goal of the Archive is to enable case research on Chinese domestic matters and China–U.S. relations, as well as to facilitate the inclusion of China in more broadly comparative studies. To that end, the Archive’s mission includes: acquiring and maintaining extant data sets and data sources on an ongoing basis, facilitating production of quantitative data from textual information when such is desirable and feasible, making all of the Archive’s data available on a user-friendly basis to scholars at and visiting Texas A&M University, and establishing web-based links to searchable electronic sources of information on China. As long-term goals, The China Archive is especially dedicated to: locating and acquiring difficult-to-obtain Chinese data such as public opinion data, making as many of the holdings as possible available online or via other computer media, and providing capability for converting textual data to numerical data suitable for quantitative analysis. In keeping with these goals, the Archive includes data sets collected by individuals and research centers/institutes as well as by government agencies. The Archive was planned by a faculty committee in 2002–2003, and is now a project of the Texas A&M University Libraries. A faculty committee continues to function as an advisory committee to the Libraries on matters related to the upkeep and expansion of The China Archive. The faculty committee is, in turn, advised by an External Advisory Panel, composed of China scholars from other academic institutions in the United States. Faculty Planning Committee Bob Harmel, Archive Planning Committee Chair; Political Science; Director, Program in the Cross-National Study of Politics Stephen Atkins, Sterling Evans Library Ben Crouch, Associate Dean, College of Liberal Arts Qi Li, Department of Economics Xinsheng Liu, Institute for Science, Technology and Public Policy, Bush School of Government and Public Service; School of Government, Peking University Rick Nader, Director, Institute for Pacific Asia Dudley Poston, Professor of Sociology and Abell Professor of Liberal Arts Raghavan (Srini) Srinivasan, Director, Spatial Sciences Laboratory, Texas Agriculture Experiment Station Di Wang, Assistant Professor of History Ben Wu, Associate Professor, Rangeland Ecology and Management Wei Zhao, Professor, Computer Science; Associate Vice President for Research Dianzhi (Dan) Sui, Department of Geography\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The recognition of issues as public problems and the ways governments prioritize them constitute focal points in the study of policy change and policy dynamics. In Brazil and other Latin American countries, social welfare systems and related policies have undergone transformations throughout the recent democratic period. This article aims to understand changes in the Brazilian social welfare agenda by means of an analysis of the attention given to social welfare policies at the federal level. The main analytical and methodological contribution of this article is its use of the research strategy developed under the Comparative Agendas Project (CAP) to analyze the Brazilian situation. We drew on a set of unpublished datasets on the attention given by governments to social welfare policies from 1988 to 2018 that involves more than one thousand observations across six different datasets. The analyses are made at two different levels: first, we seek to understand macro trends and moments of continuity and inflection in social welfare policy by federal government administration. Second, we analyze the composition of the attention given to social welfare policies, thereby identifying the themes given the highest priority.\n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The survey Aspects of Daily Life, which is part of an integrated system of social surveys – Multiscopo ISTAT – reveals the basic information relating to the daily lives of individuals and families. Since 1993, the survey is conducted annually and the information gathered provides insight into the habits of citizens and the problems they face every day. The main topics of the survey are: eating habits smoking habits (electronic cigarettes too) health conditions mass-media use PC and Internet use, IT skills cell phone use cultural consumption (films, concerts, performances, exhibition, reading books, etc.) religious, political, and social participation use of public services and public utility services and degree of satisfaction use of health services and degree of satisfaction environmental issues safety trust in people and institutions education daily commuting leisure time domestic accidents physical activity and sports family life and friendship working conditions use of the justice system characteristics of the house and the area possession of home appliances and means of transport possession of IT equipment access to Internet household economic situation\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The survey Aspects of Daily Life, which is part of an integrated system of social surveys – Multiscopo ISTAT – reveals the basic information relating to the daily lives of individuals and families. Since 1993, the survey is conducted annually and the information gathered provides insight into the habits of citizens and the problems they face every day. The main topics of the survey are: eating habits and alcohol consumption smoking habits (electronic cigarettes too) health conditions mass-media use libraries use PC and Internet use, IT skills purchase of goods and services online cell phone/smartphone use cultural consumption (cinema, theatre, books) religious, political, and social participation use of public services and public utility services and degree of satisfaction use of health services and degree of satisfaction environmental issues safety general satisfaction, trust in people and institutions relationship with local territory education and training daily commuting leisure time domestic accidents physical activity and sports relationship with friends, family and neighbourhood working conditions electricity and gas consumption characteristics of the house and the area private services to households accessibility to the area services possession of home appliances and means of transport possession of books and of IT equipment, Internet connection household economic situation\n6                                                                                                                                                                                                                                         The Australian Policy Agendas Project collects and organizes data on Australian legislation, executive speeches, opposition questions, public opinion, media coverage, and High Court decisions. Some details are listed below. Data is forthcoming. Decisions of the High Court of Australia This dataset contains information on every case decided by the High Court of Australia between the years 1970 and 2015. Cases serve as the unit of analysis. Each case was coded in terms of its policy content and several other variables controlling for the nature of the case and the nature of the court. In coding for policy content, we utilized the Comparative Agendas Project’s topics coding scheme, where each case was assigned both a major topic and a sub topic depending on its policy content. A full description of these categories and their corresponding codes may be found in the codebook. Sydney Morning Herald - Front Page Articles This dataset contains information on each article published on the Sydney Morning Herald's front page for each day from 1990 through 2015. Front page articles serve as the unit of analysis. Each article was coded in terms of its policy content and other variables of interest controlling for location, political context, and key actors. In coding for policy content, we utilized the Comparative Agendas Project’s major topics coding scheme, where each article was assigned a major topic code. A full description of the policy content categories and their corresponding codes may be found in the major topics codebook. Dr. Keith Dowding (ANU), Dr. Aaron Martin (Melbourne), and Dr. Rhonda Evans (UT-Austin) lead the Australian Policy Agendas Project. Dr. Dowding and Dr. Martin coded legislation, executive speeches, opposition questions, public opinion, and media data, and Dr. Evans collected data on decisions of the High Court of Australia as well as additional media data. Data is forthcoming. Principal Investigator: Dr. Keith Dowding, Dr. Aaron Martin, Dr. Rhonda Evans Location: Australian National University, University of Melbourne, The University of Texas at Austin Downloadable Data Series: 1 Time Span: 1970-2015 Total Observations: 2,548 Sponsoring Institutions Dr. Dowding and Dr. Martin’s research was funded by the Australian Research Council Discovery Award DP 110102622. Dr. Evans’ research is funded by the Edward A. Clark Center for Australian and New Zealand Studies at The University of Texas at Austin.\n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       raw data to use to created cem data file\n8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           &lt;NA&gt;\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           &lt;NA&gt;\n          published_at               global_id\n1 2023-06-06T17:18:53Z                    &lt;NA&gt;\n2 2016-12-09T15:13:22Z                    &lt;NA&gt;\n3 2022-09-06T07:18:10Z doi:10.70122/FK2/ZR9M6C\n4 2022-11-22T15:27:16Z doi:10.70122/FK2/ZKZPSJ\n5 2023-02-28T11:10:06Z doi:10.70122/FK2/NJ77JM\n6 2023-06-06T17:18:42Z                    &lt;NA&gt;\n7 2023-03-06T22:13:12Z                    &lt;NA&gt;\n8 2023-06-16T16:27:13Z                    &lt;NA&gt;\n9 2023-08-16T18:57:34Z                    &lt;NA&gt;\n                               publisher\n1                                   &lt;NA&gt;\n2                                   &lt;NA&gt;\n3                        Academia Xavier\n4 UniData Bicocca Data Archive Dataverse\n5 UniData Bicocca Data Archive Dataverse\n6                                   &lt;NA&gt;\n7                                   &lt;NA&gt;\n8                                   &lt;NA&gt;\n9                                   &lt;NA&gt;\n                                                                                                                                                                                                                                                                                                                     citationHtml\n1                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n2                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n3 Brasil, Felipe Gon&ccedil;alves; Bichir, Renata, 2022, \"Policy Dynamics and Government Attention over Welfare Policies: An Analysis of the Brazilian Case\", &lt;a href=\"https://doi.org/10.70122/FK2/ZR9M6C\" target=\"_blank\"&gt;https://doi.org/10.70122/FK2/ZR9M6C&lt;/a&gt;, Demo Dataverse, V1, UNF:6:AUntVYA8//WWvy37I1fn/w== [fileUNF]\n4                                                                                                                                                  Istat, 2022, \"Aspetti della vita quotidiana (2016)\", &lt;a href=\"https://doi.org/10.70122/FK2/ZKZPSJ\" target=\"_blank\"&gt;https://doi.org/10.70122/FK2/ZKZPSJ&lt;/a&gt;, Demo Dataverse, V1\n5                                                                                                                                                  Istat, 2023, \"Aspetti della vita quotidiana (2020)\", &lt;a href=\"https://doi.org/10.70122/FK2/NJ77JM\" target=\"_blank\"&gt;https://doi.org/10.70122/FK2/NJ77JM&lt;/a&gt;, Demo Dataverse, V1\n6                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n7                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n8                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n9                                                                                                                                                                                                                                                                                                                            &lt;NA&gt;\n  identifier_of_dataverse                      name_of_dataverse\n1                    &lt;NA&gt;                                   &lt;NA&gt;\n2                    &lt;NA&gt;                                   &lt;NA&gt;\n3          academiaxavier                        Academia Xavier\n4                   dassi UniData Bicocca Data Archive Dataverse\n5                   dassi UniData Bicocca Data Archive Dataverse\n6                    &lt;NA&gt;                                   &lt;NA&gt;\n7                    &lt;NA&gt;                                   &lt;NA&gt;\n8                    &lt;NA&gt;                                   &lt;NA&gt;\n9                    &lt;NA&gt;                                   &lt;NA&gt;\n                                                                                                                                                                                                                                                citation\n1                                                                                                                                                                                                                                                   &lt;NA&gt;\n2                                                                                                                                                                                                                                                   &lt;NA&gt;\n3 Brasil, Felipe Gonçalves; Bichir, Renata, 2022, \"Policy Dynamics and Government Attention over Welfare Policies: An Analysis of the Brazilian Case\", https://doi.org/10.70122/FK2/ZR9M6C, Demo Dataverse, V1, UNF:6:AUntVYA8//WWvy37I1fn/w== [fileUNF]\n4                                                                                                                                           Istat, 2022, \"Aspetti della vita quotidiana (2016)\", https://doi.org/10.70122/FK2/ZKZPSJ, Demo Dataverse, V1\n5                                                                                                                                           Istat, 2023, \"Aspetti della vita quotidiana (2020)\", https://doi.org/10.70122/FK2/NJ77JM, Demo Dataverse, V1\n6                                                                                                                                                                                                                                                   &lt;NA&gt;\n7                                                                                                                                                                                                                                                   &lt;NA&gt;\n8                                                                                                                                                                                                                                                   &lt;NA&gt;\n9                                                                                                                                                                                                                                                   &lt;NA&gt;\n         storageIdentifier\n1                     &lt;NA&gt;\n2                     &lt;NA&gt;\n3 s3://10.70122/FK2/ZR9M6C\n4 s3://10.70122/FK2/ZKZPSJ\n5 s3://10.70122/FK2/NJ77JM\n6                     &lt;NA&gt;\n7                     &lt;NA&gt;\n8                     &lt;NA&gt;\n9                     &lt;NA&gt;\n                                                                                  keywords\n1                                                                                     NULL\n2                                                                                     NULL\n3 Policy change, Welfare policy, Punctuated equilibrium, Policy dynamics, Policy attention\n4                                                                                     NULL\n5                                                                                     NULL\n6                                                                                     NULL\n7                                                                                     NULL\n8                                                                                     NULL\n9                                                                                     NULL\n         subjects fileCount versionId versionState majorVersion minorVersion\n1            NULL        NA        NA         &lt;NA&gt;           NA           NA\n2            NULL        NA        NA         &lt;NA&gt;           NA           NA\n3 Social Sciences        10    220301     RELEASED            1            1\n4 Social Sciences         0    223199     RELEASED            1            0\n5 Social Sciences         0    226493     RELEASED            1            0\n6            NULL        NA        NA         &lt;NA&gt;           NA           NA\n7            NULL        NA        NA         &lt;NA&gt;           NA           NA\n8            NULL        NA        NA         &lt;NA&gt;           NA           NA\n9            NULL        NA        NA         &lt;NA&gt;           NA           NA\n             createdAt            updatedAt\n1                 &lt;NA&gt;                 &lt;NA&gt;\n2                 &lt;NA&gt;                 &lt;NA&gt;\n3 2022-09-06T01:38:45Z 2022-09-06T11:38:01Z\n4 2022-11-22T15:26:43Z 2022-11-22T15:27:16Z\n5 2023-02-27T12:57:28Z 2023-02-28T11:10:06Z\n6                 &lt;NA&gt;                 &lt;NA&gt;\n7                 &lt;NA&gt;                 &lt;NA&gt;\n8                 &lt;NA&gt;                 &lt;NA&gt;\n9                 &lt;NA&gt;                 &lt;NA&gt;\n                                            contacts\n1                                               NULL\n2                                               NULL\n3 Oliveira, Gustavo, Academia Xavier (Bibliotecário)\n4                                            DASSI, \n5        Data Archive for Social Sciences in Italy, \n6                                               NULL\n7                                               NULL\n8                                               NULL\n9                                               NULL\n                                                                                                                                                                                                                                publications\n1                                                                                                                                                                                                                                       NULL\n2                                                                                                                                                                                                                                       NULL\n3 Brasil, Felipe Gonçalves; Bichir, Renata. \"Policy Dynamics and Government Attention over Welfare Policies: An Analysis of the Brazilian Case.\" Braz. political sci. rev. 16 (1) • 2022. DOI: https://doi.org/10.1590/1981-3821202200010006\n4                                                                                                                                                                                                                                       NULL\n5                                                                                                                                                                                                                                       NULL\n6                                                                                                                                                                                                                                       NULL\n7                                                                                                                                                                                                                                       NULL\n8                                                                                                                                                                                                                                       NULL\n9                                                                                                                                                                                                                                       NULL\n                                   authors file_id     file_type\n1                                     NULL    &lt;NA&gt;          &lt;NA&gt;\n2                                     NULL    &lt;NA&gt;          &lt;NA&gt;\n3 Brasil, Felipe Gonçalves, Bichir, Renata    &lt;NA&gt;          &lt;NA&gt;\n4                                    Istat    &lt;NA&gt;          &lt;NA&gt;\n5                                    Istat    &lt;NA&gt;          &lt;NA&gt;\n6                                     NULL    &lt;NA&gt;          &lt;NA&gt;\n7                                     NULL 2029365 Tab-Delimited\n8                                     NULL 2069367 Tab-Delimited\n9                                     NULL 2077715 Tab-Delimited\n          file_content_type size_in_bytes                              md5\n1                      &lt;NA&gt;            NA                             &lt;NA&gt;\n2                      &lt;NA&gt;            NA                             &lt;NA&gt;\n3                      &lt;NA&gt;            NA                             &lt;NA&gt;\n4                      &lt;NA&gt;            NA                             &lt;NA&gt;\n5                      &lt;NA&gt;            NA                             &lt;NA&gt;\n6                      &lt;NA&gt;            NA                             &lt;NA&gt;\n7 text/tab-separated-values        596398 37e83da30d716060ca5d7f289de2ac5f\n8 text/tab-separated-values        908083 a2d42bc2d8acf0dd224116c96b93a77e\n9 text/tab-separated-values       1565999 2160f247e7b739c19c6b123eba8d8adb\n  checksum.type                   checksum.value                            unf\n1          &lt;NA&gt;                             &lt;NA&gt;                           &lt;NA&gt;\n2          &lt;NA&gt;                             &lt;NA&gt;                           &lt;NA&gt;\n3          &lt;NA&gt;                             &lt;NA&gt;                           &lt;NA&gt;\n4          &lt;NA&gt;                             &lt;NA&gt;                           &lt;NA&gt;\n5          &lt;NA&gt;                             &lt;NA&gt;                           &lt;NA&gt;\n6          &lt;NA&gt;                             &lt;NA&gt;                           &lt;NA&gt;\n7           MD5 37e83da30d716060ca5d7f289de2ac5f UNF:6:7pIYE/ETWB2c1Jei5fwA6w==\n8           MD5 a2d42bc2d8acf0dd224116c96b93a77e UNF:6:aP35eeDYtLi99gqAeHK5yA==\n9           MD5 2160f247e7b739c19c6b123eba8d8adb UNF:6:xvDAh14r4cr+Mec/kK5suQ==\n              file_persistent_id\n1                           &lt;NA&gt;\n2                           &lt;NA&gt;\n3                           &lt;NA&gt;\n4                           &lt;NA&gt;\n5                           &lt;NA&gt;\n6                           &lt;NA&gt;\n7 doi:10.70122/FK2/W5RRCR/FLR3JK\n8 doi:10.70122/FK2/HWIRHD/IOGVHM\n9 doi:10.70122/FK2/HUUQPF/KC6EHB\n                                                                                dataset_name\n1                                                                                       &lt;NA&gt;\n2                                                                                       &lt;NA&gt;\n3                                                                                       &lt;NA&gt;\n4                                                                                       &lt;NA&gt;\n5                                                                                       &lt;NA&gt;\n6                                                                                       &lt;NA&gt;\n7 Replication Data for \"School Shootings, Protests and the Gun Culture in the United States\"\n8                                                                                      test2\n9                                                              Darwin's Finches: CC0 License\n  dataset_id   dataset_persistent_id\n1       &lt;NA&gt;                    &lt;NA&gt;\n2       &lt;NA&gt;                    &lt;NA&gt;\n3       &lt;NA&gt;                    &lt;NA&gt;\n4       &lt;NA&gt;                    &lt;NA&gt;\n5       &lt;NA&gt;                    &lt;NA&gt;\n6       &lt;NA&gt;                    &lt;NA&gt;\n7    2029363 doi:10.70122/FK2/W5RRCR\n8    2008345 doi:10.70122/FK2/HWIRHD\n9    2023467 doi:10.70122/FK2/HUUQPF\n                                                                                                                                                                                                     dataset_citation\n1                                                                                                                                                                                                                &lt;NA&gt;\n2                                                                                                                                                                                                                &lt;NA&gt;\n3                                                                                                                                                                                                                &lt;NA&gt;\n4                                                                                                                                                                                                                &lt;NA&gt;\n5                                                                                                                                                                                                                &lt;NA&gt;\n6                                                                                                                                                                                                                &lt;NA&gt;\n7 Olzak, Susan, 2023, \"Replication Data for \"School Shootings, Protests and the Gun Culture in the United States\"\", https://doi.org/10.70122/FK2/W5RRCR, Demo Dataverse, V1, UNF:6:3FFG3U7z859Y+Ujet+CeKA== [fileUNF]\n8                                                                                        Myers, Jim, 2022, \"test2\", https://doi.org/10.70122/FK2/HWIRHD, Demo Dataverse, V2, UNF:6:aP35eeDYtLi99gqAeHK5yA== [fileUNF]\n9                                                              Finch, Fiona, 2023, \"Darwin's Finches: CC0 License\", https://doi.org/10.70122/FK2/HUUQPF, Demo Dataverse, V3, UNF:6:e1mgFxJtO5XDCjJXlHrP7g== [fileUNF]\n\n$data$count_in_response\n[1] 9\n\n\nWe could look at the dataset using View(politics_datasets), which would allow us to expand the tree based on what we are interested in. We can even get the code that we need to focus on different aspects by hovering on the item and then clicking the icon with the green arrow (Figure 7.4).\n\n\n\nFigure 7.4: Example of hovering over a JSON element, “items”, where the icon with a green arrow can be clicked on to get the code that would focus on that element\n\n\nThis tells us how to obtain the dataset of interest.\n\nas_tibble(politics_datasets[[\"data\"]][[\"items\"]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Spotify\nSometimes there is an R package built around an API and allows us to interact with it in ways that are similar what we have seen before. For instance, spotifyr is a wrapper around the Spotify API. When using APIs, even when they are wrapped in an R package, in this case spotifyr, it is important to read the terms under which access is provided.\nTo access the Spotify API, we need a Spotify Developer Account. This is free but will require logging in with a Spotify account and then accepting the Developer Terms (Figure 7.5).\n\n\n\nFigure 7.5: Spotify Developer Account Terms agreement page\n\n\nContinuing with the registration process, in our case, we “do not know” what we are building and so Spotify requires us to use a non-commercial agreement which is fine. To use the Spotify API we need a “Client ID” and a “Client Secret”. These are things that we want to keep to ourselves because otherwise anyone with the details could use our developer account as though they were us. One way to keep these details secret with minimum hassle is to keep them in our “System Environment”. In this way, when we push to GitHub they should not be included. To do this we will load and use usethis to modify our System Environment. In particular, there is a file called “.Renviron” which we will open and then add our “Client ID” and “Client Secret”.\n\nedit_r_environ()\n\nWhen we run edit_r_environ(), a “.Renviron” file will open and we can add our “Spotify Client ID” and “Client Secret”. Use the same names, because spotifyr will look in our environment for keys with those specific names. Being careful to use single quotes is important here even though we normally use double quotes in this book.\n\nSPOTIFY_CLIENT_ID = 'PUT_YOUR_CLIENT_ID_HERE'\nSPOTIFY_CLIENT_SECRET = 'PUT_YOUR_SECRET_HERE'\n\nSave the “.Renviron” file, and then restart R: “Session” \\(\\rightarrow\\) “Restart R”. We can now use our “Spotify Client ID” and “Client Secret” as needed. And functions that require those details as arguments will work without them being explicitly specified again.\nTo try this out we install and load spotifyr. We will get and save some information about Radiohead, the English rock band, using get_artist_audio_features(). One of the required arguments is authorization, but as that is set, by default, to look at the “.Renviron” file, we do not need to specify it here.\n\nradiohead &lt;- get_artist_audio_features(\"radiohead\")\nsaveRDS(radiohead, \"radiohead.rds\")\n\n\nradiohead &lt;- readRDS(\"radiohead.rds\")\n\nThere is a variety of information available based on songs. We might be interested to see whether their songs are getting longer over time (Figure 7.6). Following the guidance in Chapter 5 this is a nice opportunity to additionally use a boxplot to communicate summary statistics by album at the same time.\n\nradiohead &lt;- as_tibble(radiohead)\n\nradiohead |&gt;\n  mutate(album_release_date = ymd(album_release_date)) |&gt;\n  ggplot(aes(\n    x = album_release_date,\n    y = duration_ms,\n    group = album_release_date\n  )) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.5, width = 0.3, height = 0) +\n  theme_minimal() +\n  labs(\n    x = \"Album release date\",\n    y = \"Duration of song (ms)\"\n  )\n\n\n\n\nFigure 7.6: Length of each Radiohead song, over time, as gathered from Spotify\n\n\n\n\nOne interesting variable provided by Spotify about each song is “valence”. The Spotify documentation describes this as a measure between zero and one that signals “the musical positiveness” of the track with higher values being more positive. We might be interested to compare valence over time between a few artists, for instance, Radiohead, the American rock band The National, and the American singer Taylor Swift.\nFirst, we need to gather the data.\n\ntaylor_swift &lt;- get_artist_audio_features(\"taylor swift\")\nthe_national &lt;- get_artist_audio_features(\"the national\")\n\nsaveRDS(taylor_swift, \"taylor_swift.rds\")\nsaveRDS(the_national, \"the_national.rds\")\n\nThen we can bring them together and make the graph (Figure 7.7). This appears to show that while Taylor Swift and Radiohead have largely maintained their level of valence over time, The National has decreased theirs.\n\nrbind(taylor_swift, the_national, radiohead) |&gt;\n  select(artist_name, album_release_date, valence) |&gt;\n  mutate(album_release_date = ymd(album_release_date)) |&gt; \n  ggplot(aes( x = album_release_date, y = valence, color = artist_name)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth() +\n  theme_minimal() +\n  facet_wrap(facets = vars(artist_name), dir = \"v\") +\n  labs(\n    x = \"Album release date\",\n    y = \"Valence\",\n    color = \"Artist\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 7.7: Comparing valence, over time, for Radiohead, Taylor Swift, and The National\n\n\n\n\nHow amazing that we live in a world where all that information is available with very little effort or cost! And having gathered the data, there is a lot that could be done. For instance, Pavlik (2019) uses an expanded dataset to classify musical genres and The Economist (2022) looks at how language is associated with music streaming on Spotify. Our ability to gather such data enables us to answer questions that had to be considered experimentally in the past. For instance, Salganik, Dodds, and Watts (2006) had to use experimental data to analyze the social aspect of what makes a hit song, rather than the observational data we can now access.\nThat said, it is worth thinking about what valence is purporting to measure. Little information is available in the Spotify documentation how it was created. It is doubtful that one number can completely represent how positive is a song. And what about the songs from these artists that are not on Spotify, or even publicly released? This is a nice example of how measurement and sampling pervade all aspects of telling stories with data."
  },
  {
    "objectID": "07-gather.html#web-scraping",
    "href": "07-gather.html#web-scraping",
    "title": "7  Gather data",
    "section": "7.3 Web scraping",
    "text": "7.3 Web scraping\n\n7.3.1 Principles\nWeb scraping is a way to get data from websites. Rather than going to a website using a browser and then saving a copy of it, we write code that does it for us. This opens a lot of data to us, but on the other hand, it is not typically data that are being made available for these purposes. This means that it is especially important to be respectful. While generally not illegal, the specifics about the legality of web scraping depend on jurisdictions and what we are doing, and so it is also important to be mindful. Even if our use is not commercially competitive, of particular concern is the conflict between the need for our work to be reproducible with the need to respect terms of service that may disallow data republishing (Luscombe, Dick, and Walby 2021).\nPrivacy often trumps reproducibility. There is also a considerable difference between data being publicly available on a website and being scraped, cleaned, and prepared into a dataset which is then publicly released. For instance, Kirkegaard and Bjerrekær (2016) scraped publicly available OKCupid profiles and then made the resulting dataset easily available (Hackett 2016). Zimmer (2018) details some of the important considerations that were overlooked including “minimizing harm”, “informed consent”, and ensuring those in the dataset maintain “privacy and confidentiality”. While it is correct to say that OKCupid made data public, they did so in a certain context, and when their data was scraped that context was changed.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nPolice violence is particularly concerning because of the need for trust between the police and society. Without good data it is difficult to hold police departments accountable, or know whether there is an issue, but getting data is difficult (Thomson-DeVeaux, Bronner, and Sharma 2021). The fundamental problem is that there is no way to easily simplify an encounter that results in violence into a dataset. Two popular datasets draw on web scraping:\n\n“Mapping Police Violence”; and\n“Fatal Force Database”.\n\nBor et al. (2018) use “Mapping Police Violence” to examine police killings of Black Americans, especially when unarmed, and find a substantial effect on the mental health of Black Americans. Responses to the paper, such as Nix and Lozada (2020), have special concern with the coding of the dataset, and after re-coding draw different conclusions. An example of a coding difference is the unanswerable question, because it depends on context and usage, of whether to code an individual who was killed with a toy firearm as “armed” or “unarmed”. We may want a separate category, but some simplification is necessary for the construction of a quantitative dataset. The Washington Post writes many articles using the “Fatal Force Database” (The Washington Post 2023). Jenkins et al. (2022) describes their methodology and the challenges of standardization. Comer and Ingram (2022) compare the datasets and find similarities, but document ways in which the datasets are different.\n\n\nWeb scraping is an invaluable source of data. But they are typically datasets that can be created as a by-product of someone trying to achieve another aim. And web scraping imposes a cost on the website host, and so we should reduce this to the extent possible. For instance, a retailer may have a website with their products and their prices. That has not been created deliberately as a source of data, but we can scrape it to create a dataset. The following principles may be useful to guide web scraping.\n\nAvoid it. Try to use an API wherever possible.\nAbide by their desires. Some websites have a “robots.txt” file that contains information about what they are comfortable with scrapers doing. In general, if it exists, a “robots.txt” file can be accessed by appending “robots.txt” to the base URL. For instance, the “robots.txt” file for https://www.google.com, can be accessed at https://www.google.com/robots.txt. Note if there are folders listed against “Disallow:”. These are the folders that the website would not like to be scraped. And also note any instances of “Crawl-delay:”. This is the number of seconds the website would like you to wait between visits.\nReduce the impact.\n\nSlow down the scraper, for instance, rather than having it visit the website every second, slow it down using sys.sleep(). If you only need a few hundred files, then why not just have it visit the website a few times a minute, running in the background overnight?\nConsider the timing of when you run the scraper. For instance, if you are scraping a retailer then maybe set the script to run from 10pm through to the morning, when fewer customers are likely using the site. Similarly, if it is a government website and they have a regular monthly release, then it might be polite to avoid that day.\n\nTake only what is needed. For instance, you do not need to scrape the entirety of Wikipedia if all you need is the names of the ten largest cities in Croatia. This reduces the impact on the website, and allows us to more easily justify our actions.\nOnly scrape once. This means you should save everything as you go so that you do not have to re-collect data when the scraper inevitably fails at some point. For instance, you will typically spend a lot of time getting a scraper working on one page, but typically the page structure will change at some point and the scraper will need to be updated. Once you have the data, you should save that original, unedited data separately to the modified data. If you need data over time then you will need to go back, but this is different than needlessly re-scraping a page.\nDo not republish the pages that were scraped (this contrasts with datasets that you create from it).\nTake ownership and ask permission if possible. At a minimum all scripts should have contact details in them. Depending on the circumstances, it may be worthwhile asking for permission before you scrape.\n\n\n\n7.3.2 HTML/CSS essentials\nWeb scraping is possible by taking advantage of the underlying structure of a webpage. We use patterns in the HTML/CSS to get the data that we want. To look at the underlying HTML/CSS we can either:\n\nopen a browser, right-click, and choose something like “Inspect”; or\nsave the website and then open it with a text editor rather than a browser.\n\nHTML/CSS is a markup language based on matching tags. If we want text to be bold, then we would use something like:\n&lt;b&gt;My bold text&lt;/b&gt;\nSimilarly, if we want a list, then we start and end the list as well as indicating each item.\n&lt;ul&gt;\n  &lt;li&gt;Learn webscraping&lt;/li&gt;\n  &lt;li&gt;Do data science&lt;/li&gt;\n  &lt;li&gt;Profit&lt;/li&gt;\n&lt;/ul&gt;\nWhen scraping we will search for these tags.\nTo get started, we can pretend that we obtained some HTML from a website, and that we want to get the name from it. We can see that the name is in bold, so we want to focus on that feature and extract it.\n\nwebsite_extract &lt;- \"&lt;p&gt;Hi, I’m &lt;b&gt;Rohan&lt;/b&gt; Alexander.&lt;/p&gt;\"\n\nrvest is part of the tidyverse so it does not have to be installed, but it is not part of the core, so it does need to be loaded. After that, use read_html() to read in the data.\n\nrohans_data &lt;- read_html(website_extract)\n\nrohans_data\n\n{html_document}\n&lt;html&gt;\n[1] &lt;body&gt;&lt;p&gt;Hi, I’m &lt;b&gt;Rohan&lt;/b&gt; Alexander.&lt;/p&gt;&lt;/body&gt;\n\n\nThe language used by rvest to look for tags is “node”, so we focus on bold nodes. By default html_elements() returns the tags as well. We extract the text with html_text().\n\nrohans_data |&gt;\n  html_elements(\"b\")\n\n{xml_nodeset (1)}\n[1] &lt;b&gt;Rohan&lt;/b&gt;\n\nrohans_data |&gt;\n  html_elements(\"b\") |&gt;\n  html_text()\n\n[1] \"Rohan\"\n\n\nWeb scraping is an exciting source of data, and we will now go through some examples. But in contrast to these examples, information is not usually all on one page. Web scraping quickly becomes a difficult art form that requires a lot of practice. For instance, we distinguish between an index scrape and a contents scrape. The former is scraping to build the list of URLs that have the content you want, while the latter is to get the content from those URLs. An example is provided by Luscombe, Duncan, and Walby (2022). If you end up doing a lot of web scraping, then polite (Perepolkin 2022) may be helpful to better optimize your workflow. And using GitHub Actions to allow for larger and slower scrapes over time.\n\n\n7.3.3 Book information\nIn this case study we will scrape a list of books available here. We will then clean the data and look at the distribution of the first letters of author surnames. It is slightly more complicated than the example above, but the underlying workflow is the same: download the website, look for the nodes of interest, extract the information, and clean it.\nWe use rvest to download a website, and to then navigate the HTML to find the aspects that we are interested in. And we use the tidyverse to clean the dataset. We first need to go to the website and then save a local copy.\n\nbooks_data &lt;- read_html(\"https://rohansbooks.com\")\n\nwrite_html(books_data, \"raw_data.html\")\n\nWe need to navigate the HTML to get the aspects that we want. And then try to get the data into a tibble as quickly as possible because this will allow us to more easily use dplyr verbs and other functions from the tidyverse.\nSee Online Appendix A if this is unfamiliar to you.\n\nbooks_data &lt;- read_html(\"raw_data.html\")\n\n\nbooks_data\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n    &lt;h1&gt;Books&lt;/h1&gt;\\n\\n    &lt;p&gt;\\n      This is a list of books that ...\n\n\nTo get the data into a tibble we first need to use HTML tags to identify the data that we are interested in. If we look at the website then we know we need to focus on list items (Figure 7.8 (a)). And we can look at the source, focusing particularly on looking for a list (Figure 7.8 (b)).\n\n\n\n\n\n\n\n(a) Books website as displayed\n\n\n\n\n\n\n\n(b) HTML for the top of the books website and the list of books\n\n\n\n\nFigure 7.8: Screen captures from the books website as at 16 June 2022\n\n\nThe tag for a list item is “li”, so we can use that to focus on the list.\n\ntext_data &lt;-\n  books_data |&gt;\n  html_elements(\"li\") |&gt;\n  html_text()\n\nall_books &lt;-\n  tibble(books = text_data)\n\nhead(all_books)\n\n# A tibble: 6 × 1\n  books                                                                         \n  &lt;chr&gt;                                                                         \n1 \"\\n        Agassi, Andre, 2009, Open\\n      \"                                 \n2 \"\\n        Cramer, Richard Ben, 1992, What It Takes: The Way to the White Hou…\n3 \"\\n        DeWitt, Helen, 2000, The Last Samurai\\n      \"                     \n4 \"\\n        Gelman, Andrew and Jennifer Hill, 2007, Data Analysis Using Regres…\n5 \"\\n        Halberstam, David, 1972, The Best and the Brightest\\n      \"       \n6 \"\\n        Ignatieff, Michael, 2013, Fire and Ashes: Success and Failure in P…\n\n\nWe now need to clean the data. First we want to separate the title and the author using separate() and then clean up the author and title columns. We can take advantage of the fact that the year is present and separate based on that.\n\nall_books &lt;-\n  all_books |&gt;\n  mutate(books = str_squish(books)) |&gt;\n  separate(books, into = c(\"author\", \"title\"), sep = \"\\\\, [[:digit:]]{4}\\\\, \")\n\nhead(all_books)\n\n# A tibble: 6 × 2\n  author                           title                                        \n  &lt;chr&gt;                            &lt;chr&gt;                                        \n1 Agassi, Andre                    Open                                         \n2 Cramer, Richard Ben              What It Takes: The Way to the White House    \n3 DeWitt, Helen                    The Last Samurai                             \n4 Gelman, Andrew and Jennifer Hill Data Analysis Using Regression and Multileve…\n5 Halberstam, David                The Best and the Brightest                   \n6 Ignatieff, Michael               Fire and Ashes: Success and Failure in Polit…\n\n\nFinally, we could make, say, a table of the distribution of the first letter of the names (Table 7.1).\n\nall_books |&gt;\n  mutate(\n    first_letter = str_sub(author, 1, 1)\n  ) |&gt;\n  count(.by = first_letter) |&gt;\n  kable(\n    col.names = c(\"First letter\", \"Number of times\")\n  )\n\n\n\nTable 7.1: Distribution of first letter of author names in a collection of books\n\n\nFirst letter\nNumber of times\n\n\n\n\nA\n1\n\n\nC\n1\n\n\nD\n1\n\n\nG\n1\n\n\nH\n1\n\n\nI\n1\n\n\nL\n1\n\n\nM\n1\n\n\nP\n3\n\n\nR\n1\n\n\nV\n2\n\n\nW\n4\n\n\nY\n1\n\n\n\n\n\n\n\n\n7.3.4 Prime Ministers of the United Kingdom\nIn this case study we are interested in how long prime ministers of the United Kingdom lived, based on the year they were born. We will scrape data from Wikipedia using rvest, clean it, and then make a graph. From time to time a website will change. This makes many scrapes largely bespoke, even if we can borrow some code from earlier projects. It is normal to feel frustrated at times. It helps to begin with an end in mind.\nTo that end, we can start by generating some simulated data. Ideally, we want a table that has a row for each prime minister, a column for their name, and a column each for the birth and death years. If they are still alive, then that death year can be empty. We know that birth and death years should be somewhere between 1700 and 1990, and that death year should be larger than birth year. Finally, we also know that the years should be integers, and the names should be characters. We want something that looks roughly like this:\n\nset.seed(853)\n\nsimulated_dataset &lt;-\n  tibble(\n    prime_minister = babynames |&gt;\n      filter(prop &gt; 0.01) |&gt;\n      distinct(name) |&gt;\n      unlist() |&gt;\n      sample(size = 10, replace = FALSE),\n    birth_year = sample(1700:1990, size = 10, replace = TRUE),\n    years_lived = sample(50:100, size = 10, replace = TRUE),\n    death_year = birth_year + years_lived\n  ) |&gt;\n  select(prime_minister, birth_year, death_year, years_lived) |&gt;\n  arrange(birth_year)\n\nsimulated_dataset\n\n# A tibble: 10 × 4\n   prime_minister birth_year death_year years_lived\n   &lt;chr&gt;               &lt;int&gt;      &lt;int&gt;       &lt;int&gt;\n 1 Kevin                1813       1908          95\n 2 Karen                1832       1896          64\n 3 Robert               1839       1899          60\n 4 Bertha               1846       1915          69\n 5 Jennifer             1867       1943          76\n 6 Arthur               1892       1984          92\n 7 Donna                1907       2006          99\n 8 Emma                 1957       2031          74\n 9 Ryan                 1959       2053          94\n10 Tyler                1990       2062          72\n\n\nOne of the advantages of generating a simulated dataset is that if we are working in groups then one person can start making the graph, using the simulated dataset, while the other person gathers the data. In terms of a graph, we are aiming for something like Figure 7.9.\n\n\n\nFigure 7.9: Sketch of planned graph showing how long United Kingdom prime ministers lived\n\n\nWe are starting with a question that is of interest, which is how long each prime minister of the United Kingdom lived. As such, we need to identify a source of data. While there are plenty of data sources that have the births and deaths of each prime minister, we want one that we can trust, and as we are going to be scraping, we want one that has some structure to it. The Wikipedia page about prime ministers of the United Kingdom fits both these criteria. As it is a popular page the information is likely to be correct, and the data are available in a table.\nWe load rvest and then download the page using read_html(). Saving it locally provides us with a copy that we need for reproducibility in case the website changes, and means that we do not have to keep visiting the website. But it is not ours, and so this is typically not something that should be publicly redistributed.\n\nraw_data &lt;-\n  read_html(\n    \"https://en.wikipedia.org/wiki/List_of_prime_ministers_of_the_United_Kingdom\"\n  )\nwrite_html(raw_data, \"pms.html\")\n\nAs with the earlier case study, we are looking for patterns in the HTML that we can use to help us get closer to the data that we want. This is an iterative process and requires a lot of trial and error. Even simple examples will take time.\nOne tool that may help is the SelectorGadget. This allows us to pick and choose the elements that we want, and then gives us the input for html_element() (Figure 7.10). By default, SelectorGadget uses CSS selectors. These are not the only way to specify the location of the information you want, and using an alternative, such as XPath, can be a useful option to consider.\n\n\n\nFigure 7.10: Using the Selector Gadget to identify the tag, as at 12 February 2023\n\n\n\nraw_data &lt;- read_html(\"pms.html\")\n\n\nparse_data_selector_gadget &lt;-\n  raw_data |&gt;\n  html_element(\".wikitable\") |&gt;\n  html_table()\n\nhead(parse_data_selector_gadget)\n\n# A tibble: 6 × 11\n  Portrait Portrait   Prime ministerOffice(L…¹ `Term of office` `Term of office`\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;                    &lt;chr&gt;            &lt;chr&gt;           \n1 Portrait \"Portrait\" Prime ministerOffice(Li… start            end             \n2 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n3 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n4 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n5 ​         \"\"         Robert Walpole[27]MP fo… 3 April1721      11 February1742 \n6 ​         \"\"         Spencer Compton[28]1st … 16 February1742  2 July1743      \n# ℹ abbreviated name: ¹​`Prime ministerOffice(Lifespan)`\n# ℹ 6 more variables: `Term of office` &lt;chr&gt;, `Mandate[a]` &lt;chr&gt;,\n#   `Ministerial offices held as prime minister` &lt;chr&gt;, Party &lt;chr&gt;,\n#   Government &lt;chr&gt;, MonarchReign &lt;chr&gt;\n\n\nIn this case there are many columns that we do not need, and some duplicated rows.\n\nparsed_data &lt;-\n  parse_data_selector_gadget |&gt; \n  clean_names() |&gt; \n  rename(raw_text = prime_minister_office_lifespan) |&gt; \n  select(raw_text) |&gt; \n  filter(raw_text != \"Prime ministerOffice(Lifespan)\") |&gt; \n  distinct() \n\nhead(parsed_data)\n\n# A tibble: 6 × 1\n  raw_text                                                \n  &lt;chr&gt;                                                   \n1 Robert Walpole[27]MP for King's Lynn(1676–1745)         \n2 Spencer Compton[28]1st Earl of Wilmington(1673–1743)    \n3 Henry Pelham[29]MP for Sussex(1694–1754)                \n4 Thomas Pelham-Holles[30]1st Duke of Newcastle(1693–1768)\n5 William Cavendish[31]4th Duke of Devonshire(1720–1764)  \n6 Thomas Pelham-Holles[32]1st Duke of Newcastle(1693–1768)\n\n\nNow that we have the parsed data, we need to clean it to match what we wanted. We want a names column, as well as columns for birth year and death year. We use separate() to take advantage of the fact that it looks like the names and dates are distinguished by brackets. The argument in str_extract() is a regular expression. It looks for four digits in a row, followed by a dash, followed by four more digits in a row. We use a slightly different regular expression for those prime ministers who are still alive.\n\ninitial_clean &lt;-\n  parsed_data |&gt;\n  separate(\n    raw_text, into = c(\"name\", \"not_name\"), sep = \"\\\\[\", extra = \"merge\",\n  ) |&gt; \n  mutate(date = str_extract(not_name, \"[[:digit:]]{4}–[[:digit:]]{4}\"),\n         born = str_extract(not_name, \"born[[:space:]][[:digit:]]{4}\")\n         ) |&gt;\n  select(name, date, born)\n  \nhead(initial_clean)\n\n# A tibble: 6 × 3\n  name                 date      born \n  &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;\n1 Robert Walpole       1676–1745 &lt;NA&gt; \n2 Spencer Compton      1673–1743 &lt;NA&gt; \n3 Henry Pelham         1694–1754 &lt;NA&gt; \n4 Thomas Pelham-Holles 1693–1768 &lt;NA&gt; \n5 William Cavendish    1720–1764 &lt;NA&gt; \n6 Thomas Pelham-Holles 1693–1768 &lt;NA&gt; \n\n\nFinally, we need to clean up the columns.\n\ncleaned_data &lt;-\n  initial_clean |&gt;\n  separate(date, into = c(\"birth\", \"died\"), \n           sep = \"–\") |&gt;   # PMs who have died have their birth and death years \n  # separated by a hyphen, but we need to be careful with the hyphen as it seems \n  # to be a slightly odd type of hyphen and we need to copy/paste it.\n  mutate(\n    born = str_remove_all(born, \"born[[:space:]]\"),\n    birth = if_else(!is.na(born), born, birth)\n  ) |&gt; # Alive PMs have slightly different format\n  select(-born) |&gt;\n  rename(born = birth) |&gt; \n  mutate(across(c(born, died), as.integer)) |&gt; \n  mutate(Age_at_Death = died - born) |&gt; \n  distinct() # Some of the PMs had two goes at it.\n\nhead(cleaned_data)\n\n# A tibble: 6 × 4\n  name                  born  died Age_at_Death\n  &lt;chr&gt;                &lt;int&gt; &lt;int&gt;        &lt;int&gt;\n1 Robert Walpole        1676  1745           69\n2 Spencer Compton       1673  1743           70\n3 Henry Pelham          1694  1754           60\n4 Thomas Pelham-Holles  1693  1768           75\n5 William Cavendish     1720  1764           44\n6 John Stuart           1713  1792           79\n\n\nOur dataset looks similar to the one that we said we wanted at the start (Table 7.2).\n\ncleaned_data |&gt;\n  head() |&gt;\n  kable(\n    col.names = c(\"Prime Minister\", \"Birth year\", \"Death year\", \"Age at death\")\n    )\n\n\n\nTable 7.2: UK Prime Ministers, by how old they were when they died\n\n\nPrime Minister\nBirth year\nDeath year\nAge at death\n\n\n\n\nRobert Walpole\n1676\n1745\n69\n\n\nSpencer Compton\n1673\n1743\n70\n\n\nHenry Pelham\n1694\n1754\n60\n\n\nThomas Pelham-Holles\n1693\n1768\n75\n\n\nWilliam Cavendish\n1720\n1764\n44\n\n\nJohn Stuart\n1713\n1792\n79\n\n\n\n\n\n\nAt this point we would like to make a graph that illustrates how long each prime minister lived (Figure 7.11). If they are still alive then we would like them to run to the end, but we would like to color them differently.\n\ncleaned_data |&gt;\n  mutate(\n    still_alive = if_else(is.na(died), \"Yes\", \"No\"),\n    died = if_else(is.na(died), as.integer(2023), died)\n  ) |&gt;\n  mutate(name = as_factor(name)) |&gt;\n  ggplot(\n    aes(x = born, xend = died, y = name, yend = name, color = still_alive)\n    ) +\n  geom_segment() +\n  labs(\n    x = \"Year of birth\", y = \"Prime minister\", color = \"PM is currently alive\"\n    ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 7.11: How long each prime minister of the United Kingdom lived\n\n\n\n\n\n\n7.3.5 Iteration\nConsidering text as data is exciting and opens a lot of different research questions. We will draw on it in Chapter 16. Many guides assume that we already have a nicely formatted text dataset, but that is rarely actually the case. In this case study we will download files from a few different pages. While we have already seen two examples of web scraping, those were focused on just one page, whereas we often need many. Here we will focus on this iteration. We will use download.file() to do the download, and use purrr to apply this function across multiple sites. You do not need to install or load that package because it is part of the core tidyverse so it is loaded when you load the tidyverse.\nThe Reserve Bank of Australia (RBA) is Australia’s central bank. It has responsibility for setting the cash rate, which is the interest rate used for loans between banks. This interest rate is an especially important one and has a large impact on the other interest rates in the economy. Four times a year—February, May, August, and November—the RBA publishes a statement on monetary policy, and these are available as PDFs. In this example we will download two statements published in 2023.\nFirst we set up a tibble that has the information that we need. We will take advantage of commonalities in the structure of the URLs. We need to specify both a URL and a local file name for each state.\n\nfirst_bit &lt;- \"https://www.rba.gov.au/publications/smp/2023/\"\nlast_bit &lt;- \"/pdf/overview.pdf\"\n\nstatements_of_interest &lt;-\n  tibble(\n    address =\n      c(\n        paste0(first_bit, \"feb\", last_bit),\n        paste0(first_bit, \"may\", last_bit)\n      ),\n    local_save_name = c(\"2023-02.pdf\", \"2023-05.pdf\")\n    )\n\n\nstatements_of_interest\n\n# A tibble: 2 × 2\n  address                                                        local_save_name\n  &lt;chr&gt;                                                          &lt;chr&gt;          \n1 https://www.rba.gov.au/publications/smp/2023/feb/pdf/overview… 2023-02.pdf    \n2 https://www.rba.gov.au/publications/smp/2023/may/pdf/overview… 2023-05.pdf    \n\n\nWe want to apply the function download.files() to these two statements. To do this we write a function that will download the file, let us know that it was downloaded, wait a polite amount of time, and then go get the next file.\n\nvisit_download_and_wait &lt;-\n  function(address_to_visit,\n           where_to_save_it_locally) {\n    download.file(url = address_to_visit,\n                  destfile = where_to_save_it_locally)\n    \n    print(paste(\"Done with\", address_to_visit, \"at\", Sys.time()))\n    \n    Sys.sleep(sample(5:10, 1))\n  }\n\nWe now apply that function to our tibble of URLs and save names using the function walk2().\n\nwalk2(\n  statements_of_interest$address,\n  statements_of_interest$local_save_name,\n  ~ visit_download_and_wait(.x, .y)\n)\n\nThe result is that we have downloaded these PDFs and saved them to our computer. An alternative to writing these functions ourselves would be to use heapsofpapers (Alexander and Mahfouz 2021). This includes various helpful options for downloading lists of files, especially PDF, CSV, and txt files. For instance, Collins and Alexander (2022) use this to obtain thousands of PDFs and estimate the extent to which COVID-19 research was reproducible. In the next section we will build on this to discuss getting information from PDFs."
  },
  {
    "objectID": "07-gather.html#pdfs",
    "href": "07-gather.html#pdfs",
    "title": "7  Gather data",
    "section": "7.4 PDFs",
    "text": "7.4 PDFs\nPDF files were developed in the 1990s by the technology company Adobe. They are useful for documents because they are meant to display in a consistent way independent of the environment that created them or the environment in which they are being viewed. A PDF viewed on an iPhone should look the same as on an Android phone, as on a Linux desktop. One feature of PDFs is that they can include a variety of objects, for instance, text, photos, figures, etc. However, this variety can limit the capacity of PDFs to be used directly as data. The data first needs to be extracted from the PDF.\nIt is often possible to copy and paste the data from the PDF. This is more likely when the PDF only contains text or regular tables. In particular, if the PDF has been created by an application such as Microsoft Word, or another document- or form-creation system, then often the text data can be extracted in this way because they are actually stored as text within the PDF. We begin with that case. But it is not as easy if the text has been stored as an image which is then part of the PDF. This may be the case for PDFs produced through scans or photos of physical documents, and some older document preparation software. We go through that case later.\nIn contrast to an API, a PDF is usually produced for human rather than computer consumption. The nice thing about PDFs is that they are static and constant. And it is great that data are available. But the trade-off is that:\n\nIt is not overly useful to do larger-scale data.\nWe do not know how the PDF was put together so we do not know whether we can trust it.\nWe cannot manipulate the data to get results that we are interested in.\n\nThere are two important aspects to keep in mind when extracting data from a PDF:\n\nBegin with an end in mind. Plan and sketch what we want from a final dataset/graph/paper to limit time wastage.\nStart simple, then iterate. The quickest way to make something that needs to be complicated is often to first build a simple version and then add to it. Start with just trying to get one page of the PDF working or even just one line. Then iterate from there.\n\nWe will go through several examples and then go through a case study where we will gather data on United States Total Fertility Rate, by state.\n\n7.4.1 Jane Eyre\nFigure 7.12 is a PDF that consists of just the first sentence from Charlotte Brontë’s novel Jane Eyre taken from Project Gutenberg (Brontë 1847). You can get it here. If we assume that it was saved as “first_example.pdf”, then after installing and loading pdftools to get the text from this one-page PDF into R.\n\n\n\nFigure 7.12: First sentence of Jane Eyre\n\n\n\nfirst_example &lt;- pdf_text(\"first_example.pdf\")\n\nfirst_example\n\nclass(first_example)\n\n\n\n[1] \"There was no possibility of taking a walk that day.\\n\"\n\n\n[1] \"character\"\n\n\nWe can see that the PDF has been correctly read in, as a character vector.\nWe will now try a slightly more complicated example that consists of the first few paragraphs of Jane Eyre (Figure 7.13). Now we have the chapter heading as well.\n\n\n\nFigure 7.13: First few paragraphs of Jane Eyre\n\n\nWe use the same function as before.\n\nsecond_example &lt;- pdf_text(\"second_example.pdf\")\nclass(second_example)\nsecond_example\n\n\n\n[1] \"character\"\n\n\n[1] \"CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\n\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n\\n“What does Bessie say I have done?” I asked.\\n\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\n\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\n\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n\"\n\n\nAgain, we have a character vector. The end of each line is signaled by “\\n”, but other than that it looks pretty good. Finally, we consider the first two pages.\n\nthird_example &lt;- pdf_text(\"third_example.pdf\")\nclass(third_example)\nthird_example\n\n\n\n[1] \"character\"\n\n\n[1] \"CHAPTER I\\nThere was no possibility of taking a walk that day. We had been wandering, indeed, in the\\nleafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no\\ncompany, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so\\npenetrating, that further out-door exercise was now out of the question.\\n\\nI was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the\\ncoming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the\\nchidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to\\nEliza, John, and Georgiana Reed.\\n\\nThe said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room:\\nshe lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither\\nquarrelling nor crying) looked perfectly happy. Me, she had dispensed from joining the group;\\nsaying, “She regretted to be under the necessity of keeping me at a distance; but that until she\\nheard from Bessie, and could discover by her own observation, that I was endeavouring in good\\nearnest to acquire a more sociable and childlike disposition, a more attractive and sprightly\\nmanner—something lighter, franker, more natural, as it were—she really must exclude me from\\nprivileges intended only for contented, happy, little children.”\\n\\n“What does Bessie say I have done?” I asked.\\n\\n“Jane, I don’t like cavillers or questioners; besides, there is something truly forbidding in a child\\ntaking up her elders in that manner. Be seated somewhere; and until you can speak pleasantly,\\nremain silent.”\\n\\nA breakfast-room adjoined the drawing-room, I slipped in there. It contained a bookcase: I soon\\npossessed myself of a volume, taking care that it should be one stored with pictures. I mounted\\ninto the window-seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the\\nred moreen curtain nearly close, I was shrined in double retirement.\\n\\nFolds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of\\nglass, protecting, but not separating me from the drear November day. At intervals, while\\nturning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered\\na pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless\\nrain sweeping away wildly before a long and lamentable blast.\\n\\nI returned to my book—Bewick’s History of British Birds: the letterpress thereof I cared little\\nfor, generally speaking; and yet there were certain introductory pages that, child as I was, I could\\nnot pass quite as a blank. They were those which treat of the haunts of sea-fowl; of “the solitary\\nrocks and promontories” by them only inhabited; of the coast of Norway, studded with isles from\\nits southern extremity, the Lindeness, or Naze, to the North Cape—\\n\\n“Where the Northern Ocean, in vast whirls,\\nBoils round the naked, melancholy isles\\n\"\n[2] \"Of farthest Thule; and the Atlantic surge\\nPours in among the stormy Hebrides.”\\n\\nNor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen,\\nNova Zembla, Iceland, Greenland, with “the vast sweep of the Arctic Zone, and those forlorn\\nregions of dreary space,—that reservoir of frost and snow, where firm fields of ice, the\\naccumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole,\\nand concentre the multiplied rigours of extreme cold.” Of these death-white realms I formed an\\nidea of my own: shadowy, like all the half-comprehended notions that float dim through\\nchildren’s brains, but strangely impressive. The words in these introductory pages connected\\nthemselves with the succeeding vignettes, and gave significance to the rock standing up alone in\\na sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly\\nmoon glancing through bars of cloud at a wreck just sinking.\\n\\nI cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone;\\nits gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent,\\nattesting the hour of eventide.\\n\\nThe two ships becalmed on a torpid sea, I believed to be marine phantoms.\\n\\nThe fiend pinning down the thief’s pack behind him, I passed over quickly: it was an object of\\nterror.\\n\\nSo was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a\\ngallows.\\n\\nEach picture told a story; mysterious often to my undeveloped understanding and imperfect\\nfeelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on\\nwinter evenings, when she chanced to be in good humour; and when, having brought her ironing-\\ntable to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed’s lace\\nfrills, and crimped her nightcap borders, fed our eager attention with passages of love and\\nadventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from\\nthe pages of Pamela, and Henry, Earl of Moreland.\\n\\nWith Bewick on my knee, I was then happy: happy at least in my way. I feared nothing but\\ninterruption, and that came too soon. The breakfast-room door opened.\\n\\n“Boh! Madam Mope!” cried the voice of John Reed; then he paused: he found the room\\napparently empty.\\n\\n“Where the dickens is she!” he continued. “Lizzy! Georgy! (calling to his sisters) Joan is not\\nhere: tell mama she is run out into the rain—bad animal!”\\n\\n“It is well I drew the curtain,” thought I; and I wished fervently he might not discover my hiding-\\nplace: nor would John Reed have found it out himself; he was not quick either of vision or\\nconception; but Eliza just put her head in at the door, and said at once—\\n\"                                                                                                                                                                                                            \n\n\nNotice that the first page is the first element of the character vector, and the second page is the second element. As we are most familiar with rectangular data, we will try to get it into that format as quickly as possible. And then we can use functions from the tidyverse to deal with it.\nFirst we want to convert the character vector into a tibble. At this point we may like to add page numbers as well.\n\njane_eyre &lt;- tibble(\n  raw_text = third_example,\n  page_number = c(1:2)\n)\n\nWe then want to separate the lines so that each line is an observation. We can do that by looking for “\\n” remembering that we need to escape the backslash as it is a special character.\n\njane_eyre &lt;-\n  separate_rows(jane_eyre, raw_text, sep = \"\\\\n\", convert = FALSE)\n\njane_eyre\n\n# A tibble: 93 × 2\n   raw_text                                                          page_number\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 \"CHAPTER I\"                                                                 1\n 2 \"There was no possibility of taking a walk that day. We had been…           1\n 3 \"leafless shrubbery an hour in the morning; but since dinner (Mr…           1\n 4 \"company, dined early) the cold winter wind had brought with it …           1\n 5 \"penetrating, that further out-door exercise was now out of the …           1\n 6 \"\"                                                                          1\n 7 \"I was glad of it: I never liked long walks, especially on chill…           1\n 8 \"coming home in the raw twilight, with nipped fingers and toes, …           1\n 9 \"chidings of Bessie, the nurse, and humbled by the consciousness…           1\n10 \"Eliza, John, and Georgiana Reed.\"                                          1\n# ℹ 83 more rows\n\n\n\n\n7.4.2 Total Fertility Rate in the United States\nThe United States Department of Health and Human Services Vital Statistics Report provides information about the Total Fertility Rate (TFR) for each state. The average number of births per woman if women experience the current age-specific fertility rates throughout their reproductive years. The data are available in PDFs. We can use the approaches above to get the data into a dataset.\nThe table that we are interested in is on page 40 of a PDF that is available here or here. The column of interest is labelled: “Total fertility rate” (Figure 7.14).\n\n\n\nFigure 7.14: Example Vital Statistics Report, from 2000\n\n\nThe first step when getting data out of a PDF is to sketch out what we eventually want. A PDF typically contains a lot of information, and so we should be clear about what is needed. This helps keep you focused, and prevents scope creep, but it is also helpful when thinking about data checks. We literally write down on paper what we have in mind. In this case, what is needed is a table with a column for state, year, and total fertility rate (TFR) (Figure 7.15).\n\n\n\nFigure 7.15: Planned dataset of TFR for each US state\n\n\nWe are interested in a particular column in a particular table for this PDF. Unfortunately, there is nothing magical about what is coming. This first step requires finding the PDF online, working out the link for each, and searching for the page and column name that is of interest. We have built a CSV with the details that we need and can read that in.\n\nsummary_tfr_dataset &lt;- read_csv(\n  paste0(\"https://raw.githubusercontent.com/RohanAlexander/\",\n         \"telling_stories/main/inputs/tfr_tables_info.csv\")\n  )\n\n\n\n\nYear and associated data for TFR tables {#tbl-dhslinks}\n\n\n\n\n\n\n\n\n\nYear\nPage\nTable\nColumn\nURL\n\n\n\n\n2000\n40\n10\nTotal fertility rate\nhttps://www.cdc.gov/nchs/data/nvsr/nvsr50/nvsr50_05.pdf\n\n\n\n\n\nWe first download and save the PDF using download.file().\n\ndownload.file(\n  url = summary_tfr_dataset$url[1],\n  destfile = \"year_2000.pdf\"\n)\n\nWe then read the PDF in as a character vector using pdf_text() from pdftools. And then convert it to a tibble, so that we can use familiar verbs on it.\n\ndhs_2000 &lt;- pdf_text(\"year_2000.pdf\")\n\n\ndhs_2000_tibble &lt;- tibble(raw_data = dhs_2000)\n\nhead(dhs_2000_tibble)\n\n# A tibble: 6 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 \"Volume 50, Number 5                                                         …\n2 \"2   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n3 \"                                                                            …\n4 \"4   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n\\…\n5 \"                                                                            …\n6 \"6   National Vital Statistics Report, Vol. 50, No. 5, February 12, 2002\\n\\n …\n\n\nGrab the page that is of interest (remembering that each page is an element of the character vector, hence a row in the tibble).\n\ndhs_2000_relevant_page &lt;-\n  dhs_2000_tibble |&gt;\n  slice(summary_tfr_dataset$page[1])\n\nhead(dhs_2000_relevant_page)\n\n# A tibble: 1 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\\n…\n\n\nWe want to separate the rows and use separate_rows() from tidyr, which is part of the core tidyverse.\n\ndhs_2000_separate_rows &lt;-\n  dhs_2000_relevant_page |&gt;\n  separate_rows(raw_data, sep = \"\\\\n\", convert = FALSE)\n\nhead(dhs_2000_separate_rows)\n\n# A tibble: 6 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 \"40 National Vital Statistics Report, Vol. 50, No. 5, Revised May 15, 20022\"  \n2 \"\"                                                                            \n3 \"Table 10. Number of births, birth rates, fertility rates, total fertility ra…\n4 \"United States, each State and territory, 2000\"                               \n5 \"[By place of residence. Birth rates are live births per 1,000 estimated popu…\n6 \"estimated in each area; total fertility rates are sums of birth rates for 5-…\n\n\nWe are searching for patterns that we can use. Let us look at the first ten lines of content (ignoring aspects such as headings and page numbers at the top of the page).\n\ndhs_2000_separate_rows[13:22, ] |&gt;\n  mutate(raw_data = str_remove(raw_data, \"\\\\.{40}\"))\n\n# A tibble: 10 × 1\n   raw_data                                                                     \n   &lt;chr&gt;                                                                        \n 1 \"                                  State                                    …\n 2 \"                                                                           …\n 3 \"                                                                           …\n 4 \"\"                                                                           \n 5 \"\"                                                                           \n 6 \"United States 1 ..............          4,058,814   14.7      67.5      2,1…\n 7 \"\"                                                                           \n 8 \"Alabama .......................           63,299    14.4      65.0      2,0…\n 9 \"Alaska ...........................         9,974    16.0      74.6      2,4…\n10 \"Arizona .........................         85,273    17.5      84.4      2,6…\n\n\nAnd now at just one line.\n\ndhs_2000_separate_rows[20, ] |&gt;\n  mutate(raw_data = str_remove(raw_data, \"\\\\.{40}\"))\n\n# A tibble: 1 × 1\n  raw_data                                                                      \n  &lt;chr&gt;                                                                         \n1 Alabama .......................           63,299    14.4      65.0      2,021…\n\n\nIt does not get much better than this:\n\nWe have dots separating the states from the data.\nWe have a space between each of the columns.\n\nWe can now separate this into columns. First, we want to match on when there are at least two dots (remembering that the dot is a special character and so needs to be escaped).\n\ndhs_2000_separate_columns &lt;-\n  dhs_2000_separate_rows |&gt;\n  separate(\n    col = raw_data,\n    into = c(\"state\", \"data\"),\n    sep = \"\\\\.{2,}\",\n    remove = FALSE,\n    fill = \"right\"\n  )\n\ndhs_2000_separate_columns[18:28, ] |&gt;\n  select(state, data)\n\n# A tibble: 11 × 2\n   state                   data                                                 \n   &lt;chr&gt;                   &lt;chr&gt;                                                \n 1 \"United States 1 \"      \"          4,058,814   14.7      67.5      2,130.0  …\n 2 \"\"                       &lt;NA&gt;                                                \n 3 \"Alabama \"              \"           63,299    14.4      65.0      2,021.0   …\n 4 \"Alaska \"               \"         9,974    16.0      74.6      2,437.0      …\n 5 \"Arizona \"              \"         85,273    17.5      84.4      2,652.5     …\n 6 \"Arkansas \"             \"          37,783    14.7      69.1      2,140.0    …\n 7 \"California \"           \"        531,959    15.8      70.7      2,186.0     …\n 8 \"Colorado \"             \"          65,438    15.8      73.1      2,356.5    …\n 9 \"Connecticut \"          \"           43,026    13.0      61.2      1,931.5   …\n10 \"Delaware \"             \"           11,051    14.5      63.5      2,014.0   …\n11 \"District of Columbia \" \"                7,666    14.8      63.0      1,975.…\n\n\nWe then separate the data based on spaces. There is an inconsistent number of spaces, so we first squish any example of more than one space into just one with str_squish() from stringr.\n\ndhs_2000_separate_data &lt;-\n  dhs_2000_separate_columns |&gt;\n  mutate(data = str_squish(data)) |&gt;\n  separate(\n    col = data,\n    into = c(\n      \"number_of_births\",\n      \"birth_rate\",\n      \"fertility_rate\",\n      \"TFR\",\n      \"teen_births_all\",\n      \"teen_births_15_17\",\n      \"teen_births_18_19\"\n    ),\n    sep = \"\\\\s\",\n    remove = FALSE\n  )\n\ndhs_2000_separate_data[18:28, ] |&gt;\n  select(-raw_data, -data)\n\n# A tibble: 11 × 8\n   state        number_of_births birth_rate fertility_rate TFR   teen_births_all\n   &lt;chr&gt;        &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;          \n 1 \"United Sta… 4,058,814        14.7       67.5           2,13… 48.5           \n 2 \"\"           &lt;NA&gt;             &lt;NA&gt;       &lt;NA&gt;           &lt;NA&gt;  &lt;NA&gt;           \n 3 \"Alabama \"   63,299           14.4       65.0           2,02… 62.9           \n 4 \"Alaska \"    9,974            16.0       74.6           2,43… 42.4           \n 5 \"Arizona \"   85,273           17.5       84.4           2,65… 69.1           \n 6 \"Arkansas \"  37,783           14.7       69.1           2,14… 68.5           \n 7 \"California… 531,959          15.8       70.7           2,18… 48.5           \n 8 \"Colorado \"  65,438           15.8       73.1           2,35… 49.2           \n 9 \"Connecticu… 43,026           13.0       61.2           1,93… 31.9           \n10 \"Delaware \"  11,051           14.5       63.5           2,01… 51.6           \n11 \"District o… 7,666            14.8       63.0           1,97… 80.7           \n# ℹ 2 more variables: teen_births_15_17 &lt;chr&gt;, teen_births_18_19 &lt;chr&gt;\n\n\nThis is all looking fairly great. The only thing left is to clean up.\n\ndhs_2000_cleaned &lt;-\n  dhs_2000_separate_data |&gt;\n  select(state, TFR) |&gt;\n  slice(18:74) |&gt;\n  drop_na() |&gt; \n  mutate(\n    TFR = str_remove_all(TFR, \",\"),\n    TFR = as.numeric(TFR),\n    state = str_trim(state),\n    state = if_else(state == \"United States 1\", \"Total\", state)\n  )\n\nAnd run some checks, for instance that we have all the states.\n\nall(state.name %in% dhs_2000_cleaned$state)\n\n[1] TRUE\n\n\nAnd we are done (Table 7.3). We can see that there is quite a wide distribution of TFR by US state (Figure 7.16). Utah has the highest and Vermont the lowest.\n\ndhs_2000_cleaned |&gt;\n  slice(1:10) |&gt;\n  kable(\n    col.names = c(\"State\", \"TFR\"),\n    digits = 0,\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 7.3: First ten rows of a dataset of TFR by United States state, 2000-2019\n\n\nState\nTFR\n\n\n\n\nTotal\n2,130\n\n\nAlabama\n2,021\n\n\nAlaska\n2,437\n\n\nArizona\n2,652\n\n\nArkansas\n2,140\n\n\nCalifornia\n2,186\n\n\nColorado\n2,356\n\n\nConnecticut\n1,932\n\n\nDelaware\n2,014\n\n\nDistrict of Columbia\n1,976\n\n\n\n\n\n\n\ndhs_2000_cleaned |&gt; \n  filter(state != \"Total\") |&gt; \n  ggplot(aes(x = TFR, y = fct_reorder(state, TFR))) +\n  geom_point() +\n  theme_classic() +\n  labs(y = \"State\", x = \"Total Fertility Rate\")\n\n\n\n\nFigure 7.16: Distribution of TFR by US state in 2000\n\n\n\n\nHealy (2022) provides another example of using this approach in a different context.\n\n\n7.4.3 Optical Character Recognition\nAll of the above is predicated on having a PDF that is already “digitized”. But what if it is made of images, such as the result of a scan. Such PDFs often contain unstructured data, meaning that the data are not tagged nor organized in a regular way. Optical Character Recognition (OCR) is a process that transforms an image of text into actual text. Although there may not be much difference to a human reading a PDF before and after OCR, the PDF becomes machine-readable which allows us to use scripts (Cheriet et al. 2007). OCR has been used to parse images of characters since the 1950s, initially using manual approaches. While manual approaches remain the gold standard, for reasons of cost effectiveness, this has been largely replaced with statistical models.\nIn this example we use tesseract to OCR a document. This is a R wrapper around the Tesseract open-source OCR engine. Tesseract was initially developed at HP in the 1980s, and is now mostly developed by Google. After we install and load tesseract we can use ocr().\nLet us see an example with a scan from the first page of Jane Eyre (Figure 7.17).\n\n\n\nFigure 7.17: Scan of first page of Jane Eyre\n\n\n\ntext &lt;- ocr(\n  here(\"jane_scan.png\"),\n  engine = tesseract(\"eng\")\n)\ncat(text)\n\n\n\n1 THERE was no possibility of taking a walk that day. We had\nbeen wandering, indeed, in the leafless shrubbery an hour in\nthe morning; but since dinner (Mrs Reed, when there was no com-\npany, dined early) the cold winter wind had brought with it clouds\nso sombre, and a rain so penetrating, that further out-door exercise\n\nwas now out of the question.\n\nI was glad of it: I never liked long walks, especially on chilly\nafternoons: dreadful to me was the coming home in the raw twi-\nlight, with nipped fingers and toes, and a heart saddened by the\nchidings of Bessie, the nurse, and humbled by the consciousness of\nmy physical inferiority to Eliza, John, and Georgiana Reed.\n\nThe said Eliza, John, and Georgiana were now clustered round\ntheir mama in the drawing-room: she lay reclined on a sofa by the\nfireside, and with her darlings about her (for the time neither quar-\nrelling nor crying) looked perfectly happy. Me, she had dispensed\nfrom joining the group; saying, ‘She regretted to be under the\nnecessity of keeping me at a distance; but that until she heard from\nBessie, and could discover by her own observation that I was\nendeavouring in good earnest to acquire a more sociable and\nchild-like disposition, a more attractive and sprightly manner—\nsomething lighter, franker, more natural as it were—she really\nmust exclude me from privileges intended only for contented,\nhappy, littie children.’\n\n‘What does Bessie say I have done?’ I asked.\n\n‘Jane, I don’t like cavillers or questioners: besides, there is\nsomething truly forbidding in a child taking up her elders in that\nmanner. Be seated somewhere; and until you can speak pleasantly,\nremain silent.’\n\n. a TV\n\ni; STA AEE LT JEUNE TIS Sis\na) | | | a) ee\ni Ni 4 | | | ae ST | | a eg\n\nce A FEM yi | eS ee\nPe TT (SB ag ie pe\nis \\ ie mu) i i es SS\nveal | Dy eT |\npa || i er itl |\n\naes : Oty ZR UIE OR HMR Sa ote ariel\nSEEN ed — =\n15\n\n\nIn general the result is not too bad. OCR is a useful tool but is not perfect and the resulting data may require extra attention in terms of cleaning. For instance, in the OCR results of Figure 7.17 we see irregularities that would need to be fixed. Various options, such as focusing on the particular data of interest and increasing the contrast can help. Other popular OCR engines include Amazon Textract, Google Vision API, and ABBYY."
  },
  {
    "objectID": "07-gather.html#exercises",
    "href": "07-gather.html#exercises",
    "title": "7  Gather data",
    "section": "7.5 Exercises",
    "text": "7.5 Exercises\n\nScales\n\n(Plan) Consider the following scenario: A group of five undergraduates—Matt, Ash, Jacki, Rol, and Mike—each read some number of pages from a book each day for 100 days. Two of the undergraduates are a couple and so their number of pages is positively correlated, however all the others are independent. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include five tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched using the data that you simulated. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nSeparately for both an API of interest to you, and an API that has an R package written around it, please list its name, a link, and a brief description of the data that are available.\nPlease consider the following code, which uses gh to access the GitHub API (you will need to have set up GitHub on your computer, as covered in Chapter 3). When was the repo for heapsofpapers created (pick one)?\n\n2021-02-23\n2021-03-06\n2021-05-25\n2021-04-27\n\n\n\n# Based on Tyler Bradley and Monica Alexander\nrepos &lt;- gh(\"/users/RohanAlexander/repos\", per_page = 100)\nrepo_info &lt;- tibble(\n  name = map_chr(repos, \"name\"),\n  created = map_chr(repos, \"created_at\"),\n  full_name = map_chr(repos, \"full_name\"),\n)\n\n\nPlease consider the UN’s Data API and the introductory note on how to use it by Schmertmann (2022). Argentina’s location code is 32. Modify the following code to determine what Argentina’s single-year fertility rate was for 20-year-olds in 1995 (pick one)?\n\n147.679\n172.988\n204.124\n128.665\n\n\n\nmy_indicator &lt;- 68\nmy_location &lt;- 50\nmy_startyr &lt;- 1996\nmy_endyr &lt;- 1999\n\nurl &lt;- paste0(\n  \"https://population.un.org/dataportalapi/api/v1\",\n  \"/data/indicators/\", my_indicator, \"/locations/\",\n  my_location, \"/start/\", my_startyr, \"/end/\",\n  my_endyr, \"/?format=csv\"\n)\n\nun_data &lt;- read_delim(file = url, delim = \"|\", skip = 1)\n\nun_data |&gt;\n  filter(AgeLabel == 25 & TimeLabel == 1996) |&gt;\n  select(Value)\n\n\nWhat is the main argument to GET() from httr (pick one)?\n\n“url”\n“website”\n“domain”\n“location”\n\nWhat are three reasons why we should be respectful when getting scraping data from websites (write at least two paragraphs)?\nWhat features of a website do we typically take advantage of when we parse the code (pick one)?\n\nHTML/CSS mark-up.\nCookies.\nFacebook beacons.\nCode comments.\n\nWhat are three advantages and three disadvantages of scraping compared with using an API?\nWhat are three delimiters that could be useful when trying to bring order to a PDF that you read in as a character vector (the use of dot points is fine)?\nWhich of the following, used as part of a regular expression, would match a full stop (hint: see the “strings” cheat sheet) (pick one)?\n\n“.”\n“.”\n“\\.”\n“\\.”\n\nWhat are three checks that we might like to use for demographic data, such as the number of births in a country in a particular year?\nWhich of these are functions from the purrr package (select all that apply)?\n\nmap()\nwalk()\nrun()\nsafely()\n\nWhat are some principles to follow when scraping (select all that apply)?\n\nAvoid it if possible\nFollow the site’s guidance\nSlow down\nUse a scalpel not an axe.\n\nWhat is the HTML tag for an item in a list (pick one)?\n\nli\nbody\nb\nem\n\nWhich function should we use if we have the following text data: “rohan_alexander” in a column called “names” and want to split it into first name and surname based on the underscore (pick one)?\n\nseparate()\nslice()\nspacing()\ntext_to_columns()\n\n\n\n\nTutorial\nPlease redo the web scraping example, but for one of: Australia, Canada, India, or New Zealand.\nPlan, gather, and clean the data, and then use it to create a similar table to the one created above. Write a few paragraphs about your findings. Then write a few paragraphs about the data source, what you gathered, and how you went about it. What took longer than you expected? When did it become fun? What would you do differently next time you do this? Your submission should be at least two pages, but likely more.\nUse Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations. Submit a PDF.\n\n\n\n\nAlexander, Rohan, and A Mahfouz. 2021. heapsofpapers: Easily Download Heaps of PDF and CSV Files. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nBailey, Rosemary. 2008. Design of Comparative Experiments. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511611483.\n\n\nBor, Jacob, Atheendar Venkataramani, David Williams, and Alexander Tsai. 2018. “Police Killings and Their Spillover Effects on the Mental Health of Black Americans: A Population-Based, Quasi-Experimental Study.” The Lancet 392 (10144): 302–10. https://doi.org/10.1016/s0140-6736(18)31130-9.\n\n\nBrontë, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\nBryan, Jenny, and Hadley Wickham. 2021. gh: GitHub API. https://CRAN.R-project.org/package=gh.\n\n\nCheriet, Mohamed, Nawwaf Kharma, Cheng-Lin Liu, and Ching Suen. 2007. Character Recognition Systems: A Guide for Students and Practitioner. Wiley.\n\n\nCirone, Alexandra, and Arthur Spirling. 2021. “Turning History into Data: Data Collection, Measurement, and Inference in HPE.” Journal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nCollins, Annie, and Rohan Alexander. 2022. “Reproducibility of COVID-19 Pre-Prints.” Scientometrics 127: 4655–73. https://doi.org/10.1007/s11192-022-04418-2.\n\n\nComer, Benjamin P., and Jason R. Ingram. 2022. “Comparing Fatal Encounters, Mapping Police Violence, and Washington Post Fatal Police Shooting Data from 2015-2019: A Research Note.” Criminal Justice Review, January, 073401682110710. https://doi.org/10.1177/07340168211071014.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nCummins, Neil. 2022. “The Hidden Wealth of English Dynasties, 1892–2016.” The Economic History Review 75 (3): 667–702. https://doi.org/10.1111/ehr.13120.\n\n\nEisenstein, Michael. 2022. “Need Web Data? Here’s How to Harvest Them.” Nature 607: 200–201. https://doi.org/10.1038/d41586-022-01830-9.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHackett, Robert. 2016. “Researchers Caused an Uproar By Publishing Data From 70,000 OkCupid Users.” Fortune, May. https://fortune.com/2016/05/18/okcupid-data-research/.\n\n\nHealy, Kieran. 2022. “Unhappy in Its Own Way,” July. https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/.\n\n\nJenkins, Jennifer, Steven Rich, Andrew Ba Tran, Paige Moody, Julie Tate, and Ted Mellnik. 2022. “How the Washington Post Examines Police Shootings in the United States.” https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/.\n\n\nJohnson, Kaneesha. 2021. “Two Regimes of Prison Data Collection.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.72825001.\n\n\nKirkegaard, Emil, and Julius Bjerrekær. 2016. “The OKCupid Dataset: A Very Large Public Dataset of Dating Site Users.” Open Differential Psychology, 1–10. https://doi.org/10.26775/ODP.2016.11.03.\n\n\nLuscombe, Alex, Kevin Dick, and Kevin Walby. 2021. “Algorithmic Thinking in the Public Interest: Navigating Technical, Legal, and Ethical Hurdles to Web Scraping in the Social Sciences.” Quality & Quantity 56 (3): 1–22. https://doi.org/10.1007/s11135-021-01164-0.\n\n\nLuscombe, Alex, Jamie Duncan, and Kevin Walby. 2022. “Jumpstarting the Justice Disciplines: A Computational-Qualitative Approach to Collecting and Analyzing Text and Image Data in Criminology and Criminal Justice Studies.” Journal of Criminal Justice Education 33 (2): 151–71. https://doi.org/10.1080/10511253.2022.2027477.\n\n\nMüller, Kirill. 2020. here: A Simpler Way to Find Your Files. https://CRAN.R-project.org/package=here.\n\n\nNix, Justin, and M. James Lozada. 2020. “Police Killings of Unarmed Black Americans: A Reassessment of Community Mental Health Spillover Effects,” January. https://doi.org/10.31235/osf.io/ajz2q.\n\n\nOoms, Jeroen. 2014. “The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2022a. pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2022b. tesseract: Open Source OCR Engine. https://CRAN.R-project.org/package=tesseract.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using Spotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nPerepolkin, Dmytro. 2022. polite: Be Nice on the Web. https://CRAN.R-project.org/package=polite.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSalganik, Matthew, Peter Sheridan Dodds, and Duncan Watts. 2006. “Experimental Study of Inequality and Unpredictability in an Artificial Cultural Market.” Science 311 (5762): 854–56. https://doi.org/10.1126/science.1121066.\n\n\nSaulnier, Lucile, Siddharth Karamcheti, Hugo Laurençon, Léo Tronchon, Thomas Wang, Victor Sanh, Amanpreet Singh, et al. 2022. “Putting Ethical Principles at the Core of the Research Lifecycle.” https://huggingface.co/blog/ethical-charter-multimodal.\n\n\nSchmertmann, Carl. 2022. “UN API Test,” July. https://bonecave.schmert.net/un-api-example.html.\n\n\nTaflaga, Marija, and Matthew Kerby. 2019. “Who Does What Work in a Ministerial Office: Politically Appointed Staff and the Descriptive Representation of Women in Australian Political Offices, 19792010.” Political Studies 68 (2): 463–85. https://doi.org/10.1177/0032321719853459.\n\n\nThe Economist. 2022. “What Spotify Data Show about the Decline of English,” January. https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english.\n\n\nThe Washington Post. 2023. “Fatal Force Database.” https://github.com/washingtonpost/data-police-shootings.\n\n\nThompson, Charlie, Daniel Antal, Josiah Parry, Donal Phipps, and Tom Wolff. 2022. spotifyr: R Wrapper for the “Spotify” Web API. https://CRAN.R-project.org/package=spotifyr.\n\n\nThomson-DeVeaux, Amelia, Laura Bronner, and Damini Sharma. 2021. “Cities Spend Millions On Police Misconduct Every Year. Here’s Why It’s So Difficult to Hold Departments Accountable.” FiveThirtyEight, February. https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/.\n\n\nWickham, Hadley. 2021. babynames: US Baby Names 1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2022. rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2023. httr: Tools for Working with URLs and HTTP. https://CRAN.R-project.org/package=httr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, and Lionel Henry. 2022. purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\n\n\nWong, Julia Carrie. 2020. “One Year Inside Trump’s Monumental Facebook Campaign.” The Guardian, January. https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nZimmer, Michael. 2018. “Addressing Conceptual Gaps in Big Data Research Ethics: An Application of Contextual Integrity.” Social Media + Society 4 (2): 1–11. https://doi.org/10.1177/2056305118768300."
  },
  {
    "objectID": "08-hunt.html#introduction",
    "href": "08-hunt.html#introduction",
    "title": "8  Hunt data",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nThis chapter is about obtaining data with experiments. This is a situation in which we can explicitly control and vary what we are interested in. The advantage of this is that identifying and estimating an effect should be clear. There is a treatment group that is subject to what we are interested in, and a control group that is not. These are randomly split before treatment. And so, if they end up different, then it must be because of the treatment. Unfortunately, life is rarely so smooth. Arguing about how similar the treatment and control groups were tends to carry on indefinitely. And before we can estimate an effect, we need to be able to measure whatever it is that we are interested in, which is often surprisingly difficult.\nBy way of motivation, consider the situation of someone who moved to San Francisco in 2014—as soon as they moved the Giants won the World Series and the Golden State Warriors began a historic streak of World Championships. They then moved to Chicago, and immediately the Cubs won the World Series for the first time in 100 years. They then moved to Massachusetts, and the Patriots won the Super Bowl again, and again, and again. And finally, they moved to Toronto, where the Raptors immediately won the World Championship. Should a city pay them to move, or could municipal funds be better spent elsewhere?\nOne way to get at the answer would be to run an experiment. Make a list of the North American cities with major sports teams. Then roll some dice, send them to live there for a year, and measure the outcomes of the sports teams. With enough lifetimes, we could work it out. This would take a long time because we cannot both live in a city and not live in a city. This is the fundamental problem of causal inference: a person cannot be both treated and untreated. Experiments and randomized controlled trials are circumstances in which we try to randomly allocate some treatment, to have a belief that everything else was the same (or at least ignorable). We use the Neyman-Rubin potential outcomes framework to formalize the situation (Holland 1986). \nA treatment, \\(t\\), will often be a binary variable, that is either 0 or 1. It is 0 if the person, \\(i\\), is not treated, which is to say they are in the control group, and 1 if they are treated. We will typically have some outcome, \\(Y_i\\), of interest for that person which could be binary, categorical, multinomial, ordinal, continuous, or possibly even some other type of variable. For instance, it could be vote choice, in which case we could measure whether the person is: “Conservative” or “Not Conservative”; which party they support, say: “Conservative”, “Liberal”, “Democratic”, “Green”; or maybe a probability of supporting some particular leader.\nThe effect of a treatment is then causal if \\((Y_i|t=0) \\neq (Y_i|t=1)\\). That is to say, the outcome for person \\(i\\), given they were not treated, is different to their outcome given they were treated. If we could both treat and control the one individual at the one time, then we would know that it was only the treatment that had caused any change in outcome. There could be no other factor to explain it. But the fundamental problem of causal inference remains: we cannot both treat and control the one individual at the one time. So, when we want to know the effect of the treatment, we need to compare it with a counterfactual. The counterfactual, introduced in Chapter 4, is what would have happened if the treated individual were not treated. As it turns out, this means one way to think of causal inference is as a missing data problem, where we are missing the counterfactual.\nWe cannot compare treatment and control in one individual. So we instead compare the average of two groups—those treated and those not. We are looking to estimate the counterfactual at a group level because of the impossibility of doing it at an individual level. Making this trade-off allows us to move forward but comes at the cost of certainty. We must instead rely on randomization, probabilities, and expectations.\nWe usually consider a default of there being no effect and we look for evidence that would cause us to change our mind. As we are interested in what is happening in groups, we turn to expectations and notions of probability to express ourselves. Hence, we will make claims that apply on average. Maybe wearing fun socks really does make you have a lucky day, but on average, across the group, it is probably not the case. It is worth pointing out that we do not just have to be interested in the average effect. We may consider the median, or variance, or whatever. Nonetheless, if we were interested in the average effect, then one way to proceed would be to:\n\ndivide the dataset in two—treated and not treated—and have a binary effect variable—lucky day or not;\nsum the variable, then divide it by the length of the variable; and\ncompare this value between the two groups.\n\nThis is an estimator, introduced in Chapter 4, which is a way of putting together a guess of something of interest. The estimand is the thing of interest, in this case the average effect, and the estimate is whatever our guess turns out to be. We can simulate data to illustrate the situation.\n\nset.seed(853)\n\ntreat_control &lt;-\n  tibble(\n    group = sample(x = c(\"Treatment\", \"Control\"), size = 100, replace = TRUE),\n    binary_effect = sample(x = c(0, 1), size = 100, replace = TRUE)\n    )\n\ntreat_control\n\n# A tibble: 100 × 2\n   group     binary_effect\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 Treatment             0\n 2 Control               1\n 3 Control               1\n 4 Treatment             1\n 5 Treatment             1\n 6 Treatment             0\n 7 Treatment             1\n 8 Treatment             1\n 9 Control               0\n10 Control               0\n# ℹ 90 more rows\n\n\n\ntreat_control |&gt;\n  summarise(\n    treat_result = sum(binary_effect) / length(binary_effect),\n    .by = group\n  )\n\n# A tibble: 2 × 2\n  group     treat_result\n  &lt;chr&gt;            &lt;dbl&gt;\n1 Treatment        0.552\n2 Control          0.333\n\n\nIn this case, we draw either 0 or 1, 100 times, for each the treatment and control group, and then the estimate of the average effect of being treated is 0.22.\nMore broadly, to tell causal stories we need to bring together theory and a detailed knowledge of what we are interested in (Cunningham 2021, 4). In Chapter 7 we discussed gathering data that we observed about the world. In this chapter we are going to be more active about turning the world into the data that we need. As the researcher, we will decide what to measure and how, and we will need to define what we are interested in. We will be active participants in the data-generating process. That is, if we want to use this data, then as researchers we must go out and hunt it.\nIn this chapter we cover experiments, especially constructing treatment and control groups, and appropriately considering their results. We go through implementing a survey. We discuss some aspects of ethical behavior in experiments through reference to the Tuskegee Syphilis Study and the Extracorporeal Membrane Oxygenation (ECMO) experiment and go through various case studies. Finally, we then turn to A/B testing, which is extensively used in industry, and consider a case study based on Upworthy data.\nRonald Fisher, the twentieth century statistician, and Francis Galton, the nineteenth century statistician, are the intellectual grandfathers of much of the work that we cover in this chapter. In some cases it is directly their work, in other cases it is work that built on their contributions. Both men believed in eugenics, amongst other things that are generally reprehensible. In the same way that art history acknowledges, say, Caravaggio as a murderer, while also considering his work and influence, so too must statistics and data science more generally concern themselves with this past, at the same time as we try to build a better future."
  },
  {
    "objectID": "08-hunt.html#field-experiments-and-randomized-controlled-trials",
    "href": "08-hunt.html#field-experiments-and-randomized-controlled-trials",
    "title": "8  Hunt data",
    "section": "8.2 Field experiments and randomized controlled trials",
    "text": "8.2 Field experiments and randomized controlled trials\n\n8.2.1 Randomization\nCorrelation can be enough in some settings (Hill 1965), but to be able to make forecasts when things change, and circumstances are slightly different, we should try to understand causation. Economics went through a credibility revolution in the 2000s (Angrist and Pischke 2010). Economists realized previous work was not as reliable as it could be. There was increased concern with research design and use of experiments. This also happened in other social sciences, such as political science at a similar time (Druckman and Green 2021).\nThe key is the counterfactual: what would have happened in the absence of the treatment. Ideally, we could keep everything else constant, randomly divide the world into two groups, and treat one and not the other. Then we could be confident that any difference between the two groups was due to that treatment. The reason for this is that if we have some population and we randomly select two groups from it, then those two groups (provided they are both big enough) should have the same characteristics as the population. Randomized controlled trials (RCTs) and A/B testing attempt to get us as close to this “gold standard” as we can hope.\nWhen we, and others such as Athey and Imbens (2017b), use such positive language to refer to these approaches, we do not mean to imply that they are perfect. Just that they can be better than most of the other options. For instance, in Chapter 14 we will consider causality from observational data, and while this is sometimes all that we can do, the circumstances in which it is possible to evaluate both makes it clear that approaches based on observational data are usually second-best (Gordon et al. 2019; Gordon, Moakler, and Zettelmeyer 2022). RCTs and A/B testing also bring other benefits, such as the chance to design a study that focuses on a particular question and tries to uncover the mechanism by which the effect occurs (Alsan and Finkelstein 2021). But they are not perfect, and the embrace of RCTs has not been unanimous (Deaton 2010).\nOne bedrock of experimental practice is that it be blinded, that is, a participant does not know whether they are in the treatment or control group. A failure to blind, especially with subjective outcomes, is grounds for the dismissal of an entire experiment in some disciplines (Edwards 2017). Ideally experiments should be double-blind, that is, even the researcher does not know. Stolberg (2006) discusses an early example of a randomized double-blind trial in 1835 to evaluate the effect of homeopathic drugs where neither the participants nor the organizers knew who was in which group. This is rarely the case for RCTs and A/B testing. Again, this is not to say they are not useful—after all in 1847 Semmelweis identified the benefit of having an intern wash their hands before delivering babies without a blinded study (Morange 2016, 121). Another major concern is with the extent to which the result found in the RCT generalizes to outside of that setting. There are typically few RCTs conducted over a long time, although it is possible this is changing and Bouguen et al. (2019) provide some RCTs that could be followed up on to assess long-term effects. Finally, the focus on causality has not been without cost in social sciences. Some argue that a causality-focused approach centers attention on the types of questions that it can answer at the expense of other types of questions.\n\n\n8.2.2 Simulated example: cats or dogs\nWe hope to be able to establish treatment and control groups that are the same, but for the treatment. This means creating the control group is critical because when we do that, we establish the counterfactual. We might be worried about, say, underlying trends, which is one issue with a before-and-after comparison, or selection bias, which could occur when we allow self-selection into the treatment group. Either of these issues could result in biased estimates. We use randomization to go some way to addressing these.\nTo get started, we simulate a population, and then randomly sample from it. We will set it up so that half the population likes blue, and the other half likes white. And further, if someone likes blue then they almost surely prefer dogs, but if they like white then they almost surely prefer cats. Simulation is a critical part of the workflow advocated in this book. This is because we know what the outcomes should be from the analysis of simulated data. Whereas if we go straight to analyzing real data, then we do not know if unexpected outcomes are due to our own analysis errors, or actual results. Another good reason it is useful to take this approach of simulation is that when you are working in teams the analysis can get started before the data collection and cleaning is completed. The simulation will also help the collection and cleaning team think about tests they should run on their data.\n\nset.seed(853)\n\nnum_people &lt;- 5000\n\npopulation &lt;- tibble(\n  person = 1:num_people,\n  favorite_color = sample(c(\"Blue\", \"White\"), size = num_people, replace = TRUE),\n  prefers_dogs = if_else(favorite_color == \"Blue\", \n                         rbinom(num_people, 1, 0.9), \n                         rbinom(num_people, 1, 0.1))\n  )\n\npopulation |&gt;\n  count(favorite_color, prefers_dogs)\n\n# A tibble: 4 × 3\n  favorite_color prefers_dogs     n\n  &lt;chr&gt;                 &lt;int&gt; &lt;int&gt;\n1 Blue                      0   256\n2 Blue                      1  2291\n3 White                     0  2239\n4 White                     1   214\n\n\nBuilding on the terminology and concepts introduced in Chapter 6, we now construct a sampling frame that contains about 80 per cent of the target population.\n\nset.seed(853)\n\nframe &lt;-\n  population |&gt;\n  mutate(in_frame = rbinom(n = num_people, 1, prob = 0.8)) |&gt; \n  filter(in_frame == 1)\n\nframe |&gt;\n  count(favorite_color, prefers_dogs)\n\n# A tibble: 4 × 3\n  favorite_color prefers_dogs     n\n  &lt;chr&gt;                 &lt;int&gt; &lt;int&gt;\n1 Blue                      0   201\n2 Blue                      1  1822\n3 White                     0  1803\n4 White                     1   177\n\n\nFor now, we will set aside dog or cat preferences and focus on creating treatment and control groups with favorite color only.\n\nset.seed(853)\n\nsample &lt;-\n  frame |&gt;\n  select(-prefers_dogs) |&gt;\n  mutate(\n    group = \n      sample(x = c(\"Treatment\", \"Control\"), size = nrow(frame), replace = TRUE\n  ))\n\nWhen we look at the mean for the two groups, we can see that the proportions that prefer blue or white are very similar to what we specified (Table 8.1).\n\nsample |&gt;\n  count(group, favorite_color) |&gt;\n  mutate(prop = n / sum(n),\n         .by = group) |&gt;\n  kable(\n    col.names = c(\"Group\", \"Prefers\", \"Number\", \"Proportion\"),\n    digits = 2,\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 8.1: Proportion of the groups that prefer blue or white\n\n\nGroup\nPrefers\nNumber\nProportion\n\n\n\n\nControl\nBlue\n987\n0.50\n\n\nControl\nWhite\n997\n0.50\n\n\nTreatment\nBlue\n1,036\n0.51\n\n\nTreatment\nWhite\n983\n0.49\n\n\n\n\n\n\nWe randomized with favorite color only. But we should also find that we took dog or cat preferences along at the same time and will have a “representative” share of people who prefer dogs to cats. We can look at our dataset (Table 8.2).\n\nsample |&gt;\n  left_join(\n    frame |&gt; select(person, prefers_dogs),\n    by = \"person\"\n  ) |&gt;\n  count(group, prefers_dogs) |&gt;\n  mutate(prop = n / sum(n),\n         .by = group) |&gt;\n  kable(\n    col.names = c(\n      \"Group\",\n      \"Prefers dogs to cats\",\n      \"Number\",\n      \"Proportion\"\n    ),\n    digits = 2,\n    format.args = list(big.mark = \",\")\n  )\n\n\n\nTable 8.2: Proportion of the treatment and control group that prefer dogs or cats\n\n\nGroup\nPrefers dogs to cats\nNumber\nProportion\n\n\n\n\nControl\n0\n1,002\n0.51\n\n\nControl\n1\n982\n0.49\n\n\nTreatment\n0\n1,002\n0.50\n\n\nTreatment\n1\n1,017\n0.50\n\n\n\n\n\n\nIt is exciting to have a representative share on “unobservables”. (In this case, we do “observe” them—to illustrate the point—but we did not select on them). We get this because the variables were correlated. But it will break down in several ways that we will discuss. It also assumes large enough groups. For instance, if we considered specific dog breeds, instead of dogs as an entity, we may not find ourselves in this situation. To check that the two groups are the same, we look to see if we can identify a difference between the two groups based on observables, theory, experience, and expert opinion. In this case we looked at the mean, but we could look at other aspects as well.\nThis would traditionally bring us to Analysis of Variance (ANOVA). ANOVA was introduced around 100 years ago by Fisher while he was working on statistical problems in agriculture. (Stolley (1991) provides additional background on Fisher.) This is less unexpected than it may seem because historically agricultural research was closely tied to statistical innovation. Often statistical methods were designed to answer agricultural questions such as “does fertilizer work?” and were only later adapted to clinical trials (Yoshioka 1998). It was relatively easily to divide a field into “treated” and “non-treated”, and the magnitude of any effect was likely to be large. While appropriate for that context, often these same statistical approaches are still taught today in introductory material, even when they are being applied in different circumstances to those they were designed for. It almost always pays to take a step back and think about what is being done and whether it is appropriate to the circumstances. We mention ANOVA here because of its importance historically. There is nothing wrong with it in the right setting. But the number of modern use-cases where it is the best option tends to be small. It might be better to build the model that underpins ANOVA ourselves, which we cover in Chapter 12.\n\n\n8.2.3 Treatment and control\nIf the treatment and control groups are the same in all ways and remain that way, but for the treatment, then we have internal validity, which is to say that our control will work as a counterfactual and our results can speak to a difference between the groups in that study. Internal validity means that our estimates of the effect of the treatment speak to the treatment and not some other aspect. It means that we can use our results to make claims about what happened in the experiment.\nIf the group to which we applied our randomization were representative of the broader population, and the experimental set-up was like outside conditions, then we further could have external validity. That would mean that the difference that we find does not just apply in our own experiment, but also in the broader population. External validity means that we can use our experiment to make claims about what would happen outside the experiment. It is randomization that has allowed that to happen. In practice we would not just rely on one experiment but would instead consider that a contribution to a broader evidence-collection effort (Duflo 2020, 1955).\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Esther Duflo is Abdul Latif Jameel Professor of Poverty Alleviation and Development Economics at MIT. After earning a PhD in Economics from MIT in 1999, she remained at MIT as an assistant professor, being promoted to full professor in 2003. One area of her research is economic development where she uses randomized controlled trials to understand how to address poverty. One of her most important books is Poor Economics (Banerjee and Duflo 2011). One of her most important papers is Banerjee et al. (2015) which uses randomization to examine the effect of microfinance. She was awarded the Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel in 2019.\n\n\nBut this means we need randomization twice. Firstly, into the group that was subject to the experiment, and then secondly, between treatment and control. How do we think about this randomization, and to what extent does it matter?\nWe are interested in the effect of being treated. It may be that we charge different prices, which would be a continuous treatment variable, or that we compare different colors on a website, which would be a discrete treatment variable. Either way, we need to make sure that the groups are otherwise the same. How can we be convinced of this? One way is to ignore the treatment variable and to examine all other variables, looking for whether we can detect a difference between the groups based on any other variables. For instance, if we are conducting an experiment on a website, then are the groups roughly similar in terms of, say:\n\nMicrosoft and Apple users?\nSafari, Chrome, and Firefox users?\nMobile and desktop users?\nUsers from certain locations?\n\nFurther, are the groups representative of the broader population? These are all threats to the validity of our claims. For instance, the Nationscape survey which we consider later in this chapter was concerned about the number of Firefox users who completed the survey. In the end they exclude a subset of those respondents (Vavreck and Tausanovitch 2021, 5).\nWhen done properly, that is if the treatment is truly independent, then we can estimate the average treatment effect (ATE). In a binary treatment variable setting this is:\n\\[\\mbox{ATE} = \\mathbb{E}[Y|t=1] - \\mathbb{E}[Y|t=0].\\]\nThat is, the difference between the treated group, \\(t = 1\\), and the control group, \\(t = 0\\), when measured by the expected value of the outcome, \\(Y\\). The ATE becomes the difference between two conditional expectations.\nTo illustrate this concept, we simulate some data that shows an average difference of one between the treatment and control groups.\n\nset.seed(853)\n\nate_example &lt;- \n  tibble(person = 1:1000,\n         treated = sample(c(\"Yes\", \"No\"), size = 1000, replace = TRUE)) |&gt;\n  mutate(outcome = case_when(\n    treated == \"No\" ~ rnorm(n(), mean = 5, sd = 1),\n    treated == \"Yes\" ~ rnorm(n(), mean = 6, sd = 1),\n  ))\n\nWe can see the difference, which we simulated to be one, between the two groups in Figure 8.1. And we can compute the average between the groups and then the difference to see also that we roughly get back the result that we put in (Table 8.3).\n\nate_example |&gt;\n  ggplot(aes(x = outcome, fill = treated)) +\n  geom_histogram(position = \"dodge2\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(x = \"Outcome\", \n       y = \"Number of people\", \n       fill = \"Person was treated\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 8.1: Simulated data showing a difference between the treatment and control group\n\n\n\n\n\nate_example |&gt;\n  summarise(mean = mean(outcome),\n            .by = treated) |&gt; \n  kable(\n    col.names = c(\n      \"Was treated?\",\n      \"Average effect\"\n    ),\n    digits = 2\n  )\n\n\n\nTable 8.3: Average difference between the treatment and control groups for data simulated to have an average difference of one\n\n\nWas treated?\nAverage effect\n\n\n\n\nYes\n6.06\n\n\nNo\n5.03\n\n\n\n\n\n\nUnfortunately, there is often a difference between simulated data and reality. For instance, an experiment cannot run for too long otherwise people may be treated many times or become inured to the treatment; but it cannot be too short otherwise we cannot measure longer-term outcomes. We cannot have a “representative” sample across every facet of a population, but if not, then the treatment and control may be different. Practical difficulties may make it difficult to follow up with certain groups and so we end up with a biased collection. Some questions to explore when working with real experimental data include:\n\nHow are the participants being selected into the frame for consideration?\nHow are they being selected for treatment? We would hope this is being done randomly, but this term is applied to a variety of situations. Additionally, early “success” can lead to pressure to treat everyone, especially in medical settings.\nHow is treatment being assessed?\nTo what extent is random allocation ethical and fair? Some argue that shortages mean it is reasonable to randomly allocate, but that may depend on how linear the benefits are. It may also be difficult to establish definitions, and the power imbalance between those making these decisions and those being treated should be considered.\n\nBias and other issues are not the end of the world. But we need to think about them carefully. Selection bias, introduced in Chapter 4, can be adjusted for, but only if it is recognized. For instance, how would the results of a survey about the difficulty of a university course differ if only students who completed the course were surveyed, and not those who dropped out? We should always work to try to make our dataset as representative as possible when we are creating it, but it may be possible to use a model to adjust for some of the bias after the fact. For instance, if there were a variable that was correlated with, say, attrition, then it could be added to the model either by itself, or as an interaction. Similarly, if there was correlation between the individuals. For instance, if there was some “hidden variable” that we did not know about that meant some individuals were correlated, then we could use wider standard errors. This needs to be done carefully and we discuss this further in Chapter 14. That said, if such issues can be anticipated, then it may be better to change the experiment. For instance, perhaps it would be possible to stratify by that variable.\n\n\n8.2.4 Fisher’s tea party\nThe British are funny when it comes to tea. There is substantial, persistent, debate in Britain about how to make the perfect “cuppa” with everyone from George Orwell to John Lennon weighing in. Some say to add the milk first. Others, to add it last. YouGov, a polling company, found that most respondents put milk in last (Smith 2018). But one might wonder whether the order matters at all.\nFisher introduced an experiment designed to see if a person can distinguish between a cup of tea where the milk was added first, or last. We begin by preparing eight cups of tea: four with milk added first and the other four with milk added last. We then randomize the order of all eight cups. We tell the taster, whom we will call “Ian”, about the experimental set-up: there are eight cups of tea, four of each type, he will be given cups of tea in a random order, and his task is to group them into two groups.\nOne of the nice aspects of this experiment is that we can do it ourselves. There are a few things to be careful of in practice. These include:\n\nthat the quantities of milk and tea are consistent,\nthe groups are marked in some way that the taster cannot see, and\nthe order is randomized.\n\nAnother nice aspect of this experiment is that we can calculate the chance that Ian is able to randomly get the groupings correct. To decide if his groupings were likely to have occurred at random, we need to calculate the probability this could happen. First, we count the number of successes out of the four that were chosen. There are: \\({8 \\choose 4} = \\frac{8!}{4!(8-4)!}=70\\) possible outcomes (Fisher [1935] 1949, 14). This notation means there are eight items in the set, and we are choosing four of them, and is used when the order of choice does not matter.\nWe are asking Ian to group the cups, not to identify which is which, and so there are two ways for him to be perfectly correct. He could either correctly identify all the ones that were milk-first (one outcome out of 70) or correctly identify all the ones that were tea-first (one outcome out of 70). This means the probability of this event is: \\(\\frac{2}{70}\\), or about three per cent.\nAs Fisher ([1935] 1949, 15) makes clear, this now becomes a judgement call. We need to consider the weight of evidence that we require before we accept the groupings did not occur by chance and that Ian was aware of what he was doing. We need to decide what evidence it takes for us to be convinced. If there is no possible evidence that would dissuade us from the view that we held coming into the experiment, say, that there is no difference between milk-first and tea-first, then what is the point of doing an experiment? We expect that if Ian got it completely right, then the reasonable person would accept that he was able to tell the difference.\nWhat if he is almost perfect? By chance, there are 16 ways for a person to be “off-by-one”. Either Ian thinks there was one cup that was milk-first when it was tea-first—there are, \\({4 \\choose 1} = 4\\), four ways this could happen—or he thinks there was one cup that was tea-first when it was milk-first—again, there are, \\({4 \\choose 1}\\) = 4, four ways this could happen. These outcomes are independent, so the probability is \\(\\frac{4\\times 4}{70}\\), or about 23 per cent. Given there is an almost 23 per cent chance of being off-by-one just by randomly grouping the teacups, this outcome probably would not convince us that Ian could tell the difference between tea-first and milk-first.\nWhat we are looking for, in order to claim something is experimentally demonstrable is that we have come to know the features of an experiment where such a result is reliably found (Fisher [1935] 1949, 16). We need a weight of evidence rather than just one experiment. We are looking to thoroughly interrogate our data and our experiments, and to think precisely about the analysis methods we are using. Rather than searching for meaning in constellations of stars, we want to make it as easy as possible for others to reproduce our work. It is in that way that our conclusions stand a better chance of holding up in the long term.\n\n\n8.2.5 Ethical foundations\nThe weight of evidence in medical settings can be measured in lost lives. One reason ethical practice in medical experiments developed is to prevent the unnecessary loss of life. We now detail two cases where human life may have been unnecessarily lost that helped establish foundations of ethical practice. We consider the need to obtain informed consent by discussing the Tuskegee Syphilis Study. And the need to ensure that an experiment is necessary by discussing the ECMO experiments.\n\n8.2.5.1 Tuskegee Syphilis Study\nFollowing Brandt (1978) and Alsan and Wanamaker (2018), the Tuskegee Syphilis Study is an infamous medical trial that began in 1932. As part of this experiment, 400 Black Americans with syphilis were not given appropriate treatment, nor even told they had syphilis, well after a standard treatment for syphilis was established and widely available. A control group, without syphilis, were also given non-effective drugs. These financially poor Black Americans in the United States South were offered minimal compensation and not told they were part of an experiment. Further, extensive work was undertaken to ensure the men would not receive treatment from anywhere, including writing to local doctors and the local health department. Even after some of the men were drafted and told to immediately get treatment, the draft board complied with a request to have the men excluded from treatment. By the time the study was stopped in 1972, more than half of the men were deceased and many of deaths were from syphilis-related causes.\nThe effect of the Tuskegee Syphilis Study was felt not just by the men in the study, but more broadly. Alsan and Wanamaker (2018) found that it is associated with a decrease in life expectancy at age 45 of up to 1.5 years for Black men located around central Alabama because of medical mistrust and decreased interactions with physicians. In response the United States established requirements for Institutional Review Boards and President Clinton made a formal apology in 1997. Brandt (1978, 27) says:\n\nIn retrospect the Tuskegee Study revealed more about the pathology of racism than the pathology of syphilis; more about the nature of scientific inquiry than the nature of the disease process\\(\\dots\\) [T]he notion that science is a value-free discipline must be rejected. The need for greater vigilance in assessing the specific ways in which social values and attitudes affect professional behavior is clearly indicated.\n\nHeller (2022) provides further background on the Tuskegee Syphilis Study.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Marcella Alsan is a Professor of Public Policy at Harvard University. She has an MD from Loyola University and earned a PhD in Economics from Harvard University in 2012. She was appointed as an assistant professor at Stanford, being promoted to full professor in 2019 when she returned to Harvard. One area of her research is health inequality, and one particularly important paper is Alsan and Wanamaker (2018), which we discussed above. She was awarded a MacArthur Foundation Fellowship in 2021.\n\n\n\n\n8.2.5.2 Extracorporeal membrane oxygenation (ECMO)\nTurning to the evaluation of extracorporeal membrane oxygenation (ECMO), Ware (1989) describes how they viewed ECMO as a possible treatment for persistent pulmonary hypertension in newborn children. They enrolled 19 patients and used conventional medical therapy on ten of them, and ECMO on nine of them. It was found that six of the ten in the control group survived while all in the treatment group survived. Ware (1989) used randomized consent whereby only the parents of infants randomly selected to be treated with ECMO were asked to consent.\nWe are concerned with “equipoise”, by which we refer to a situation in which there is genuine uncertainty about whether the treatment is more effective than conventional procedures. In medical settings even if there is initial equipoise it could be undermined if the treatment is found to be effective early in the study. Ware (1989) describes how after the results of these first 19 patients, randomization stopped and only ECMO was used. The recruiters and those treating the patients were initially not told that randomization had stopped. It was decided that this complete allocation to ECMO would continue “until either the 28th survivor or the 4th death was observed”. After 19 of 20 additional patients survived the trial was terminated. The experiment was effectively divided into two phases: in the first there was randomized use of ECMO, and in the second only ECMO was used.\nOne approach in these settings is a “randomized play-the-winner” rule following Wei and Durham (1978). Treatment is still randomized, but the probability shifts with each successful treatment to make treatment more likely, and there is some stopping rule. Berry (1989) argues that far from the need for a more sophisticated stopping rule, there was no need for this study of ECMO because equipoise never existed. Berry (1989) re-visits the literature mentioned by Ware (1989) and finds extensive evidence that ECMO was already known to be effective. Berry (1989) points out that there is almost never complete consensus and so one could almost always argue, inappropriately, for the existence of equipoise even in the face of a substantial weight of evidence. Berry (1989) further criticizes Ware (1989) for the use of randomized consent because of the potential that there may have been different outcomes for the infants subject to conventional medical therapy had their parents known there were other options.\nThe Tuskegee Syphilis Study and ECMO experiments may seem quite far from our present circumstances. While it may be illegal to do this exact research these days, it does not mean that unethical research does not still happen. For instance, we see it in machine learning applications in health and other areas; while we are not meant to explicitly discriminate and we are meant to get consent, it does not mean that we cannot implicitly discriminate without any type of consumer buy-in. For instance, Obermeyer et al. (2019) describes how many health care systems in the United States use algorithms to score the severity of how sick a patient is. They show that for the same score, Black patients are sicker, and that if Black patients were scored in the same way as White patients, then they would receive considerably more care. They find that the discrimination occurs because the algorithm is based on health care costs, rather than sickness. But because access to healthcare is unequally distributed between Black and White patients, the algorithm, however inadvertently, perpetuates racial bias."
  },
  {
    "objectID": "08-hunt.html#surveys",
    "href": "08-hunt.html#surveys",
    "title": "8  Hunt data",
    "section": "8.3 Surveys",
    "text": "8.3 Surveys\nHaving decided what to measure, one common way to get values is to use a survey. This is especially challenging, and there is an entire field—survey research—focused on it. Edelman, Vittert, and Meng (2021) make it clear that there are no new problems here, and the challenges that we face today are closely related to those that were faced in the past. There are many ways to implement surveys, and this decision matters. For some time, the only option was face-to-face surveys, where an enumerator conducted the survey in-person with the respondent. Eventually surveys began to be conducted over the telephone, again by an enumerator. One issue in both these settings was a considerable interviewer effect (Elliott et al. 2022). The internet brought a third era of survey research, characterized by low participation rates (Groves 2011). Surveys are a popular and invaluable way to get data. Face-to-face and telephone surveys are still used and have an important role to play, but many surveys are now internet-based.\nThere are many dedicated survey platforms, such as Survey Monkey and Qualtrics, that are largely internet-based. One especially common approach, because it is free, is to use Google Forms. In general, the focus of those platforms is enabling the user to construct and send a survey form. They typically expect the user already has contact details for some sampling frame.\nOther platforms, such as Amazon Mechanical Turk, mentioned in Chapter 3, and Prolific, focus on providing respondents. When using platforms like those we should try to understand who those respondents are and how they might differ from the population of interest (Levay, Freese, and Druckman 2016; Enns and Rothschild 2022).\nThe survey form needs to be considered within the context of the broader research and with special concern for the respondent. Try to conduct a test of the survey before releasing it. Light, Singer, and Willett (1990, 213), in the context of studies to evaluate higher education, say that there is no occasion in which a pilot study will not bring improvements, and that they are almost always worth it. In the case of surveys, we go further. If you do not have the time, or budget, to test a survey then it might be better to re-consider whether the survey should be done.\nTry to test the wording of a survey (Tourangeau, Rips, and Rasinski 2000, 23). When designing the survey, we need to have survey questions that are conversational and flow from one to the next, grouped within topics (Elson 2018). But we should also consider the cognitive load that we place on the respondent, and vary the difficulty of the questions.\nWhen designing a survey, the critical task is to keep the respondent front-of-mind (Dillman, Smyth, and Christian [1978] 2014, 94). Drawing on Swain (1985), all questions need to be relevant and able to be answered by the respondent. The wording of the questions should be based on what the respondent would be comfortable with. The decision between different question types turns on minimizing both error and the burden that we impose on the respondent. In general, if there are a small number of clear options then multiple-choice questions are appropriate. In that case, the responses should usually be mutually exclusive and collectively exhaustive. If they are not mutually exclusive, then this needs to be signaled in the text of the question. It is also important that units are specified, and that standard concepts are used, to the extent possible.\nOpen text boxes may be appropriate if there are many potential answers. This will increase both the time the respondent spends completing the survey and the time it will take to analyze the answers. Only ask one question at a time and try to ask questions in a neutral way that does not lead to one particular response. Testing the survey helps avoid ambiguous or double-barreled questions, which could confuse respondents. The subject matter of the survey will also affect the appropriate choice of question type. For instance, potentially “threatening” topics may be better considered with open-ended questions (Blair et al. 1977).\nAll surveys need to have an introduction that specifies a title for the survey, who is conducting it, their contact details, and the purpose. It should also include a statement about the confidentiality protections that are in place, and any ethics review board clearances that were obtained.\nWhen doing surveys, it is critical to ask the right person. For instance, Lichand and Wolf (2022) consider child labor. The extent of child labor is typically based on surveys of parents. When children were surveyed a considerable under-reporting by parents was found.\nOne aspect of particular concern is questions about sexual orientation and gender identity. While this is an evolving area, The White House (2023) provides recommendations for best practice, such as considering how the data will be used, and ensuring sufficient sample size. With regard to asking about sexual orientation they recommend the following question:\n\n“Which of the following best represents how you think of yourself?”\n\n“Gay or lesbian”\n“Straight, that is not gay or lesbian”\n“Bisexual”\n“I use a different term [free-text]”\n“I don’t know”\n\n\nAnd with regard to gender, they recommend a multi-question approach:\n\n“What sex were you assigned at birth, on your original birth certificate?”\n\n“Female”\n“Male”\n\n“How do you currently describe yourself (mark all that apply)?”\n\n“Female”\n“Male”\n“Transgender”\n“I use a different term [free-text]”\n\n\nAgain, this is an evolving area and best practice is likely to change.\nFinally, returning to the reason for doing surveys in the first place, while doing all this, it is important to also keep what we are interested in measuring in mind. Check that the survey questions relate to the estimand.\n\n8.3.1 Democracy Fund Voter Study Group\nAs an example of survey data, we will consider the Democracy Fund Voter Study Group Nationscape dataset (Tausanovitch and Vavreck 2021). This is a large series of surveys conducted between July 2019 and January 2021. It is weighted on a number of variables including: gender, major census regions, race, Hispanic ethnicity, household income, education, and age. Holliday et al. (2021) describe it as a convenience sample, which was introduced in Chapter 6, based on demographics. In this case, Holliday et al. (2021) detail how the sample was provided by Lucid, who operate an online platform for survey respondents, based on certain demographic quotas. Holliday et al. (2021) found that results are similar to government and commercial surveys.\nTo get the dataset, go to the Democracy Fund Voter Study Group website, then look for “Nationscape” and request access to the data. This could take a day or two. After getting access, focus on the “.dta” files. Nationscape conducted many surveys in the lead-up to the 2020 United States election, so there are many files. The filename is the reference date, where “ns20200625” refers to 25 June 2020. That is the file that we use here, but many of them are similar. We download and save it as “ns20200625.dta”.\nAs introduced in Online Appendix A, we can import “.dta” files after installing haven and labelled. The code that we use to import and prepare the survey dataset is based on that of Mitrovski, Yang, and Wankiewicz (2020).\n\nraw_nationscape_data &lt;-\n  read_dta(\"ns20200625.dta\")\n\n\n# The Stata format separates labels so reunite those\nraw_nationscape_data &lt;-\n  to_factor(raw_nationscape_data)\n\n# Just keep relevant variables\nnationscape_data &lt;-\n  raw_nationscape_data |&gt;\n  select(vote_2020, gender, education, state, age)\n\n\nnationscape_data\n\n# A tibble: 6,479 × 5\n   vote_2020                gender education                         state   age\n * &lt;fct&gt;                    &lt;fct&gt;  &lt;fct&gt;                             &lt;chr&gt; &lt;dbl&gt;\n 1 Donald Trump             Female Associate Degree                  WI       49\n 2 I am not sure/don't know Female College Degree (such as B.A., B.… VA       39\n 3 Donald Trump             Female College Degree (such as B.A., B.… VA       46\n 4 Donald Trump             Female High school graduate              TX       75\n 5 Donald Trump             Female High school graduate              WA       52\n 6 I would not vote         Female Other post high school vocationa… OH       44\n 7 Joe Biden                Female Completed some college, but no d… MA       21\n 8 Joe Biden                Female Completed some college, but no d… TX       38\n 9 Donald Trump             Female Completed some college, but no d… CA       69\n10 Donald Trump             Female College Degree (such as B.A., B.… NC       59\n# ℹ 6,469 more rows\n\n\nAt this point we want to clean up a few issues. For instance, for simplicity, remove anyone not voting for Trump or Biden.\n\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  filter(vote_2020 %in% c(\"Joe Biden\", \"Donald Trump\")) |&gt;\n  mutate(vote_biden = if_else(vote_2020 == \"Joe Biden\", 1, 0)) |&gt;\n  select(-vote_2020)\n\nWe then want to create some variables of interest.\n\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  mutate(\n    age_group = case_when(\n      age &lt;= 29 ~ \"18-29\",\n      age &lt;= 44 ~ \"30-44\",\n      age &lt;= 59 ~ \"45-59\",\n      age &gt;= 60 ~ \"60+\",\n      TRUE ~ \"Trouble\"\n    ),\n    gender = case_when(\n      gender == \"Female\" ~ \"female\",\n      gender == \"Male\" ~ \"male\",\n      TRUE ~ \"Trouble\"\n    ),\n    education_level = case_when(\n      education %in% c(\n        \"3rd Grade or less\",\n        \"Middle School - Grades 4 - 8\",\n        \"Completed some high school\",\n        \"High school graduate\"\n      ) ~ \"High school or less\",\n      education %in% c(\n        \"Other post high school vocational training\",\n        \"Completed some college, but no degree\"\n      ) ~ \"Some post sec\",\n      education %in% c(\n        \"Associate Degree\",\n        \"College Degree (such as B.A., B.S.)\",\n        \"Completed some graduate, but no degree\"\n      ) ~ \"Post sec +\",\n      education %in% c(\"Masters degree\",\n                       \"Doctorate degree\") ~ \"Grad degree\",\n      TRUE ~ \"Trouble\"\n    )\n  ) |&gt;\n  select(-education,-age)\n\nWe will draw on this dataset in Chapter 15, so we will save it.\n\nwrite_csv(x = nationscape_data,\n          file = \"nationscape_data.csv\")\n\nWe can also have a look at some of the variables (Figure 8.2).\n\nnationscape_data |&gt;\n  mutate(supports = if_else(vote_biden == 1, \"Biden\", \"Trump\")) |&gt; \n  mutate(supports = factor(supports, levels = c(\"Trump\", \"Biden\"))) |&gt; \n  ggplot(mapping = aes(x = age_group, fill = supports)) +\n  geom_bar(position = \"dodge2\") +\n  theme_minimal() +\n  labs(\n    x = \"Age-group of respondent\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  facet_wrap(vars(gender)) +\n  guides(x = guide_axis(angle = 90)) +\n  theme(legend.position = \"bottom\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nFigure 8.2: Examining some of the variables from the Nationscape survey dataset"
  },
  {
    "objectID": "08-hunt.html#rct-examples",
    "href": "08-hunt.html#rct-examples",
    "title": "8  Hunt data",
    "section": "8.4 RCT examples",
    "text": "8.4 RCT examples\n\n8.4.1 The Oregon Health Insurance Experiment\nIn the United States, unlike many developed countries, basic health insurance is not necessarily available to all residents, even those on low incomes. The Oregon Health Insurance Experiment involved low-income adults in Oregon, a state in the northwest of the United States, from 2008 to 2010 (Finkelstein et al. 2012).\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Amy Finkelstein is John & Jennie S. Macdonald Professor of Economics at MIT. After earning a PhD in Economics from MIT in 2001, she was a Junior Fellow at the Harvard Society of Fellows, before returning to MIT as an assistant professor in 2005, being promoted to full professor in 2008. One area of her research is health economics where she uses randomized controlled trials to understand insurance. She was one of the lead researchers on Finkelstein et al. (2012) which examined the Oregon Health Insurance Experiment. She was awarded the John Bates Clark Medal in 2012 and a MacArthur Foundation Fellowship in 2018.\n\n\nOregon funded 10,000 places in the state-run Medicaid program, which provides health insurance for people with low incomes. A lottery was used to allocate these places, and this was judged fair because it was expected, correctly as it turned out, that demand for places would exceed the supply. In the end, 89,824 individuals signed up.\nThe draws were conducted over a six-month period and 35,169 individuals were selected (the household of those who won the draw were given the opportunity) but only 30 per cent of them turned out to be eligible and completed the paperwork. The insurance lasted indefinitely. This random allocation of insurance allowed the researchers to understand the effect of health insurance.\nThe reason that this random allocation is important is that it is not usually possible to compare those with and without insurance because the type of people that sign up to get health insurance differ to those who do not. That decision is “confounded” with other variables and results in selection bias.\nAs the opportunity to apply for health insurance was randomly allocated, the researchers were able to evaluate the health and earnings of those who received health insurance and compare them to those who did not. To do this they used administrative data, such as hospital discharge data, matched credit reports, and, uncommonly, mortality records. The extent of this data is limited and so they also conducted a survey.\nThe specifics of this are not important, and we will have more to say in Chapter 12, but they estimate the model:\n\\[\ny_{ihj} = \\beta_0 + \\beta_1\\mbox{Lottery} + X_{ih}\\beta_2 + V_{ih}\\beta_3 + \\epsilon_{ihj}\n\\tag{8.1}\\]\nEquation 8.1 explains various \\(j\\) outcomes (such as health) for an individual \\(i\\) in household \\(h\\) as a function of an indicator variable as to whether household \\(h\\) was selected by the lottery. It is the \\(\\beta_1\\) coefficient that is of particular interest. That is the estimate of the mean difference between the treatment and control groups. \\(X_{ih}\\) is a set of variables that are correlated with the probability of being treated. These adjust for that impact to a certain extent. An example of that is the number of individuals in a household. And finally, \\(V_{ih}\\) is a set of variables that are not correlated with the lottery, such as demographics and previous hospital discharges.\nLike earlier studies such as Brook et al. (1984), Finkelstein et al. (2012) found that the treatment group used more health care including both primary and preventive care as well as hospitalizations but had lower out-of-pocket medical expenditures. More generally, the treatment group reported better physical and mental health.\n\n\n8.4.2 Civic Honesty Around The Globe\nTrust is not something that we think regularly about, but it is fundamental to most interactions, both economic and personal. For instance, many people get paid after they do some work—they are trusting their employer will make good, and vice versa. If you get paid in advance, then they are trusting you. In a strictly naive, one-shot, world without transaction costs, this does not make sense. If you get paid in advance, the incentive is for you to take the money and run in the last pay period before you quit, and through backward induction everything falls apart. We do not live in such a world. For one thing there are transaction costs, for another, generally, we have repeated interactions, and finally, the world usually ends up being fairly small.\nUnderstanding the extent of honesty in different countries may help us to explain economic development and other aspects of interest such as tax compliance, but it is hard to measure. We cannot ask people how honest they are—the liars would lie, resulting in a lemons problem (Akerlof 1970). This is a situation of adverse selection, where the liars know they are liars, but others do not. To get around this Cohn et al. (2019a) conduct an experiment in 355 cities across 40 countries where they “turned in” a wallet that was either empty or contained the local equivalent of US$13.45. They were interested in whether the “recipient” attempted to return the wallet. They found that generally wallets with money were more likely to be returned (Cohn et al. 2019a, 1).\nIn total Cohn et al. (2019a) “turn in” 17,303 wallets to various institutions including banks, museums, hotels, and police stations. The importance of such institutions to an economy is well accepted (Acemoglu, Johnson, and Robinson 2001) and they are common across most countries. Importantly, for the experiment, they usually have a reception area where the wallet could be turned in (Cohn et al. 2019a, 1).\nIn the experiment a research assistant turned in the wallet to an employee at the reception area, using a set form of words. The research assistant had to note various features of the setting, such as the gender, age-group, and busyness of the “recipient”. The wallets were transparent and contained a key, a grocery list, and a business card with a name and email address. The outcome of interest was whether an email was sent to the unique email address on the business card in the wallet. The grocery list was included to signal that the owner of the wallet was a local. The key was included as something that was only useful to the owner of the wallet, and never the recipient, in contrast to the cash, to adjust for altruism. The language and currency were adapted to local conditions.\nThe primary treatment in the experiment is whether the wallet contained money or not. The key outcome was whether the wallet was attempted to be returned or not. It was found that the median response time was 26 minutes, and that if an email was sent then it usually happened within a day (Cohn et al. 2019b, 10).\nUsing the data for the paper that is made available (Cohn 2019) we can see that considerable differences were found between countries (Figure 8.3). In almost all countries wallets with money were more likely to be returned than wallets without. The experiments were conducted across 40 countries, which were chosen based on them having enough cities with populations of at least 100,000, as well as the ability for the research assistants to safely visit and withdraw cash. Within those countries, the cities were chosen starting with the largest ones and there were usually 400 observations in each country (Cohn et al. 2019b, 5). Cohn et al. (2019a) further conducted the experiment with the equivalent of US$94.15 in three countries—Poland, the UK, and the US—and found that reporting rates further increased.\n\n\n\n\n\nFigure 8.3: Comparison of the proportion of wallets handed in, by country, depending on whether they contained money\n\n\n\n\nIn addition to the experiments, Cohn et al. (2019a) conducted surveys that allowed them to understand some reasons for their findings. During the survey, participants were given one of the scenarios and then asked to answer questions. The use of surveys also allowed them to be specific about the respondents. The survey involved 2,525 respondents (829 in the UK, 809 in Poland, and 887 in the US) (Cohn et al. 2019b, 36). Participants were chosen using attention checks and demographic quotas based on age, gender, and residence, and they received US$4.00 for their participation (Cohn et al. 2019b, 36). The survey did not find that larger rewards were expected for turning in a wallet with more money. But it did find that failure to turn in a wallet with more money caused the respondent to feel more like they had stolen money."
  },
  {
    "objectID": "08-hunt.html#ab-testing",
    "href": "08-hunt.html#ab-testing",
    "title": "8  Hunt data",
    "section": "8.5 A/B testing",
    "text": "8.5 A/B testing\nThe past two decades have probably seen the most experiments ever run, likely by several orders of magnitude. This is because of the extensive use of A/B testing at tech firms (Kohavi et al. 2012). For a long time decisions such as what font to use were based on the Highest Paid Person’s Opinion (HIPPO) (Christian 2012). These days, many large tech companies have extensive infrastructure for experiments. They term them A/B tests because of the comparison of two groups: one that gets treatment A and the other that either gets treatment B or does not see any change (Salganik 2018, 185). We could additionally consider more than two options at which point we typically use the terminology of “arms” of the experiment.\nThe proliferation of experiments in the private sector has brought with it a host of ethical concerns. Some private companies do not have ethical review boards, and there are different ethical concerns in the private sector compared with academia. For instance, many A/B tests are designed, explicitly, to make a consumer more likely to spend money. While society may not generally have a concern with that in the case of an online grocery retailer, society may have a problem in the case of an online gambling website. More extensive legislation and the development of private-sector ethical best practice are both likely as the extent of experimentation in the private sector becomes better known.\nEvery time you are online you are probably subject to tens, hundreds, or potentially thousands, of different A/B tests. While, at their heart, they are just experiments that use sensors to measure data that need to be analyzed, they have many special features that are interesting in their own light. For instance, Kohavi, Tang, and Xu (2020, 3) discuss the example of Microsoft’s search engine Bing. They used A/B testing to examine how to display advertisements. Based on these tests they ended up lengthening the title on the advertisement. They found this caused revenue to increase by 12 per cent, or around $100 million annually, without any significant measured trade-off.\nIn this book we use the term A/B test to refer to the situation in which we primarily implement an experiment through a technology stack about something that is primarily of the internet, such as a change to a website or similar and measured with sensors rather than a survey. While at their heart they are just experiments, A/B tests have a range of specific concerns. Bosch and Revilla (2022) detail some of these from a statistical perspective. There is something different about doing tens of thousands of small experiments all the time, compared with the typical RCT set-up of conducting one experiment over the course of months.\nRCTs are often, though not exclusively, done in academia or by government agencies, but much of A/B testing occurs in industry. This means that if you are in industry and want to introduce A/B testing to your firm there can be aspects such as culture and relationship building that become important. It can be difficult to convince a manager to run an experiment. Indeed, sometimes it can be easier to experiment by not delivering, or delaying, a change that has been decided to create a control group rather than a treatment group (Salganik 2018, 188). Sometimes the most difficult aspect of A/B testing is not the analysis, it is the politics. This is not unique to A/B testing and, for instance, looking at the history of biology, we see that even aspects such as germ theory were not resolved by experiment, but instead by ideology and social standing (Morange 2016, 124).\nFollowing Kohavi, Tang, and Xu (2020, 153), when conducting A/B testing, as with all experiments, we need to be concerned with delivery. In the case of an experiment, it is usually clear how it is being delivered. For instance, we may have the person come to a doctor’s clinic and then inject them with either a drug or a placebo. But in the case of A/B testing, it is less obvious. For instance, should we make a change to a website, or to an app? This decision affects our ability to both conduct the experiment and to gather data from it. (Urban, Sreenivasan, and Kannan (2016) provide an overview of A/B testing at Netflix, assuming an app is installed on a PlayStation 4.)\nIt is relatively easy and normal to update a website all the time. This means that small changes can be easily implemented if the A/B test is delivered that way. But in the case of an app, conducting an A/B test becomes a bigger deal. For instance, the release may need to go through an app store, and so would need to be part of a regular release cycle. There is also a selection concern: some users will not update the app and it is possible they are different to those that do regularly update the app.\nThe delivery decision also affects our ability to gather data from the A/B test. A website change is less of a big deal because we get data from a website whenever a user interacts with it. But in the case of an app, the user may use the app offline or with limited data upload which can add complications.\nWe need to plan! For instance, results are unlikely to be available the day after a change to an app, but they could be available the day after a change to a website. Further, we may need to consider our results in the context of different devices and platforms, potentially using, say, regression which will be covered in Chapter 12.\nThe second aspect of concern, as introduced in Chapter 6, is instrumentation. When we conduct a traditional experiment we might, for instance, ask respondents to fill out a survey. But this is usually not done with A/B testing. Instead we usually use various sensors (Kohavi, Tang, and Xu 2020, 162). One approach is to use cookies but different types of users will clear these at different rates. Another approach is to force the user to download a tiny image from a server, so that we know when they have completed some action. For instance, this is commonly used to track whether a user has opened an email. But again different types of users will block these at different rates.\nThe third aspect of concern is what are we randomizing over (Kohavi, Tang, and Xu 2020, 166)? In the case of traditional experiments, this is often a person, or sometimes various groups of people. But in the case of A/B testing it can be less clear. For instance, are we randomizing over the page, the session, or the user?\nTo think about this, let us consider color. For instance, say we are interested in whether we should change our logo from red to blue on the homepage. If we are randomizing at the page level, then if the user goes to some other page of our website, and then back to the homepage, the logo could change colors. If we are randomizing at the session level, then it could be blue while they use the website this time, if they close it and come back, then it could be red. Finally, if we are randomizing at a user level then possibly it would always be red for one user, but always blue for another.\nThe extent to which this matters depends on a trade-off between consistency and importance. For instance, if we are A/B testing product prices then consistency is likely an important feature. But if we are A/B testing background colors then consistency might not be as important. On the other hand, if we are A/B testing the position of a log-in button then it might be important that we not move that around too much for the one user, but between users it might matter less.\nIn A/B testing, as in traditional experiments, we are concerned that our treatment and control groups are the same, but for the treatment. In the case of traditional experiments, we satisfy ourselves of this by conducting analysis based on the data that we have after the experiment is conducted. That is usually all we can do because it would be weird to treat or control both groups. But in the case of A/B testing, the pace of experimentation allows us to randomly create the treatment and control groups, and then check, before we subject the treatment group to the treatment, that the groups are the same. For instance, if we were to show each group the same website, then we would expect the same outcomes across the two groups. If we found different outcomes then we would know that we may have a randomization issue (Taddy 2019, 129). This is termed an A/A test and was mentioned in Chapter 4.\nWe usually run A/B tests not because we desperately care about the specific outcome, but because that feeds into some other measure that we care about. For instance, do we care whether the website is quite-dark-blue or slightly-dark-blue? Probably not. We probably actually care a lot about the company share price. But what if the A/B test outcome of what is the best blue comes at a cost to the share price?\nTo illustrate this, pretend that we work at a food delivery app, and we are concerned with driver retention. Say we do some A/B tests and find that drivers are always more likely to be retained when they can deliver food to the customer faster. Our hypothetical finding is that faster is better, for driver retention, always. But one way to achieve faster deliveries is for the driver to not put the food into a hot box that would maintain the food’s temperature. Something like that might save 30 seconds, which is significant on a ten-minute delivery. Unfortunately, although we would decide to encourage that based on A/B tests designed to optimize driver-retention, such a decision would likely make the customer experience worse. If customers receive cold food that is meant to be hot, then they may stop using the app, which would be bad for the business. Chen et al. (2022) describe how they found a similar situation at Facebook in terms of notifications—although reducing the number of notifications reduced user engagement in the short-term, over the long-term it increased both user satisfaction and app usage.\nThis trade-off could become known during the hypothetical driver experiment if we were to look at customer complaints. It is possible that on a small team the A/B test analyst would be exposed to those tickets, but on a larger team they may not be. Ensuring that A/B tests are not resulting in false optimization is especially important. This is not something that we typically have to worry about in normal experiments. As another example of this Aprameya (2020) describes testing a feature of Duolingo, a language-learning application, which served an ad for Duolingo Plus when a regular Duolingo user was offline. The feature was found to be positive for Duolingo’s revenue, but negative for customer learning habits. Presumably enough customer negativity would eventually have resulted in the feature having a negative effect on revenue. Related to this, we want to think carefully about the nature of the result that we expect. For instance, in the shades of blues example, we are unlikely to find substantial surprises, and so it might be sufficient to try a small range of blues. But what if we considered a wider variety of colors?\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Susan Athey is the Economics of Technology Professor at Stanford University. After earning a PhD in Economics from Stanford in 1995, she joined MIT as an assistant professor, returning to Stanford in 2001, where she was promoted to full professor in 2004. One area of her research is applied economics, and one particularly important paper is Abadie et al. (2017), which considers when standard errors need to be clustered. Another is Athey and Imbens (2017a), which considers how to analyze randomized experiments. In addition to her academic appointments, she has worked at Microsoft and other technology firms and been extensively involved in running experiments in this context. She was awarded the John Bates Clark Medal in 2007.\n\n\n\n8.5.1 Upworthy\nThe trouble with much of A/B testing is that it is done by private firms and so we typically do not have access to their datasets. But Matias et al. (2021) provide access to a dataset of A/B tests from Upworthy, a media website that used A/B testing to optimize their content. Fitts (2014) provides more background information about Upworthy. And the datasets of A/B tests are available here.\nWe can look at what the dataset looks like and get a sense for it by looking at the names and an extract.\n\nupworthy &lt;- read_csv(\"https://osf.io/vy8mj/download\")\n\n\nupworthy |&gt;\n  names()\n\n [1] \"...1\"                 \"created_at\"           \"updated_at\"          \n [4] \"clickability_test_id\" \"excerpt\"              \"headline\"            \n [7] \"lede\"                 \"slug\"                 \"eyecatcher_id\"       \n[10] \"impressions\"          \"clicks\"               \"significance\"        \n[13] \"first_place\"          \"winner\"               \"share_text\"          \n[16] \"square\"               \"test_week\"           \n\nupworthy |&gt;\n  head()\n\n# A tibble: 6 × 17\n   ...1 created_at          updated_at          clickability_test_id     excerpt\n  &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;                    &lt;chr&gt;  \n1     0 2014-11-20 06:43:16 2016-04-02 16:33:38 546d88fb84ad38b2ce000024 Things…\n2     1 2014-11-20 06:43:44 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things…\n3     2 2014-11-20 06:44:59 2016-04-02 16:25:54 546d88fb84ad38b2ce000024 Things…\n4     3 2014-11-20 06:54:36 2016-04-02 16:25:54 546d902c26714c6c44000039 Things…\n5     4 2014-11-20 06:54:57 2016-04-02 16:31:45 546d902c26714c6c44000039 Things…\n6     5 2014-11-20 06:55:07 2016-04-02 16:25:54 546d902c26714c6c44000039 Things…\n# ℹ 12 more variables: headline &lt;chr&gt;, lede &lt;chr&gt;, slug &lt;chr&gt;,\n#   eyecatcher_id &lt;chr&gt;, impressions &lt;dbl&gt;, clicks &lt;dbl&gt;, significance &lt;dbl&gt;,\n#   first_place &lt;lgl&gt;, winner &lt;lgl&gt;, share_text &lt;chr&gt;, square &lt;chr&gt;,\n#   test_week &lt;dbl&gt;\n\n\nIt is also useful to look at the documentation for the dataset. This describes the structure of the dataset, which is that there are packages within tests. A package is a collection of headlines and images that were shown randomly to different visitors to the website, as part of a test. A test can include many packages. Each row in the dataset is a package and the test that it is part of is specified by the “clickability_test_id” column.\nThere are many variables. We will focus on:\n\n“created_at”;\n“clickability_test_id”, so that we can create comparison groups;\n“headline”;\n“impressions”, which is the number of people that saw the package; and\n“clicks” which is the number of clicks on that package.\n\nWithin each batch of tests, we are interested in the effect of the varied headlines on impressions and clicks.\n\nupworthy_restricted &lt;-\n  upworthy |&gt;\n  select(\n    created_at, clickability_test_id, headline, impressions, clicks\n    )\n\n\nhead(upworthy_restricted)\n\n# A tibble: 6 × 5\n  created_at          clickability_test_id     headline       impressions clicks\n  &lt;dttm&gt;              &lt;chr&gt;                    &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n1 2014-11-20 06:43:16 546d88fb84ad38b2ce000024 They're Being…        3052    150\n2 2014-11-20 06:43:44 546d88fb84ad38b2ce000024 They're Being…        3033    122\n3 2014-11-20 06:44:59 546d88fb84ad38b2ce000024 They're Being…        3092    110\n4 2014-11-20 06:54:36 546d902c26714c6c44000039 This Is What …        3526     90\n5 2014-11-20 06:54:57 546d902c26714c6c44000039 This Is What …        3506    120\n6 2014-11-20 06:55:07 546d902c26714c6c44000039 This Is What …        3380     98\n\n\nWe will focus on the text contained in headlines, and look at whether headlines that asked a question got more clicks than those that did not. We want to remove the effect of different images and so will focus on those tests that have the same image. To identify whether a headline asks a question, we search for a question mark. Although there are more complicated constructions that we could use, this is enough to get started.\n\nupworthy_restricted &lt;-\n  upworthy_restricted |&gt;\n  mutate(\n    asks_question =\n      str_detect(string = headline, pattern = \"\\\\?\")\n    )\n\nupworthy_restricted |&gt;\n  count(asks_question)\n\n# A tibble: 2 × 2\n  asks_question     n\n  &lt;lgl&gt;         &lt;int&gt;\n1 FALSE         19130\n2 TRUE           3536\n\n\nFor every test, and for every picture, we want to know whether asking a question affected the number of clicks.\n\nquestion_or_not &lt;-\n  upworthy_restricted |&gt;\n  summarise(\n    ave_clicks = mean(clicks),\n    .by = c(clickability_test_id, asks_question)\n  ) \n\nquestion_or_not |&gt;\n  pivot_wider(names_from = asks_question,\n              values_from = ave_clicks,\n              names_prefix = \"ave_clicks_\") |&gt;\n  drop_na(ave_clicks_FALSE, ave_clicks_TRUE) |&gt;\n  mutate(difference_in_clicks = ave_clicks_TRUE - ave_clicks_FALSE) |&gt; \n  summarise(average_differce = mean(difference_in_clicks))\n\n# A tibble: 1 × 1\n  average_differce\n             &lt;dbl&gt;\n1            -4.89\n\n\nWe could also consider a cross-tab (Table 8.4).\n\nquestion_or_not |&gt; \n  summarise(mean = mean(ave_clicks),\n            .by = asks_question) |&gt; \n  kable(\n    col.names = c(\"Asks a question?\", \"Mean clicks\"),\n    digits = 0\n  )\n\n\n\nTable 8.4: Difference between the average number of clicks\n\n\nAsks a question?\nMean clicks\n\n\n\n\nFALSE\n57\n\n\nTRUE\n44\n\n\n\n\n\n\nWe find that in general, having a question in the headline may slightly decrease the number of clicks on a headline, although if there is an effect it does not appear to be very large (Figure 8.4).\n\n\n\n\n\nFigure 8.4: Comparison of the average number of clicks when a headline contains a question mark or not"
  },
  {
    "objectID": "08-hunt.html#exercises",
    "href": "08-hunt.html#exercises",
    "title": "8  Hunt data",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\nScales\n\n(Plan) Consider the following scenario: A political candidate is interested in how two polling values change over the course of an election campaign: approval rating and vote-share. The two are measured as percentages, and are somewhat correlated. There tends to be large changes when there is a debate between candidates. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include five tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please identify and document a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched using the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nIn your own words, what is the fundamental problem of causal inference (write at least three paragraphs and include both examples and references)?\nIn your own words, what is the role of randomization in constructing a counterfactual (write at least three paragraphs and include both examples and references)?\nWhat is external validity (pick one)?\n\nFindings from an experiment hold in that setting.\nFindings from an experiment hold outside that setting.\nFindings from an experiment that has been repeated many times.\nFindings from an experiment for which code and data are available.\n\nWhat is internal validity (pick one)?\n\nFindings from an experiment hold in that setting.\nFindings from an experiment hold outside that setting.\nFindings from an experiment that has been repeated many times.\nFindings from an experiment for which code and data are available.\n\nPlease write some code for the following dataset that would randomly assign people into one of two groups.\n\n\nnetflix_data &lt;-\n  tibble(\n    person = c(\"Ian\", \"Ian\", \"Roger\", \"Roger\",\n      \"Roger\", \"Patricia\", \"Patricia\", \"Helen\"\n    ),\n    tv_show = c(\n      \"Broadchurch\", \"Duty-Shame\", \"Broadchurch\", \"Duty-Shame\",\n      \"Shetland\", \"Broadchurch\", \"Shetland\", \"Duty-Shame\"\n    ),\n    hours = c(6.8, 8.0, 0.8, 9.2, 3.2, 4.0, 0.2, 10.2)\n  )\n\n\nHow could you check that your randomization had been done appropriately (write at least three paragraphs and include both examples and references)?\nPretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for a government border security department. Write at least three paragraphs, with examples and references, discussing your thoughts, with regard to ethics, on this matter.\nWare (1989, 299) mentions “randomized-consent” and continues that it was “attractive in this setting because a standard approach to informed consent would require that parents of infants near death be approached to give informed consent for an invasive surgical procedure that would then, in some instances, not be administered. Those familiar with the agonizing experience of having a child in a neonatal intensive care unit can appreciate that the process of obtaining informed consent would be both frightening and stressful to parents.” To what extent do you agree with this position, especially given, as Ware (1989, 305), mentions “the need to withhold information about the study from parents of infants receiving Conventional Medical Therapy (CMT)”?\nWare (1989, 300) mentions “equipoise”. In your own words, could you please define and discuss it (write at least three paragraphs and include both examples and references)?\nPlease redo the Upworthy analysis, but for “!” instead of “?”. What is the difference in clicks?\n\n-8.3\n-7.2\n-4.5\n-5.6\n\n\n\n\nTutorial\nPlease consider the Special Virtual Issue on Nonresponse Rates and Nonresponse Adjustments of the Journal of Survey Statistics and Methodology. Focus on one aspect of the editorial, and with reference to relevant literature, please discuss it in at least two pages. Use Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations. Submit a PDF.\n\n\nPaper\nAt about this point the Howrah Paper from Online Appendix D would be appropriate.\n\n\n\n\nAbadie, Alberto, Susan Athey, Guido Imbens, and Jeffrey Wooldridge. 2017. “When Should You Adjust Standard Errors for Clustering?” Working Paper 24003. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24003.\n\n\nAcemoglu, Daron, Simon Johnson, and James Robinson. 2001. “The Colonial Origins of Comparative Development: An Empirical Investigation.” American Economic Review 91 (5): 1369–1401. https://doi.org/10.1257/aer.91.5.1369.\n\n\nAkerlof, George. 1970. “The Market for ‘Lemons’: Quality Uncertainty and the Market Mechanism.” The Quarterly Journal of Economics 84 (3): 488–500. https://doi.org/10.2307/1879431.\n\n\nAlsan, Marcella, and Amy Finkelstein. 2021. “Beyond Causality: Additional Benefits of Randomized Controlled Trials for Improving Health Care Delivery.” The Milbank Quarterly 99 (4): 864–81. https://doi.org/10.1111/1468-0009.12521.\n\n\nAlsan, Marcella, and Marianne Wanamaker. 2018. “Tuskegee and the Health of Black Men.” The Quarterly Journal of Economics 133 (1): 407–55. https://doi.org/10.1093/qje/qjx029.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2010. “The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con Out of Econometrics.” Journal of Economic Perspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nAprameya, Lavanya. 2020. “Improving Duolingo, One Experiment at a Time.” Duolingo Blog, January. https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/.\n\n\nAthey, Susan, and Guido Imbens. 2017a. “The Econometrics of Randomized Experiments.” In Handbook of Field Experiments, 73–140. Elsevier. https://doi.org/10.1016/bs.hefe.2016.10.003.\n\n\n———. 2017b. “The State of Applied Econometrics: Causality and Policy Evaluation.” Journal of Economic Perspectives 31 (2): 3–32. https://doi.org/10.1257/jep.31.2.3.\n\n\nBanerjee, Abhijit, and Esther Duflo. 2011. Poor Economics: A Radical Rethinking of the Way to Fight Global Poverty. New York: PublicAffairs.\n\n\nBanerjee, Abhijit, Esther Duflo, Rachel Glennerster, and Cynthia Kinnan. 2015. “The Miracle of Microfinance? Evidence from a Randomized Evaluation.” American Economic Journal: Applied Economics 7 (1): 22–53. https://doi.org/10.1257/app.20130533.\n\n\nBerry, Donald. 1989. “Comment: Ethics and ECMO.” Statistical Science 4 (4): 306–10. https://www.jstor.org/stable/2245830.\n\n\nBlair, Ed, Seymour Sudman, Norman M Bradburn, and Carol Stocking. 1977. “How to Ask Questions about Drinking and Sex: Response Effects in Measuring Consumer Behavior.” Journal of Marketing Research 14 (3): 316–21. https://doi.org/10.2307/3150769.\n\n\nBosch, Oriol, and Melanie Revilla. 2022. “When survey science met web tracking: Presenting an error framework for metered data.” Journal of the Royal Statistical Society: Series A (Statistics in Society), November, 1–29. https://doi.org/10.1111/rssa.12956.\n\n\nBouguen, Adrien, Yue Huang, Michael Kremer, and Edward Miguel. 2019. “Using Randomized Controlled Trials to Estimate Long-Run Impacts in Development Economics.” Annual Review of Economics 11 (1): 523–61. https://doi.org/10.1146/annurev-economics-080218-030333.\n\n\nBrandt, Allan. 1978. “Racism and Research: The Case of the Tuskegee Syphilis Study.” Hastings Center Report, 21–29. https://doi.org/10.2307/3561468.\n\n\nBrook, Robert, John Ware, William Rogers, Emmett Keeler, Allyson Ross Davies, Cathy Sherbourne, George Goldberg, Kathleen Lohr, Patricia Camp, and Joseph Newhouse. 1984. “The Effect of Coinsurance on the Health of Adults: Results from the RAND Health Insurance Experiment.” https://www.rand.org/pubs/reports/R3055.html.\n\n\nChen, Weijun, Yan Qi, Yuwen Zhang, Christina Brown, Akos Lada, and Harivardan Jayaraman. 2022. “Notifications: Why Less Is More,” December. https://medium.com/@AnalyticsAtMeta/notifications-why-less-is-more-how-facebook-has-been-increasing-both-user-satisfaction-and-app-9463f7325e7d.\n\n\nChristian, Brian. 2012. “The A/B Test: Inside the Technology That’s Changing the Rules of Business.” Wired, April. https://www.wired.com/2012/04/ff-abtesting/.\n\n\nCohn, Alain. 2019. “Data and code for: Civic Honesty Around the Globe.” Harvard Dataverse. https://doi.org/10.7910/dvn/ykbodn.\n\n\nCohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian Lukas Zünd. 2019a. “Civic Honesty Around the Globe.” Science 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\n\n\n———. 2019b. “Supplementary Materials for: Civic Honesty Around the Globe.” Science 365 (6448): 70–73.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nDeaton, Angus. 2010. “Instruments, Randomization, and Learning about Development.” Journal of Economic Literature 48 (2): 424–55. https://doi.org/10.1257/jel.48.2.424.\n\n\nDillman, Don, Jolene Smyth, and Leah Christian. (1978) 2014. Internet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design Method. 4th ed. Wiley.\n\n\nDruckman, James, and Donald Green. 2021. “A New Era of Experimental Political Science.” In Advances in Experimental Political Science, 1–16. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108777919.002.\n\n\nDuflo, Esther. 2020. “Field Experiments and the Practice of Policy.” American Economic Review 110 (7): 1952–73. https://doi.org/10.1257/aer.110.7.1952.\n\n\nEdelman, Murray, Liberty Vittert, and Xiao-Li Meng. 2021. “An Interview with Murray Edelman on the History of the Exit Poll.” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.3a25cd24.\n\n\nEdwards, Jonathan. 2017. “PACE team response shows a disregard for the principles of science.” Journal of Health Psychology 22 (9): 1155–58. https://doi.org/10.1177/1359105317700886.\n\n\nElliott, Michael, Brady West, Xinyu Zhang, and Stephanie Coffey. 2022. “The Anchoring Method: Estimation of Interviewer Effects in the Absence of Interpenetrated Sample Assignment.” Survey Methodology 48 (1): 25–48. http://www.statcan.gc.ca/pub/12-001-x/2022001/article/00005-eng.htm.\n\n\nElson, Malte. 2018. “Question Wording and Item Formulation.” https://doi.org/10.31234/osf.io/e4ktc.\n\n\nEnns, Peter, and Jake Rothschild. 2022. “Do You Know Where Your Survey Data Come From?” May. https://medium.com/3streams/surveys-3ec95995dde2.\n\n\nFinkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan Gruber, Joseph Newhouse, Heidi Allen, Katherine Baicker, and Oregon Health Study Group. 2012. “The Oregon Health Insurance Experiment: Evidence from the First Year.” The Quarterly Journal of Economics 127 (3): 1057–1106. https://doi.org/10.1093/qje/qjs020.\n\n\nFisher, Ronald. (1935) 1949. The Design of Experiments. 5th ed. London: Oliver; Boyd.\n\n\nFitts, Alexis Sobel. 2014. “The King of Content: How Upworthy Aims to Alter the Web, and Could End up Altering the World.” Columbia Journalism Review 53: 34–38. https://archives.cjr.org/feature/the%5Fking%5Fof%5Fcontent.php.\n\n\nFry, Hannah. 2020. “Big Tech Is Testing You.” The New Yorker, February, 61–65. https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you.\n\n\nGertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and Christel Vermeersch. 2016. Impact Evaluation in Practice. 2nd ed. The World Bank. https://doi.org/10.1596/978-1-4648-0779-4.\n\n\nGordon, Brett, Robert Moakler, and Florian Zettelmeyer. 2022. “Close Enough? A Large-Scale Exploration of Non-Experimental Approaches to Advertising Measurement.” Marketing Science, November. https://doi.org/10.1287/mksc.2022.1413.\n\n\nGordon, Brett, Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky. 2019. “A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook.” Marketing Science 38 (2): 193–225. https://doi.org/10.1287/mksc.2018.1135.\n\n\nGroves, Robert. 2011. “Three Eras of Survey Research.” Public Opinion Quarterly 75 (5): 861–71. https://doi.org/10.1093/poq/nfr057.\n\n\nHeller, Jean. 2022. “AP Exposes the Tuskegee Syphilis Study: The 50th Anniversary.” AP, July. https://apnews.com/article/tuskegee-study-ap-story-investigation-syphilis-53403657e77d76f52df6c2e2892788c9.\n\n\nHill, Austin Bradford. 1965. “The Environment and Disease: Association or Causation?” Proceedings of the Royal Society of Medicine 58 (5): 295–300.\n\n\nHolland, Paul. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81 (396): 945–60. https://doi.org/10.2307/2289064.\n\n\nHolliday, Derek, Tyler Reny, Alex Rossell Hayes, Aaron Rudkin, Chris Tausanovitch, and Lynn Vavreck. 2021. “Democracy Fund + UCLA Nationscape Methodology and Representativeness Assessment.”\n\n\nKohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and Ya Xu. 2012. “Trustworthy Online Controlled Experiments.” In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 12, 1st ed. ACM Press. https://doi.org/10.1145/2339530.2339653.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n\n\nLarmarange, Joseph. 2023. labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLetterman, Clark. 2021. “Q&A: How Pew Research Center surveyed nearly 30,000 people in India,” July. https://medium.com/pew-research-center-decoded/q-a-how-pew-research-center-surveyed-nearly-30-000-people-in-india-7c778f6d650e.\n\n\nLevay, Kevin, Jeremy Freese, and James Druckman. 2016. “The Demographic and Political Composition of Mechanical Turk Samples.” SAGE Open 6 (1): 1–17. https://doi.org/10.1177/2158244016636433.\n\n\nLichand, Guilherme, and Sharon Wolf. 2022. “Measuring Child Labor: Whom Should Be Asked, and Why It Matters,” March. https://doi.org/10.21203/rs.3.rs-1474562/v1.\n\n\nLight, Richard, Judith Singer, and John Willett. 1990. By Design: Planning Research on Higher Education. 1st ed. Cambridge: Harvard University Press.\n\n\nMatias, Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2021. “The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media.” Scientific Data 8 (1): 1–8. https://doi.org/10.1038/s41597-021-00934-7.\n\n\nMitrovski, Alen, Xiaoyan Yang, and Matthew Wankiewicz. 2020. “Joe Biden Projected to Win Popular Vote in 2020 US Election.” https://github.com/matthewwankiewicz/US_election_forecast.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSalganik, Matthew. 2018. Bit by Bit: Social Research in the Digital Age. New Jersey: Princeton University Press.\n\n\nSmith, Matthew. 2018. “Should Milk Go in a Cup of Tea First or Last?” July. https://yougov.co.uk/topics/consumer/articles-reports/2018/07/30/should-milk-go-cup-tea-first-or-last.\n\n\nStantcheva, Stefanie. 2023. “How to Run Surveys: A Guide to Creating Your Own Identifying Variation and Revealing the Invisible.” Annual Review of Economics. https://scholar.harvard.edu/files/stantcheva/files/How_to_run_surveys_Stantcheva.pdf.\n\n\nStolberg, Michael. 2006. “Inventing the Randomized Double-Blind Trial: The Nuremberg Salt Test of 1835.” Journal of the Royal Society of Medicine 99 (12): 642–43. https://doi.org/10.1177/014107680609901216.\n\n\nStolley, Paul. 1991. “When Genius Errs: R. A. Fisher and the Lung Cancer Controversy.” American Journal of Epidemiology 133 (5): 416–25. https://doi.org/10.1093/oxfordjournals.aje.a115904.\n\n\nSwain, Larry. 1985. “Basic Principles of Questionnaire Design.” Survey Methodology 11 (2): 161–70.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nTausanovitch, Chris, and Lynn Vavreck. 2021. “Democracy Fund + UCLA Nationscape Project.” https://www.voterstudygroup.org/data/nationscape.\n\n\nThe White House. 2023. “Recommendations on the Best Practices for the Collection of Sexual Orientation and Gender Identity Data on Federal Statistical Survey,” January. https://www.whitehouse.gov/wp-content/uploads/2023/01/SOGI-Best-Practices.pdf.\n\n\nTourangeau, Roger, Lance Rips, and Kenneth Rasinski. 2000. The Psychology of Survey Response. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9780511819322.003.\n\n\nUrban, Steve, Rangarajan Sreenivasan, and Vineet Kannan. 2016. “It’s All A/Bout Testing: The Netflix Experimentation Platform.” Netflix Technology Blog, April. https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15.\n\n\nVavreck, Lynn, and Chris Tausanovitch. 2021. “Democracy Fund + UCLA Nationscape Project User Guide.” https://www.voterstudygroup.org/data/nationscape.\n\n\nWare, James. 1989. “Investigating Therapies of Potentially Great Benefit: ECMO.” Statistical Science 4 (4): 298–306. https://doi.org/10.1214/ss/1177012384.\n\n\nWei, LJ, and S Durham. 1978. “The Randomized Play-the-Winner Rule in Medical Trials.” Journal of the American Statistical Association 73 (364): 840–43. https://doi.org/10.2307/2286290.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nXu, Ya. 2020. “Causal Inference Challenges in Industry: A Perspective from Experiences at LinkedIn.” YouTube, July. https://youtu.be/OoKsLAvyIYA.\n\n\nYoshioka, Alan. 1998. “Use of Randomisation in the Medical Research Council’s Clinical Trial of Streptomycin in Pulmonary Tuberculosis in the 1940s.” BMJ 317 (7167): 1220–23. https://doi.org/10.1136/bmj.317.7167.1220."
  },
  {
    "objectID": "09-clean_and_prepare.html#introduction",
    "href": "09-clean_and_prepare.html#introduction",
    "title": "9  Clean and prepare",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\n\n“Well, Lyndon, you may be right and they may be every bit as intelligent as you say,” said Rayburn, “but I’d feel a whole lot better about them if just one of them had run for sheriff once.”\nSam Rayburn reacting to Lyndon Johnson’s enthusiasm about John Kennedy’s incoming cabinet, as quoted in The Best and the Brightest (Halberstam 1972, 41).\n\nIn this chapter we put in place more formal approaches for data cleaning and preparation. These are centered around:\n\nvalidity;\ninternal consistency; and\nexternal consistency.\n\nYour model does not care whether you validated your data, but you should. Validity means that the values in the dataset are not obviously wrong. For instance, with few exceptions, currencies should not have letters in them, names should not have numbers, and velocities should not be faster than the speed of light. Internal consistency means the dataset does not contradict itself. For instance, that might mean that constituent columns add to the total column. External consistency means that the dataset does not, in general, contradict outside sources, and is deliberate when it does. For instance, if our dataset purports to be about the population of cities, then we would expect that they are the same as, to a rough approximation, say, those available from relevant censuses on Wikipedia.\nSpaceX, the United States rocket company, uses cycles of ten or 50 Hertz (equivalent to 0.1 and 0.02 seconds, respectively) to control their rockets. Each cycle, the inputs from sensors, such as temperature and pressure, are read, processed, and used to make a decision, such as whether to adjust some setting (Martin and Popper 2021). We recommend a similar iterative approach of small adjustments during data cleaning and preparation. Rather than trying to make everything perfect from the start, just get started, and iterate through a process of small, continuous improvements.\nTo a large extent, the role of data cleaning and preparation is so great that the only people that understand a dataset are those that have cleaned it. Yet, the paradox of data cleaning is that often those that do the cleaning and preparation are those that have the least trust in the resulting dataset. At some point in every data science workflow, those doing the modeling should do some data cleaning. Even though few want to do it (Sambasivan et al. 2021), it can be as influential as modeling. To clean and prepare data is to make many decisions, some of which may have important effects on our results. For instance, Northcutt, Athalye, and Mueller (2021) find the test sets of some popular datasets in computer science contain, on average, labels that are wrong in around three per cent of cases. Banes et al. (2022) re-visit the Sumatran orang-utan (Pongo abelii) reference genome and find that nine of the ten samples had some issue. And Du, Huddart, and Jiang (2022) find a substantial difference between the as-filed and standardized versions of a company’s accounting data, especially for complex financial situations. Like Sam Rayburn wishing that Kennedy’s cabinet despite their intelligence, had experience in the nitty-gritty, a data scientist needs to immerse themselves in the messy reality of their dataset.\nThe reproducibility crisis, which was identified early in psychology (Open Science Collaboration 2015) but since extended to many other disciplines in the physical and social sciences, brought to light issues such as p-value “hacking”, researcher degrees of freedom, file-drawer issues, and even data and results fabrication (Gelman and Loken 2013). Steps are now being put in place to address these. But there has been relatively little focus on the data gathering, cleaning, and preparation aspects of applied statistics, despite evidence that decisions made during these steps greatly affect statistical results (Huntington-Klein et al. 2021). In this chapter we focus on these issues.\nWhile the statistical practices that underpin data science are themselves correct and robust when applied to simulated datasets, data science is not typically conducted with data that follow the assumptions underlying the models that are commonly fit. For instance, data scientists are interested in “messy, unfiltered, and possibly unclean data—tainted by heteroskedasticity, complex dependence and missingness patterns—that until recently were avoided in polite conversations between more traditional statisticians” (Craiu 2019). Big data does not resolve this issue and may even exacerbate it. For instance, population inference based on larger amounts of poor-quality data, without adjusting for data issues, will just lead to more confidently wrong conclusions (Meng 2018). The problems that are found in much of applied statistics research are not necessarily associated with researcher quality, or their biases (Silberzahn et al. 2018). Instead, they are a result of the context within which data science is conducted. This chapter provides an approach and tools to explicitly think about this work.\nGelman and Vehtari (2021), writing about the most important statistical ideas of the past 50 years, say that each of them enabled new ways of thinking about data analysis. These ideas brought into the tent of statistics, approaches that “had been considered more a matter of taste or philosophy”. The focus on data cleaning and preparation in this chapter is analogous, insofar as it represents a codification, or bringing inside the tent, of aspects that are typically, incorrectly, considered those of taste rather than core statistical concerns.\nThe workflow for data cleaning and preparation that we advocate is:\n\nSave the original, unedited data.\nBegin with an end in mind by sketching and simulating.\nWrite tests and documentation.\nExecute the plan on a small sample.\nIterate the plan.\nGeneralize the execution.\nUpdate tests and documentation.\n\nWe will need a variety of skills to be effective, but this is the very stuff of data science. The approach needed is some combination of dogged and sensible. Perfect is very much the enemy of good enough when it comes to data cleaning. And to be specific, it is better to have 90 per cent of the data cleaned and prepared, and to start exploring that, before deciding whether it is worth the effort to clean and prepare the remaining 10 per cent. Because that remainder will likely take an awful lot of time and effort.\nAll data regardless of whether they were obtained from farming, gathering, or hunting, will have issues. We need approaches that can deal with a variety of concerns, and more importantly, understand how they might affect our modeling (Van den Broeck et al. 2005). To clean data is to analyze data. This is because the process forces us to make choices about what we value in our results (Au 2020)."
  },
  {
    "objectID": "09-clean_and_prepare.html#workflow",
    "href": "09-clean_and_prepare.html#workflow",
    "title": "9  Clean and prepare",
    "section": "9.2 Workflow",
    "text": "9.2 Workflow\n\n9.2.1 Save the original, unedited data\nThe first step is to save the original, unedited data into a separate, local folder. The original, unedited data establishes the foundation for reproducibility (Wilson et al. 2017). If we obtained our data from a third-party, such as a government website, then we have no control over whether they will continue to host that data, update it, or change the address at which it is available. Saving a local copy also reduces the burden that we impose on their servers.\nHaving locally saved the original, unedited data we must maintain a copy of it in that state, and not modify it. As we begin to clean and prepare it, we instead make these changes to a copy of the dataset. Maintaining the original, unedited dataset, and using scripts to create the dataset that we are interested in analyzing, ensures that our entire workflow is reproducible. It may be that the changes that we decide to make today, are not ones that we would make tomorrow, having learnt more about the dataset. We need to ensure that we have that data in the original, unedited state in case we need to return to it (Borer et al. 2009).\nWe may not always be allowed to share that original, unedited data, but we can almost always create something similar. For instance, if we are using a restricted-use computer, then it may be that the best we can do is create a simulated version of the original, unedited data that conveys the main features, and include detailed access instructions in a README file.\n\n\n9.2.2 Plan\nPlanning the endpoint forces us to begin with an end in mind and is important for a variety of reasons. As with scraping data, introduced in Chapter 7, it helps us to be proactive about scope-creep. But with data cleaning it additionally forces us to really think about what we want the final dataset to look like.\nThe first step is to sketch the dataset that we are interested in. The key features of the sketch will be aspects such as the names of the columns, their class, and the possible range of values. For instance, we might be interested in the populations of US states. Our sketch might look like Figure 9.1.\n\n\n\nFigure 9.1: Planned dataset of US states and their populations\n\n\nIn this case, the sketch forces us to decide that we want full names rather than abbreviations for the state names, and the population to be measured in millions. The process of sketching this endpoint has forced us to make decisions early on and be clear about our desired endpoint.\nWe then implement that using code to simulate data. Again, this process forces us to think about what reasonable values look like in our dataset because we must decide which functions to use. We need to think carefully about the unique values of each variable. For instance, if the variable is meant to be “gender” then unique values such as “male”, “female”, “other”, and “unknown” may be expected, but a number such as “1,000” would likely be wrong. It also forces us to be explicit about names because we must assign the output of those functions to a variable. For instance, we could simulate some population data for the US states.\n\nset.seed(853)\n\nsimulated_population &lt;-\n  tibble(\n    state = state.name,\n    population = runif(n = 50, min = 0, max = 50) |&gt;\n      round(digits = 2)\n  )\n\nsimulated_population\n\n# A tibble: 50 × 2\n   state       population\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 Alabama          18.0 \n 2 Alaska            6.01\n 3 Arizona          24.2 \n 4 Arkansas         15.8 \n 5 California        1.87\n 6 Colorado         20.2 \n 7 Connecticut       6.54\n 8 Delaware         12.1 \n 9 Florida           7.9 \n10 Georgia           9.44\n# ℹ 40 more rows\n\n\nOur purpose, during data cleaning and preparation, is to then bring our original, unedited data close to that plan. Ideally, we would plan so that the desired endpoint of our dataset is “tidy data”. This is introduced in Online Appendix A, but briefly, it means that (Wickham, Çetinkaya-Rundel, and Grolemund [2016] 2023; Wickham 2014, 4):\n\neach variable is in its own column;\neach observation is in its own row; and\neach value is in its own cell.\n\nBegin thinking about validity and internal consistency at this stage. What are some of the features that these data should have? Note these as you go through the process of simulating the dataset because we will draw on them to write tests.\n\n\n9.2.3 Start small\nHaving thoroughly planned we can turn to the original, unedited data that we are dealing with. Usually we want to manipulate the original, unedited data into a rectangular dataset as quickly as possible. This allows us to use familiar functions from the tidyverse. For instance, let us assume that we are starting with a .txt file.\nThe first step is to look for regularities in the dataset. We want to end up with tabular data, which means that we need some type of delimiter to distinguish different columns. Ideally this might be features such as a comma, a semicolon, a tab, a double space, or a line break. In the following case we could take advantage of the comma.\nAlabama, 5\nAlaska, 0.7\nArizona, 7\nArkansas, 3\nCalifornia, 40\nIn more challenging cases there may be some regular feature of the dataset that we can take advantage of. Sometimes various text is repeated, as in the following case.\nState is Alabama and population is 5 million.\nState is Alaska and population is 0.7 million.\nState is Arizona and population is 7 million.\nState is Arkansas and population is 3 million.\nState is California and population is 40 million.\nIn this case, although we do not have a traditional delimiter, we can use the regularity of “State is”, “and population is”, and “million” to get what we need. A more difficult case is when we do not have line breaks. This final case is illustrative of that.\nAlabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\nOne way to approach this is to take advantage of the different classes and values that we are looking for. For instance, we know that we are after US states, so there are only 50 possible options (setting D.C. to one side for the time being), and we could use the these as a delimiter. We could also use the fact that population is a number, and so separate based on a space followed by a number.\nWe will now convert this final case into tidy data.\n\nunedited_data &lt;-\n  c(\"Alabama 5 Alaska 0.7 Arizona 7 Arkansas 3 California 40\")\n\ntidy_data &lt;-\n  tibble(raw = unedited_data) |&gt;\n  separate(\n    col = raw,\n    into = letters[1:5],\n    sep = \"(?&lt;=[[:digit:]]) \" # A bracket preceded by numbers\n  ) |&gt;\n  pivot_longer(\n    cols = letters[1:5],\n    names_to = \"drop_me\",\n    values_to = \"separate_me\"\n  ) |&gt;\n  separate(\n    col = separate_me,\n    into = c(\"state\", \"population\"),\n    sep = \" (?=[[:digit:]])\" # A space followed by a number\n  ) |&gt;\n  select(-drop_me)\n\ntidy_data\n\n# A tibble: 5 × 2\n  state      population\n  &lt;chr&gt;      &lt;chr&gt;     \n1 Alabama    5         \n2 Alaska     0.7       \n3 Arizona    7         \n4 Arkansas   3         \n5 California 40        \n\n\n\n\n9.2.4 Write tests and documentation\nHaving established a rectangular dataset, albeit a messy one, we should begin to look at the classes that we have. We do not necessarily want to fix the classes at this point, because that can result in lost data. But we look at the class to see what it is, compare it to our simulated dataset, and note the columns where it is different to see what changes need to be made. Background on class() is available in Online Appendix A.\nBefore changing the class and before going on to more bespoke issues, we should deal with some common issues including:\n\nCommas and other punctuation, such as denomination signs ($, €, £, etc.), in variables that should be numeric.\nInconsistent formatting of dates, such as “December” and “Dec” and “12” all in the one variable.\nUnexpected character encoding, especially in Unicode, which may not display consistently.1\n\nTypically, we want to fix anything immediately obvious. For instance, we should remove commas that have been used to group digits in currencies. However, the situation will often feel overwhelming. What we need to do is to look at the unique values in each variable, and then triage what we will fix. We make the triage decision based on what is likely to have the largest impact. That usually means creating counts of the observations, sorting them in descending order, and then dealing with them in this order.\nWhen the tests of membership are passed—which we initially establish based on simulation and experience—then we can change the class, and run all the tests again. We have adapted this idea from the software development approach of unit testing. Tests are crucial because they enable us to understand whether software (or in this case data) is fit for our purpose (Irving et al. 2021). Tests, especially in data science, are not static things that we just write once and then forget. Instead they should update and evolve as needed.\n\n\n\n\n\n\nOh, you think we have good data on that!\n\n\n\nThe simplification of reality can be especially seen in sports records, which necessarily must choose what to record. Sports records are fit for some purposes and not for others. For instance, chess is played on an 8 x 8 board of alternating black and white squares. The squares are detonated by a unique combination of both a letter (A-G) and a number (1-8). Most pieces have a unique abbreviation, for instance knights are N and bishops are B. Each game is independently recorded using this “algebraic notation” by each player. These records allow us to recreate the moves of the game. The 2021 Chess World Championship was contested by Magnus Carlsen and Ian Nepomniachtchi. There were a variety of reasons this game was particularly noteworthy—including it being the longest world championship game—but one is the uncharacteristic mistakes that both Carlsen and Nepomniachtchi made. For instance, at Move 33 Carlsen did not exploit an opportunity; and at Move 36 a different move would have provided Nepomniachtchi with a promising endgame (Doggers 2021). One reason for these mistakes may have been that both players at that point in the game had very little time remaining—they had to decide on their moves very quickly. But there is no sense of that in the representation provided by the game sheet because it does not record time remaining. The record is fit for purpose as a “correct” representation of what happened in the game; but not necessarily why it happened.\n\n\nLet us run through an example with a collection of strings, some of which are slightly wrong. This type of output is typical of OCR, introduced in Chapter 7, which often gets most of the way there, but not quite.\n\nmessy_string &lt;- paste(\n  c(\"Patricia, Ptricia, PatricIa, Patric1a, PatricIa\"),\n  c(\"PatrIcia, Patricia, Patricia, Patricia , 8atricia\"),\n  sep = \", \"\n)\n\nAs before, we first get this into a rectangular dataset.\n\nmessy_dataset &lt;-\n  tibble(names = messy_string) |&gt;\n  separate_rows(names, sep = \", \")\n\nmessy_dataset\n\n# A tibble: 10 × 1\n   names      \n   &lt;chr&gt;      \n 1 \"Patricia\" \n 2 \"Ptricia\"  \n 3 \"PatricIa\" \n 4 \"Patric1a\" \n 5 \"PatricIa\" \n 6 \"PatrIcia\" \n 7 \"Patricia\" \n 8 \"Patricia\" \n 9 \"Patricia \"\n10 \"8atricia\" \n\n\nWe now need to decide which of these errors we are going to fix. To help us decide which are most important, we create a count.\n\nmessy_dataset |&gt;\n  count(names, sort = TRUE)\n\n# A tibble: 7 × 2\n  names           n\n  &lt;chr&gt;       &lt;int&gt;\n1 \"Patricia\"      3\n2 \"PatricIa\"      2\n3 \"8atricia\"      1\n4 \"PatrIcia\"      1\n5 \"Patric1a\"      1\n6 \"Patricia \"     1\n7 \"Ptricia\"       1\n\n\nThe most common unique observation is the correct one. The next one—“PatricIa”—looks like the “i” has been incorrectly capitalized. This is true for “PatrIcia” as well. We can fix the capitalization issues with str_to_title(), which converts the first letter of each word in a string to uppercase and the rest to lowercase, and then redo the count.\nBackground on strings is available in Online Appendix A.\n\nmessy_dataset_fix_I_8 &lt;-\n  messy_dataset |&gt;\n  mutate(\n    names = str_to_title(names)\n  )\n\nmessy_dataset_fix_I_8 |&gt;\n  count(names, sort = TRUE)\n\n# A tibble: 5 × 2\n  names           n\n  &lt;chr&gt;       &lt;int&gt;\n1 \"Patricia\"      6\n2 \"8atricia\"      1\n3 \"Patric1a\"      1\n4 \"Patricia \"     1\n5 \"Ptricia\"       1\n\n\nAlready this is much better with 60 per cent of the values are correct, compared with the earlier 30 per cent. There are two more clear errors—“8tricia” and “Ptricia”—with the first distinguished by an “8” instead of a “P”, and the second missing an “a”. We can fix these issues with str_replace_all().\n\nmessy_dataset_fix_a_n &lt;-\n  messy_dataset_fix_I_8 |&gt;\n  mutate(\n    names = str_replace_all(names, \"8atricia\", \"Patricia\"),\n    names = str_replace_all(names, \"Ptricia\", \"Patricia\")\n  )\n\nmessy_dataset_fix_a_n |&gt;\n  count(names, sort = TRUE)\n\n# A tibble: 3 × 2\n  names           n\n  &lt;chr&gt;       &lt;int&gt;\n1 \"Patricia\"      8\n2 \"Patric1a\"      1\n3 \"Patricia \"     1\n\n\nWe have achieved an 80 per cent outcome with not too much effort. The final two issues are more subtle. The first has occurred because the “i” has been incorrectly coded as a “1”. In some fonts this will show up, but in others it will be more difficult to see. This is a common issue, especially with OCR, and something to be aware of. The second occurs because of a trailing space. Trailing and leading spaces are another common issue and we can address them with str_trim(). After we fix these two remaining issues we have all entries corrected.\n\ncleaned_data &lt;-\n  messy_dataset_fix_a_n |&gt;\n  mutate(\n    names = str_replace_all(names, \"Patric1a\", \"Patricia\"),\n    names = str_trim(names, side = c(\"right\"))\n  )\n\ncleaned_data |&gt;\n  count(names, sort = TRUE)\n\n# A tibble: 1 × 2\n  names        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Patricia    10\n\n\nWe have been doing the tests in our head in this example. We know that we are hoping for “Patricia”. But we can start to document this test as well. One way is to look to see if values other than “Patricia” exist in the dataset.\n\ncheck_me &lt;-\n  cleaned_data |&gt;\n  filter(names != \"Patricia\")\n\nif (nrow(check_me) &gt; 0) {\n  print(\"Still have values that are not Patricia!\")\n}\n\nWe can make things a little more imposing by stopping our code execution if the condition is not met with stopifnot(). To use this function we define a condition that we would like met. We could implement this type of check throughout our code. For instance if we expected there to be a certain number of observations in the dataset, or for a certain variable to have various properties, such as being an integer or a factor.\n\nstopifnot(nrow(check_me) == 0)\n\nWe can use stopifnot() to ensure that our script is working as expected as it runs.\nAnother way to write tests for our dataset is to use testthat. Although developed for testing packages, we can use the functionality to test our datasets. For instance, we can use expect_length() to check the length of a dataset and expect_equal() to check the content.\n\n# Is the dataset of length one?\nexpect_length(check_me, 1) \n# Are the observations characters?\nexpect_equal(class(cleaned_data$names), \"character\") \n# Is every unique observation \"Patricia\"?\nexpect_equal(unique(cleaned_data$names), \"Patricia\") \n\nIf the tests pass then nothing happens, but if the tests fail then the script will stop.\nWhat do we test? It is a difficult problem, and we detail a range of more-specific tests in the next section. But broadly we test what we have, against what we expect. The engineers working on the software for the Apollo program in the 1960s initially considered writing tests to be “busy work” (Mindell 2008, 170). But they eventually came to realize that NASA would not have faith that software could be used to send men to the moon unless it was accompanied by a comprehensive suite of tests. And it is the same for data science.\nStart with tests for validity. These will typically check the class of the variables, their unique values, and the number of observations. For instance, if we were using a recent dataset then columns that are years could be tested to ensure that all elements have four digits and start with a “2”. Baumgartner (2021) describes this as tests on the schema.\nAfter that, turn to checks of internal consistency. For instance, if there are variables of different numeric responses, then check that the sum of those equals a total variable, or if it does not then this difference is explainable. Finally, turn to tests for external consistency. Here we want to use outside information to inform our tests. For instance, if we had a variable of the neonatal mortality rate (NMR) for Germany (this concept was introduced in Chapter 2), then we could look at the estimates from the World Health Organization (WHO), and ensure our NMR variable aligns. Experienced analysts do this all in their head. The issue is that it does not scale, can be inconsistent, and overloads on reputation. We return to this issue in Chapter 12 in the context of modeling.\nWe write tests throughout our code, rather than right at the end. In particular, using stopifnot() statements on intermediate steps ensures that the dataset is being cleaned in a way that we expect. For instance, when merging two datasets we could check:\n\nThe variable names in the datasets are unique, apart from the column/s to be used as the key/s.\nThe number of observations of each type is being carried through appropriately.\nThe dimensions of the dataset are not being unexpectedly changed.\n\n\n\n9.2.5 Iterate, generalize, and update\nWe could now iterate the plan. In this most recent case, we started with ten entries. There is no reason that we could not increase this to 100 or even 1,000. We may need to generalize the cleaning procedures and tests. But eventually we would start to bring the dataset into some sort of order."
  },
  {
    "objectID": "09-clean_and_prepare.html#checking-and-testing",
    "href": "09-clean_and_prepare.html#checking-and-testing",
    "title": "9  Clean and prepare",
    "section": "9.3 Checking and testing",
    "text": "9.3 Checking and testing\nRobert Caro, the biographer of Lyndon Johnson introduced in Chapter 4, spent years tracking down everyone connected to the 36th President of the United States. Caro and his wife Ina went so far as to live in Texas Hill Country for three years so that they could better understand where Johnson was from. When Caro heard that Johnson, as a senator, would run to the Senate from where he stayed in D.C., he ran that route multiple times himself to try to understand why Johnson was running. Caro eventually understood it only when he ran the route as the sun was rising, just as Johnson had done; it turns out that the sun hits the Senate Rotunda in a particularly inspiring way (Caro 2019, 156). This background work enabled him to uncover aspects that no one else knew. For instance, Johnson almost surely stole his first election win (Caro 2019, 116). We need to understand our data to this same extent. We want to metaphorically turn every page.\nThe idea of negative space is well established in design. It refers to that which surrounds the subject. Sometimes negative space is used as an effect. For instance the logo of FedEx, an American logistics company, has negative space between the E and X that creates an arrow. In a similar way, we want to be cognizant of the data that we have, and the data that we do not have (Hodgetts 2022). We are worried that the data that we do not have somehow has meaning, potentially even to the extent of changing our conclusions. When we are cleaning data, we are looking for anomalies. We are interested in values that are in the dataset that should not be, but also the opposite situation—values that should be in the dataset but are not. There are three tools that we use to identify these situations: graphs, counts, and tests.\nWe also use these tools to ensure that we are not changing correct observations to incorrect. Especially when our cleaning and preparation requires many steps, it may be that fixes at one stage are undone later. We use graphs, counts, and especially tests, to prevent this. The importance of these grows exponentially with the size of the dataset. Small and medium datasets are more amenable to manual inspection and other aspects that rely on the analyst, while larger datasets especially require more efficient strategies (Hand 2018).\n\n9.3.1 Graphs\nGraphs are an invaluable tool when cleaning data, because they show each observation in the dataset, potentially in relation to the other observations. They are useful for identifying when a value does not belong. For instance, if a value is expected to be numerical, but is still a character then it will not plot, and a warning will be displayed. Graphs will be especially useful for numerical data, but are still useful for text and categorical data. Let us pretend that we have a situation where we are interested in a person’s age, for some youth survey. We have the following data:\n\nyouth_survey_data &lt;-\n  tibble(ages = c(\n    15.9, 14.9, 16.6, 15.8, 16.7, 17.9, 12.6, 11.5, 16.2, 19.5, 150\n  ))\n\n\nyouth_survey_data |&gt;\n  ggplot(aes(x = ages)) +\n  geom_histogram(binwidth = 1) +\n  theme_minimal() +\n  labs(\n    x = \"Age of respondent\",\n    y = \"Number of respondents\"\n  )\n\nyouth_survey_data_fixed |&gt;\n  ggplot(aes(x = ages)) +\n  geom_histogram(binwidth = 1) +\n  theme_minimal() +\n  labs(\n    x = \"Age of respondent\",\n    y = \"Number of respondents\"\n  )\n\n\n\n\n\n\n\n(a) Before cleaning\n\n\n\n\n\n\n\n(b) After cleaning\n\n\n\n\nFigure 9.2: The ages in the simulated youth survey dataset identify a data issue\n\n\n\nFigure 9.2 (a) shows an unexpected value of 150. The most likely explanation is that the data were incorrectly entered, missing the decimal place, and should be 15.0. We could fix that, document it, and then redo the graph, which would show that everything seemed more valid (Figure 9.2 (b)).\n\n\n9.3.2 Counts\nWe want to focus on getting most of the data right, so we are interested in the counts of unique values. Hopefully most of the data are concentrated in the most common counts. But it can also be useful to invert it and see what is especially uncommon. The extent to which we want to deal with these depends on what we need. Ultimately, each time we fix one we are getting very few additional observations, potentially even just one. Counts are especially useful with text or categorical data but can be helpful with numerical data as well.\nLet us see an example of text data, each of which is meant to be “Australia”.\n\naustralian_names_data &lt;-\n  tibble(\n    country = c(\n      \"Australie\", \"Austrelia\", \"Australie\", \"Australie\", \"Aeustralia\",\n      \"Austraia\", \"Australia\", \"Australia\", \"Australia\", \"Australia\"\n    )\n  )\n\naustralian_names_data |&gt;\n  count(country, sort = TRUE)\n\n# A tibble: 5 × 2\n  country        n\n  &lt;chr&gt;      &lt;int&gt;\n1 Australia      4\n2 Australie      3\n3 Aeustralia     1\n4 Austraia       1\n5 Austrelia      1\n\n\nThe use of this count identifies where we should spend our time: changing “Australie” to “Australia” would almost double the amount of usable data.\nTurning, briefly to numeric data, Preece (1981) recommends plotting counts of the final digit of each observation in a variable. For instance, if the observations of the variable were “41.2”, “80.3”, “20.7”, “1.2”, “46.5”, “96.2”, “32.7”, “44.3”, “5.1”, and “49.0”. Then we note that 0, 1 and 5 all occur once, 3 and 7 occur twice, and 2 occurs three times. We might expect that there should be a uniform distribution of these final digits. But that is surprisingly often not the case, and the ways in which it differs can be informative. For instance, it may be that data were rounded, or recorded by different collectors.\nFor instance, later in this chapter we will gather, clean, and prepare some data from the 2019 Kenyan census. We pre-emptively use that dataset here and look at the count of the final digits of the ages. That is, say, from age 35 we take “5”, from age 74, we take “4”. Table 9.1 shows the expected age-heaping that occurs because some respondents reply to questions about age with a value to the closest 5 or 10. If we had an age variable without that pattern then we might expect it had been constructed from a different type of question.\n\n\n\n\nTable 9.1: Excess of 0 and 5 digits in counts of the final digits of single-year ages in Nairobi from the 2019 Kenyan census\n\n\nFinal digit of age\nNumber of times\n\n\n\n\n0\n347,233\n\n\n1\n278,930\n\n\n2\n308,933\n\n\n3\n285,745\n\n\n4\n270,355\n\n\n5\n303,817\n\n\n6\n246,582\n\n\n7\n242,688\n\n\n8\n207,739\n\n\n9\n216,355\n\n\n\n\n\n\n\n\n9.3.3 Tests\nAs we said in Chapter 3, if you write code, then you are a programmer, but there is a difference between someone coding for fun, and, say, writing the code that runs the James Webb Telescope. Following Weinberg (1971, 122), we can distinguish between amateurs and professionals based the existence of subsequent users. When you first start out coding, you typically write code that only you will use. For instance, you may write some code for a class paper. After you get a grade, then in most cases, the code will not be run again. In contrast, a professional writes code for, and often with, other people.\nMuch academic research these days relies on code. If that research is to contribute to lasting knowledge, then the code that underpins it is being written for others and must work for others well after the researcher has moved to other projects. A professional places appropriate care on tasks that ensure code can be considered by others. A large part of that is tests.\nJet Propulsion Laboratory (2009, 14) claim that analysis after the fact “often find at least one defect per one hundred lines of code written”. There is no reason to believe that code without tests is free of defects, just that they are not known. As such, we should strive to include tests in our code when possible. There is some infrastructure for testing data science code. For instance, in Python there is the Test-Driven Data Analysis library of Radcliffe (2023), but more is needed.\nSome things are so important that we require that the cleaned dataset have them. These are conditions that we should check. They would typically come from experience, expert knowledge, or the planning and simulation stages. For instance, there should be no negative numbers in an age variable, and few ages above 110. For these we could specifically require that the condition is met. Another example is when doing cross-country analysis, a list of country names that we know should be in our dataset would be useful. Our test would then be that there were:\n\nvalues not in that list that were in our dataset, or vice versa; and\ncountries that we expected to be in our dataset that were not.\n\nTo have a concrete example, let us consider if we were doing some analysis about the five largest counties in Kenya. From looking it up, we find these are: “Nairobi”, “Kiambu”, “Nakuru”, “Kakamega”, and “Bungoma”. We can create that variable.\n\ncorrect_kenya_counties &lt;-\n  c(\n    \"Nairobi\", \"Kiambu\", \"Nakuru\", \"Kakamega\", \"Bungoma\"\n  )\n\nThen pretend we have the following dataset, which contains errors.\n\ntop_five_kenya &lt;-\n  tibble(county = c(\n    \"Nairobi\",  \"Nairob1\", \"Nakuru\", \"Kakamega\", \"Nakuru\",\n    \"Kiambu\", \"Kiambru\", \"Kabamega\", \"Bun8oma\", \"Bungoma\"\n  ))\n\ntop_five_kenya |&gt;\n  count(county, sort = TRUE)\n\n# A tibble: 9 × 2\n  county       n\n  &lt;chr&gt;    &lt;int&gt;\n1 Nakuru       2\n2 Bun8oma      1\n3 Bungoma      1\n4 Kabamega     1\n5 Kakamega     1\n6 Kiambru      1\n7 Kiambu       1\n8 Nairob1      1\n9 Nairobi      1\n\n\nBased on the count we know that we must fix some of them. There are two with numbers in the names.\n\ntop_five_kenya_fixed_1_8 &lt;-\n  top_five_kenya |&gt;\n  mutate(\n    county = str_replace_all(county, \"Nairob1\", \"Nairobi\"),\n    county = str_replace_all(county, \"Bun8oma\", \"Bungoma\")\n  )\n\ntop_five_kenya_fixed_1_8 |&gt;\n  count(county, sort = TRUE)\n\n# A tibble: 7 × 2\n  county       n\n  &lt;chr&gt;    &lt;int&gt;\n1 Bungoma      2\n2 Nairobi      2\n3 Nakuru       2\n4 Kabamega     1\n5 Kakamega     1\n6 Kiambru      1\n7 Kiambu       1\n\n\nAt this point we can compare this with our known correct variable. We check both ways, i.e. is there anything in the correct variable not in our dataset, and is there anything in the dataset not in our correct variable. We use our check conditions to decide whether we are finished.\n\nif (all(top_five_kenya_fixed_1_8$county |&gt;\n  unique() %in% correct_kenya_counties)) {\n  \"The cleaned counties match the expected countries\"\n} else {\n  \"Not all of the counties have been cleaned completely\"\n}\n\n[1] \"Not all of the counties have been cleaned completely\"\n\nif (all(correct_kenya_counties %in% top_five_kenya_fixed_1_8$county |&gt;\n  unique())) {\n  \"The expected countries are in the cleaned counties\"\n} else {\n  \"Not all the expected countries are in the cleaned counties\"\n}\n\n[1] \"The expected countries are in the cleaned counties\"\n\n\nIt is clear that we still have cleaning to do because not all the counties match what we were expecting.\n\n9.3.3.1 Aspects to test\nWe will talk about explicit tests for class and dates, given their outsized importance, and how common it is for them to go wrong. But other aspects to explicitly consider testing include:\n\nVariables of monetary values should be tested for reasonable bounds given the situation. In some cases negative values will not be possible. Sometimes an upper bound can be identified. Monetary variables should be numeric. They should not have commas or other separators. They should not contain symbols such as currency signs or semicolons.\nVariables of population values should likely not be negative. Populations of cities should likely be somewhere between 100,000 and 50,000,000. They again should be numeric, and contain only numbers, no symbols.\nNames should be character variables. They likely do not contain numbers. They may contain some limited set of symbols, and this would be context specific.\nThe number of observations is surprisingly easy to inadvertently change. While it is fine for this to happen deliberately, when it happens accidentally it can create substantial problems. The number of observations should be tested at the start of any data cleaning process against the data simulation and this expectation updated as necessary. It should be tested throughout the data cleaning process, but especially before and after any joins.\n\nMore generally, work with experts and draw on prior knowledge to work out some reasonable features for the variables of interest and then implement these. For instance, consider how Baker (2023) was able to quickly identify an error in a claim about user numbers by roughly comparing it with how many institutions in the US receive federal financial aid.\nWe can use validate to set up a series of tests. For instance, here we will simulate some data with clear issues.\n\nset.seed(853)\n\ndataset_with_issues &lt;-\n  tibble(\n    age = c(\n      runif(n = 9, min = 0, max = 100) |&gt; round(),\n      1000\n    ),\n    gender = c(\n      sample(\n        x = c(\"female\", \"male\", \"other\", \"prefer not to disclose\"),\n        size = 9,\n        replace = TRUE,\n        prob = c(0.4, 0.4, 0.1, 0.1)\n      ),\n      \"tasmania\"\n    ),\n    income = rexp(n = 10, rate = 0.10) |&gt; round() |&gt; as.character()\n  )\n\ndataset_with_issues\n\n# A tibble: 10 × 3\n     age gender                 income\n   &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt; \n 1    36 female                 20    \n 2    12 prefer not to disclose 16    \n 3    48 male                   0     \n 4    32 female                 2     \n 5     4 female                 1     \n 6    40 female                 13    \n 7    13 female                 13    \n 8    24 female                 7     \n 9    16 male                   3     \n10  1000 tasmania               2     \n\n\nIn this case, there is an impossible age, one observation in the gender variable that should not be there, and finally, income is a character variable instead of a numeric. We use validator() to establish rules we expect the data to satisfy and confront() to determine whether it does.\n\nrules &lt;- validator(\n  is.numeric(age),\n  is.character(gender),\n  is.numeric(income),\n  age &lt; 120,\n  gender %in% c(\"female\", \"male\", \"other\", \"prefer not to disclose\")\n)\n\nout &lt;-\n  confront(dataset_with_issues, rules)\n\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1     1      1     0   0 FALSE   FALSE\n2   V2     1      1     0   0 FALSE   FALSE\n3   V3     1      0     1   0 FALSE   FALSE\n4   V4    10      9     1   0 FALSE   FALSE\n5   V5    10      9     1   0 FALSE   FALSE\n                                                           expression\n1                                                     is.numeric(age)\n2                                                is.character(gender)\n3                                                  is.numeric(income)\n4                                                           age &lt; 120\n5 gender %vin% c(\"female\", \"male\", \"other\", \"prefer not to disclose\")\n\n\nIn this case, we can see that there are issues with the final three rules that we established. More generally, van der Loo (2022) provides many example tests that can be used.\nAs mentioned in Chapter 6, gender is something that we need to be especially careful about. We will typically have a small number of responses that are neither “male” or “female”. The correct way to deal with the situation depends on context. But if responses other than “male” or “female” are going to be removed from the dataset and ignored, because there are too few of them, showing respect for the respondent might mean including a brief discussion of how they were similar or different to the rest of the dataset. Plots and a more extensive discussion could then be included in an appendix.\n\n\n9.3.3.2 Class\nIt is sometimes said that Americans are obsessed with money, while the English are obsessed with class. In the case of data cleaning and preparation we need to be English. Class is critical and worthy of special attention. We introduce class in Online Appendix A and here we focus on “numeric”, “character”, and “factor”. Explicit checks of the class of variables are essential. Accidentally assigning the wrong class to a variable can have a large effect on subsequent analysis. It is important to:\n\ncheck whether some value should be a number or a factor; and\ncheck that values are numbers not characters.\n\nTo understand why it is important to be clear about whether a value is a number or a factor, consider the following situation:\n\nsimulated_class_data &lt;-\n  tibble(\n    response = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n    group = c(1, 2, 1, 1, 2, 3, 1, 2, 3)\n  ) |&gt;\n  mutate(\n    group_as_integer = as.integer(group),\n    group_as_factor = as.factor(group),\n  )\n\nWe use logistic regression, which we cover in more detail in Chapter 12, and first include “group” as an integer, then we include it as a factor. Table 9.2 shows how different the results are and highlights the importance of getting the class of variables used in regression right. In the former, where group is an integer, we impose a consistent relationship between the different levels of the observations, whereas in the latter, where it is a factor, we enable more freedom.\n\nmodels &lt;- list(\n  \"Group as integer\" = glm(\n    response ~ group_as_integer,\n    data = simulated_class_data,\n    family = \"binomial\"\n  ),\n  \"Group as factor\" = glm(\n    response ~ group_as_factor,\n    data = simulated_class_data,\n    family = \"binomial\"\n  )\n)\nmodelsummary(models)\n\n\n\n\n\nTable 9.2:  Examining the effect of class on regression results \n  \n    \n    \n       \n      Group as integer\n      Group as factor\n    \n  \n  \n    (Intercept)\n1.417\n1.099\n    \n(1.755)\n(1.155)\n    group_as_integer\n-0.666\n\n    \n(0.894)\n\n    group_as_factor2\n\n-1.792\n    \n\n(1.683)\n    group_as_factor3\n\n-1.099\n    \n\n(1.826)\n    Num.Obs.\n9\n9\n    AIC\n15.8\n17.1\n    BIC\n16.2\n17.7\n    Log.Lik.\n-5.891\n-5.545\n    F\n0.554\n0.579\n    RMSE\n0.48\n0.46\n  \n  \n  \n\n\n\n\n\nClass is so important, subtle, and can have such a pernicious effect on analysis, that analysis with a suite of tests that check class is easier to believe. Establishing this suite is especially valuable just before modeling, but it is worthwhile setting this up as part of data cleaning and preparation. One reason that Jane Street, the US proprietary trading firm, uses a particular programming language, OCaml, is that its type system makes it more reliable with regard to class (Somers 2015). When code matters, class is of vital concern.\nThere are many open questions around the effect and implications of type in computer science more generally but there has been some work. For instance, Gao, Bird, and Barr (2017) find that the use of a static type system would have caught around 15 per cent of errors in production JavaScript systems. Languages have been developed, such as Typescript, where the primary difference, in this case from JavaScript, is that they are strongly typed. Turcotte et al. (2020) examine some of the considerations for adding a type system in R. They develop a prototype that goes some way to addressing the technical issues, but acknowledge that large-scale implementation would be challenging for many reasons including the need for users to change.\nTo this point in this book when we have used read_csv(), and other functions for importing data, we have allowed the function to guess the class of the variables. Moving forward we will be more deliberate and instead specify it ourselves using “col_types”. For instance, instead of:\n\nraw_igme_data &lt;-\n  read_csv(\n    file = \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    show_col_types = FALSE\n  )\n\nWe recommend using:\n\nraw_igme_data &lt;-\n  read_csv(\n    file = \"https://childmortality.org/wp-content/uploads/2021/09/UNIGME-2021.csv\",\n    col_select = c(`Geographic area`, TIME_PERIOD, OBS_VALUE),\n    col_types = cols(\n      `Geographic area` = col_character(),\n      TIME_PERIOD = col_character(),\n      OBS_VALUE = col_double(),\n    )\n  )\n\nThis is typically an iterative process of initially reading in the dataset, getting a quick sense of it, and then reading it in properly with only the necessary columns and classes specified. While this will require a little extra work of us, it is important that we are clear about class.\n\n\n9.3.3.3 Dates\nA shibboleth for whether someone has worked with dates is their reaction when you tell them you are going to be working with dates. If they share a horror story, then they have likely worked with dates before!\nExtensive checking of dates is important. Ideally, we would like dates to be in the following format: YYYY-MM-DD. There are differences of opinion as to what is an appropriate date format in the broader world. Reasonable people can differ on whether 1 July 2022 or July 1, 2022 is better, but YYYY-MM-DD is the international standard and we should use that in our date variables where possible.\nA few tests that could be useful include:\n\nIf a column is days of the week, then test that the only components are Monday, Tuesday, \\(\\dots\\), Sunday. Further, test that all seven days are present. Similarly, for month.\nTest that the number of days is appropriate for each month, for instance, check that September has 30 days, etc.\nCheck whether the dates are in order in the dataset. This need not necessarily be the case, but often when it is not, there are issues worth exploring.\nCheck that the years are complete and appropriate to the analysis period.\n\nIn Chapter 2 we introduced a dataset of shelter usage in Toronto in 2021 using opendatatoronto. Here we examine that same dataset, but for 2017, to illustrate some issues with dates. We first need to download the data.2\n\ntoronto_shelters_2017 &lt;-\n  search_packages(\"Daily Shelter Occupancy\") |&gt;\n  list_package_resources() |&gt;\n  filter(name == \"Daily shelter occupancy 2017.csv\") |&gt;\n  group_split(name) |&gt;\n  map_dfr(get_resource, .id = \"file\")\n\nwrite_csv(\n  x = toronto_shelters_2017,\n  file = \"toronto_shelters_2017.csv\"\n)\n\nWe need to make the names easier to type and only keep relevant columns.\n\ntoronto_shelters_2017 &lt;-\n  toronto_shelters_2017 |&gt;\n  clean_names() |&gt;\n  select(occupancy_date, sector, occupancy, capacity)\n\nThe main issue with this dataset will be the dates. We will find that the dates appear to be mostly year-month-day, but certain observations may be year-day-month. We use ymd() from lubridate to parse the date in that order.\n\ntoronto_shelters_2017 &lt;-\n  toronto_shelters_2017 |&gt;\n  mutate(\n    # remove times\n    occupancy_date =\n      str_remove(\n        occupancy_date,\n        \"T[:digit:]{2}:[:digit:]{2}:[:digit:]{2}\"\n        )) |&gt;\n  mutate(generated_date = ymd(occupancy_date, quiet = TRUE))\n\ntoronto_shelters_2017\n\n# A tibble: 38,700 × 5\n   occupancy_date sector   occupancy capacity generated_date\n   &lt;chr&gt;          &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;        \n 1 2017-01-01     Co-ed           16       16 2017-01-01    \n 2 2017-01-01     Men             13       17 2017-01-01    \n 3 2017-01-01     Men             63       63 2017-01-01    \n 4 2017-01-01     Families        66       70 2017-01-01    \n 5 2017-01-01     Men             58       60 2017-01-01    \n 6 2017-01-01     Families       168      160 2017-01-01    \n 7 2017-01-01     Families       119      150 2017-01-01    \n 8 2017-01-01     Men             23       28 2017-01-01    \n 9 2017-01-01     Families         8        0 2017-01-01    \n10 2017-01-01     Co-ed           14       40 2017-01-01    \n# ℹ 38,690 more rows\n\n\nThe plot of the distribution of what purports to be the day component makes it clear that there are concerns (Figure 9.3 (a)). In particular we are concerned that the distribution of the days is not roughly uniform.\n\ntoronto_shelters_2017 |&gt;\n  separate(\n    generated_date,\n    into = c(\"one\", \"two\", \"three\"),\n    sep = \"-\",\n    remove = FALSE\n  ) |&gt;\n  count(three) |&gt;\n  ggplot(aes(x = three, y = n)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Third component of occupancy date\",\n       y = \"Number\")\n\ntoronto_shelters_2017 |&gt;\n  mutate(row_number = c(seq_len(nrow(toronto_shelters_2017)))) |&gt;\n  ggplot(aes(x = row_number, y = generated_date), alpha = 0.1) +\n  geom_point(alpha = 0.3) +\n  theme_minimal() +\n  labs(\n    x = \"Row number\",\n    y = \"Date\"\n  )\n\n\n\n\n\n\n\n(a) Counts, by third component of occupancy date\n\n\n\n\n\n\n\n(b) Comparison of row number with date\n\n\n\n\nFigure 9.3: Examining the date in more detail\n\n\n\nAs mentioned, one graph that is especially useful when cleaning a dataset is the order the observations appear in the dataset. For instance, we would generally expect that there would be a rough ordering in terms of date. To examine whether this is the case, we can graph the date variable in the order it appears in the dataset (Figure 9.3 (b)).\nWhile this is just a quick graph it illustrates the point—there are a lot in order, but not all. If they were in order, then we would expect them to be along the diagonal. It is odd that the data are not in order, especially as there appears to be something systematic initially. We can summarize the data to get a count of occupancy by day.\n\n# Idea from Lisa Lendway\ntoronto_shelters_by_day &lt;-\n  toronto_shelters_2017 |&gt;\n  drop_na(occupancy, capacity) |&gt;\n  summarise(\n    occupancy = sum(occupancy),\n    capacity = sum(capacity),\n    usage = occupancy / capacity,\n    .by = generated_date\n  )\n\nWe are interested in the availability of shelter spots in Toronto for each day (Figure 9.4).\n\ntoronto_shelters_by_day |&gt;\n  ggplot(aes(x = day(generated_date), y = occupancy)) +\n  geom_point(alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(\n    color = \"Type\",\n    x = \"Day\",\n    y = \"Occupancy (number)\"\n  ) +\n  facet_wrap(\n    vars(month(generated_date, label = TRUE)),\n    scales = \"free_x\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 9.4: Occupancy per day in Toronto shelters\n\n\n\n\nIt is clear there seems to be an issue with the first 12 days of the month. We noted that when we look at the data it is a bit odd that it is not in order. From Figure 9.3 (b) it looks like there are some systematic issue that affects many observations. In general, it seems that it might be the case that in the date variable the first 12 days are the wrong way around, i.e. we think it is year-month-day, but it is actually year-day-month. But there are exceptions. As a first pass, we can flip those first 12 days of each month and see if that helps. It will be fairly blunt, but hopefully gets us somewhere.\n\n# Code by Monica Alexander\npadded_1_to_12 &lt;- sprintf(\"%02d\", 1:12)\n\nlist_of_dates_to_flip &lt;-\n  paste(2017, padded_1_to_12, \n        rep(padded_1_to_12, each = 12), sep = \"-\")\n\ntoronto_shelters_2017_flip &lt;-\n  toronto_shelters_2017 |&gt;\n  mutate(\n    year = year(generated_date),\n    month = month(generated_date),\n    day = day(generated_date),\n    generated_date = as.character(generated_date),\n    changed_date = if_else(\n      generated_date %in% list_of_dates_to_flip,\n      paste(year, day, month, sep = \"-\"),\n      paste(year, month, day, sep = \"-\"),\n    ),\n    changed_date = ymd(changed_date)\n  ) |&gt;\n  select(-year, -month, -day)\n\nNow let us take a look (Figure 9.5).\n\ntoronto_shelters_2017_flip |&gt;\n  mutate(counter = seq_len(nrow(toronto_shelters_2017_flip))) |&gt;\n  ggplot(aes(x = counter, y = changed_date)) +\n  geom_point(alpha = 0.3) +\n  labs(x = \"Row in the dataset\",\n       y = \"Date of that row\") +\n  theme_minimal()\n\ntoronto_shelters_2017_flip |&gt;\n  drop_na(occupancy, capacity) |&gt;\n  summarise(occupancy = sum(occupancy),\n            .by = changed_date) |&gt;\n  ggplot(aes(x = day(changed_date), y = occupancy)) +\n  geom_point(alpha = 0.3) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(color = \"Type\",\n       x = \"Changed day\",\n       y = \"Occupancy (number)\") +\n  facet_wrap(vars(month(changed_date, label = TRUE)),\n             scales = \"free_x\") +\n  theme_minimal()\n\n\n\n\n\n\n\n(a) Date of each row in order after adjustment\n\n\n\n\n\n\n\n(b) Toronto shelters daily occupancy after adjustment\n\n\n\n\nFigure 9.5: Adjusted dates, occupancy in Toronto shelters\n\n\n\nIt has not fixed all the issues. For instance, notice there are now no entries below the diagonal (Figure 9.5 (a)). But we can see that has almost entirely taken care of the systematic differences (Figure 9.5 (b)). This is where we will leave this example."
  },
  {
    "objectID": "09-clean_and_prepare.html#simulated-example-running-times",
    "href": "09-clean_and_prepare.html#simulated-example-running-times",
    "title": "9  Clean and prepare",
    "section": "9.4 Simulated example: running times",
    "text": "9.4 Simulated example: running times\nTo provide a specific example, which we will return to in Chapter 12, consider the time it takes someone to run five kilometers (which is a little over three miles), compared with the time it takes them to run a marathon (Figure 12.2 (a)).\nHere we consider “simulate” and “acquire”, focused on testing. In the simulation we specify a relationship of 8.4, as that is roughly the ratio between a five-kilometer run and the 42.2 kilometer distance of a marathon (a little over 26 miles).\n\nset.seed(853)\n\nnum_observations &lt;- 200\nexpected_relationship &lt;- 8.4\nfast_time &lt;- 15\ngood_time &lt;- 30\n\nsim_run_data &lt;-\n  tibble(\n    five_km_time =\n      runif(n = num_observations, min = fast_time, max = good_time),\n    noise = rnorm(n = num_observations, mean = 0, sd = 20),\n    marathon_time = five_km_time * expected_relationship + noise\n  ) |&gt;\n  mutate(\n    five_km_time = round(x = five_km_time, digits = 1),\n    marathon_time = round(x = marathon_time, digits = 1)\n  ) |&gt;\n  select(-noise)\n\nsim_run_data\n\n# A tibble: 200 × 2\n   five_km_time marathon_time\n          &lt;dbl&gt;         &lt;dbl&gt;\n 1         20.4          164.\n 2         16.8          158 \n 3         22.3          196.\n 4         19.7          160.\n 5         15.6          121.\n 6         21.1          178.\n 7         17            157.\n 8         18.6          169.\n 9         17.4          150.\n10         17.8          126.\n# ℹ 190 more rows\n\n\nWe can use our simulation to put in place various tests that we would want the actual data to satisfy. For instance, we want the class of the five kilometer and marathon run times to be numeric. And we want 200 observations.\n\nstopifnot(\n  class(sim_run_data$marathon_time) == \"numeric\",\n  class(sim_run_data$five_km_time) == \"numeric\",\n  nrow(sim_run_data) == 200\n)\n\nWe know that any value that is less than 15 minutes or more than 30 minutes for the five-kilometer run time is likely something that needs to be followed up on.\n\nstopifnot(\n  min(sim_run_data$five_km_time) &gt;= 15,\n  max(sim_run_data$five_km_time) &lt;= 30\n)\n\nBased on this maximum and the simulated relationship of 8.4, we would be surprised if we found any marathon times that were substantially over \\(30\\times8.4=252\\) minutes, after we allow for a little bit of drift, say 300 minutes. (To be clear, there is nothing wrong with taking longer than this to run a marathon, but it is just unlikely based on our simulation parameters). And we would be surprised if the world record marathon time, 121 minutes as at the start of 2023, were improved by anything more than a minute or two, say, anything faster than 118 minutes. (It will turn out that our simulated data do not satisfy this and result in a implausibly fast 88 minute marathon time, which suggests a need to improve the simulation.)\n\nstopifnot(\n  min(sim_run_data$marathon_time) &gt;= 118,\n  max(sim_run_data$marathon_time) &lt;= 300\n)\n\nWe can then take these tests to real data. Actual survey data on the relationship between five kilometer and marathon run times are available from Vickers and Vertosick (2016). After downloading the data, which Vickers and Vertosick (2016) make available as an “Additional file”, we can focus on the variables of interest and only individuals with both a five-kilometer time and a marathon time.\n\nvickers_data &lt;- \n  read_excel(\"13102_2016_52_MOESM2_ESM.xlsx\") |&gt; \n  select(k5_ti, mf_ti) |&gt; \n  drop_na()\n\nvickers_data\n\n\n\n# A tibble: 430 × 2\n   k5_ti mf_ti\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  1075 10295\n 2  1292 12292\n 3  1222 13452\n 4   893  9515\n 5  1050 10875\n 6  1603 16580\n 7  1457 15440\n 8  1256 13113\n 9  1572 17190\n10  2575 22139\n# ℹ 420 more rows\n\n\nThe first thing that we notice is that our data are in seconds, whereas we were expecting them to be in minutes. This is fine. Our simulation and tests can update, or we can adjust our data. Our simulation and tests retain their value even when the data turn out to be slightly different, which they inevitably will.\nIn this case, we will divide by sixty, and round, to shift our data into minutes.\n\nvickers_data &lt;- \n  vickers_data |&gt; \n  mutate(five_km_time = round(k5_ti / 60, 1),\n         marathon_time = round(mf_ti / 60, 1)\n         ) |&gt; \n  select(five_km_time, marathon_time)\n\nvickers_data\n\n# A tibble: 430 × 2\n   five_km_time marathon_time\n          &lt;dbl&gt;         &lt;dbl&gt;\n 1         17.9          172.\n 2         21.5          205.\n 3         20.4          224.\n 4         14.9          159.\n 5         17.5          181.\n 6         26.7          276.\n 7         24.3          257.\n 8         20.9          219.\n 9         26.2          286.\n10         42.9          369 \n# ℹ 420 more rows\n\n\n\nstopifnot(\n  class(vickers_data$marathon_time) == \"numeric\",\n  class(vickers_data$five_km_time) == \"numeric\",\n  min(vickers_data$five_km_time) &gt;= 15,\n  max(vickers_data$five_km_time) &lt;= 30,\n  min(vickers_data$marathon_time) &gt;= 118,\n  max(vickers_data$marathon_time) &lt;= 300\n)\n\nIn this case, our tests, which were written for the simulated data, identify that we have five kilometer run times that are faster that 15 minutes and longer than 30 minutes. They also identify marathon times that are longer than 300 minutes. If we were actually using this data for analysis, then our next step would be to plot the data, taking care to examine each of these points that our tests identified, and then either adjust the tests or the dataset."
  },
  {
    "objectID": "09-clean_and_prepare.html#names",
    "href": "09-clean_and_prepare.html#names",
    "title": "9  Clean and prepare",
    "section": "9.5 Names",
    "text": "9.5 Names\n\nAn improved scanning software we developed identified gene name errors in 30.9% (3,436/11,117) of articles with supplementary Excel gene lists; a figure significantly higher than previously estimated. This is due to gene names being converted not just to dates and floating-point numbers, but also to internal date format (five-digit numbers).\nAbeysooriya et al. (2021)\n\nNames matter. The land on which much of this book was written is today named Canada, but for a long time was known as Turtle Island. Similarly, there is a big rock in the center of Australia. For a long time, it was called Uluru, then it was known as Ayers Rock. Today it has a dual name that combines both. And in parts of the US South, including signage surrounding the South Carolina State House, the US Civil War is referred to as the War of Northern Aggression. In these examples, the name that is used conveys information, not only about the user, but about the circumstances. Even the British Royal Family recognizes the power of names. In 1917 they changed from the House of Saxe-Coburg and Gotha to the House of Windsor. It was felt that the former was too Germanic given World War I. Names matter in everyday life. And they matter in our code, too.\nWhen coding, names are critical and worthy of special attention because (Hermans 2021):\n\nthey help document our code as they are contained in the code;\nthey make up a large proportion of any script;\nthey are referred to a lot by others; and\nthey help the reader understand what is happening in the code.\n\nIn addition to respecting the nature of the data, names need to satisfy two additional considerations:\n\nthey need to be machine readable, and\nthey need to be human readable.\n\n\n9.5.1 Machine-readable\nEnsuring machine-readable names can be an easier standard to meet. It usually means avoiding spaces and special characters. A space can be replaced with an underscore. For instance, we prefer “my_data” to “my data”. Avoiding spaces enables tab-completion which makes us more efficient. It also helps with reproducibility because spaces are considered differently by different operating systems.\nUsually, special characters should be removed because they can be inconsistent between different computers and languages. This is especially the case with slash, backslash, asterisk, and both single, and double quotation marks. Try to avoid using those in names.\nNames should also be unique within a dataset, and unique within a collection of datasets unless that particular variable is being deliberately used as a key to join different datasets. This usually means that the domain is critical for effective names, and when working as part of a team this all gets much more difficult (Hermans 2017). Names need to not only be unique, but notably different when there is a potential for confusion. For instance, for many years, the language PHP had both mysql_escape_string and mysql_real_escape_string (Somers 2015). It is easy to see how programmers may have accidentally written one when they meant the other.\nAn especially useful function to use to get closer to machine-readable names is clean_names() from janitor. This deals with those issues mentioned above as well as a few others.\n\nsome_bad_names &lt;-\n  tibble(\n    \"Second Name has spaces\" = c(1),\n    \"weird#symbol\" = c(1),\n    \"InCoNsIsTaNtCaPs\" = c(1)\n  )\n\nbad_names_made_better &lt;-\n  some_bad_names |&gt;\n  clean_names()\n\nsome_bad_names\n\n# A tibble: 1 × 3\n  `Second Name has spaces` `weird#symbol` InCoNsIsTaNtCaPs\n                     &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1                        1              1                1\n\nbad_names_made_better\n\n# A tibble: 1 × 3\n  second_name_has_spaces weird_number_symbol in_co_ns_is_ta_nt_ca_ps\n                   &lt;dbl&gt;               &lt;dbl&gt;                   &lt;dbl&gt;\n1                      1                   1                       1\n\n\n\n\n9.5.2 Human-readable\n\nPrograms must be written for people to read, and only incidentally for machines to execute\nAbelson and Sussman (1996)\n\nIn the same way that we emphasized in Chapter 4 that we write papers for the reader, here we emphasize that we write code for the reader. Human-readable names require an additional layer, and extensive consideration. Following Lockheed Martin (2005, 25), we should avoid names that only differ by the use of the letter “O”, instead of the number “0” or the letter “D”. Similarly, “S” with “5”.\nWe should consider other cultures and how they may interpret some of the names that we use. We also need to consider different experience levels that subsequent users of the dataset may have. This is both in terms of experience with data science, but also experience with similar datasets. For instance, a variable called “flag” is often used to signal that a variable contains data that needs to be followed up with or treated carefully in some way. An experienced analyst will know this, but a beginner will not. Try to use meaningful names wherever possible (Lin, Ali, and Wilson 2021). It has been found that shorter names may take longer to comprehend (Hofmeister, Siegmund, and Holt 2017), and so it is often useful to avoid uncommon abbreviations where possible.\nBryan (2015) recommends that file names, in particular, should consider the default ordering that a file manager will impose. This might mean adding prefixes such as “00-”, “01-”, etc to filenames (which might involve left-padding with zeros depending on the number of files). Critically it means using ISO 8601 for dates. That was introduced earlier and means that 2 December 2022 would be written “2022-12-02”. The reason for using such file names is to provide information to other people about the order of the files.\nOne interesting feature of R is that in certain cases partial matching on names is possible. For instance:\n\npartial_matching_example &lt;-\n  data.frame(\n    my_first_name = c(1, 2),\n    another_name = c(\"wow\", \"great\")\n  )\n\npartial_matching_example$my_first_name\n\n[1] 1 2\n\npartial_matching_example$my\n\n[1] 1 2\n\n\nThis behavior is not possible within the tidyverse (for instance, if data.frame were replaced with tibble in the above code). Partial matching should rarely be used. It makes it more difficult to understand code after a break, and for others to come to it fresh.\nVariable names should have a consistent structure. For instance, imposing the naming pattern verb_noun, as in read_csv(), then having one function that was noun_verb, perhaps csv_read(), would be inconsistent. That inconsistency imposes a significant cost because it makes it more difficult to remember the name of the function.\nR, Python, and many of the other languages that are commonly used for data science are dynamically typed, as opposed to static typed. This means that class can be defined independently of declaring a variable. One interesting area of data science research is going partially toward static typed and understanding what that might mean. For instance, Python enabled type hints in 2014 (Boykis 2019). While not required, this goes someway to being more explicit about types.\nRiederer (2020) advises using variable names as contracts. We do this by establishing a controlled vocabulary for them. In this way, we would define a set of words that we can use in names. In the controlled vocabulary of Riederer (2020) a variable could start with an abbreviation for its class, then something specific to what it pertains to, and then various details.\nFor instance, we could consider column names of “age” and “sex”. Following Riederer (2020) we may change these to be more informative of the class and other information. This issue is not settled, and there is not yet best practice. For instance, there are arguments against this in terms of readability.\n\nsome_names &lt;-\n  tibble(\n    age = as.integer(c(1, 3, 35, 36)),\n    sex = factor(c(\"male\", \"male\", \"female\", \"male\"))\n  )\n\nriederer_names &lt;-\n  some_names |&gt;\n  rename(\n    integer_age_respondent = age,\n    factor_sex_respondent = sex\n  )\n\nsome_names\n\n# A tibble: 4 × 2\n    age sex   \n  &lt;int&gt; &lt;fct&gt; \n1     1 male  \n2     3 male  \n3    35 female\n4    36 male  \n\nriederer_names\n\n# A tibble: 4 × 2\n  integer_age_respondent factor_sex_respondent\n                   &lt;int&gt; &lt;fct&gt;                \n1                      1 male                 \n2                      3 male                 \n3                     35 female               \n4                     36 male                 \n\n\nEven just trying to be a little more explicit and consistent about names throughout a project typically brings substantial benefits when we come to revisit the project later. Would a rose by any other name smell as sweet? Of course. But we call it a rose—or even better Rosa rubiginosa—because that helps others know what we are talking about, compared with, say, “red_thing”, “five_petaled_smell_nice”, “flower”, or “r_1”. It is clearer, and helps others efficiently understand."
  },
  {
    "objectID": "09-clean_and_prepare.html#tanzanian-dhs",
    "href": "09-clean_and_prepare.html#tanzanian-dhs",
    "title": "9  Clean and prepare",
    "section": "9.6 1996 Tanzanian DHS",
    "text": "9.6 1996 Tanzanian DHS\nWe will now go through the first of two examples. The Demographic and Health Surveys (DHS) play an important role in gathering data in areas where we may not have other datasets. Here we will clean and prepare a DHS table about household populations in Tanzania in 1996. As a reminder, the workflow that we advocate in this book is:\n\\[\n\\mbox{Plan}\\rightarrow\\mbox{Simulate}\\rightarrow\\mbox{Acquire}\\rightarrow\\mbox{Explore}\\rightarrow\\mbox{Share}\n\\]\nWe are interested in the distribution of age-groups, gender, and urban/rural. A quick sketch might look like Figure 9.6.\n\n\n\nFigure 9.6: Quick sketch of a dataset that we might be interested in\n\n\nWe can then simulate a dataset.\n\nset.seed(853)\n\nage_group &lt;- tibble(starter = 0:19) |&gt;\n  mutate(lower = starter * 5, upper = starter * 5 + 4) |&gt;\n  unite(string_sequence, lower, upper, sep = \"-\") |&gt;\n  pull(string_sequence)\n\nmean_value &lt;- 10\n\nsimulated_tanzania_dataset &lt;-\n  tibble(\n    age_group = age_group,\n    urban_male = round(rnorm(length(age_group), mean_value)),\n    urban_female = round(rnorm(length(age_group), mean_value)),\n    rural_male = round(rnorm(length(age_group), mean_value)),\n    rural_female = round(rnorm(length(age_group), mean_value)),\n    total_male = round(rnorm(length(age_group), mean_value)),\n    total_female = round(rnorm(length(age_group), mean_value))\n  ) |&gt;\n  mutate(\n    urban_total = urban_male + urban_female,\n    rural_total = rural_male + rural_female,\n    total_total = total_male + total_female\n  )\n\nsimulated_tanzania_dataset\n\n# A tibble: 20 × 10\n   age_group urban_male urban_female rural_male rural_female total_male\n   &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 0-4               10           10          9            9         11\n 2 5-9               10            9          9           10          9\n 3 10-14              8           11         11           11         10\n 4 15-19              9           11         10            9         10\n 5 20-24              9            8         11           10          9\n 6 25-29             12            9         10           10         10\n 7 30-34              9            8         10           10          8\n 8 35-39             10           11          8           10         10\n 9 40-44              9            9          9           10         11\n10 45-49              9           10         11           10         11\n11 50-54             12           10          9           13         10\n12 55-59              9           11         10            9          9\n13 60-64             10            9         11           11         10\n14 65-69             10           10         10           10         11\n15 70-74             10           10         12            9          8\n16 75-79             10            8         10            9         10\n17 80-84             10            9          9           10          9\n18 85-89             10            9         10           11         11\n19 90-94             11           11         11           10         11\n20 95-99             10           10         10           11         11\n# ℹ 4 more variables: total_female &lt;dbl&gt;, urban_total &lt;dbl&gt;, rural_total &lt;dbl&gt;,\n#   total_total &lt;dbl&gt;\n\n\nBased on this simulation we are interested to test:\n\nWhether there are only numbers.\nWhether the sum of urban and rural match the total column.\nWhether the sum of the age-groups match the total.\n\nWe begin by downloading the data.3\n\ndownload.file(\n  url = \"https://dhsprogram.com/pubs/pdf/FR83/FR83.pdf\",\n  destfile = \"1996_Tanzania_DHS.pdf\",\n  mode = \"wb\"\n)\n\nWhen we have a PDF and want to read the content into R, then pdf_text() from pdftools is useful, as introduced in Chapter 7. It works well for many recently produced PDFs because the content is text which it can extract. But if the PDF is an image, then pdf_text() will not work. Instead, the PDF will first need to go through OCR, which was also introduced in Chapter 7.\n\ntanzania_dhs &lt;-\n  pdf_text(\n    pdf = \"1996_Tanzania_DHS.pdf\"\n  )\n\nIn this case we are interested in Table 2.1, which is on the 33rd page of the PDF (Figure 9.7).\n\n\n\nFigure 9.7: The page of interest in the 1996 Tanzanian DHS\n\n\nWe use stri_split_lines() from stringi to focus on that particular page.\n\n# From Bob Rudis: https://stackoverflow.com/a/47793617\ntanzania_dhs_page_33 &lt;- stri_split_lines(tanzania_dhs[[33]])[[1]]\n\nWe first want to remove all the written content and focus on the table. We then want to convert that into a tibble so that we can use our familiar tidyverse approaches.\n\ntanzania_dhs_page_33_only_data &lt;- tanzania_dhs_page_33[31:55]\n\ntanzania_dhs_raw &lt;- tibble(all = tanzania_dhs_page_33_only_data)\n\ntanzania_dhs_raw\n\n# A tibble: 25 × 1\n   all                                                                          \n   &lt;chr&gt;                                                                        \n 1 \"                                  Urban                              Rural …\n 2 \"\"                                                                           \n 3 \" Age group             Male      Female       Total          Male   Female …\n 4 \"\"                                                                           \n 5 \"\"                                                                           \n 6 \" 0-4                   16.4        13.8        15.1          18.1     17.1 …\n 7 \" 5-9                   13.5        13.0        13.2          17.5     16,0 …\n 8 \" 10-14                 12.6        13.1        12.8          15.3     13.5 …\n 9 \" 15-19                 10.8        11.3        11.1           9.8      8.8 …\n10 \" 20-~                   9.4        12.2        10,8           5.9      8.2 …\n# ℹ 15 more rows\n\n\nAll the columns have been collapsed into one, so we need to separate them. We will do this based on the existence of a space, which means we first need to change “Age group” to “Age-group” because we do not want that separated.\n\n# Separate columns\ntanzania_dhs_separated &lt;-\n  tanzania_dhs_raw |&gt;\n  mutate(all = str_squish(all)) |&gt;\n  mutate(all = str_replace(all, \"Age group\", \"Age-group\")) |&gt;\n  separate(\n    col = all,\n    into = c(\n      \"age_group\", \n      \"male_urban\", \"female_urban\", \"total_urban\",\n      \"male_rural\", \"female_rural\", \"total_rural\",\n      \"male_total\", \"female_total\", \"total_total\"\n    ),\n    sep = \" \",\n    remove = TRUE,\n    fill = \"right\",\n    extra = \"drop\"\n  )\n\ntanzania_dhs_separated\n\n# A tibble: 25 × 10\n   age_group   male_urban female_urban total_urban male_rural female_rural\n   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;       \n 1 \"Urban\"     Rural      Total        &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;        \n 2 \"\"          &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;        \n 3 \"Age-group\" Male       Female       Total       Male       Female      \n 4 \"\"          &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;        \n 5 \"\"          &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;        \n 6 \"0-4\"       16.4       13.8         15.1        18.1       17.1        \n 7 \"5-9\"       13.5       13.0         13.2        17.5       16,0        \n 8 \"10-14\"     12.6       13.1         12.8        15.3       13.5        \n 9 \"15-19\"     10.8       11.3         11.1        9.8        8.8         \n10 \"20-~\"      9.4        12.2         10,8        5.9        8.2         \n# ℹ 15 more rows\n# ℹ 4 more variables: total_rural &lt;chr&gt;, male_total &lt;chr&gt;, female_total &lt;chr&gt;,\n#   total_total &lt;chr&gt;\n\n\nNow we need to clean the rows and columns. One helpful “negative space” approach to work out what we need to remove, is to look at what is left if we temporarily remove everything that we know we want. Whatever is left is then a candidate for being removed. In this case we know that we want the columns to contain numbers, so we remove numeric digits from all columns to see what might stand in our way of converting from string to numeric.\n\ntanzania_dhs_separated |&gt;\n  mutate(across(everything(), ~ str_remove_all(., \"[:digit:]\"))) |&gt;\n  distinct()\n\n# A tibble: 15 × 10\n   age_group   male_urban female_urban total_urban male_rural female_rural\n   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;       \n 1 \"Urban\"     Rural      Total        &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;        \n 2 \"\"          &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;        &lt;NA&gt;       &lt;NA&gt;        \n 3 \"Age-group\" Male       Female       Total       Male       Female      \n 4 \"-\"         .          .            .           .          .           \n 5 \"-\"         .          .            .           .          ,           \n 6 \"-\"         .          .            .           .          .           \n 7 \"-~\"        .          .            ,           .          .           \n 8 \"-\"         .          .            ,           ,          ,           \n 9 \"-\"         ,          .            .           .          .           \n10 \"-\"         .          .            .           .          .           \n11 \"-\"         ,          .            .           ;          .           \n12 \"-\"         .          .            .           ,          .           \n13 \"+\"         .          .            .           .          .           \n14 \"Total\"     .          .            .           .          .           \n15 \"Number\"    ,          ,            ,           .          ,           \n# ℹ 4 more variables: total_rural &lt;chr&gt;, male_total &lt;chr&gt;, female_total &lt;chr&gt;,\n#   total_total &lt;chr&gt;\n\n\nIn this case we can see that some commas and semicolons have been incorrectly considered decimal places. Also, some tildes and blank lines need to be removed. After that we can impose the correct class.\n\ntanzania_dhs_cleaned &lt;-\n  tanzania_dhs_separated |&gt;\n  slice(6:22, 24, 25) |&gt;\n  mutate(across(everything(), ~ str_replace_all(., \"[,;]\", \".\"))) |&gt;\n  mutate(\n    age_group = str_replace(age_group, \"20-~\", \"20-24\"),\n    age_group = str_replace(age_group, \"40-~\", \"40-44\"),\n    male_rural = str_replace(male_rural, \"14.775\", \"14775\")\n  ) |&gt;\n  mutate(across(starts_with(c(\n    \"male\", \"female\", \"total\"\n  )),\n  as.numeric))\n\ntanzania_dhs_cleaned\n\n# A tibble: 19 × 10\n   age_group male_urban female_urban total_urban male_rural female_rural\n   &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 0-4            16.4         13.8        15.1        18.1         17.1\n 2 5-9            13.5         13          13.2        17.5         16  \n 3 10-14          12.6         13.1        12.8        15.3         13.5\n 4 15-19          10.8         11.3        11.1         9.8          8.8\n 5 20-24           9.4         12.2        10.8         5.9          8.2\n 6 25-29           8.4          9.8         9.1         5.6          7.1\n 7 30-34           6.6          6.3         6.4         5.2          5.6\n 8 35-39           5.8          5.9         5.8         4            4.5\n 9 40-44           4.4          3.5         3.9         3.3          3.5\n10 45-49           3.2          2.3         2.7         3.2          3.3\n11 50-54           2            2.4         2.2         2.2          3.4\n12 55-59           1.8          1.8         1.8         2.1          2.9\n13 60-64           2.1          1.7         1.9         2.4          2  \n14 65-69           1.3          1.3         1.3         2.2          1.6\n15 70-74           0.9          0.7         0.8         1.3          1.2\n16 75-79           0.3          0.4         0.4         0.8          0.6\n17 80+             0.3          0.5         0.4         0.9          0.7\n18 Total         100          100         100         100          100  \n19 Number          3.69         3.88        7.57    14775           15.9\n# ℹ 4 more variables: total_rural &lt;dbl&gt;, male_total &lt;dbl&gt;, female_total &lt;dbl&gt;,\n#   total_total &lt;dbl&gt;\n\n\nFinally, we may wish to check that the sum of the constituent parts equals the total.\n\ntanzania_dhs_cleaned |&gt;\n  filter(!age_group %in% c(\"Total\", \"Number\")) |&gt;\n  summarise(sum = sum(total_total))\n\n# A tibble: 1 × 1\n    sum\n  &lt;dbl&gt;\n1  99.7\n\n\nIn this case we can see that it is a few tenths of a percentage point off."
  },
  {
    "objectID": "09-clean_and_prepare.html#kenyan-census",
    "href": "09-clean_and_prepare.html#kenyan-census",
    "title": "9  Clean and prepare",
    "section": "9.7 2019 Kenyan census",
    "text": "9.7 2019 Kenyan census\nAs a final example, let us consider a more extensive situation and gather, clean, and prepare some data from the 2019 Kenyan census. We will focus on creating a dataset of single-year counts, by gender, for Nairobi.\nThe distribution of population by age, sex, and administrative unit from the 2019 Kenyan census can be downloaded here. While this format as a PDF makes it easy to look up a particular result, it is not overly useful if we want to model the data. In order to be able to do that, we need to convert this PDF into a tidy dataset that can be analyzed.\n\n9.7.1 Gather and clean\nWe first need to download and read in the PDF of the 2019 Kenyan census.4\n\ncensus_url &lt;-\n  paste0(\n    \"https://www.knbs.or.ke/download/2019-kenya-population-and-\",\n    \"housing-census-volume-iii-distribution-of-population-by-age-\",\n    \"sex-and-administrative-units/?wpdmdl=5729&refresh=\",\n    \"620561f1ce3ad1644519921\"\n  )\n\ndownload.file(\n  url = census_url,\n  destfile = \"2019_Kenya_census.pdf\",\n  mode = \"wb\"\n)\n\nWe can use pdf_text() from pdftools again here.\n\nkenya_census &lt;-\n  pdf_text(\n    pdf = \"2019_Kenya_census.pdf\"\n  )\n\nIn this example we will focus on the page of the PDF about Nairobi (Figure 9.8).\n\n\n\nFigure 9.8: Page from the 2019 Kenyan census about Nairobi\n\n\n\n9.7.1.1 Make rectangular\nThe first challenge is to get the dataset into a format that we can more easily manipulate. We will extract the relevant parts of the page. In this case, data about Nairobi is on page 410.\n\n# Focus on the page of interest\njust_nairobi &lt;- stri_split_lines(kenya_census[[410]])[[1]]\n\n# Remove blank lines\njust_nairobi &lt;- just_nairobi[just_nairobi != \"\"]\n\n# Remove titles, headings and other content at the top of the page\njust_nairobi &lt;- just_nairobi[5:length(just_nairobi)]\n\n# Remove page numbers and other content at the bottom of the page\njust_nairobi &lt;- just_nairobi[1:62]\n\n# Convert into a tibble\ndemography_data &lt;- tibble(all = just_nairobi)\n\nAt this point the data are in a tibble. This allows us to use our familiar dplyr verbs. In particular we want to separate the columns.\n\ndemography_data &lt;-\n  demography_data |&gt;\n  mutate(all = str_squish(all)) |&gt;\n  mutate(all = str_replace(all, \"10 -14\", \"10-14\")) |&gt;\n  mutate(all = str_replace(all, \"Not Stated\", \"NotStated\")) |&gt;\n  # Deal with the two column set-up\n  separate(\n    col = all,\n    into = c(\n      \"age\", \"male\", \"female\", \"total\",\n      \"age_2\", \"male_2\", \"female_2\", \"total_2\"\n    ),\n    sep = \" \",\n    remove = TRUE,\n    fill = \"right\",\n    extra = \"drop\"\n  )\n\nThey are side by side at the moment. We need to instead append to the bottom.\n\ndemography_data_long &lt;-\n  rbind(\n    demography_data |&gt; select(age, male, female, total),\n    demography_data |&gt;\n      select(age_2, male_2, female_2, total_2) |&gt;\n      rename(\n        age = age_2,\n        male = male_2,\n        female = female_2,\n        total = total_2\n        )\n    )\n\n\n# There is one row of NAs, so remove it\ndemography_data_long &lt;-\n  demography_data_long |&gt;\n  remove_empty(which = c(\"rows\"))\n\ndemography_data_long\n\n# A tibble: 123 × 4\n   age   male      female    total    \n   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n 1 Total 2,192,452 2,204,376 4,396,828\n 2 0     57,265    56,523    113,788  \n 3 1     56,019    54,601    110,620  \n 4 2     52,518    51,848    104,366  \n 5 3     51,115    51,027    102,142  \n 6 4     47,182    46,889    94,071   \n 7 0-4   264,099   260,888   524,987  \n 8 5     45,203    44,711    89,914   \n 9 6     43,635    44,226    87,861   \n10 7     43,507    43,655    87,162   \n# ℹ 113 more rows\n\n\nHaving got it into a rectangular format, we now need to clean the dataset to make it useful.\n\n\n9.7.1.2 Validity\nTo attain validity requires a number of steps. The first step is to make the numbers into actual numbers, rather than characters. Before we can convert the type, we need to remove anything that is not a number otherwise that cell will be converted into an NA. We first identify any values that are not numbers so that we can remove them, and distinct() is especially useful.\n\ndemography_data_long |&gt;\n  select(male, female, total) |&gt;\n  mutate(across(everything(), ~ str_remove_all(., \"[:digit:]\"))) |&gt;\n  distinct()\n\n# A tibble: 5 × 3\n  male  female total\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;\n1 \",,\"  \",,\"   \",,\" \n2 \",\"   \",\"    \",\"  \n3 \"\"    \",\"    \",\"  \n4 \"\"    \"\"     \",\"  \n5 \"\"    \"\"     \"\"   \n\n\nWe need to remove commas. While we could use janitor here, it is worthwhile to at least first look at what is going on because sometimes there is odd stuff that janitor (and other packages) will not deal with in a way that we want. Nonetheless, having identified everything that needs to be removed, we can do the actual removal and convert our character column of numbers to integers.\n\ndemography_data_long &lt;-\n  demography_data_long |&gt;\n  mutate(across(c(male, female, total), ~ str_remove_all(., \",\"))) |&gt;\n  mutate(across(c(male, female, total), ~ as.integer(.)))\n\ndemography_data_long\n\n# A tibble: 123 × 4\n   age      male  female   total\n   &lt;chr&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n 1 Total 2192452 2204376 4396828\n 2 0       57265   56523  113788\n 3 1       56019   54601  110620\n 4 2       52518   51848  104366\n 5 3       51115   51027  102142\n 6 4       47182   46889   94071\n 7 0-4    264099  260888  524987\n 8 5       45203   44711   89914\n 9 6       43635   44226   87861\n10 7       43507   43655   87162\n# ℹ 113 more rows\n\n\n\n\n9.7.1.3 Internal consistency\n\nThe census has done some of the work of putting together age-groups for us, but we want to make it easy to just focus on the counts by single-year age. As such we will add a flag as to the type of age it is: an age-group, such as “ages 0 to 5”, or a single age, such as “1”.\n\ndemography_data_long &lt;-\n  demography_data_long |&gt;\n  mutate(\n    age_type = if_else(str_detect(age, \"-\"), \n                       \"age-group\", \n                       \"single-year\"),\n    age_type = if_else(str_detect(age, \"Total\"), \n                       \"age-group\", \n                       age_type)\n  )\n\nAt the moment, age is a character variable. We have a decision to make here. We do not want it to be a character variable (because it will not graph properly), but we do not want it to be numeric, because there is total and 100+ in there. For now, we will just make it into a factor, and at least that will be able to be nicely graphed.\n\ndemography_data_long &lt;-\n  demography_data_long |&gt;\n  mutate(\n    age = as_factor(age)\n  )\n\n\n\n\n9.7.2 Check and test\nHaving gathered and cleaned the data, we would like to run a few checks. Given the format of the data, we can check that “total” is the sum of “male” and “female”, which are the only two gender categories available.\n\ndemography_data_long |&gt;\n  mutate(\n    check_sum = male + female,\n    totals_match = if_else(total == check_sum, 1, 0)\n  ) |&gt;\n  filter(totals_match == 0)\n\n# A tibble: 0 × 7\n# ℹ 7 variables: age &lt;fct&gt;, male &lt;int&gt;, female &lt;int&gt;, total &lt;int&gt;,\n#   age_type &lt;chr&gt;, check_sum &lt;int&gt;, totals_match &lt;dbl&gt;\n\n\nFinally, we want to check that the single-age counts sum to the age-groups.\n\ndemography_data_long |&gt;\n  mutate(age_groups = if_else(age_type == \"age-group\", \n                              age, \n                              NA_character_)) |&gt;\n  fill(age_groups, .direction = \"up\") |&gt;\n  mutate(\n    group_sum = sum(total),\n    group_sum = group_sum / 2,\n    difference = total - group_sum,\n    .by = c(age_groups)\n  ) |&gt;\n  filter(age_type == \"age-group\" & age_groups != \"Total\") |&gt; \n  head()\n\n# A tibble: 6 × 8\n  age     male female  total age_type  age_groups group_sum difference\n  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 0-4   264099 260888 524987 age-group 0-4           524987          0\n2 5-9   215230 217482 432712 age-group 5-9           432712          0\n3 10-14 185008 193542 378550 age-group 10-14         378550          0\n4 15-19 159098 192755 351853 age-group 15-19         351853          0\n5 20-24 249534 313485 563019 age-group 20-24         563019          0\n6 25-29 282703 300845 583548 age-group 25-29         583548          0\n\n\n\n\n9.7.3 Tidy-up\nNow that we are reasonably confident that everything is looking good, we can convert it to tidy format. This will make it easier to work with.\n\ndemography_data_tidy &lt;-\n  demography_data_long |&gt;\n  rename_with(~paste0(., \"_total\"), male:total) |&gt;\n  pivot_longer(cols = contains(\"_total\"), \n               names_to = \"type\", \n               values_to = \"number\") |&gt;\n  separate(\n    col = type,\n    into = c(\"gender\", \"part_of_area\"),\n    sep = \"_\"\n  ) |&gt;\n  select(age, age_type, gender, number)\n\nThe original purpose of cleaning this dataset was to make a table that is used by Alexander and Alkema (2022). We will return to this dataset, but just to bring this all together, we may like to make a graph of single-year counts, by gender, for Nairobi (Figure 9.9).\n\ndemography_data_tidy |&gt;\n  filter(age_type == \"single-year\") |&gt;\n  select(age, gender, number) |&gt;\n  filter(gender != \"total\") |&gt;\n  ggplot(aes(x = age, y = number, fill = gender)) +\n  geom_col(aes(x = age, y = number, fill = gender), \n           position = \"dodge\") +\n  scale_y_continuous(labels = comma) +\n  scale_x_discrete(breaks = c(seq(from = 0, to = 99, by = 5), \"100+\")) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    y = \"Number\",\n    x = \"Age\",\n    fill = \"Gender\",\n    caption = \"Data source: 2019 Kenya Census\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  coord_flip()\n\n\n\n\nFigure 9.9: Distribution of age and gender in Nairobi in 2019, based on Kenyan census\n\n\n\n\nA variety of features are clear from Figure 9.9, including age-heaping, a slight difference in the ratio of male-female birth, and a substantial difference between ages 15 and 25.\nFinally, we may wish to use more informative names. For instance, in the Kenyan data example earlier we have the following column names: “area”, “age”, “gender”, and “number”. If we were to use our column names as contracts, then these could be: “chr_area”, “fctr_group_age”, “chr_group_gender”, and “int_group_count”.\n\ncolumn_names_as_contracts &lt;-\n  demography_data_tidy |&gt;\n  filter(age_type == \"single-year\") |&gt;\n  select(age, gender, number) |&gt;\n  rename(\n    \"fctr_group_age\" = \"age\",\n    \"chr_group_gender\" = \"gender\",\n    \"int_group_count\" = \"number\"\n  )\n\nWe can then use pointblank to set-up tests for us.\n\nagent &lt;-\n  create_agent(tbl = column_names_as_contracts) |&gt;\n  col_is_character(columns = vars(chr_group_gender)) |&gt;\n  col_is_factor(columns = vars(fctr_group_age)) |&gt;\n  col_is_integer(columns = vars(int_group_count)) |&gt;\n  col_vals_in_set(\n    columns = chr_group_gender,\n    set = c(\"male\", \"female\", \"total\")\n  ) |&gt;\n  interrogate()\n\nagent\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Pointblank Validation\n    \n    \n      [2023-08-18|13:37:20]\ntibble\ncolumn_names_as_contracts\n\n    \n    \n      \n      \n      STEP\n      COLUMNS\n      VALUES\n      TBL\n      EVAL\n      UNITS\n      PASS\n      FAIL\n      W\n      S\n      N\n      EXT\n    \n  \n  \n    \n1\n\n      col_is_character                                                            \n   col_is_character()\n\n\n\n  \n    ▮chr_group_gender\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n2\n\n      col_is_factor                                                            \n   col_is_factor()\n\n\n\n  \n    ▮fctr_group_age\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n3\n\n      col_is_integer                                                            \n   col_is_integer()\n\n\n\n  \n    ▮int_group_count\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n—\n\n—\n\n—\n\n    \n4\n\n      col_vals_in_set                                                \n   col_vals_in_set()\n\n\n\n  \n    ▮chr_group_gender\n  \n\n\nmale, female, total\n\n                                                            \n\n✓\n\n306\n3061.00\n00.00\n—\n\n—\n\n—\n\n—\n\n  \n  \n    \n      2023-08-18 13:37:20 EDT\n&lt; 1 s\n2023-08-18 13:37:20 EDT"
  },
  {
    "objectID": "09-clean_and_prepare.html#exercises",
    "href": "09-clean_and_prepare.html#exercises",
    "title": "9  Clean and prepare",
    "section": "9.8 Exercises",
    "text": "9.8 Exercises\n\nScales\n\n(Plan) Consider the following scenario: You manage a shop with two employees and are interested in modeling their efficiency. The shop opens at 9am and closes at 5pm. The efficiency of the employees is mildly correlated and defined by the number of customers that they serve each hour. Be clear about whether you assume a negative or positive correlation. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include five tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched using the simulated data from step 1. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nIf we had a character variable “some_words” with one observation \"You know what\" within a dataset called sayings, then which of the following would split it into its constituent words (pick one)?\n\nseparate(data = sayings, col = some_words, into = c(\"one\", \"two\", \"three\"), sep = \" \")\nsplit(data = sayings, col = some_words, into = c(\"one\", \"two\", \"three\"), sep = \" \")\ndivide(data = sayings, col = some_words, into = c(\"one\", \"two\", \"three\"), sep = \" \")\npart(data = sayings, col = some_words, into = c(\"one\", \"two\", \"three\"), sep = \" \")\nunattach(data = sayings, col = some_words, into = c(\"one\", \"two\", \"three\"), sep = \" \")\n\nIs the following an example of tidy data? Why or why not?\n\n\ntibble(\n  name = c(\"Ian\", \"Patricia\", \"Ville\", \"Karen\"),\n  age_group = c(\"18-29\", \"30-44\", \"45-60\", \"60+\"),\n)\n\n\nWhich function would change “lemons” into “lemonade”?\n\nstr_replace(string = \"lemons\", pattern = \"lemons\", replacement = \"lemonade\")\nchr_replace(string = \"lemons\", pattern = \"lemons\", replacement = \"lemonade\")\nstr_change(string = \"lemons\", pattern = \"lemons\", replacement = \"lemonade\")\nchr_change(string = \"lemons\", pattern = \"lemons\", replacement = \"lemonade\")\n\nWhen dealing with ages, what are some desirable classes for the variable (select all that apply)?\n\ninteger\nmatrix\nnumeric\n\nPlease consider the following cities in Germany: “Berlin”, “Hamburg”, “Munich”, “Cologne”, “Frankfurt”, and “Rostock”. Use testthat to define three tests that could apply if we had a dataset with a variable “german_cities” that claimed to contain these, and only these, cities. Submit a link to a GitHub Gist.\nWhich is the most acceptable format for dates in data science?\n\nYYYY-DD-MM\nYYYY-MM-DD\nDD-MM-YYYY\nMM-MM-YYYY\n\nWhich of the following does not belong? c(15.9, 14.9, 16.6, 15.8, 16.7, 17.9, I2.6, 11.5, 16.2, 19.5, 15.0)\nWith regard to “AV Rule 48” from Lockheed Martin (2005, 25) which of the following are not allowed to differ identifiers (select all that apply)?\n\nOnly a mixture of case\nThe presence/absence of the underscore character\nThe interchange of the letter “O” with the number “0” or the letter “D”\nThe interchange of the letter “I” with the number “1” or the letter “l”\n\nWith regard to Preece (1981) please discuss two ways in which final digits can be informative. Write at least a paragraph about each and include examples.\n\n\n\nTutorial\nWith regard to Jordan (2019), D’Ignazio and Klein (2020, chap. 6), Au (2020), and other relevant work, to what extent do you think we should let the data speak for themselves? Please write at least two pages.\nUse Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations to produce a draft. After this, please pair with another student and exchange your written work. Update it based on their feedback, and be sure to acknowledge them by name in your paper. Submit a PDF.\n\n\n\n\nAbelson, Harold, and Gerald Jay Sussman. 1996. Structure and Interpretation of Computer Programs. Cambridge: The MIT Press.\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann. 2021. “Gene Name Errors: Lessons Not Learned.” PLOS Computational Biology 17 (7): 1–13. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nAlexander, Monica, and Leontine Alkema. 2022. “A Bayesian Cohort Component Projection Model to Estimate Women of Reproductive Age at the Subnational Level in Data-Sparse Settings.” Demography 59 (5): 1713–37. https://doi.org/10.1215/00703370-10216406.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nAu, Randy. 2020. “Data Cleaning IS Analysis, Not Grunt Work,” September. https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt.\n\n\nBaker, Dominique. 2023. “Scams Will Not Save Us (Tuition Dollars),” February. http://www.dominiquebaker.com/blog/2023/2/16/scams-will-not-save-us-tuition-dollars.\n\n\nBanes, Graham, Emily Fountain, Alyssa Karklus, Robert Fulton, Lucinda Antonacci-Fulton, and Joanne Nelson. 2022. “Nine out of ten samples were mistakenly switched by The Orang-utan Genome Consortium.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01602-0.\n\n\nBaumgartner, Peter. 2021. “Ways I Use Testing as a Data Scientist,” December. https://www.peterbaumgartner.com/blog/testing-for-data-science/.\n\n\nBorer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” Bulletin of the Ecological Society of America 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nBoykis, Vicki. 2019. “A Deep Dive on Python Type Hints,” July. https://vickiboykis.com/2019/07/08/a-deep-dive-on-python-type-hints/.\n\n\nBryan, Jenny. 2015. “Naming Things.” Reproducible Science Workshop, May. https://speakerdeck.com/jennybc/how-to-name-files.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nChan, Duo. 2021. “Combining Statistical, Physical, and Historical Evidence to Improve Historical Sea-Surface Temperature Records.” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.edcee38f.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer Data Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nDe Jonge, Edwin, and Mark van der Loo. 2013. An introduction to data cleaning with R. Statistics Netherlands Heerlen. https://cran.r-project.org/doc/contrib/de%5FJonge+van%5Fder%5FLoo-Introduction%5Fto%5Fdata%5Fcleaning%5Fwith%5FR.pdf.\n\n\nDoggers, Peter. 2021. “Carlsen Wins Game 6, Longest World Chess Championship Game of All Time,” December. https://www.chess.com/news/view/fide-world-chess-championship-2021-game-6.\n\n\nDu, Kai, Steven Huddart, and Xin Daniel Jiang. 2022. “Lost in Standardization: Effects of Financial Statement Database Discrepancies on Inference.” Journal of Accounting and Economics, December, 101573. https://doi.org/10.1016/j.jacceco.2022.101573.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGagolewski, Marek. 2022. “stringi: Fast and Portable Character String Processing in R.” Journal of Statistical Software 103 (2): 1–59. https://doi.org/10.18637/jss.v103.i02.\n\n\nGao, Zheng, Christian Bird, and Earl T. Barr. 2017. “To Type or Not to Type: Quantifying Detectable Bugs in JavaScript.” In 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE. https://doi.org/10.1109/icse.2017.75.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No ‘Fishing Expedition’ or ‘p-Hacking’ and the Research Hypothesis Was Posited Ahead of Time.” Department of Statistics, Columbia University. http://www.stat.columbia.edu/~gelman/research/unpublished/p%5Fhacking.pdf.\n\n\nGelman, Andrew, and Aki Vehtari. 2021. “What Are the Most Important Statistical Ideas of the Past 50 Years?” Journal of the American Statistical Association 116 (536): 2087–97. https://doi.org/10.1080/01621459.2021.1938081.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHalberstam, David. 1972. The Best and the Brightest. 1st ed. New York: Random House.\n\n\nHand, David. 2018. “Statistical Challenges of Administrative and Transaction Data.” Journal of the Royal Statistical Society: Series A (Statistics in Society) 181 (3): 555–605. https://doi.org/10.1111/rssa.12315.\n\n\nHermans, Felienne. 2017. “Peter Hilton on Naming.” IEEE Software 34 (3): 117–20. https://doi.org/10.1109/MS.2017.81.\n\n\n———. 2021. The Programmer’s Brain: What Every Programmer Needs to Know about Cognition. 1st ed. New York: Simon; Schuster. https://www.manning.com/books/the-programmers-brain.\n\n\nHodgetts, Paul. 2022. “The Negative Space of Data,” March. https://hodgettsp.netlify.app/post/data-negativespace/.\n\n\nHofmeister, Johannes, Janet Siegmund, and Daniel Holt. 2017. “Shorter Identifier Names Take Longer to Comprehend.” In 2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER), 217–27. https://doi.org/10.1109/saner.2017.7884623.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nIannone, Richard, and Mauricio Vargas. 2022. pointblank: Data Validation and Organization of Metadata for Local and Remote Tables. https://CRAN.R-project.org/package=pointblank.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte Wickham, and Greg Wilson. 2021. Research Software Engineering with Python. Chapman; Hall/CRC.\n\n\nJet Propulsion Laboratory. 2009. “JPL Institutional Coding Standard for the C Programming Language.” Document Number D-60411, March. https://web.archive.org/web/20111015064908/http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf.\n\n\nJordan, Michael. 2019. “Artificial Intelligence–The Revolution Hasn’t Happened Yet.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nLin, Sarah, Ibraheem Ali, and Greg Wilson. 2021. “Ten Quick Tips for Making Things Findable.” PLOS Computational Biology 16 (12): 1–10. https://doi.org/10.1371/journal.pcbi.1008469.\n\n\nLiu, Emily, Lenny Bronner, and Jeremy Bowers. 2022. “What the Washington Post Elections Engineering Team Had to Learn about Election Data.” Washington Post Engineering, April. https://washpost.engineering/what-the-washington-post-elections-engineering-team-had-to-learn-about-election-data-a41603daf9ca.\n\n\nLockheed Martin. 2005. “Joint Strike Fighter Air Vehicle C++ Coding Standards For The System Development And Demonstration Program.” Document Number 2RDU00001 Rev C, December. https://www.stroustrup.com/JSF-AV-rules.pdf.\n\n\nMartin, Charles, and Ben Popper. 2021. “Don’t Push That Button: Exploring the Software That Flies SpaceX Rockets and Starships.” The Overflow, December. https://stackoverflow.blog/2021/12/27/dont-push-that-button-exploring-the-software-that-flies-spacex-starships/.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nMindell, David. 2008. Digital Apollo: Human and Machine in Spaceflight. 1st ed. New York: The MIT Press.\n\n\nNorthcutt, Curtis, Anish Athalye, and Jonas Mueller. 2021. “Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.” https://doi.org/10.48550/arXiv.2103.14749.\n\n\nOoms, Jeroen. 2022. pdftools: Text Extraction, Rendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716.\n\n\nPreece, Donald Arthur. 1981. “Distributions of Final Digits in Data.” The Statistician 30 (1): 31. https://doi.org/10.2307/2987702.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRadcliffe, Nicholas. 2023. Test-Driven Data Analysis (Python TDDA library). https://tdda.readthedocs.io/en/latest/index.html.\n\n\nRiederer, Emily. 2020. “Column Names as Contracts,” September. https://emilyriederer.netlify.app/post/column-name-contracts/.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora Aroyo. 2021. “‘Everyone Wants to Do the Model Work, Not the Data Work’: Data Cascades in High-Stakes AI.” In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSilberzahn, Raphael, Eric Uhlmann, Daniel Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.” Advances in Methods and Practices in Psychological Science 1 (3): 337–56. https://doi.org/10.1177/2515245917747646.\n\n\nSomers, James. 2015. “Toolkits for the Mind.” MIT Technology Review, April. https://www.technologyreview.com/2015/04/02/168469/toolkits-for-the-mind/.\n\n\nTurcotte, Alexi, Aviral Goel, Filip Křikava, and Jan Vitek. 2020. “Designing Types for r, Empirically.” Proceedings of the ACM on Programming Languages 4 (OOPSLA): 1–25. https://doi.org/10.1145/3428249.\n\n\nVan den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and Kobus Herbst. 2005. “Data Cleaning: Detecting, Diagnosing, and Editing Data Abnormalities.” PLOS Medicine 2 (10): e267. https://doi.org/10.1371/journal.pmed.0020267.\n\n\nvan der Loo, Mark. 2022. The Data Validation Cookbook. https://data-cleaning.github.io/validate/.\n\n\nvan der Loo, Mark, and Edwin De Jonge. 2021. “Data Validation Infrastructure for R.” Journal of Statistical Software 97 (10): 1–33. https://doi.org/10.18637/jss.v097.i10.\n\n\nVickers, Andrew, and Emily Vertosick. 2016. “An Empirical Study of Race Times in Recreational Endurance Runners.” BMC Sports Science, Medicine and Rehabilitation 8 (1). https://doi.org/10.1186/s13102-016-0052-y.\n\n\nWeinberg, Gerald. 1971. The Psychology of Computer Programming. New York: Van Nostrand Reinhold Company.\n\n\nWickham, Hadley. 2011. “testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, and Dana Seidel. 2022. scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "09-clean_and_prepare.html#footnotes",
    "href": "09-clean_and_prepare.html#footnotes",
    "title": "9  Clean and prepare",
    "section": "",
    "text": "By way of background, character encoding is needed for computers, which are based on strings of 0s and 1s, to be able to consider symbols such as alphabets. One source of particularly annoying data cleaning issues is different character encoding. This is especially common when dealing with foreign languages and odd characters. In general, we use an encoding called UTF-8. The encoding of a character vector can be found using Encoding().↩︎\nIf this does not work, then the City of Toronto government may have moved the datasets. Instead use: earlier_toronto_shelters &lt;- read_csv(\"https://www.tellingstorieswithdata.com/inputs/data/earlier_toronto_shelters.csv\").↩︎\nOr use: https://www.tellingstorieswithdata.com/inputs/pdfs/1996_Tanzania_DHS.pdf.↩︎\nIf the Kenyan government link breaks then replace their URL with: https://www.tellingstorieswithdata.com/inputs/pdfs/2019_Kenya_census.pdf.↩︎"
  },
  {
    "objectID": "10-store_and_share.html#introduction",
    "href": "10-store_and_share.html#introduction",
    "title": "10  Store and share",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nAfter we have put together a dataset we must store it appropriately and enable easy retrieval both for ourselves and others. There is no completely agreed on approach, but there are best standards, and this is an evolving area of research (Lewis 2023). Wicherts, Bakker, and Molenaar (2011) found that a reluctance to share data was associated with research papers that had weaker evidence and more potential errors. While it is possible to be especially concerned about this—and entire careers and disciplines are based on the storage and retrieval of data—to a certain extent, the baseline is not onerous. If we can get our dataset off our own computer, then we are much of the way there. Further confirming that someone else can retrieve and use it, ideally without our involvement, puts us much further than most. Just achieving that for our data, models, and code meets the “bronze” standard of Heil et al. (2021).\nThe FAIR principles are useful when we come to think more formally about data sharing and management. This requires that datasets are (Wilkinson et al. 2016):\n\nFindable. There is one, unchanging, identifier for the dataset and the dataset has high-quality descriptions and explanations.\nAccessible. Standardized approaches can be used to retrieve the data, and these are open and free, possibly with authentication, and their metadata persist even if the dataset is removed.\nInteroperable. The dataset and its metadata use a broadly-applicable language and vocabulary.\nReusable. There are extensive descriptions of the dataset and the usage conditions are made clear along with provenance.\n\nOne reason for the rise of data science is that humans are at the heart of it. And often the data that we are interested in directly concern humans. This means that there can be tension between sharing a dataset to facilitate reproducibility and maintaining privacy. Medicine developed approaches to this over a long time. And out of that we have seen the Health Insurance Portability and Accountability Act (HIPAA) in the US, the broader General Data Protection Regulation (GDPR) in Europe introduced in 2016, and the California Consumer Privacy Act (CCPA) introduced in 2018, among others.\nOur concerns in data science tend to be about personally identifying information. We have a variety of ways to protect especially private information, such as emails and home addresses. For instance, we can hash those variables. Sometimes we may simulate data and distribute that instead of sharing the actual dataset. More recently, approaches based on differential privacy are being implemented, for instance for the US census. The fundamental problem of data privacy is that increased privacy reduces the usefulness of a dataset. The trade-off means the appropriate decision is nuanced and depends on costs and benefits, and we should be especially concerned about differentiated effects on population minorities.\nJust because a dataset is FAIR, it is not necessarily an unbiased representation of the world. Further, it is not necessarily fair in the everyday way that word is used, i.e. impartial and honest (Lima et al. 2022). FAIR reflects whether a dataset is appropriately available, not whether it is appropriate.\nFinally, in this chapter we consider efficiency. As datasets and code bases get larger it becomes more difficult to deal with them, especially if we want them to be shared. We come to concerns around efficiency, not for its own sake, but to enable us to tell stories that could not otherwise be told. This might mean moving beyond CSV files to formats with other properties, or even using databases, such as Postgres, although even as we do so acknowledging that the simplicity of a CSV, as it is text-based which lends itself to human inspection, can be a useful feature."
  },
  {
    "objectID": "10-store_and_share.html#plan",
    "href": "10-store_and_share.html#plan",
    "title": "10  Store and share",
    "section": "10.2 Plan",
    "text": "10.2 Plan\nThe storage and retrieval of information is especially connected with libraries, in the traditional sense of a collection of books. These have existed since antiquity and have well-established protocols for deciding what information to store and what to discard, as well as information retrieval. One of the defining aspects of libraries is deliberate curation and organization. The use of a cataloging system ensures that books on similar topics are located close to each other, and there are typically also deliberate plans for ensuring the collection is up to date. This enables information storage and retrieval that is appropriate and efficient.\nData science relies heavily on the internet when it comes to storage and retrieval. Vannevar Bush, the twentieth century engineer, defined a “memex” in 1945 as a device to store books, records, and communications in a way that supplements memory (Bush 1945). The key to it was the indexing, or linking together, of items. We see this concept echoed just four decades later in the proposal by Tim Berners-Lee for hypertext (Berners-Lee 1989). This led to the World Wide Web and defines the way that resources are identified. They are then transported over the internet, using Hypertext Transfer Protocol (HTTP).\nAt its most fundamental, the internet is about storing and retrieving data. It is based on making various files on a computer available to others. When we consider the storage and retrieval of our datasets we want to especially contemplate for how long they should be stored and for whom (Michener 2015). For instance, if we want some dataset to be available for a decade, and widely available, then it becomes important to store it in open and persistent formats (Hart et al. 2016). But if we are just using a dataset as part of an intermediate step, and we have the original, unedited data and the scripts to create it, then it might be fine to not worry too much about such considerations. The evolution of physical storage media has similar complicated issues. For instance, datasets and recordings made on media such as wax cylinders, magnetic tapes, and proprietary optical disks, now have a variable ease of use.\nStoring the original, unedited data is important and there are many cases where unedited data have revealed or hinted at fraud (Simonsohn 2013). Shared data also enhances the credibility of our work, by enabling others to verify it, and can lead to the generation of new knowledge as others use it to answer different questions (Christensen, Freese, and Miguel 2019). Christensen et al. (2019) suggest that research that shares its data may be more highly cited, although Tierney and Ram (2021) caution that widespread data sharing may require a cultural change.\nWe should try to invite scrutiny and make it as easy as possible to receive criticism. We should try to do this even when it is the difficult choice and results in discomfort because that is the only way to contribute to the stock of lasting knowledge. For instance, Piller (2022) details potential fabrication in research about Alzheimer’s disease. In that case, one of the issues that researchers face when trying to understand whether the results are legitimate is a lack of access to unpublished images.\nData provenance is especially important. This refers to documenting “where a piece of data came from and the process by which it arrived in the database” (Buneman, Khanna, and Wang-Chiew 2001, 316). Documenting and saving the original, unedited dataset, using scripts to manipulate it to create the dataset that is analyzed, and sharing all of this—as recommended in this book—goes some way to achieving this. In some fields it is common for just a handful of databases to be used by many different teams, for instance, in genetics, the UK BioBank, and in the life sciences a cloud-based platform called ORCESTRA (Mammoliti et al. 2021) has been established to help."
  },
  {
    "objectID": "10-store_and_share.html#share",
    "href": "10-store_and_share.html#share",
    "title": "10  Store and share",
    "section": "10.3 Share",
    "text": "10.3 Share\n\n10.3.1 GitHub\nThe easiest place for us to get started with storing a dataset is GitHub because that is already built into our workflow. For instance, if we push a dataset to a public repository, then our dataset becomes available. One benefit of this is that if we have set up our workspace appropriately, then we likely store our original, unedited data and the tidy data, as well as the scripts that are needed to transform one to the other. We are most of the way to the “bronze” standard of Heil et al. (2021) without changing anything.\nAs an example of how we have stored some data, we can access “raw_data.csv” from the “starter_folder”. We navigate to the file in GitHub (“inputs” \\(\\rightarrow\\) “data” \\(\\rightarrow\\) “raw_data.csv”), and then click “Raw” (Figure 10.1).\n\n\n\nFigure 10.1: Getting the necessary link to be able to read a CSV from a GitHub repository\n\n\nWe can then add that URL as an argument to read_csv().\n\ndata_location &lt;-\n  paste0(\n    \"https://raw.githubusercontent.com/RohanAlexander/\",\n    \"starter_folder/main/inputs/data/raw_data.csv\"\n  )\n\nstarter_data &lt;-\n  read_csv(file = data_location,\n           col_types = cols(\n             first_col = col_character(),\n             second_col = col_character(),\n             third_col = col_character()\n             )\n           )\n\nstarter_data\n\n# A tibble: 1 × 3\n  first_col second_col third_col\n  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    \n1 some      raw        data     \n\n\nWhile we can store and retrieve a dataset easily in this way, it lacks explanation, a formal dictionary, and aspects such as a license that would bring it closer to aligning with the FAIR principles. Another practical concern is that the maximum file size on GitHub is 100MB, although Git Large File Storage (LFS) can be used if needed. And a final concern, for some, is that GitHub is owned by Microsoft, a for-profit US technology firm.\n\n\n10.3.2 R packages for data\nTo this point we have largely used R packages for their code, although we have seen a few that were focused on sharing data, for instance, troopdata and babynames in Appendix E. We can build a R package for our dataset and then add it to GitHub and potentially eventually CRAN. This will make it easy to store and retrieve because we can obtain the dataset by loading the package. In contrast to the CSV-based approach, it also means a dataset brings its documentation along with it.\nThis will be the first R package that we build, and so we will jump over a number of steps. The key is to just try to get something working. In Appendix I, we return to R packages and use them to deploy models. This gives us another chance to further develop experience with them.\nTo get started, create a new package: “File” \\(\\rightarrow\\) “New project” \\(\\rightarrow\\) “New Directory” \\(\\rightarrow\\) “R Package”. Give the package a name, such as “favcolordata” and select “Open in new session”. Create a new folder called “data”. We will simulate a dataset of people and their favorite colors to include in our R package.\n\nset.seed(853)\n\ncolor_data &lt;-\n  tibble(\n    name =\n      c(\n        \"Edward\", \"Helen\", \"Hugo\", \"Ian\", \"Monica\",\n        \"Myles\", \"Patricia\", \"Roger\", \"Rohan\", \"Ruth\"\n      ),\n    fav_color =\n      sample(\n        x = colors(),\n        size = 10,\n        replace = TRUE\n      )\n  )\n\nTo this point we have largely been using CSV files for our datasets. To include our data in this R package, we save our dataset in a different format, “.rda”, using save().\n\nsave(color_data, file = \"data/color_data.rda\")\n\nThen we create a R file “data.R” in the “R” folder. This file will only contain documentation using roxygen2 comments. These start with #', and we follow the documentation for troopdata closely.\n\n#' Favorite color of various people data\n#'\n#' @description \\code{favcolordata} returns a dataframe\n#' of the favorite color of various people.\n#'\n#' @return Returns a dataframe of the favorite color\n#' of various people.\n#'\n#' @docType data\n#'\n#' @usage data(color_data)\n#'\n#' @format A dataframe of individual-level observations\n#' with the following variables:\n#'\n#' \\describe{\n#' \\item{\\code{name}}{A character vector of individual names.}\n#' \\item{\\code{fav_color}}{A character vector of colors.}\n#' }\n#'\n#' @keywords datasets\n#'\n#' @source \\url{tellingstorieswithdata.com/10-store_and_share.html}\n#'\n\"color_data\"\n\nFinally, add a README that provides a summary of all of this for someone coming to the project for the first time. Examples of packages with excellent READMEs include ggplot2, pointblank, modelsummary, and janitor.\nWe can now go to the “Build” tab and click “Install and Restart”. After this, the package “favcolordata”, will be loaded and the data can be accessed locally using “color_data”. If we were to push this package to GitHub, then anyone would be able to install the package using devtools and use our dataset. Indeed, the following should work.\n\ninstall_github(\"RohanAlexander/favcolordata\")\n\nlibrary(favcolordata)\n\ncolor_data\n\nThis has addressed many of the issues that we faced earlier. For instance, we have included a README and a data dictionary, of sorts, in terms of the descriptions that we added. But if we were to try to put this package onto CRAN, then we might face some issues. For instance, the maximum size of a package is 5MB and we would quickly come up against that. We have also largely forced users to use R. While there are benefits of that, we may like to be more language agnostic (Tierney and Ram 2020), especially if we are concerned about the FAIR principles.\nWickham (2022, chap. 8) provides more information about including data in R packages.\n\n\n10.3.3 Depositing data\nWhile it is possible that a dataset will be cited if it is available through GitHub or a R package, this becomes more likely if the dataset is deposited somewhere. There are several reasons for this, but one is that it seems a bit more formal. Another is that it is associated with a DOI. Zenodo and the Open Science Framework (OSF) are two depositories that are commonly used. For instance, Carleton (2021) uses Zenodo to share the dataset and analysis supporting Carleton, Campbell, and Collard (2021), Geuenich et al. (2021b) use Zenodo to share the dataset that underpins Geuenich et al. (2021a), and Katz and Alexander (2023b) use Zenodo to share the dataset that underpins Katz and Alexander (2023a). Similarly, Arel-Bundock et al. (2022) use OSF to share code and data.\nAnother option is to use a dataverse, such as the Harvard Dataverse or the Australian Data Archive. This is a common requirement for journal publications. One nice aspect of this is that we can use dataverse to retrieve the dataset as part of a reproducible workflow. We have an example of this in Chapter 13.\nIn general, these options are free and provide a DOI that can be useful for citation purposes. The use of data deposits such as these is a way to offload responsibility for the continued hosting of the dataset (which in this case is a good thing) and prevent the dataset from being lost. It also establishes a single point of truth, which should act to reduce errors (Byrd et al. 2020). Finally, it makes access to the dataset independent of the original researchers, and results in persistent metadata. That all being said, the viability of these options rests on their underlying institutions. For instance, Zenodo is operated by CERN and many dataverses are operated by universities. These institutions are subject to, as we all are, social and political forces."
  },
  {
    "objectID": "10-store_and_share.html#data-documentation",
    "href": "10-store_and_share.html#data-documentation",
    "title": "10  Store and share",
    "section": "10.4 Data documentation",
    "text": "10.4 Data documentation\nDataset documentation has long consisted of a data dictionary. This may be as straight-forward a list of the variables, a few sentences of description, and ideally a source. The data dictionary of the ACS, which was introduced in Chapter 6, is particularly comprehensive. And OSF provides instructions for how to make a data dictionary. Given the workflow advocated in this book, it might be worthwhile to actually begin putting together a data dictionary as part of the simulation step i.e. before even collecting the data. While it would need to be updated, it would be another opportunity to think deeply about the data situation.\nDatasheets (Gebru et al. 2021) are an increasingly common addition to documentation. If we think of a data dictionary as a list of ingredients for a dataset, then we could think of a datasheet as basically a nutrition label for datasets. The process of creating them enables us to think more carefully about what we will feed our model. More importantly, they enable others to better understand what we fed our model. One important task is going back and putting together datasheets for datasets that are widely used. For instance, researchers went back and wrote a datasheet for “BookCorpus”, which is one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated (Bandy and Vincent 2021).\n\n\n\n\n\n\nShoulders of giants\n\n\n\nTimnit Gebru is the founder of the Distributed Artificial Intelligence Research Institute (DAIR). After earning a PhD in Computer Science from Stanford University, Gebru joined Microsoft and then Google. In addition to Bandy and Vincent (2021), which introduced datasheets, one notable paper is Bender et al. (2021), which discussed the dangers of language models being too large. She has made many other substantial contributions to fairness and accountability, especially Buolamwini and Gebru (2018), which demonstrated racial bias in facial analysis algorithms.\n\n\nInstead of telling us how unhealthy various foods are, a datasheet tells us things like:\n\nWho put the dataset together?\nWho paid for the dataset to be created?\nHow complete is the dataset? (Which is, of course, unanswerable, but detailing the ways in which it is known to be incomplete is valuable.)\nWhich variables are present, and, equally, not present, for particular observations?\n\nSometimes, a lot of work is done to create a datasheet. In that case, we may like to publish and share it on its own, for instance, Biderman, Bicheno, and Gao (2022) and Bandy and Vincent (2021). But typically a datasheet might live in an appendix to the paper, for instance Zhang et al. (2022), or be included in a file adjacent to the dataset.\nAs an example, a datasheet for the dataset that underpins Alexander and Hodgetts (2021) is included in Online Appendix F. The text of the questions directly comes from Gebru et al. (2021). When creating a datasheet for a dataset, especially a dataset that we did not put together ourselves, it is possible that the answer to some questions will simply be “Unknown”, but we should do what we can to minimize that.\nThe datasheet template created by Gebru et al. (2021) is not the final word. It is possible to improve on it, and add additional detail sometimes. For instance, Miceli, Posada, and Yang (2022) argue for the addition of questions to do with power relations."
  },
  {
    "objectID": "10-store_and_share.html#personally-identifying-information",
    "href": "10-store_and_share.html#personally-identifying-information",
    "title": "10  Store and share",
    "section": "10.5 Personally identifying information",
    "text": "10.5 Personally identifying information\nBy way of background, Christensen, Freese, and Miguel (2019, 180) define a variable as “confidential” if the researchers know who is associated with each observation, but the public version of the dataset removes this association. A variable is “anonymous” if even the researchers do not know.\nPersonally identifying information (PII) is that which enables us to link an observation in our dataset with an actual person. This is a significant concern in fields focused on data about people. Email addresses are often PII, as are names and addresses. While some variables may not be PII for many respondents, it could be PII for some. For instance, consider a survey that is representative of the population age distribution. There is not likely to be many respondents aged over 100, and so the variable age may then become PII. The same scenario applies to income, wealth, and many other variables. One response to this is for data to be censored, which was discussed in Chapter 6. For instance, we may record age between zero and 90, and then group everyone over that into “90+”. Another is to construct age-groups: “18-29”, “30-44”, \\(\\dots\\). Notice that with both these solutions we have had to trade-off privacy and usefulness. More concerningly, a variable may be PII, not by itself, but when combined with another variable.\nOur primary concern should be with ensuring that the privacy of our dataset is appropriate, given the expectations of the reasonable person. This requires weighing costs and benefits. In national security settings there has been considerable concern about the over-classification of documents (Lin 2014). The reduced circulation of information because of this may result in unrealized benefits. To avoid this in data science, the test of the need to protect a dataset needs to be made by the reasonable person weighing up costs and benefits. It is easy, but incorrect, to argue that data should not be released unless it is perfectly anonymized. The fundamental problem of data privacy implies that such data would have limited utility. That approach, possibly motivated by the precautionary principle, would be too conservative and could cause considerable loss in terms of unrealized benefits.\nRandomized response (Greenberg et al. 1969) is a clever way to enable anonymity without much overhead. Each respondent flips a coin before they answer a question but does not show the researcher the outcome of the coin flip. The respondent is instructed to respond truthfully to the question if the coin lands on heads, but to always give some particular (but still plausible) response if tails. The results of the other options can then be re-weighted to enable an estimate, without a researcher ever knowing the truth about any particular respondent. This is especially used in association with snowball sampling, discussed in Chapter 6. One issue with randomized response is that the resulting dataset can be only used to answer specific questions. This requires careful planning, and the dataset will be of less general value.\nZook et al. (2017) recommend considering whether data even need to be gathered in the first place. For instance, if a phone number is not absolutely required then it might be better to not ask for it, rather than need to worry about protecting it before data dissemination. GDPR and HIPAA are two legal structures that govern data in Europe, and the United States, respectively. Due to the influence of these regions, they have a significant effect outside those regions also. GDPR concerns data generally, while HIPAA is focused on healthcare. GDPR applies to all personal data, which is defined as:\n\n\\(\\dots\\)any information relating to an identified or identifiable natural person (“data subject”); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;\nCouncil of European Union (2016), Article 4, “Definitions”\n\nHIPAA refers to the privacy of medical records in the US and codifies the idea that the patient should have access to their medical records, and that only the patient should be able to authorize access to their medical records (Annas 2003). HIPAA only applies to certain entities. This means it sets a standard, but coverage is inconsistent. For instance, a person’s social media posts about their health would generally not be subject to it, nor would knowledge of a person’s location and how active they are, even though based on that information we may be able to get some idea of their health (Cohen and Mello 2018). Such data are hugely valuable (Ross 2022).\nThere are a variety of ways of protecting PII, while still sharing some data, that we will now go through. We focus here initially on what we can do when the dataset is considered by itself, which is the main concern. But sometimes the combination of several variables, none of which are PII in and of themselves, can be PII. For instance, age is unlikely PII by itself, but age combined with city, education, and a few other variables could be. One concern is that re-identification could occur by combining datasets and this is a potential role for differential privacy.\n\n10.5.1 Hashing\nA cryptographic hash is a one-way transformation, such that the same input always provides the same output, but given the output, it is not reasonably possible to obtain the input. For instance, a function that doubled its input always gives the same output, for the same input, but is also easy to reverse, so would not work well as a hash. In contrast, the modulo, which for a non-negative number is the remainder after division and can be implemented in R using %%, would be difficult to reverse.\nKnuth (1998, 514) relates an interesting etymology for “hash”. He first defines “to hash” as relating to chop up or make a mess, and then explaining that hashing relates to scrambling the input and using this partial information to define the output. A collision is when different inputs map to the same output, and one feature of a good hashing algorithm is that collisions are reduced. As mentioned, one simple approach is to rely on the modulo operator. For instance, if we were interested in ten different groupings for the integers 1 through to 10, then modulo would enable this. A better approach would be for the number of groupings to be a larger number, because this would reduce the number of values with the same hash outcome.\nFor instance, consider some information that we would like to keep private, such as names and ages of respondents.\n\nsome_private_information &lt;-\n  tibble(\n    names = c(\"Rohan\", \"Monica\"),\n    ages = c(36, 35)\n  )\n\nsome_private_information\n\n# A tibble: 2 × 2\n  names   ages\n  &lt;chr&gt;  &lt;dbl&gt;\n1 Rohan     36\n2 Monica    35\n\n\nOne option for the names would be to use a function that just took the first letter of each name. And one option for the ages would be to convert them to Roman numerals.\n\nsome_private_information |&gt;\n  mutate(\n    names = substring(names, 1, 1),\n    ages = as.roman(ages)\n  )\n\n# A tibble: 2 × 2\n  names ages   \n  &lt;chr&gt; &lt;roman&gt;\n1 R     XXXVI  \n2 M     XXXV   \n\n\nWhile the approach for the first variable, names, is good because the names cannot be backed out, the issue is that as the dataset grows there are likely to be lots of “collisions”—situations where different inputs, say “Rohan” and “Robert”, both get the same output, in this case “R”. It is the opposite situation for the approach for the second variable, ages. In this case, there will never be any collisions—“36” will be the only input that ever maps to “XXXVI”. However, it is easy to back out the actual data, for anyone who knows roman numerals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRather than write our own hash functions, we can use cryptographic hash functions such as md5() from openssl.\n\nsome_private_information |&gt;\n  mutate(\n    md5_names = md5(names),\n    md5_ages = md5(ages |&gt; as.character())\n  )\n\n# A tibble: 2 × 4\n  names   ages md5_names                        md5_ages                        \n  &lt;chr&gt;  &lt;dbl&gt; &lt;hash&gt;                           &lt;hash&gt;                          \n1 Rohan     36 02df8936eee3d4d2568857ed530671b2 19ca14e7ea6328a42e0eb13d585e4c22\n2 Monica    35 09084cc0cda34fd80bfa3cc0ae8fe3dc 1c383cd30b7c298ab50293adfecb7b18\n\n\nWe could share either of these transformed variables and be comfortable that it would be difficult for someone to use only that information to recover the names of our respondents. That is not to say that it is impossible. Knowledge of the key, which is the term given to the string used to encrypt the data, would allow someone to reverse this. If we made a mistake, such as accidentally pushing the original dataset to GitHub then they could be recovered. And it is likely that governments and some private companies can reverse the cryptographic hashes used here.\nOne issue that remains is that anyone can take advantage of the key feature of hashes to back out the input. In particular, the same input always gets the same output. So they could test various options for inputs. For instance, they could themselves try to hash “Rohan”, and then noticing that the hash is the same as the one that we published in our dataset, know that data relates to that individual. We could try to keep our hashing approach secret, but that is difficult as there are only a few that are widely used. One approach is to add a salt that we keep secret. This slightly changes the input. For instance, we could add the salt “_is_a_person” to all our names and then hash that, although a large random number might be a better option. Provided the salt is not shared, then it would be difficult for most people to reverse our approach in that way.\n\nsome_private_information |&gt;\n  mutate(names = paste0(names, \"_is_a_person\")) |&gt;\n  mutate(\n    md5_of_salt = md5(names)\n  )\n\n# A tibble: 2 × 3\n  names               ages md5_of_salt                     \n  &lt;chr&gt;              &lt;dbl&gt; &lt;hash&gt;                          \n1 Rohan_is_a_person     36 3ab064d7f746fde604122d072fd4fa97\n2 Monica_is_a_person    35 50bb9dfffa926c855b830845ac61b659\n\n\n\n\n10.5.2 Simulation\nOne common approach to deal with the issue of being unable to share the actual data that underpins an analysis, is to use data simulation. We have used data simulation throughout this book toward the start of the workflow to help us to think more deeply about our dataset. We can use data simulation again at the end, to ensure that others cannot access the actual dataset.\nThe approach is to understand the critical features of the dataset and the appropriate distribution. For instance, if our data were the ages of some population, then we may want to use the Poisson distribution and experiment with different parameters for the rate. Having simulated a dataset, we conduct our analysis using this simulated dataset and ensure that the results are broadly similar to when we use the real data. We can then release the simulated dataset along with our code.\nFor more nuanced situations, Koenecke and Varian (2020) recommend using the synthetic data vault (Patki, Wedge, and Veeramachaneni 2016) and then the use of Generative Adversarial Networks, such as implemented by Athey et al. (2021).\n\n\n10.5.3 Differential privacy\nDifferential privacy is a mathematical definition of privacy (Dwork and Roth 2013, 6). It is not just one algorithm, it is a definition that many algorithms satisfy. Further, there are many definitions of privacy, of which differential privacy is just one. The main issue it solves is that there are many datasets available. This means there is always the possibility that some combination of them could be used to identify respondents even if PII were removed from each of these individual datasets. For instance, experience with the Netflix prize found that augmenting the available dataset with data from IMBD resulted in better predictions, which points to why this would so commonly happen. Rather than needing to anticipate how various datasets could be combined to re-identify individuals and adjust variables to remove this possibility, a dataset that is created using a differentially private approach provides assurances that privacy will be maintained.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nCynthia Dwork is the Gordon McKay Professor of Computer Science at Harvard University. After earning a PhD in Computer Science from Cornell University, she was a Post-Doctoral Research Fellow at MIT and then worked at IBM, Compaq, and Microsoft Research where she is a Distinguished Scientist. She joined Harvard in 2017. One of her major contributions is differential privacy (Dwork et al. 2006), which has become widely used.\n\n\nTo motivate the definition, consider a dataset of responses and PII that only has one person in it. The release of that dataset, as is, would perfectly identify them. At the other end of the scale, consider a dataset that does not contain a particular person. The release of that dataset could, in general, never be linked to them because they are not in it.1 Differential privacy, then, is about the inclusion or exclusion of particular individuals in a dataset. An algorithm is differentially private if the inclusion or exclusion of any particular person in a dataset has at most some given factor of an effect on the probability of some output (Oberski and Kreuter 2020).      The fundamental problem of data privacy is that we cannot have completely anonymized data that remains useful (Dwork and Roth 2013, 6). Instead, we must trade-off utility and privacy.\nA dataset is differentially private to different levels of privacy, based on how much it changes when one person’s results are included or excluded. This is the key parameter, because at the same time as deciding how much of an individual’s information we are prepared to give up, we are deciding how much random noise to add, which will impact our output. The choice of this level is a nuanced one and should involve consideration of the costs of undesired disclosures, compared with the benefits of additional research. For public data that will be released under differential privacy, the reasons for the decision should be public because of the costs that are being imposed. Indeed, Tang et al. (2017) argue that even in the case of private companies that use differential privacy, such as Apple, users should have a choice about the level of privacy loss.\nConsider a situation in which a professor wants to release the average mark for a particular assignment. The professor wants to ensure that despite that information, no student can work out the grade that another student got. For instance, consider a small class with the following marks.\n\nset.seed(853)\n\ngrades &lt;- \n  tibble(ps_1 = sample(x = (1:100), size = 10, replace = TRUE))\n\nmean(grades$ps_1)\n\n[1] 50.5\n\n\nThe professor could announce the exact mean, for instance, “The mean for the first problem set was 50.5”. Theoretically, all-but-one student could let the others know their mark. It would then be possible for that group to determine the mark of the student who did not agree to make their mark public.\nA non-statistical approach would be for the professor to add the word “roughly”. For instance, the professor could say “The mean for the first problem set was roughly 50.5”. The students could attempt the same strategy, but they would never know with certainty. The professor could implement a more statistical approach to this by adding noise to the mean.\n\nmean(grades$ps_1) + runif(n = 1, min = -2, max = 2)\n\n[1] 48.91519\n\n\nThe professor could then announce this modified mean. This would make the students’ plan more difficult. One thing to notice about that approach is that it would not work with persistent questioning. For instance, eventually the students would be able to back out the distribution of the noise that the professor added. One implication is that the professor would need to limit the number of queries they answered about the mean of the problem set.\nA differentially private approach is a sophisticated version of this. We can implement it using diffpriv. This results in a mean that we could announce (Table 10.1).\n\n# Code based on the diffpriv example\ntarget &lt;- function(X) mean(X)\n\nmech &lt;- DPMechLaplace(target = target)\n\ndistr &lt;- function(n) rnorm(n)\n\nmech &lt;- sensitivitySampler(mech, oracle = distr, n = 5, gamma = 0.1)\n\nr &lt;- releaseResponse(mech, \n                     privacyParams = DPParamsEps(epsilon = 1), \n                     X = grades$ps_1)\n\n\n\n\n\nTable 10.1: Comparing the actual mean with a differentially private mean\n\n\nActual mean\nAnnounceable mean\n\n\n\n\n50.5\n52.46028\n\n\n\n\n\n\nThe implementation of differential privacy is a costs and benefits issue (Hotz et al. 2022; Kenny et al. 2022). Stronger privacy protection fundamentally must mean less information (Bowen 2022, 39), and this differently affects various aspects of society. For instance, Suriyakumar et al. (2021) found that, in the context of health care, differentially private learning can result in models that are disproportionately affected by large demographic groups. A variant of differential privacy has recently been implemented by the US census. It may have a significant effect on redistricting (Kenny et al. 2021) and result in some publicly available data that are unusable in the social sciences (Ruggles et al. 2019)."
  },
  {
    "objectID": "10-store_and_share.html#data-efficiency",
    "href": "10-store_and_share.html#data-efficiency",
    "title": "10  Store and share",
    "section": "10.6 Data efficiency",
    "text": "10.6 Data efficiency\nFor the most part, done is better than perfect, and unnecessary optimization is a waste of resources. However, at a certain point, we need to adapt new ways of dealing with data, especially as our datasets start to get larger. Here we discuss iterating through multiple files, and then turn to the use of Apache Arrow and parquet. Another natural step would be the use of SQL, which is covered in Online Appendix G.\n\n10.6.1 Iteration\nThere are several ways to become more efficient with our data, especially as it becomes larger. The first, and most obvious, is to break larger datasets into smaller pieces. For instance, if we have a dataset for a year, then we could break it into months, or even days. To enable this, we need a way of quickly reading in many different files.\nThe need to read in multiple files and combine them into the one tibble is a surprisingly common task. For instance, it may be that the data for a year, are saved into individual CSV files for each month. We can use purrr and fs to do this. To illustrate this situation we will simulate data from the exponential distribution using rexp(). Such data may reflect, say, comments on a social media platform, where the vast majority of comments are made by a tiny minority of users. We will use dir_create() from fs to create a folder, simulate monthly data, and save it. We will then illustrate reading it in.\n\ndir_create(path = \"user_data\")\n\nset.seed(853)\n\nsimulate_and_save_data &lt;- function(month) {\n  num_obs &lt;- 1000\n  file_name &lt;- paste0(\"user_data/\", month, \".csv\")\n  user_comments &lt;-\n    tibble(\n      user = c(1:num_obs),\n      month = rep(x = month, times = num_obs),\n      comments = rexp(n = num_obs, rate = 0.3) |&gt; round()\n    )\n  write_csv(\n    x = user_comments,\n    file = file_name\n  )\n}\n\nwalk(month.name |&gt; tolower(), simulate_and_save_data)\n\nHaving created our dataset with each month saved to a different CSV, we can now read it in. There are a variety of ways to do this. The first step is that we need to get a list of all the CSV files in the directory. We use the “glob” argument here to specify that we are interested only in the “.csv” files, and that could change to whatever files it is that we are interested in.\n\nfiles_of_interest &lt;-\n  dir_ls(path = \"user_data/\", glob = \"*.csv\")\n\nfiles_of_interest\n\n\n\n [1] \"april.csv\"     \"august.csv\"    \"december.csv\"  \"february.csv\" \n [5] \"january.csv\"   \"july.csv\"      \"june.csv\"      \"march.csv\"    \n [9] \"may.csv\"       \"november.csv\"  \"october.csv\"   \"september.csv\"\n\n\nWe can pass this list to read_csv() and it will read them in and combine them.\n\nyear_of_data &lt;-\n  read_csv(\n    files_of_interest,\n    col_types = cols(\n      user = col_double(),\n      month = col_character(),\n      comments = col_double(),\n    )\n  )\n\nyear_of_data\n\n\n\n# A tibble: 12,000 × 3\n    user month comments\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 april        0\n 2     2 april        2\n 3     3 april        2\n 4     4 april        5\n 5     5 april        1\n 6     6 april        3\n 7     7 april        2\n 8     8 april        1\n 9     9 april        4\n10    10 april        3\n# ℹ 11,990 more rows\n\n\nIt prints out the first ten days of April, because alphabetically April is the first month of the year and so that was the first CSV that was read.\nThis works well when we have CSV files, but we might not always have CSV files and so will need another way, and can use map_dfr() to do this. One nice aspect of this approach is that we can include the name of the file alongside the observation using “.id”. Here we specify that we would like that column to be called “file”, but it could be anything.\n\nyear_of_data_using_purrr &lt;-\n  files_of_interest |&gt;\n  map_dfr(read_csv, .id = \"file\")\n\n\n\n# A tibble: 12,000 × 4\n   file       user month comments\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 april.csv     1 april        0\n 2 april.csv     2 april        2\n 3 april.csv     3 april        2\n 4 april.csv     4 april        5\n 5 april.csv     5 april        1\n 6 april.csv     6 april        3\n 7 april.csv     7 april        2\n 8 april.csv     8 april        1\n 9 april.csv     9 april        4\n10 april.csv    10 april        3\n# ℹ 11,990 more rows\n\n\n\n\n10.6.2 Apache Arrow\nCSVs are commonly used without much thought in data science. And while CSVs are good because they have little overhead and can be manually inspected, this also means they are quite minimal. This can lead to issues, for instance class is not preserved, and file sizes can become large leading to storage and performance issues. There are various alternatives, including Apache Arrow, which stores data in columns rather than rows like CSV. We focus on the “.parquet” format from Apache Arrow. Like a CSV, parquet is an open standard. The R package, arrow, enables us to use this format. The use of parquet has the advantage of requiring little change from us while delivering significant benefits.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nWes McKinney holds an undergraduate degree in theoretical mathematics from MIT. Starting in 2008, while working at AQR Capital Management, he developed the Python package, pandas, which has become a cornerstone of data science. He later wrote Python for Data Analysis (McKinney [2011] 2022). In 2016, with Hadley Wickham, he designed Feather, which was released in 2016. He now works as CTO of Voltron Data, which focuses on the Apache Arrow project.\n\n\nIn particular, we focus on the benefit of using parquet for data storage, such as when we want to save a copy of an analysis dataset that we cleaned and prepared. Among other aspects, parquet brings two specific benefits, compared with CSV:\n\nthe file sizes are typically smaller; and\nclass is preserved because parquet attaches a schema, which makes dealing with, say, dates and factors considerably easier.\n\nHaving loaded arrow, we can use parquet files in a similar way to CSV files. Anywhere in our code that we used write_csv() and read_csv() we could alternatively, or additionally, use write_parquet() and read_parquet(), respectively. The decision to use parquet needs to consider both costs and benefits, and it is an active area of development.\n\nnum_draws &lt;- 1000000\n\n# Homage: https://www.rand.org/pubs/monograph_reports/MR1418.html\na_million_random_digits &lt;-\n  tibble(\n    numbers = runif(n = num_draws),\n    letters = sample(x = letters, size = num_draws, replace = TRUE),\n    states = sample(x = state.name, size = num_draws, replace = TRUE),\n  )\n\nwrite_csv(x = a_million_random_digits,\n          file = \"a_million_random_digits.csv\")\n\nwrite_parquet(x = a_million_random_digits,\n              sink = \"a_million_random_digits.parquet\")\n\nfile_size(\"a_million_random_digits.csv\")\n\n29.3M\n\nfile_size(\"a_million_random_digits.parquet\")\n\n8.17M\n\n\nWe can write a parquet file with write_parquet() and we can read a parquet with read_parquet(). We get significant reductions in file size when we compare the size of the same datasets saved in each format, especially as they get larger (Table 10.2). The speed benefits of using parquet are most notable for larger datasets. It turns them from being impractical to being usable.\n\n\n\n\nTable 10.2: Comparing the file sizes, and read and write times, of CSV and parquet as the file size increases\n\n\n\n\n\n\n\n\n\n\n\nNumber\nCSV size\nCSV write time (sec)\nCSV read time (sec)\nParquet size\nParquet write time (sec)\nParquet read time (sec)\n\n\n\n\n1e+02\n3.03K\n0.005\n0.262\n2.65K\n0.007\n0.004\n\n\n1e+03\n30K\n0.019\n0.272\n11.1K\n0.011\n0.004\n\n\n1e+04\n300.21K\n0.023\n0.305\n99.58K\n0.010\n0.005\n\n\n1e+05\n2.93M\n0.029\n0.281\n1016.49K\n0.043\n0.008\n\n\n1e+06\n29.29M\n0.151\n0.580\n8.17M\n0.224\n0.046\n\n\n1e+07\n292.89M\n0.998\n2.953\n79.11M\n1.763\n0.416\n\n\n1e+08\n2.86G\n7.648\n32.892\n788.82M\n16.124\n4.847\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrane, Hazlitt, and Arrow (2023) provides further information about specific tasks, Navarro (2022) provides helpful examples of implementation, and Navarro, Keane, and Hazlitt (2022) provides an extensive set of materials. There is no settled consensus on whether parquet files should be used exclusively for dataset. But it is indisputable that the persistence of class alone provides a compelling reason for including them in addition to a CSV.\nWe will use parquet more in the remainder of this book."
  },
  {
    "objectID": "10-store_and_share.html#exercises",
    "href": "10-store_and_share.html#exercises",
    "title": "10  Store and share",
    "section": "10.7 Exercises",
    "text": "10.7 Exercises\n\nScales\n\n(Plan) Consider the following scenario: You work for a large news media company and focus on subscriber management. Over the course of a year most subscribers will never post a comment beneath a news article, but a few post an awful lot. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Carefully pick an appropriate distribution. Please include five tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe one possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nFollowing Wilkinson et al. (2016), please discuss the FAIR principles in the context of a dataset that you are familiar with (begin with a one-paragraph summary of the dataset, then write one paragraph per principle).\nPlease create a R package for a simulated dataset, push it to GitHub, and submit code to install the package (e.g. devtools::install_github(\"RohanAlexander/favcolordata\")).\nAccording to Gebru et al. (2021), a datasheet should document a dataset’s (please select all that apply):\n\ncomposition.\nrecommended uses.\nmotivation.\ncollection process.\n\nDiscuss, with the help of examples and references, whether a person’s name is PII (please write at least three paragraphs)?\nUsing md5() what is the hash of “Monica” (pick one)?\n\n243f63354f4c1cc25d50f6269b844369\n09084cc0cda34fd80bfa3cc0ae8fe3dc\n09084cc0cda34fd80bfa3cc0ae8fe3dc\n1b3840b0b70d91c17e70014c8537dbba\n\nPlease save the penguins data from from palmerpenguins as a CSV file and as a Parquet file. How big are they?\n\n12.5K; 6.03K\n14.9K; 6.03K\n14.9K; 5.02K\n12.5K; 5.02K\n\n\n\n\nTutorial\nPlease identify a dataset you consider interesting and important, that does not have a datasheet (Gebru et al. 2021). As a reminder, datasheets accompany datasets and document “motivation, composition, collection process, recommended uses,” among other aspects. Please put together a datasheet for this dataset. You are welcome to use the template in the starter folder.\nUse Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations to produce a draft. Following this, please pair with another student and exchange your written work. Update it based on their feedback, and be sure to acknowledge them by name in your paper. Submit a PDF.\n\n\nPaper\nAt about this point the Dysart Paper from Online Appendix D would be appropriate.\n\n\n\n\nAlexander, Rohan, and Paul Hodgetts. 2021. AustralianPoliticians: Provides Datasets About Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAnnas, George. 2003. “HIPAA Regulations: A New Era of Medical-Record Privacy?” New England Journal of Medicine 348 (15): 1486–90. https://doi.org/10.1056/NEJMlim035027.\n\n\nArel-Bundock, Vincent, Ryan Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T. D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” https://osf.io/bzj9y/.\n\n\nAthey, Susan, Guido Imbens, Jonas Metzger, and Evan Munro. 2021. “Using Wasserstein Generative Adversarial Networks for the Design of Monte Carlo Simulations.” Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2020.09.013.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing ‘Documentation Debt’ in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” arXiv. https://doi.org/10.48550/arXiv.2105.05241.\n\n\nBender, Emily, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerners-Lee, Timothy. 1989. “Information Management: A Proposal.” https://www.w3.org/History/1989/proposal.html.\n\n\nBiderman, Stella, Kieran Bicheno, and Leo Gao. 2022. “Datasheet for the Pile.” https://arxiv.org/abs/2201.07311.\n\n\nBorghi, John, and Ana Van Gulick. 2022. “Promoting Open Science Through Research Data Management.” Harvard Data Science Review 4 (3). https://doi.org/10.1162/99608f92.9497f68e.\n\n\nBowen, Claire McKay. 2022. Protecting Your Privacy in a Data-Driven World. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003122043.\n\n\nBuneman, Peter, Sanjeev Khanna, and Tan Wang-Chiew. 2001. “Why and Where: A Characterization of Data Provenance.” In Database Theory  ICDT 2001, 316–30. Springer Berlin Heidelberg. https://doi.org/10.1007/3-540-44503-x_20.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91.\n\n\nBush, Vannevar. 1945. “As We May Think.” The Atlantic Monthly, July. https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/.\n\n\nByrd, James Brian, Anna Greene, Deepashree Venkatesh Prasad, Xiaoqian Jiang, and Casey Greene. 2020. “Responsible, Practical Genomic Data Sharing That Accelerates Research.” Nature Reviews Genetics 21 (10): 615–29. https://doi.org/10.1038/s41576-020-0257-5.\n\n\nCarleton, Chris. 2021. “wccarleton/conflict-europe: Acce.” Zenodo. https://doi.org/10.5281/zenodo.4550688.\n\n\nCarleton, Chris, Dave Campbell, and Mark Collard. 2021. “A Reassessment of the Impact of Temperature Change on European Conflict During the Second Millennium CE Using a Bespoke Bayesian Time-Series Model.” Climatic Change 165 (1): 1–16. https://doi.org/10.1007/s10584-021-03022-2.\n\n\nChristensen, Garret, Allan Dafoe, Edward Miguel, Don Moore, and Andrew Rose. 2019. “A Study of the Impact of Data Sharing on Article Citations Using Journal Policies as a Natural Experiment.” PLOS ONE 14 (12): e0225883. https://doi.org/10.1371/journal.pone.0225883.\n\n\nChristensen, Garret, Jeremy Freese, and Edward Miguel. 2019. Transparent and Reproducible Social Science Research. California: University of California Press.\n\n\nCohen, Glenn, and Michelle Mello. 2018. “HIPAA and Protecting Health Information in the 21st Century.” JAMA 320 (3): 231. https://doi.org/10.1001/jama.2018.5630.\n\n\nCouncil of European Union. 2016. “General Data Protection Regulation 2016/679.” https://eur-lex.europa.eu/eli/reg/2016/679/oj.\n\n\nCrane, Nicola, Stephanie Hazlitt, and Apache Arrow. 2023. Apache Arrow R Cookbook. https://arrow.apache.org/cookbook/r/.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. “Calibrating Noise to Sensitivity in Private Data Analysis.” In Theory of Cryptography Conference, 265–84. Springer. https://doi.org/10.1007/11681878_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations of Differential Privacy.” Foundations and Trends in Theoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGeuenich, Michael, Jinyu Hou, Sunyun Lee, Shanza Ayub, Hartland Jackson, and Kieran Campbell. 2021a. “Automated Assignment of Cell Identity from Single-Cell Multiplexed Imaging and Proteomic Data.” Cell Systems 12 (12): 1173–86. https://doi.org/10.1016/j.cels.2021.08.012.\n\n\n———. 2021b. “Replication Materials: \"Automated Assignment of Cell Identity from Single-Cell Multiplexed Imaging and Proteomic Data\".” https://doi.org/10.5281/ZENODO.5156049.\n\n\nGreenberg, Bernard, Abdel-Latif Abul-Ela, Walt Simmons, and Daniel Horvitz. 1969. “The Unrelated Question Randomized Response Model: Theoretical Framework.” Journal of the American Statistical Association 64 (326): 520–39. https://doi.org/10.1080/01621459.1969.10500991.\n\n\nHart, Edmund, Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick Mulrooney, Timothée Poisot, Kara Woo, Naupaka Zimmerman, and Jeffrey Hollister. 2016. “Ten Simple Rules for Digital Data Storage.” PLOS Computational Biology 12 (10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, and Stephanie Hicks. 2021. “Reproducibility Standards for Machine Learning in the Life Sciences.” Nature Methods 18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2021. fs: Cross-Platform File System Operations Based on “libuv”. https://CRAN.R-project.org/package=fs.\n\n\nHotz, Joseph, Christopher Bollinger, Tatiana Komarova, Charles Manski, Robert Moffitt, Denis Nekipelov, Aaron Sojourner, and Bruce Spencer. 2022. “Balancing Data Privacy and Usability in the Federal Statistical System.” Proceedings of the National Academy of Sciences 119 (31): 1–10. https://doi.org/10.1073/pnas.2104906119.\n\n\nIzrailev, Sergei. 2022. tictoc: Functions for Timing R Scripts, as Well as Implementations of “Stack” and “List” Structures. https://CRAN.R-project.org/package=tictoc.\n\n\nKatz, Lindsay, and Rohan Alexander. 2023a. “Digitization of the Australian Parliamentary Debates, 1998-2022.” arXiv. https://doi.org/10.48550/arXiv.2304.04561.\n\n\n———. 2023b. “A new, comprehensive database of all proceedings of the Australian Parliamentary Debates (1998-2022).” Zenodo. https://doi.org/10.5281/zenodo.7799678.\n\n\nKenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, Tyler Simko, and Kosuke Imai. 2021. “The use of differential privacy for census data and its impact on redistricting: The case of the 2020 U.S. Census.” Science Advances 7 (41). https://doi.org/10.1126/sciadv.abk3283.\n\n\n———. 2022. “Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System.” Harvard Data Science Review. https://doi.org/10.48550/arXiv.2210.08383.\n\n\nKnuth, Donald. 1998. Art of Computer Programming, Volume 2: Seminumerical Algorithms. 2nd ed.\n\n\nKoenecke, Allison, and Hal Varian. 2020. “Synthetic Data Generation for Economists.” https://arxiv.org/abs/2011.01374.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research. https://datamgmtinedresearch.com/index.html.\n\n\nLima, Renato de, Oliver Phillips, Alvaro Duque, Sebastian Tello, Stuart Davies, Alexandre Adalardo de Oliveira, Sandra Muller, et al. 2022. “Making Forest Data Fair and Open.” Nature Ecology & Evolution 6 (April): 656–58. https://doi.org/10.1038/s41559-022-01738-7.\n\n\nLin, Herbert. 2014. “A Proposal to Reduce Government Overclassification of Information Related to National Security.” Journal of National Security Law and Policy 7: 443–63.\n\n\nMammoliti, Anthony, Petr Smirnov, Minoru Nakano, Zhaleh Safikhani, Christopher Eeles, Heewon Seo, Sisira Kadambat Nair, et al. 2021. “Orchestrating and Sharing Large Multimodal Data for Transparent and Reproducible Research.” Nature Communications 12 (1). https://doi.org/10.1038/s41467-021-25974-w.\n\n\nMcKinney, Wes. (2011) 2022. Python for Data Analysis. 3rd ed. https://wesmckinney.com/book/.\n\n\nMiceli, Milagros, Julian Posada, and Tianling Yang. 2022. “Studying up Machine Learning Data.” Proceedings of the ACM on Human-Computer Interaction 6 (January): 1–14. https://doi.org/10.1145/3492853.\n\n\nMichener, William. 2015. “Ten Simple Rules for Creating a Good Data Management Plan.” PLOS Computational Biology 11 (10): e1004525. https://doi.org/10.1371/journal.pcbi.1004525.\n\n\nNavarro, Danielle. 2022. “Binding Apache Arrow to R,” January. https://blog.djnavarro.net/posts/2022-01-18%5Fbinding-arrow-to-r/.\n\n\nNavarro, Danielle, Jonathan Keane, and Stephanie Hazlitt. 2022. “Larger-Than-Memory Data Workflows with Apache Arrow,” June. https://arrow-user2022.netlify.app.\n\n\nOberski, Daniel, and Frauke Kreuter. 2020. “Differential Privacy and Social Science: An Urgent Puzzle.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.63a22079.\n\n\nOoms, Jeroen. 2022. openssl: Toolkit for Encryption, Signatures and Certificates Based on OpenSSL. https://CRAN.R-project.org/package=openssl.\n\n\nPatki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The Synthetic Data Vault.” In 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), 399–410. https://doi.org/10.1109/DSAA.2016.49.\n\n\nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily Bender, Emily Denton, and Alex Hanna. 2021. “Data and Its (Dis)contents: A Survey of Dataset Development and Use in Machine Learning Research.” Patterns 2 (11): 100336. https://doi.org/10.1016/j.patter.2021.100336.\n\n\nPiller, Charles. 2022. “Blots on a Field?” Science 377 (6604): 358–63. https://doi.org/10.1126/science.ade0209.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nRoss, Casey. 2022. “How a Decades-Old Database Became a Hugely Profitable Dossier on the Health of 270 Million Americans.” Stat, February. https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/.\n\n\nRubinstein, Benjamin, and Francesco Alda. 2017. “Pain-Free Random Differential Privacy with Sensitivity Sampling.” In 34th International Conference on Machine Learning (ICML’2017).\n\n\nRuggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan Schroeder. 2019. “Differential Privacy and Census Data: Implications for Social and Economic Research.” AEA Papers and Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nSimonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of Fabricated Data Detected by Statistics Alone.” Psychological Science 24 (10): 1875–88. https://doi.org/10.1177/0956797613480366.\n\n\nSuriyakumar, Vinith, Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi. 2021. “Chasing Your Long Tails.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. https://doi.org/10.1145/3442188.3445934.\n\n\nTang, Jun, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, and Xiaofeng Wang. 2017. “Privacy Loss in Apple’s Implementation of Differential Privacy on MacOS 10.12.” arXiv. https://doi.org/10.48550/arXiv.1709.02753.\n\n\nTierney, Nicholas, and Karthik Ram. 2020. “A Realistic Guide to Making Data Available Alongside Code to Improve Reproducibility.” https://arxiv.org/abs/2002.11626.\n\n\n———. 2021. “Common-Sense Approaches to Sharing Tabular Data Alongside Publication.” Patterns 2 (12): 100368. https://doi.org/10.1016/j.patter.2021.100368.\n\n\nWicherts, Jelte, Marjan Bakker, and Dylan Molenaar. 2011. “Willingness to Share Research Data Is Related to the Strength of the Evidence and the Quality of Reporting of Statistical Results.” PLOS ONE 6 (11): e26828. https://doi.org/10.1371/journal.pone.0026828.\n\n\nWickham, Hadley. 2022. R Packages. 2nd ed. O’Reilly Media. https://r-pkgs.org.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jenny Bryan. 2022. devtools: Tools to Make Developing R Packages Easier. https://CRAN.R-project.org/package=devtools.\n\n\nWilkinson, Mark, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nZhang, Susan, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, et al. 2022. “OPT: Open Pre-Trained Transformer Language Models.” arXiv. https://doi.org/10.48550/arXiv.2205.01068.\n\n\nZook, Matthew, Solon Barocas, danah boyd, Kate Crawford, Emily Keller, Seeta Peña Gangadharan, Alyssa Goodman, et al. 2017. “Ten Simple Rules for Responsible Big Data Research.” PLOS Computational Biology 13 (3): e1005399. https://doi.org/10.1371/journal.pcbi.1005399."
  },
  {
    "objectID": "10-store_and_share.html#footnotes",
    "href": "10-store_and_share.html#footnotes",
    "title": "10  Store and share",
    "section": "",
    "text": "An interesting counterpoint is the recent use, by law enforcement, of DNA databases to find suspects. The suspect themselves might not be in the database, but the nature of DNA means that some related individuals can nonetheless still be identified.↩︎"
  },
  {
    "objectID": "11-eda.html#introduction",
    "href": "11-eda.html#introduction",
    "title": "11  Exploratory data analysis",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\n\nThe future of data analysis can involve great progress, the overcoming of real difficulties, and the provision of a great service to all fields of science and technology. Will it? That remains to us, to our willingness to take up the rocky road of real problems in preference to the smooth road of unreal assumptions, arbitrary criteria, and abstract results without real attachments. Who is for the challenge?\nTukey (1962, 64).\n\nExploratory data analysis is never finished. It is the active process of exploring and becoming familiar with our data. Like a farmer with their hands in the earth, we need to know every contour and aspect of our data. We need to know how it changes, what it shows, hides, and what are its limits. Exploratory data analysis (EDA) is the unstructured process of doing this.\nEDA is a means to an end. While it will inform the entire paper, especially the data section, it is not typically something that ends up in the final paper. The way to proceed is to make a separate Quarto document. Add code and brief notes on-the-go. Do not delete previous code, just add to it. By the end of it we will have created a useful notebook that captures your exploration of the dataset. This is a document that will guide the subsequent analysis and modeling.\nEDA draws on a variety of skills and there are a lot of options when conducting EDA (Staniak and Biecek 2019). Every tool should be considered. Look at the data and scroll through it. Make tables, plots, summary statistics, even some models. The key is to iterate, move quickly rather than perfectly, and come to a thorough understanding of the data. Interestingly, coming to thoroughly understand the data that we have often helps us understand what we do not have.\nWe are interested in the following process:\n\nUnderstand the distribution and properties of individual variables.\nUnderstand relationships between variables.\nUnderstand what is not there.\n\nThere is no one correct process or set of steps that are required to undertake and complete EDA. Instead, the relevant steps and tools depend on the data and question of interest. As such, in this chapter we will illustrate approaches to EDA through various examples of EDA including US state populations, subway delays in Toronto, and Airbnb listings in London. We also build on Chapter 6 and return to missing data."
  },
  {
    "objectID": "11-eda.html#united-states-population-and-income-data",
    "href": "11-eda.html#united-states-population-and-income-data",
    "title": "11  Exploratory data analysis",
    "section": "11.2 1975 United States population and income data",
    "text": "11.2 1975 United States population and income data\nAs a first example we consider US state populations as of 1975. This dataset is built into R with state.x77. Here is what the dataset looks like:\n\nus_populations &lt;-\n  state.x77 |&gt;\n  as_tibble() |&gt;\n  clean_names() |&gt;\n  mutate(state = rownames(state.x77)) |&gt;\n  select(state, population, income)\n\nus_populations\n\n# A tibble: 50 × 3\n   state       population income\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624\n 2 Alaska             365   6315\n 3 Arizona           2212   4530\n 4 Arkansas          2110   3378\n 5 California       21198   5114\n 6 Colorado          2541   4884\n 7 Connecticut       3100   5348\n 8 Delaware           579   4809\n 9 Florida           8277   4815\n10 Georgia           4931   4091\n# ℹ 40 more rows\n\n\nWe want to get a quick sense of the data. The first step is to have a look at the top and bottom of it with head() and tail(), then a random selection, and finally to focus on the variables and their class with glimpse(). The random selection is an important aspect, and when you use head() you should also quickly consider a random selection.\n\nus_populations |&gt;\n  head()\n\n# A tibble: 6 × 3\n  state      population income\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1 Alabama          3615   3624\n2 Alaska            365   6315\n3 Arizona          2212   4530\n4 Arkansas         2110   3378\n5 California      21198   5114\n6 Colorado         2541   4884\n\nus_populations |&gt;\n  tail()\n\n# A tibble: 6 × 3\n  state         population income\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n1 Vermont              472   3907\n2 Virginia            4981   4701\n3 Washington          3559   4864\n4 West Virginia       1799   3617\n5 Wisconsin           4589   4468\n6 Wyoming              376   4566\n\nus_populations |&gt;\n  slice_sample(n = 6)\n\n# A tibble: 6 × 3\n  state        population income\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1 Indiana            5313   4458\n2 Virginia           4981   4701\n3 South Dakota        681   4167\n4 Colorado           2541   4884\n5 Oregon             2284   4660\n6 Alabama            3615   3624\n\nus_populations |&gt;\n  glimpse()\n\nRows: 50\nColumns: 3\n$ state      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"…\n$ population &lt;dbl&gt; 3615, 365, 2212, 2110, 21198, 2541, 3100, 579, 8277, 4931, …\n$ income     &lt;dbl&gt; 3624, 6315, 4530, 3378, 5114, 4884, 5348, 4809, 4815, 4091,…\n\n\nWe are then interested in understanding key summary statistics, such as the minimum, median, and maximum values for numeric variables with summary() from base R and the number of observations.\n\nus_populations |&gt;\n  summary()\n\n    state             population        income    \n Length:50          Min.   :  365   Min.   :3098  \n Class :character   1st Qu.: 1080   1st Qu.:3993  \n Mode  :character   Median : 2838   Median :4519  \n                    Mean   : 4246   Mean   :4436  \n                    3rd Qu.: 4968   3rd Qu.:4814  \n                    Max.   :21198   Max.   :6315  \n\n\nFinally, it is especially important to understand the behavior of these key summary statistics at the limits. In particular, one approach is to randomly remove some observations and compare what happens to them. For instance, we can randomly create five datasets that differ on the basis of which observations were removed. We can then compare the summary statistics. If any of them are especially different, then we would want to look at the observations that were removed as they may contain observations with high influence.\n\nsample_means &lt;- tibble(seed = c(), mean = c(), states_ignored = c())\n\nfor (i in c(1:5)) {\n  set.seed(i)\n  dont_get &lt;- c(sample(x = state.name, size = 5))\n  sample_means &lt;-\n    sample_means |&gt;\n    rbind(tibble(\n      seed = i,\n      mean =\n        us_populations |&gt;\n          filter(!state %in% dont_get) |&gt;\n          summarise(mean = mean(population)) |&gt;\n          pull(),\n      states_ignored = str_c(dont_get, collapse = \", \")\n    ))\n}\n\nsample_means |&gt;\n  kable(\n    col.names = c(\"Seed\", \"Mean\", \"Ignored states\"),\n    digits = 0,\n    format.args = list(big.mark = \",\"),\n    booktabs = TRUE\n  )\n\n\n\nTable 11.1: Comparing the mean population when different states are randomly removed\n\n\n\n\n\n\n\nSeed\nMean\nIgnored states\n\n\n\n\n1\n4,469\nArkansas, Rhode Island, Alabama, North Dakota, Minnesota\n\n\n2\n4,027\nMassachusetts, Iowa, Colorado, West Virginia, New York\n\n\n3\n4,086\nCalifornia, Idaho, Rhode Island, Oklahoma, South Carolina\n\n\n4\n4,391\nHawaii, Arizona, Connecticut, Utah, New Jersey\n\n\n5\n4,340\nAlaska, Texas, Iowa, Hawaii, South Dakota\n\n\n\n\n\n\nIn the case of the populations of US states, we know that larger states, such as California and New York, will have an out sized effect on our estimate of the mean. Table 11.1 supports that, as we can see that when we use seeds 2 and 3, there is a lower mean."
  },
  {
    "objectID": "11-eda.html#missing-data",
    "href": "11-eda.html#missing-data",
    "title": "11  Exploratory data analysis",
    "section": "11.3 Missing data",
    "text": "11.3 Missing data\nWe have discussed missing data a lot throughout this book, especially in Chapter 6. Here we return to it because understanding missing data tends to be a substantial focus of EDA. When we find missing data—and there are always missing data of some sort or another—we want to establish what type of missingness we are dealing with. Focusing on known-missing observations, that is where there are observations that we can see are missing in the dataset, based on Gelman, Hill, and Vehtari (2020, 323) we consider three main categories of missing data:\n\nMissing Completely At Random;\nMissing at Random; and\nMissing Not At Random.\n\nWhen data are Missing Completely At Random (MCAR), observations are missing from the dataset independent of any other variables—whether in the dataset or not. As discussed in Chapter 6, when data are MCAR there are fewer concerns about summary statistics and inference, but data are rarely MCAR. Even if they were it would be difficult to be convinced of this. Nonetheless we can simulate an example. For instance we can remove the population data for three randomly selected states.\n\nset.seed(853)\n\nremove_random_states &lt;-\n  sample(x = state.name, size = 3, replace = FALSE)\n\nus_states_MCAR &lt;-\n  us_populations |&gt;\n  mutate(\n    population =\n      if_else(state %in% remove_random_states, NA_real_, population)\n  )\n\nsummary(us_states_MCAR)\n\n    state             population        income    \n Length:50          Min.   :  365   Min.   :3098  \n Class :character   1st Qu.: 1174   1st Qu.:3993  \n Mode  :character   Median : 2861   Median :4519  \n                    Mean   : 4308   Mean   :4436  \n                    3rd Qu.: 4956   3rd Qu.:4814  \n                    Max.   :21198   Max.   :6315  \n                    NA's   :3                     \n\n\nWhen observations are Missing at Random (MAR) they are missing from the dataset in a way that is related to other variables in the dataset. For instance, it may be that we are interested in understanding the effect of income and gender on political participation, and so we gather information on these three variables. But perhaps for some reason males are less likely to respond to a question about income.\nIn the case of the US states dataset, we can simulate a MAR dataset by making the three US states with the highest population not have an observation for income.\n\nhighest_income_states &lt;-\n  us_populations |&gt;\n  slice_max(income, n = 3) |&gt;\n  pull(state)\n\nus_states_MAR &lt;-\n  us_populations |&gt;\n  mutate(population =\n           if_else(state %in% highest_income_states, NA_real_, population)\n         )\n\nsummary(us_states_MAR)\n\n    state             population        income    \n Length:50          Min.   :  376   Min.   :3098  \n Class :character   1st Qu.: 1101   1st Qu.:3993  \n Mode  :character   Median : 2816   Median :4519  \n                    Mean   : 4356   Mean   :4436  \n                    3rd Qu.: 5147   3rd Qu.:4814  \n                    Max.   :21198   Max.   :6315  \n                    NA's   :3                     \n\n\nFinally when observations are Missing Not At Random (MNAR) they are missing from the dataset in a way that is related to either unobserved variables, or the missing variable itself. For instance, it may be that respondents with a higher income, or that respondents with higher education (a variable that we did not collect), are less likely to fill in their income.\nIn the case of the US states dataset, we can simulate a MNAR dataset by making the three US states with the highest population not have an observation for population.\n\nhighest_population_states &lt;-\n  us_populations |&gt;\n  slice_max(population, n = 3) |&gt;\n  pull(state)\n\nus_states_MNAR &lt;-\n  us_populations |&gt;\n  mutate(population =\n           if_else(state %in% highest_population_states,\n                   NA_real_,\n                   population))\n\nus_states_MNAR\n\n# A tibble: 50 × 3\n   state       population income\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624\n 2 Alaska             365   6315\n 3 Arizona           2212   4530\n 4 Arkansas          2110   3378\n 5 California          NA   5114\n 6 Colorado          2541   4884\n 7 Connecticut       3100   5348\n 8 Delaware           579   4809\n 9 Florida           8277   4815\n10 Georgia           4931   4091\n# ℹ 40 more rows\n\n\nThe best approach will be bespoke to the circumstances, but in general we want to use simulation to better understand the implications of our choices. From a data side we can choose to remove observations that are missing or input a value. (There are also options on the model side, but those are beyond the scope of this book.) These approaches have their place, but need to be used with humility and well communicated. The use of simulation is critical.\nWe can return to our US states dataset, generate some missing data, and consider a few common approaches for dealing with missing data, and compare the implied values for each state, and the overall US mean population. We consider the following options:\n\nDrop observations with missing data.\nImpute the mean of observations without missing data.\nUse multiple imputation.\n\nTo drop the observations with missing data, we can use mean(). By default it will exclude observations with missing values in its calculation. To impute the mean, we construct a second dataset with the observations with missing data removed. We then compute the mean of the population column, and impute that into the missing values in the original dataset. Multiple imputation involves creating many potential datasets, conducting inference, and then bringing them together potentially though averaging (Gelman and Hill 2007, 542). We can implement multiple imputation with mice() from mice.\n\nmultiple_imputation &lt;-\n  mice(\n    us_states_MCAR,\n    print = FALSE\n  )\n\nmice_estimates &lt;-\n  complete(multiple_imputation) |&gt;\n  as_tibble()\n\n\n\n\n\nTable 11.2: Comparing the imputed values of population for three US states and the overall mean population\n\n\nObservation\nDrop missing\nInput mean\nMultiple imputation\nActual\n\n\n\n\nFlorida\nNA\n4,308\n11,197\n8,277\n\n\nMontana\nNA\n4,308\n4,589\n746\n\n\nNew Hampshire\nNA\n4,308\n813\n812\n\n\nOverall\n4,308\n4,308\n4,382\n4,246\n\n\n\n\n\n\nTable 11.2 makes it clear that none of these approaches should be naively imposed. For instance, Florida’s population should be 8,277. Imputing the mean across all the states would result in an estimate of 4,308, and multiple imputation results in an estimate of 5,814, both of which are too low. If imputation is the answer, it may be better to look for a different question. It is worth pointing out that it was developed for specific circumstances of limiting public disclosure of private information (Horton and Lipsitz 2001).\nNothing can make up for missing data (Manski 2022). The conditions under which it makes sense to impute the mean or the prediction based on multiple imputation are not common, and even more rare is our ability to verify them. What to do depends on the circumstances and purpose of the analysis. Simulating the removal of observations that we have and then implementing various options can help us better understand the trade-offs we face. Whatever choice is made—and there is rarely a clear-cut solution—try to document and communicate what was done, and explore the effect of different choices on subsequent estimates. We recommend proceeding by simulating different scenarios that remove some of the data that we have, and evaluating how the approaches differ.\nFinally, more prosaically, but just as importantly, sometimes missing data is encoded in the variable with particular values. For instance, while R has the option of “NA”, sometimes numerical data is entered as “-99” or alternatively as a very large integer such as “9999999”, if it is missing. In the case of the Nationscape survey dataset introduced in Chapter 8, there are three types of known missing data:\n\n“888”: “Asked in this wave, but not asked of this respondent”\n“999”: “Not sure, don’t know”\n“.”: Respondent skipped\n\nIt is always worth looking explicitly for values that seem like they do not belong and investigating them. Graphs and tables are especially useful for this purpose."
  },
  {
    "objectID": "11-eda.html#ttc-subway-delays",
    "href": "11-eda.html#ttc-subway-delays",
    "title": "11  Exploratory data analysis",
    "section": "11.4 TTC subway delays",
    "text": "11.4 TTC subway delays\nAs a second, and more involved, example of EDA we use opendatatoronto, introduced in Chapter 2, and the tidyverse to obtain and explore data about the Toronto subway system. We want to get a sense of the delays that have occurred.\nTo begin, we download the data on Toronto Transit Commission (TTC) subway delays in 2021. The data are available as an Excel file with a separate sheet for each month. We are interested in 2021 so we filter to just that year then download it using get_resource() from opendatatoronto and bring the months together with bind_rows().\n\nall_2021_ttc_data &lt;-\n  list_package_resources(\"996cfe8d-fb35-40ce-b569-698d51fc683b\") |&gt;\n  filter(name == \"ttc-subway-delay-data-2021\") |&gt;\n  get_resource() |&gt;\n  bind_rows() |&gt;\n  clean_names()\n\nwrite_csv(all_2021_ttc_data, \"all_2021_ttc_data.csv\")\n\nall_2021_ttc_data\n\n\n\n# A tibble: 16,370 × 10\n   date                time   day    station code  min_delay min_gap bound line \n   &lt;dttm&gt;              &lt;time&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2021-01-01 00:00:00 00:33  Friday BLOOR … MUPAA         0       0 N     YU   \n 2 2021-01-01 00:00:00 00:39  Friday SHERBO… EUCO          5       9 E     BD   \n 3 2021-01-01 00:00:00 01:07  Friday KENNED… EUCD          5       9 E     BD   \n 4 2021-01-01 00:00:00 01:41  Friday ST CLA… MUIS          0       0 &lt;NA&gt;  YU   \n 5 2021-01-01 00:00:00 02:04  Friday SHEPPA… MUIS          0       0 &lt;NA&gt;  YU   \n 6 2021-01-01 00:00:00 02:35  Friday KENNED… MUIS          0       0 &lt;NA&gt;  BD   \n 7 2021-01-01 00:00:00 02:39  Friday VAUGHA… MUIS          0       0 &lt;NA&gt;  YU   \n 8 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  YU   \n 9 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n10 2021-01-01 00:00:00 06:00  Friday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n# ℹ 16,360 more rows\n# ℹ 1 more variable: vehicle &lt;dbl&gt;\n\n\nThe dataset has a variety of columns, and we can find out more about each of them by downloading the codebook. The reason for each delay is coded, and so we can also download the explanations. One variable of interest appears is “min_delay”, which gives the extent of the delay in minutes.\n\n# Data codebook\ndelay_codebook &lt;-\n  list_package_resources(\n    \"996cfe8d-fb35-40ce-b569-698d51fc683b\"\n  ) |&gt;\n  filter(name == \"ttc-subway-delay-data-readme\") |&gt;\n  get_resource() |&gt;\n  clean_names()\n\nwrite_csv(delay_codebook, \"delay_codebook.csv\")\n\n# Explanation for delay codes\ndelay_codes &lt;-\n  list_package_resources(\n    \"996cfe8d-fb35-40ce-b569-698d51fc683b\"\n  ) |&gt;\n  filter(name == \"ttc-subway-delay-codes\") |&gt;\n  get_resource() |&gt;\n  clean_names()\n\nwrite_csv(delay_codes, \"delay_codes.csv\")\n\nThere is no one way to explore a dataset while conducting EDA, but we are usually especially interested in:\n\nWhat should the variables look like? For instance, what is their class, what are the values, and what does the distribution of these look like?\nWhat aspects are surprising, both in terms of data that are there that we do not expect, such as outliers, but also in terms of data that we may expect but do not have, such as missing data.\nDeveloping a goal for our analysis. For instance, in this case, it might be understanding the factors such as stations and the time of day that are associated with delays. While we would not answer these questions formally here, we might explore what an answer could look like.\n\nIt is important to document all aspects as we go through and note anything surprising. We are looking to create a record of the steps and assumptions that we made as we were going because these will be important when we come to modeling. In the natural sciences, a research notebook of this type can even be a legal document (Ryan 2015).\n\n11.4.1 Distribution and properties of individual variables\nWe should check that the variables are what they say they are. If they are not, then we need to work out what to do. For instance, should we change them, or possibly even remove them? It is also important to ensure that the class of the variables is as we expect. For instance, variables that should be a factor are a factor and those that should be a character are a character. And that we do not accidentally have, say, factors as numbers, or vice versa. One way to do this is to use unique(), and another is to use table(). There is no universal answer to which variables should be of certain classes, because the answer depends on the context.\n\nunique(all_2021_ttc_data$day)\n\n[1] \"Friday\"    \"Saturday\"  \"Sunday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\"\n[7] \"Thursday\" \n\nunique(all_2021_ttc_data$line)\n\n [1] \"YU\"                     \"BD\"                     \"SHP\"                   \n [4] \"SRT\"                    \"YU/BD\"                  NA                      \n [7] \"YONGE/UNIVERSITY/BLOOR\" \"YU / BD\"                \"YUS\"                   \n[10] \"999\"                    \"SHEP\"                   \"36 FINCH WEST\"         \n[13] \"YUS & BD\"               \"YU & BD LINES\"          \"35 JANE\"               \n[16] \"52\"                     \"41 KEELE\"               \"YUS/BD\"                \n\ntable(all_2021_ttc_data$day)\n\n\n   Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday \n     2600      2434      2073      1942      2425      2481      2415 \n\ntable(all_2021_ttc_data$line)\n\n\n               35 JANE          36 FINCH WEST               41 KEELE \n                     1                      1                      1 \n                    52                    999                     BD \n                     1                      1                   5734 \n                  SHEP                    SHP                    SRT \n                     1                    657                    656 \nYONGE/UNIVERSITY/BLOOR                     YU                YU / BD \n                     1                   8880                     17 \n         YU & BD LINES                  YU/BD                    YUS \n                     1                    346                     18 \n              YUS & BD                 YUS/BD \n                     1                      1 \n\n\nWe have likely issues in terms of the subway lines. Some of them have a clear fix, but not all. One option would be to drop them, but we would need to think about whether these errors might be correlated with something that is of interest. If they were then we may be dropping important information. There is usually no one right answer, because it will usually depend on what we are using the data for. We would note the issue, as we continued with EDA and then decide later about what to do. For now, we will remove all the lines that are not the ones that we know to be correct based on the codebook.\n\ndelay_codebook |&gt;\n  filter(field_name == \"Line\")\n\n# A tibble: 1 × 3\n  field_name description                               example\n  &lt;chr&gt;      &lt;chr&gt;                                     &lt;chr&gt;  \n1 Line       TTC subway line i.e. YU, BD, SHP, and SRT YU     \n\nall_2021_ttc_data_filtered_lines &lt;-\n  all_2021_ttc_data |&gt;\n  filter(line %in% c(\"YU\", \"BD\", \"SHP\", \"SRT\"))\n\nEntire careers are spent understanding missing data, and the presence, or lack, of missing values can haunt an analysis. To get started we could look at known-unknowns, which are the NAs for each variable. For instance, we could create counts by variable.\nIn this case we have many missing values in “bound” and two in “line”. For these known-unknowns, as discussed in Chapter 6, we are interested in whether they are missing at random. We want to, ideally, show that data happened to just drop out. But this is unlikely, and so we are usually trying to look at what is systematic about how the data are missing.\nSometimes data happen to be duplicated. If we did not notice this, then our analysis would be wrong in ways that we would not be able to consistently expect. There are a variety of ways to look for duplicated rows, but get_dupes() from janitor is especially useful.\n\nget_dupes(all_2021_ttc_data_filtered_lines)\n\n# A tibble: 36 × 11\n   date                time   day    station code  min_delay min_gap bound line \n   &lt;dttm&gt;              &lt;time&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2021-09-13 00:00:00 06:00  Monday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n 2 2021-09-13 00:00:00 06:00  Monday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n 3 2021-09-13 00:00:00 06:00  Monday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n 4 2021-09-13 00:00:00 06:00  Monday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n 5 2021-09-13 00:00:00 06:00  Monday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n 6 2021-09-13 00:00:00 06:00  Monday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n 7 2021-03-31 00:00:00 05:45  Wedne… DUNDAS… MUNCA         0       0 &lt;NA&gt;  BD   \n 8 2021-03-31 00:00:00 05:45  Wedne… DUNDAS… MUNCA         0       0 &lt;NA&gt;  BD   \n 9 2021-06-08 00:00:00 14:40  Tuesd… VAUGHA… MUNOA         3       6 S     YU   \n10 2021-06-08 00:00:00 14:40  Tuesd… VAUGHA… MUNOA         3       6 S     YU   \n# ℹ 26 more rows\n# ℹ 2 more variables: vehicle &lt;dbl&gt;, dupe_count &lt;int&gt;\n\n\nThis dataset has many duplicates. We are interested in whether there is something systematic going on. Remembering that during EDA we are trying to quickly come to terms with a dataset, one way forward is to flag this as an issue to come back to and explore later, and to just remove duplicates for now using distinct().\n\nall_2021_ttc_data_no_dupes &lt;-\n  all_2021_ttc_data_filtered_lines |&gt;\n  distinct()\n\nThe station names have many errors.\n\nall_2021_ttc_data_no_dupes |&gt;\n  count(station) |&gt;\n  filter(str_detect(station, \"WEST\"))\n\n# A tibble: 17 × 2\n   station                    n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 DUNDAS WEST STATION      198\n 2 EGLINTON WEST STATION    142\n 3 FINCH WEST STATION       126\n 4 FINCH WEST TO LAWRENCE     3\n 5 FINCH WEST TO WILSON       1\n 6 LAWRENCE WEST CENTRE       1\n 7 LAWRENCE WEST STATION    127\n 8 LAWRENCE WEST TO EGLIN     1\n 9 SHEPPARD WEST - WILSON     1\n10 SHEPPARD WEST STATION    210\n11 SHEPPARD WEST TO LAWRE     3\n12 SHEPPARD WEST TO ST CL     2\n13 SHEPPARD WEST TO WILSO     7\n14 ST CLAIR WEST STATION    205\n15 ST CLAIR WEST TO ST AN     1\n16 ST. CLAIR WEST TO KING     1\n17 ST.CLAIR WEST TO ST.A      1\n\n\nWe could try to quickly bring a little order to the chaos by just taking just the first word or first few words, accounting for names like “ST. CLAIR” and “ST. PATRICK” by checking if the name starts with “ST”, as well as distinguishing between stations like “DUNDAS” and “DUNDAS WEST” by checking if the name contains “WEST”. Again, we are just trying to get a sense of the data, not necessarily make binding decisions here. We use word() from stringr to extract specific words from the station names.\n\nall_2021_ttc_data_no_dupes &lt;-\n  all_2021_ttc_data_no_dupes |&gt;\n  mutate(\n    station_clean =\n      case_when(\n        str_starts(station, \"ST\") &\n          str_detect(station, \"WEST\") ~ word(station, 1, 3),\n        str_starts(station, \"ST\") ~ word(station, 1, 2),\n        str_detect(station, \"WEST\") ~ word(station, 1, 2),\n        TRUE ~ word(station, 1)\n      )\n  )\n\nall_2021_ttc_data_no_dupes\n\n# A tibble: 15,908 × 11\n   date                time   day    station code  min_delay min_gap bound line \n   &lt;dttm&gt;              &lt;time&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 2021-01-01 00:00:00 00:33  Friday BLOOR … MUPAA         0       0 N     YU   \n 2 2021-01-01 00:00:00 00:39  Friday SHERBO… EUCO          5       9 E     BD   \n 3 2021-01-01 00:00:00 01:07  Friday KENNED… EUCD          5       9 E     BD   \n 4 2021-01-01 00:00:00 01:41  Friday ST CLA… MUIS          0       0 &lt;NA&gt;  YU   \n 5 2021-01-01 00:00:00 02:04  Friday SHEPPA… MUIS          0       0 &lt;NA&gt;  YU   \n 6 2021-01-01 00:00:00 02:35  Friday KENNED… MUIS          0       0 &lt;NA&gt;  BD   \n 7 2021-01-01 00:00:00 02:39  Friday VAUGHA… MUIS          0       0 &lt;NA&gt;  YU   \n 8 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  YU   \n 9 2021-01-01 00:00:00 06:00  Friday TORONT… MUO           0       0 &lt;NA&gt;  SHP  \n10 2021-01-01 00:00:00 06:00  Friday TORONT… MRO           0       0 &lt;NA&gt;  SRT  \n# ℹ 15,898 more rows\n# ℹ 2 more variables: vehicle &lt;dbl&gt;, station_clean &lt;chr&gt;\n\n\nWe need to see the data in its original state to understand it, and we often use bar charts, scatterplots, line plots, and histograms for this. During EDA we are not so concerned with whether the graph looks nice, but are instead trying to acquire a sense of the data as quickly as possible. We can start by looking at the distribution of “min_delay”, which is one outcome of interest.\n\nall_2021_ttc_data_no_dupes |&gt;\n  ggplot(aes(x = min_delay)) +\n  geom_histogram(bins = 30)\n\nall_2021_ttc_data_no_dupes |&gt;\n  ggplot(aes(x = min_delay)) +\n  geom_histogram(bins = 30) +\n  scale_x_log10()\n\n\n\n\n\n\n\n(a) Distribution of delay\n\n\n\n\n\n\n\n(b) With a log scale\n\n\n\n\nFigure 11.1: Distribution of delay, in minutes\n\n\n\nThe largely empty graph in Figure 11.1 (a) suggests the presence of outliers. There are a variety of ways to try to understand what could be going on, but one quick way to proceed is to use logarithms, remembering that we would expect values of zero to drop away (Figure 11.1 (b)).\nThis initial exploration suggests there are a small number of large delays that we might like to explore further. We will join this dataset with “delay_codes” to understand what is going on.\n\nfix_organization_of_codes &lt;-\n  rbind(\n    delay_codes |&gt;\n      select(sub_rmenu_code, code_description_3) |&gt;\n      mutate(type = \"sub\") |&gt;\n      rename(\n        code = sub_rmenu_code,\n        code_desc = code_description_3\n      ),\n    delay_codes |&gt;\n      select(srt_rmenu_code, code_description_7) |&gt;\n      mutate(type = \"srt\") |&gt;\n      rename(\n        code = srt_rmenu_code,\n        code_desc = code_description_7\n      )\n  )\n\nall_2021_ttc_data_no_dupes_with_explanation &lt;-\n  all_2021_ttc_data_no_dupes |&gt;\n  mutate(type = if_else(line == \"SRT\", \"srt\", \"sub\")) |&gt;\n  left_join(\n    fix_organization_of_codes,\n    by = c(\"type\", \"code\")\n  )\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  select(station_clean, code, min_delay, code_desc) |&gt;\n  arrange(-min_delay)\n\n# A tibble: 15,908 × 4\n   station_clean code  min_delay code_desc                                  \n   &lt;chr&gt;         &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;                                      \n 1 MUSEUM        PUTTP       348 Traction Power Rail Related                \n 2 EGLINTON      PUSTC       343 Signals - Track Circuit Problems           \n 3 WOODBINE      MUO         312 Miscellaneous Other                        \n 4 MCCOWAN       PRSL        275 Loop Related Failures                      \n 5 SHEPPARD WEST PUTWZ       255 Work Zone Problems - Track                 \n 6 ISLINGTON     MUPR1       207 Priority One - Train in Contact With Person\n 7 SHEPPARD WEST MUPR1       191 Priority One - Train in Contact With Person\n 8 ROYAL         SUAP        182 Assault / Patron Involved                  \n 9 ROYAL         MUPR1       180 Priority One - Train in Contact With Person\n10 SHEPPARD      MUPR1       171 Priority One - Train in Contact With Person\n# ℹ 15,898 more rows\n\n\nFrom this we can see that the 348 minute delay was due to “Traction Power Rail Related”, the 343 minute delay was due to “Signals - Track Circuit Problems”, and so on.\nAnother thing that we are looking for is various groupings of the data, especially where sub-groups may end up with only a small number of observations in them. This is because our analysis could be especially influenced by them. One quick way to do this is to group the data by a variable that is of interest, for instance, “line”, using color.\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(\n      x = min_delay,\n      y = ..density..,\n      fill = line\n    ),\n    position = \"dodge\",\n    bins = 10\n  ) +\n  scale_x_log10()\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(x = min_delay, fill = line),\n    position = \"dodge\",\n    bins = 10\n  ) +\n  scale_x_log10()\n\n\n\n\n\n\n\n(a) Density\n\n\n\n\n\n\n\n(b) Frequency\n\n\n\n\nFigure 11.2: Distribution of delay, in minutes\n\n\n\nFigure 11.2 (a) uses density so that we can look at the distributions more comparably, but we should also be aware of differences in frequency (Figure 11.2 (b)). In this case, we see that “SHP” and “SRT” have much smaller counts.\nTo group by another variable, we can add facets (Figure 11.3).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(x = min_delay, fill = line),\n    position = \"dodge\",\n    bins = 10\n  ) +\n  scale_x_log10() +\n  facet_wrap(vars(day)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 11.3: Frequency of the distribution of delay, in minutes, by day\n\n\n\n\nWe can also plot the top five stations by mean delay, faceted by line (Figure 11.4). This raises something that we would need to follow up on, which is what is “ZONE” in “YU”?\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  summarise(mean_delay = mean(min_delay), n_obs = n(),\n            .by = c(line, station_clean)) |&gt;\n  filter(n_obs &gt; 1) |&gt;\n  arrange(line, -mean_delay) |&gt;\n  slice(1:5, .by = line) |&gt;\n  ggplot(aes(station_clean, mean_delay)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(vars(line), scales = \"free_y\")\n\n\n\n\nFigure 11.4: Top five stations, by mean delay and line\n\n\n\n\nAs discussed in Chapter 9, dates are often difficult to work with because they are so prone to having issues. For this reason, it is especially important to consider them during EDA. Let us create a graph by week, to see if there is any seasonality over the course of a year. When using dates, lubridate is especially useful. For instance, we can look at the average delay, of those that were delayed, by week, using week() to construct the weeks (Figure 11.5).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  filter(min_delay &gt; 0) |&gt;\n  mutate(week = week(date)) |&gt;\n  summarise(mean_delay = mean(min_delay),\n            .by = c(week, line)) |&gt;\n  ggplot(aes(week, mean_delay, color = line)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(line), scales = \"free_y\")\n\n\n\n\nFigure 11.5: Average delay, in minutes, by week, for the Toronto subway\n\n\n\n\nNow let us look at the proportion of delays that were greater than ten minutes (Figure 11.6).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  mutate(week = week(date)) |&gt;\n  summarise(prop_delay = sum(min_delay &gt; 10) / n(),\n            .by = c(week, line)) |&gt;\n  ggplot(aes(week, prop_delay, color = line)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(line), scales = \"free_y\")\n\n\n\n\nFigure 11.6: Delays longer than ten minutes, by week, for the Toronto subway\n\n\n\n\nThese figures, tables, and analysis may not have a place in a final paper. Instead, they allow us to become comfortable with the data. We note aspects about each that stand out, as well as the warnings and any implications or aspects to return to.\n\n\n11.4.2 Relationships between variables\nWe are also interested in looking at the relationship between two variables. We will draw heavily on graphs for this. Appropriate types, for different circumstances, were discussed in Chapter 5. Scatter plots are especially useful for continuous variables, and are a good precursor to modeling. For instance, we may be interested in the relationship between the delay and the gap, which is the number of minutes between trains (Figure 11.7).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  ggplot(aes(x = min_delay, y = min_gap, alpha = 0.1)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\nFigure 11.7: Relationship between delay and gap for the Toronto subway in 2021\n\n\n\n\nThe relationship between categorical variables takes more work, but we could also, for instance, look at the top five reasons for delay by station. We may be interested in whether they differ, and how any difference could be modelled (Figure 11.8).\n\nall_2021_ttc_data_no_dupes_with_explanation |&gt;\n  summarise(mean_delay = mean(min_delay),\n            .by = c(line, code_desc)) |&gt;\n  arrange(-mean_delay) |&gt;\n  slice(1:5) |&gt;\n  ggplot(aes(x = code_desc, y = mean_delay)) +\n  geom_col() +\n  facet_wrap(vars(line), scales = \"free_y\", nrow = 4) +\n  coord_flip()\n\n\n\n\nFigure 11.8: Relationship between categorical variables for the Toronto subway in 2021"
  },
  {
    "objectID": "11-eda.html#airbnb-listings-in-london-england",
    "href": "11-eda.html#airbnb-listings-in-london-england",
    "title": "11  Exploratory data analysis",
    "section": "11.5 Airbnb listings in London, England",
    "text": "11.5 Airbnb listings in London, England\nIn this case study we look at Airbnb listings in London, England, as at 14 March 2023. The dataset is from Inside Airbnb (Cox 2021) and we will read it from their website, and then save a local copy. We can give read_csv() a link to where the dataset is and it will download it. This helps with reproducibility because the source is clear. But as that link could change at any time, longer-term reproducibility, as well as wanting to minimize the effect on the Inside Airbnb servers, suggests that we should also save a local copy of the data and then use that.\nTo get the dataset that we need, go to Inside Airbnb \\(\\rightarrow\\) “Data” \\(\\rightarrow\\) “Get the Data”, then scroll down to London. We are interested in the “listings dataset”, and we right click to get the URL that we need (Figure 11.9). Inside Airbnb update the data that they make available, and so the particular dataset that is available will change over time.\n\n\n\nFigure 11.9: Obtaining the Airbnb data from Inside Airbnb\n\n\nAs the original dataset is not ours, we should not make that public without first getting written permission. For instance, we may want to add it to our inputs folder, but use a “.gitignore” entry, covered in Chapter 3, to ensure that we do not push it to GitHub. The “guess_max” option in read_csv() helps us avoid having to specify the column types. Usually read_csv() takes a best guess at the column types based on the first few rows. But sometimes those first ones are misleading and so “guess_max” forces it to look at a larger number of rows to try to work out what is going on. Paste the URL that we copied from Inside Airbnb into the URL part. And once it is downloaded, save a local copy.\n\nurl &lt;-\n  paste0(\n    \"http://data.insideairbnb.com/united-kingdom/england/\",\n    \"london/2023-03-14/data/listings.csv.gz\"\n  )\n\nairbnb_data &lt;-\n  read_csv(\n    file = url,\n    guess_max = 20000\n  )\n\nwrite_csv(airbnb_data, \"airbnb_data.csv\")\n\nairbnb_data\n\nWe should refer to this local copy of our data when we run our scripts to explore the data, rather than asking the Inside Airbnb servers for the data each time. It might be worth even commenting out this call to their servers to ensure that we do not accidentally stress their service.\nAgain, add this filename—“airbnb_data.csv”—to the “.gitignore” file so that it is not pushed to GitHub. The size of the dataset will create complications that we would like to avoid.\nWhile we need to archive this CSV because that is the original, unedited data, at more than 100MB it is a little unwieldy. For exploratory purposes we will create a parquet file with selected variables (we do this in an iterative way, using names(airbnb_data) to work out the variable names).\n\nairbnb_data_selected &lt;-\n  airbnb_data |&gt;\n  select(\n    host_id,\n    host_response_time,\n    host_is_superhost,\n    host_total_listings_count,\n    neighbourhood_cleansed,\n    bathrooms,\n    bedrooms,\n    price,\n    number_of_reviews,\n    review_scores_rating,\n    review_scores_accuracy,\n    review_scores_value\n  )\n\nwrite_parquet(\n  x = airbnb_data_selected, \n  sink = \n    \"2023-03-14-london-airbnblistings-select_variables.parquet\"\n  )\n\nrm(airbnb_data)\n\n\n11.5.1 Distribution and properties of individual variables\nFirst we might be interested in price. It is a character at the moment and so we need to convert it to a numeric. This is a common problem and we need to be a little careful that it does not all just convert to NAs. If we just force the price variable to be a numeric then it will go to NA because there are a lot of characters where it is unclear what the numeric equivalent is, such as “$”. We need to remove those characters first.\n\nairbnb_data_selected$price |&gt;\n  head()\n\n[1] \"$100.00\" \"$65.00\"  \"$132.00\" \"$100.00\" \"$120.00\" \"$43.00\" \n\nairbnb_data_selected$price |&gt;\n  str_split(\"\") |&gt;\n  unlist() |&gt;\n  unique()\n\n [1] \"$\" \"1\" \"0\" \".\" \"6\" \"5\" \"3\" \"2\" \"4\" \"9\" \"8\" \"7\" \",\"\n\nairbnb_data_selected |&gt;\n  select(price) |&gt;\n  filter(str_detect(price, \",\"))\n\n# A tibble: 1,629 × 1\n   price    \n   &lt;chr&gt;    \n 1 $3,070.00\n 2 $1,570.00\n 3 $1,480.00\n 4 $1,000.00\n 5 $1,100.00\n 6 $1,433.00\n 7 $1,800.00\n 8 $1,000.00\n 9 $1,000.00\n10 $1,000.00\n# ℹ 1,619 more rows\n\nairbnb_data_selected &lt;-\n  airbnb_data_selected |&gt;\n  mutate(\n    price = str_remove_all(price, \"[\\\\$,]\"),\n    price = as.integer(price)\n  )\n\nNow we can look at the distribution of prices (Figure 11.10 (a)). There are outliers, so again we might like to consider it on the log scale (Figure 11.10 (b)).\n\nairbnb_data_selected |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  )\n\nairbnb_data_selected |&gt;\n  filter(price &gt; 1000) |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  ) +\n  scale_y_log10()\n\n\n\n\n\n\n\n(a) Distribution of prices\n\n\n\n\n\n\n\n(b) Using the log scale for prices more than $1,000\n\n\n\n\nFigure 11.10: Distribution of prices of London Airbnb rentals in March 2023\n\n\n\nIf we focus on prices that are less than $1,000, then we see that most properties have a nightly price less than $250 (Figure 11.11 (a)). In the same way that we saw some bunching in ages in Chapter 9, it looks like there is some bunching of prices here. It might be that this is happening around numbers ending in zero or nine. Let us just zoom in on prices between $90 and $210, out of interest, but change the bins to be smaller (Figure 11.11 (b)).\n\nairbnb_data_selected |&gt;\n  filter(price &lt; 1000) |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  )\n\nairbnb_data_selected |&gt;\n  filter(price &gt; 90) |&gt;\n  filter(price &lt; 210) |&gt;\n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Number of properties\"\n  )\n\n\n\n\n\n\n\n(a) Prices less than $1,000 suggest some bunching\n\n\n\n\n\n\n\n(b) Prices between $90 and $210 illustrate the bunching more clearly\n\n\n\n\nFigure 11.11: Distribution of prices for Airbnb listings in London in March 2023\n\n\n\nFor now, we will just remove all prices that are more than $999.\n\nairbnb_data_less_1000 &lt;-\n  airbnb_data_selected |&gt;\n  filter(price &lt; 1000)\n\nSuperhosts are especially experienced Airbnb hosts, and we might be interested to learn more about them. For instance, a host either is or is not a superhost, and so we would not expect any NAs. But we can see that there are NAs. It might be that the host removed a listing or similar, but this is something that we would need to look further into.\n\nairbnb_data_less_1000 |&gt;\n  filter(is.na(host_is_superhost))\n\n# A tibble: 13 × 12\n     host_id host_response_time host_is_superhost host_total_listings_count\n       &lt;dbl&gt; &lt;chr&gt;              &lt;lgl&gt;                                 &lt;dbl&gt;\n 1 317054510 within an hour     NA                                        5\n 2 316090383 within an hour     NA                                        6\n 3 315016947 within an hour     NA                                        2\n 4 374424554 within an hour     NA                                        2\n 5  97896300 N/A                NA                                       10\n 6 316083765 within an hour     NA                                        7\n 7 310628674 N/A                NA                                        5\n 8 179762278 N/A                NA                                       10\n 9 315037299 N/A                NA                                        1\n10 316090018 within an hour     NA                                        6\n11 375515965 within an hour     NA                                        2\n12 341372520 N/A                NA                                        7\n13 180634347 within an hour     NA                                        5\n# ℹ 8 more variables: neighbourhood_cleansed &lt;chr&gt;, bathrooms &lt;lgl&gt;,\n#   bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;,\n#   review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;\n\n\nWe will also want to create a binary variable from this. It is true/false at the moment, which is fine for the modeling, but there are a handful of situations where it will be easier if we have a 0/1. And for now we will just remove anyone with a NA for whether they are a superhost.\n\nairbnb_data_no_superhost_nas &lt;-\n  airbnb_data_less_1000 |&gt;\n  filter(!is.na(host_is_superhost)) |&gt;\n  mutate(\n    host_is_superhost_binary =\n      as.numeric(host_is_superhost)\n  )\n\nOn Airbnb, guests can give one to five star ratings across a variety of different aspects, including cleanliness, accuracy, value, and others. But when we look at the reviews in our dataset, it is clear that it is effectively a binary, and almost entirely the case that either the rating is five stars or not (Figure 11.12).\n\nairbnb_data_no_superhost_nas |&gt;\n  ggplot(aes(x = review_scores_rating)) +\n  geom_bar() +\n  theme_classic() +\n  labs(\n    x = \"Review scores rating\",\n    y = \"Number of properties\"\n  )\n\n\n\n\nFigure 11.12: Distribution of review scores rating for London Airbnb rentals in March 2023\n\n\n\n\nWe would like to deal with the NAs in “review_scores_rating”, but this is more complicated as there are a lot of them. It may be that this is just because they do not have any reviews.\n\nairbnb_data_no_superhost_nas |&gt;\n  filter(is.na(review_scores_rating)) |&gt;\n  nrow()\n\n[1] 17681\n\nairbnb_data_no_superhost_nas |&gt;\n  filter(is.na(review_scores_rating)) |&gt;\n  select(number_of_reviews) |&gt;\n  table()\n\nnumber_of_reviews\n    0 \n17681 \n\n\nThese properties do not have a review rating yet because they do not have enough reviews. It is a large proportion of the total, at almost a fifth of them so we might like to look at this in more detail using counts.  We are interested to see whether there is something systematic happening with these properties. For instance, if the NAs were being driven by, say, some requirement of a minimum number of reviews, then we would expect they would all be missing.   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne approach would be to just focus on those that are not missing and the main review score (Figure 11.13).\n\nairbnb_data_no_superhost_nas |&gt;\n  filter(!is.na(review_scores_rating)) |&gt;\n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(\n    x = \"Average review score\",\n    y = \"Number of properties\"\n  )\n\n\n\n\nFigure 11.13: Distribution of review scores for London Airbnb rentals in March 2023\n\n\n\n\nFor now, we will remove anyone with an NA in their main review score, even though this will remove roughly 20 per cent of observations. If we ended up using this dataset for actual analysis, then we would want to justify this decision in an appendix or similar.\n\nairbnb_data_has_reviews &lt;-\n  airbnb_data_no_superhost_nas |&gt;\n  filter(!is.na(review_scores_rating))\n\nAnother important factor is how quickly a host responds to an inquiry. Airbnb allows hosts up to 24 hours to respond, but encourages responses within an hour.\n\nairbnb_data_has_reviews |&gt;\n  count(host_response_time)\n\n# A tibble: 5 × 2\n  host_response_time     n\n  &lt;chr&gt;              &lt;int&gt;\n1 N/A                19479\n2 a few days or more   712\n3 within a day        4512\n4 within a few hours  6894\n5 within an hour     24321\n\n\nIt is unclear how a host could have a response time of NA. It may be this is related to some other variable. Interestingly it seems like what looks like “NAs” in “host_response_time” variable are not coded as proper NAs, but are instead being treated as another category. We will recode them to be actual NAs and change the variable to be a factor.\n\nairbnb_data_has_reviews &lt;-\n  airbnb_data_has_reviews |&gt;\n  mutate(\n    host_response_time = if_else(\n      host_response_time == \"N/A\",\n      NA_character_,\n      host_response_time\n    ),\n    host_response_time = factor(host_response_time)\n  )\n\nThere is an issue with NAs as there are a lot of them. For instance, we might be interested to see if there is a relationship with the review score (Figure 11.14). There are a lot that have an overall review of 100.\n\nairbnb_data_has_reviews |&gt;\n  filter(is.na(host_response_time)) |&gt;\n  ggplot(aes(x = review_scores_rating)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(\n    x = \"Average review score\",\n    y = \"Number of properties\"\n  )\n\n\n\n\nFigure 11.14: Distribution of review scores for properties with NA response time, for London Airbnb rentals in March 2023\n\n\n\n\nUsually missing values are dropped by ggplot2. We can use geom_miss_point() from naniar to include them in the graph (Figure 11.15).\n\nairbnb_data_has_reviews |&gt;\n  ggplot(aes(\n    x = host_response_time,\n    y = review_scores_accuracy\n  )) +\n  geom_miss_point() +\n  labs(\n    x = \"Host response time\",\n    y = \"Review score accuracy\",\n    color = \"Is missing?\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\nFigure 11.15: Missing values in London Airbnb data, by host response time\n\n\n\n\nFor now, we will remove anyone with a NA in their response time. This will again remove roughly another 20 per cent of the observations.\n\nairbnb_data_selected &lt;-\n  airbnb_data_has_reviews |&gt;\n  filter(!is.na(host_response_time))\n\nWe might be interested in how many properties a host has on Airbnb (Figure 11.16).\n\nairbnb_data_selected |&gt;\n  ggplot(aes(x = host_total_listings_count)) +\n  geom_histogram() +\n  scale_x_log10() +\n  labs(\n    x = \"Total number of listings, by host\",\n    y = \"Number of hosts\"\n  )\n\n\n\n\nFigure 11.16: Distribution of the number of properties a host has on Airbnb, for London Airbnb rentals in March 2023\n\n\n\n\nBased on Figure 11.16 we can see there are a large number who have somewhere in the 2-500 properties range, with the usual long tail. The number with that many listings is unexpected and worth following up on. And there are a bunch with NA that we will need to deal with.\n\nairbnb_data_selected |&gt;\n  filter(host_total_listings_count &gt;= 500) |&gt;\n  head()\n\n# A tibble: 6 × 13\n    host_id host_response_time host_is_superhost host_total_listings_count\n      &lt;dbl&gt; &lt;fct&gt;              &lt;lgl&gt;                                 &lt;dbl&gt;\n1 439074505 within an hour     FALSE                                  3627\n2 156158778 within an hour     FALSE                                   558\n3 156158778 within an hour     FALSE                                   558\n4 156158778 within an hour     FALSE                                   558\n5 156158778 within an hour     FALSE                                   558\n6 156158778 within an hour     FALSE                                   558\n# ℹ 9 more variables: neighbourhood_cleansed &lt;chr&gt;, bathrooms &lt;lgl&gt;,\n#   bedrooms &lt;dbl&gt;, price &lt;int&gt;, number_of_reviews &lt;dbl&gt;,\n#   review_scores_rating &lt;dbl&gt;, review_scores_accuracy &lt;dbl&gt;,\n#   review_scores_value &lt;dbl&gt;, host_is_superhost_binary &lt;dbl&gt;\n\n\nThere is nothing that immediately jumps out as odd about the people with more than ten listings, but at the same time it is still not clear. For now, we will move on and focus on only those with one property for simplicity.\n\nairbnb_data_selected &lt;-\n  airbnb_data_selected |&gt;\n  add_count(host_id) |&gt;\n  filter(n == 1) |&gt;\n  select(-n)\n\n\n\n11.5.2 Relationships between variables\nWe might like to make some graphs to see if there are any relationships between variables that become clear. Some aspects that come to mind are looking at prices and comparing with reviews, superhosts, number of properties, and neighborhood.\nWe can look at the relationship between price and reviews, and whether they are a super-host, for properties with more than one review (Figure 11.17).\n\nairbnb_data_selected |&gt;\n  filter(number_of_reviews &gt; 1) |&gt;\n  ggplot(aes(x = price, y = review_scores_rating, \n             color = host_is_superhost)) +\n  geom_point(size = 1, alpha = 0.1) +\n  theme_classic() +\n  labs(\n    x = \"Price per night\",\n    y = \"Average review score\",\n    color = \"Superhost\"\n  ) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nFigure 11.17: Relationship between price and review and whether a host is a superhost, for London Airbnb rentals in March 2023\n\n\n\n\nOne of the aspects that may make someone a superhost is how quickly they respond to inquiries. One could imagine that being a superhost involves quickly saying yes or no to inquiries. Let us look at the data. First, we want to look at the possible values of superhost by their response times.\n\nairbnb_data_selected |&gt;\n  count(host_is_superhost) |&gt;\n  mutate(\n    proportion = n / sum(n),\n    proportion = round(proportion, digits = 2)\n  )\n\n# A tibble: 2 × 3\n  host_is_superhost     n proportion\n  &lt;lgl&gt;             &lt;int&gt;      &lt;dbl&gt;\n1 FALSE             10480       0.74\n2 TRUE               3672       0.26\n\n\nFortunately, it looks like when we removed the reviews rows we removed any NAs from whether they were a superhost, but if we go back and look into that we may need to check again. We could build a table that looks at a hosts response time by whether they are a superhost using tabyl() from janitor. It is clear that if a host does not respond within an hour then it is unlikely that they are a superhost.\n\nairbnb_data_selected |&gt;\n  tabyl(host_response_time, host_is_superhost) |&gt;\n  adorn_percentages(\"col\") |&gt;\n  adorn_pct_formatting(digits = 0) |&gt;\n  adorn_ns() |&gt;\n  adorn_title()\n\n                    host_is_superhost            \n host_response_time             FALSE        TRUE\n a few days or more        5%   (489)  0%     (8)\n       within a day       22% (2,322) 11%   (399)\n within a few hours       23% (2,440) 25%   (928)\n     within an hour       50% (5,229) 64% (2,337)\n\n\nFinally, we could look at neighborhood. The data provider has attempted to clean the neighborhood variable for us, so we will use that variable for now. Although if we ended up using this variable for our actual analysis we would want to examine how it was constructed.\n\nairbnb_data_selected |&gt;\n  tabyl(neighbourhood_cleansed) |&gt;\n  adorn_pct_formatting() |&gt;\n  arrange(-n) |&gt;\n  filter(n &gt; 100) |&gt;\n  adorn_totals(\"row\") |&gt;\n  head()\n\n neighbourhood_cleansed    n percent\n                Hackney 1172    8.3%\n            Westminster  965    6.8%\n          Tower Hamlets  956    6.8%\n              Southwark  939    6.6%\n                Lambeth  914    6.5%\n             Wandsworth  824    5.8%\n\n\nWe will quickly run a model on our dataset. We will cover modeling in more detail in Chapter 12, but we can use models during EDA to help get a better sense of relationships that may exist between multiple variables in a dataset. For instance, we may like to see whether we can forecast whether someone is a superhost, and the factors that go into explaining that. As the outcome is binary, this is a good opportunity to use logistic regression. We expect that superhost status will be associated with faster responses and better reviews. Specifically, the model that we estimate is:\n\\[\\mbox{Prob(Is superhost} = 1) = \\mbox{logit}^{-1}\\left( \\beta_0 + \\beta_1 \\mbox{Response time} + \\beta_2 \\mbox{Reviews} + \\epsilon\\right)\\]\nWe estimate the model using glm.\n\nlogistic_reg_superhost_response_review &lt;-\n  glm(\n    host_is_superhost ~\n      host_response_time +\n      review_scores_rating,\n    data = airbnb_data_selected,\n    family = binomial\n  )\n\nAfter installing and loading modelsummary we can have a quick look at the results using modelsummary() (Table 11.3).\n\nmodelsummary(logistic_reg_superhost_response_review)\n\n\n\n\n\nTable 11.3:  Explaining whether a host is a superhost based on their response\ntime \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n-16.369\n    \n(0.673)\n    host_response_timewithin a day\n2.230\n    \n(0.361)\n    host_response_timewithin a few hours\n3.035\n    \n(0.359)\n    host_response_timewithin an hour\n3.279\n    \n(0.358)\n    review_scores_rating\n2.545\n    \n(0.116)\n    Num.Obs.\n14152\n    AIC\n14948.4\n    BIC\n14986.2\n    Log.Lik.\n-7469.197\n    F\n197.407\n    RMSE\n0.42\n  \n  \n  \n\n\n\n\n\nWe see that each of the levels is positively associated with the probability of being a superhost. However, having a host that responds within an hour is associated with individuals that are superhosts in our dataset.\nWe will save this analysis dataset.\n\nwrite_parquet(\n  x = airbnb_data_selected, \n  sink = \"2023-05-05-london-airbnblistings-analysis_dataset.parquet\"\n  )"
  },
  {
    "objectID": "11-eda.html#concluding-remarks",
    "href": "11-eda.html#concluding-remarks",
    "title": "11  Exploratory data analysis",
    "section": "11.6 Concluding remarks",
    "text": "11.6 Concluding remarks\nIn this chapter we have considered exploratory data analysis (EDA), which is the active process of getting to know a dataset. We focused on missing data, the distributions of variables, and the relationships between variables. And we extensively used graphs and tables to do this.\nThe approaches to EDA will vary depending on context, and the issues and features that are encountered in the dataset. It will also depend on your skills, for instance it is common to consider regression models, and dimensionality reduction approaches."
  },
  {
    "objectID": "11-eda.html#exercises",
    "href": "11-eda.html#exercises",
    "title": "11  Exploratory data analysis",
    "section": "11.7 Exercises",
    "text": "11.7 Exercises\n\nScales\n\n(Plan) Consider the following scenario: We have some data on age from a social media company that has about 80 per cent of the US population on the platform. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Use parquet due to size. Please include ten tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write one page about what you did, and be careful to discuss some of the threats to the estimate that you make based on the sample.\n\n\n\nQuestions\n\nSummarize Tukey (1962) in a few paragraphs and then relate it to data science.\nIn your own words what is exploratory data analysis (please write at least three paragraphs, and include citations and examples)?\nSuppose you have a dataset called “my_data”, which has two columns: “first_col” and “second_col”. Please write some R code that would generate a graph (the type of graph does not matter). Submit a link to a GitHub Gist that contains your code.\nConsider a dataset that has 500 observations and three variables, so there are 1,500 cells. If 100 of the rows are missing a cell for at least one of the columns, then would you: a) remove the whole row from your dataset, b) try to run your analysis on the data as is, or c) some other procedure? What if your dataset had 10,000 rows instead, but the same number of missing rows? Discuss, with examples and citations, in at least three paragraphs.\nPlease discuss three ways of identifying unusual values, writing at least one paragraph for each.\nWhat is the difference between a categorical and continuous variable?\nWhat is the difference between a factor and an integer variable?\nHow can we think about who is systematically excluded from a dataset?\nUsing opendatatoronto, download the data on mayoral campaign contributions for 2014. (Note: the 2014 file you will get from get_resource() contains many sheets, so just keep the sheet that relates to the mayor election).\n\nClean up the data format (fixing the parsing issue and standardizing the column names using janitor).\nSummarize the variables in the dataset. Are there missing values, and if so, should we be worried about them? Is every variable in the format it should be? If not, create new variable(s) that are in the right format.\nVisually explore the distribution of values of the contributions. What contributions are notable outliers? Do they share similar characteristic(s)? It may be useful to plot the distribution of contributions without these outliers to get a better sense of most of the data.\nList the top five candidates in each of these categories: 1) total contributions; 2) mean contribution; and 3) number of contributions.\nRepeat that process, but without contributions from the candidates themselves.\nHow many contributors gave money to more than one candidate?\n\nList three geoms that produce graphs that have bars in ggplot().\nConsider a dataset with 10,000 observations and 27 variables. For each observation, there is at least one missing variable. Please discuss, in a paragraph or two, the steps that you would take to understand what is going on.\nKnown missing data are those that leave holes in your dataset. But what about data that were never collected? Please look at McClelland (2019) and Luscombe and McClelland (2020). Look into how they gathered their dataset and what it took to put this together. What is in the dataset and why? What is missing and why? How could this affect the results? How might similar biases enter into other datasets that you have used or read about?\n\n\n\nTutorial\nPick one of the following options. Use Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations. Submit a PDF.\nOption 1:\nRepeat the missing data exercise conducted for the US states and population, but for the “bill_length_mm” variable in the penguins() dataset available from palmerpenguins. Compare the imputed value with the actual value.\nWrite at least two pages about what you did and what you found.\nFollowing this, please pair with another student and exchange your written work. Update it based on their feedback, and be sure to acknowledge them by name in your paper.\nOption 2:\nCarry out an Airbnb EDA but for Paris.\nOption 3:\nPlease write at least two pages about the topic: “what is missing data and what should you do about it?”\nFollowing this, please pair with another student and exchange your written work. Update it based on their feedback, and be sure to acknowledge them by name in your paper.\n\n\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nCox, Murray. 2021. “Inside Airbnb—Toronto Data.” http://insideairbnb.com/get-the-data.html.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHorton, Nicholas, and Stuart Lipsitz. 2001. “Multiple Imputation in Practice.” The American Statistician 55 (3): 244–54. https://doi.org/10.1198/000313001317098266.\n\n\nLuscombe, Alex, and Alexander McClelland. 2020. “Policing the Pandemic: Tracking the Policing of Covid-19 Across Canada,” April. https://doi.org/10.31235/osf.io/9pn27.\n\n\nManski, Charles. 2022. “Inference with Imputed Data: The Allure of Making Stuff Up.” arXiv. https://doi.org/10.48550/arXiv.2205.07388.\n\n\nMcClelland, Alexander. 2019. “‘Lock This Whore up’: Legal Violence and Flows of Information Precipitating Personal Violence Against People Criminalised for HIV-Related Crimes in Canada.” European Journal of Risk Regulation 10 (1): 132–47. https://doi.org/10.1017/err.2019.20.\n\n\nOsborne, Jason. 2012. Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data. SAGE Publications.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nRyan, Philip. 2015. “Keeping a Lab Notebook.” YouTube, May. https://youtu.be/-MAIuaOL64I.\n\n\nStaniak, Mateusz, and Przemysław Biecek. 2019. “The Landscape of R Packages for Automated Exploratory Data Analysis.” The R Journal 11 (2): 347–69. https://doi.org/10.32614/RJ-2019-033.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations for Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTukey, John. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.\n\n\nWickham, Hadley. 2018. “Whole Game.” YouTube, January. https://youtu.be/go5Au01Jrvs.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "12-ijalm.html#introduction",
    "href": "12-ijalm.html#introduction",
    "title": "12  Linear models",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nLinear models have been used in various forms for a long time. Stigler (1986, 16) describes how least squares, which is a method to fit simple linear regression, was associated with foundational problems in astronomy in the 1700s, such as determining the motion of the moon and reconciling the non-periodic motion of Jupiter and Saturn. The fundamental issue at the time with least squares was that of hesitancy by those coming from a statistical background to combine different observations. Astronomers were early to develop a comfort with doing this, possibly because they had typically gathered their observations themselves and knew that the conditions of the data gathering were similar, even though the value of the observation was different. For instance, Stigler (1986, 28) characterizes Leonhard Euler, the eighteenth century mathematician mentioned in Chapter 6, as considering that errors increase as they are aggregated, in comparison to Tobias Mayer, the eighteenth century astronomer, who was comfortable that errors would cancel each other. It took longer, again, for social scientists to become comfortable with linear models, possibly because they were hesitant to group together data they worried were not alike. In one sense astronomers had an advantage because they could compare their predictions with what happened whereas this was more difficult for social scientists (Stigler 1986, 163).\nWhen we build models, we are not discovering “the truth”. A model is not, and cannot be, a true representation of reality. We are using the model to help us explore and understand our data. There is no one best model, there are just useful models that help us learn something about the data that we have and hence, hopefully, something about the world from which the data were generated. When we use models, we are trying to understand the world, but there are constraints on the perspective we bring to this. We should not just throw data into a model and hope that it will sort it out. It will not.\n\nRegression is indeed an oracle, but a cruel one. It speaks in riddles and delights in punishing us for asking bad questions.\nMcElreath ([2015] 2020, 162)\n\nWe use models to understand the world. We poke, push, and test them. We build them and rejoice in their beauty, and then seek to understand their limits and ultimately destroy them. It is this process that is important, it is this process that allows us to better understand the world; not the outcome, although they may be coincident. When we build models, we need to keep in mind both the world of the model and the broader world that we want to be able to speak about. The datasets that we have are often unrepresentative of real-world populations in certain ways. Models trained on such data are not worthless, but they are also not unimpeachable. To what extent does the model teach us about the data that we have? To what extent do the data that we have reflect the world about which we would like to draw conclusions? We need to keep such questions front of mind.\nA lot of statistical methods commonly used today were developed for situations such as astronomy and agriculture. Ronald Fisher, introduced in Chapter 8, published Fisher ([1925] 1928) while he worked at an agricultural research institution. But many of the subsequent uses in the twentieth and twenty-first centuries concern applications that may have different properties. Statistical validity relies on assumptions, and so while what is taught is correct, our circumstances may not meet the starting criteria. Statistics is often taught as though it proceeds through some idealized process where a hypothesis appears, is tested against some data that similarly appears, and is either confirmed or not. But that is not what happens, in practice. We react to incentives. We dabble, guess, and test, and then follow our intuition, backfilling as we need. All of this is fine. But it is not a world in which a traditional null hypothesis, which we will get to later, holds completely. This means concepts such as p-values and power lose some of their meaning. While we need to understand these foundations, we also need to be sophisticated enough to know when we need to move away from them.\nStatistical checks are widely used in modeling. And an extensive suite of them is available. But automated testing of code and data is also important. For instance, Knutson et al. (2022) built a model of excess deaths in a variety of countries, to estimate the overall death toll from the pandemic. After initially releasing the model, which had been extensively manually checked for statistical issues and reasonableness, some of the results were reexamined and it was found that the estimates for Germany and Sweden were oversensitive. The authors quickly addressed the issue, but the integration of automated testing of expected values for the coefficients, in addition to the usual manual statistical checks, would go some way to enabling us to have more faith in the models of others.\nIn this chapter we begin with simple linear regression, and then move to multiple linear regression, the difference being the number of explanatory variables that we allow. We go through two approaches for each of these: base R, in particular the lm() and glm() functions, which are useful when we want to quickly use the models in EDA; and rstanarm for when we are interested in inference. In general, a model is either optimized for inference or prediction. A focus on prediction is one of the hallmarks of machine learning. For historical reasons that has tended to be dominated by Python, although tidymodels (Kuhn and Wickham 2020) has been developed in R. Due to the need to also introduce Python, we devote the Appendix H to various approaches focused on prediction. Regardless of the approach we use, the important thing to remember that we are just doing something akin to fancy averaging, and our results always reflect the biases and idiosyncrasies of the dataset.\nFinally a note on terminology and notation. For historical and context-specific reasons there are a variety of terms used to describe the same idea across the literature. We follow Gelman, Hill, and Vehtari (2020) and use the terms “outcome” and “predictor”, we follow the frequentist notation of James et al. ([2013] 2021), and the Bayesian model specification of McElreath ([2015] 2020)."
  },
  {
    "objectID": "12-ijalm.html#simple-linear-regression",
    "href": "12-ijalm.html#simple-linear-regression",
    "title": "12  Linear models",
    "section": "12.2 Simple linear regression",
    "text": "12.2 Simple linear regression\nWhen we are interested in the relationship of some continuous outcome variable, say \\(y\\), and some predictor variable, say \\(x\\), we can use simple linear regression. This is based on the Normal, also called “Gaussian”, distribution, but it is not these variables themselves that are normally distributed. The Normal distribution is determined by two parameters, the mean, \\(\\mu\\), and the standard deviation, \\(\\sigma\\) (Pitman 1993, 94):\n\\[y = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}z^2},\\] where \\(z = (x - \\mu)/\\sigma\\) is the difference between \\(x\\) and the mean, scaled by the standard deviation. Altman and Bland (1995) provide an overview of the Normal distribution.\nAs introduced in Appendix A, we use rnorm() to simulate data from the Normal distribution.\n\nset.seed(853)\n\nnormal_example &lt;-\n  tibble(draws = rnorm(n = 20, mean = 0, sd = 1))\n\nnormal_example |&gt; pull(draws)\n\n [1] -0.35980342 -0.04064753 -1.78216227 -1.12242282 -1.00278400  1.77670433\n [7] -1.38825825 -0.49749494 -0.55798959 -0.82438245  1.66877818 -0.68196486\n[13]  0.06519751 -0.25985911  0.32900796 -0.43696568 -0.32288891  0.11455483\n[19]  0.84239206  0.34248268\n\n\nHere we specified 20 draws from a Normal distribution with a true mean, \\(\\mu\\), of zero and a true standard deviation, \\(\\sigma\\), of one. When we deal with real data, we will not know the true value of these, and we want to use our data to estimate them. We can create an estimate of the mean, \\(\\hat{\\mu}\\), and an estimate of the standard deviation, \\(\\hat{\\sigma}\\), with the following estimators:\n\\[\n\\begin{aligned}\n\\hat{\\mu} &= \\frac{1}{n} \\times \\sum_{i = 1}^{n}x_i\\\\\n\\hat{\\sigma} &= \\sqrt{\\frac{1}{n-1} \\times \\sum_{i = 1}^{n}\\left(x_i - \\hat{\\mu}\\right)^2}\n\\end{aligned}\n\\]\nIf \\(\\hat{\\sigma}\\) is the estimate of the standard deviation, then the standard error (SE) of the estimate of the mean, \\(\\hat{\\mu}\\), is:\n\\[\\mbox{SE}(\\hat{\\mu}) = \\frac{\\hat{\\sigma}}{\\sqrt{n}}.\\]\nThe standard error is a comment about the estimate of the mean compared with the actual mean, while the standard deviation is a comment about how widely distributed the data are.1\nWe can implement these in code using our simulated data to see how close our estimates are.\n\nestimated_mean &lt;-\n  sum(normal_example$draws) / nrow(normal_example)\n\nnormal_example &lt;-\n  normal_example |&gt;\n  mutate(diff_square = (draws - estimated_mean) ^ 2)\n\nestimated_standard_deviation &lt;-\n  sqrt(sum(normal_example$diff_square) / (nrow(normal_example) - 1))\n\nestimated_standard_error &lt;-\n  estimated_standard_deviation / sqrt(nrow(normal_example))\n\nkable(\n  tibble(mean = estimated_mean,\n         sd = estimated_standard_deviation,\n         se = estimated_standard_error),\n  col.names = c(\n    \"Estimated mean\",\n    \"Estimated standard deviation\",\n    \"Estimated standard error\"\n  ),\n  digits = 2,\n  align = c(\"l\", \"r\", \"r\"),\n  booktabs = TRUE,\n  linesep = \"\"\n  )\n\n\n\nTable 12.1: Estimates of the mean and standard deviation based on the simulated data\n\n\n\n\n\n\n\nEstimated mean\nEstimated standard deviation\nEstimated standard error\n\n\n\n\n-0.21\n0.91\n0.2\n\n\n\n\n\n\nWe should not be too worried that our estimates are slightly off (Table 12.1), relative to the “true” mean and SD of 0 and 1. We only considered 20 observations. It will typically take a larger number of draws before we get the expected shape, and our estimated parameters get close to the actual parameters, but it will almost surely happen (Figure 12.1). Wasserman (2005, 76) considers our certainty of this, which is due to the Law of Large Numbers, as a crowning accomplishment of probability, although Wood (2015, 15), perhaps more prosaically, describes it as “almost” a statement “of the obvious”!\n\nset.seed(853)\n\nnormal_takes_shapes &lt;-\n  map_dfr(c(2, 5, 10, 50, 100, 500, 1000, 10000, 100000),\n          ~ tibble(\n            number_draws = rep(paste(.x, \"draws\"), .x),\n            draws = rnorm(.x, mean = 0, sd = 1)\n          ))\n\nnormal_takes_shapes |&gt;\n  mutate(number_draws = as_factor(number_draws)) |&gt;\n  ggplot(aes(x = draws)) +\n  geom_density() +\n  theme_minimal() +\n  facet_wrap(vars(number_draws),\n             scales = \"free_y\") +\n  labs(x = \"Draw\",\n       y = \"Density\")\n\n\n\n\nFigure 12.1: The Normal distribution takes its familiar shape as the number of draws increases\n\n\n\n\nWhen we use simple linear regression, we assume that our relationship is characterized by the variables and the parameters. If we have two variables, \\(Y\\) and \\(X\\), then we could characterize a linear relationship between these as:\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon.\n\\tag{12.1}\\]\nHere, there are two parameters, also referred to as coefficients: the intercept, \\(\\beta_0\\), and the slope, \\(\\beta_1\\). In Equation 12.1 we are saying that the expected value of \\(Y\\) is \\(\\beta_0\\) when \\(X\\) is 0, and that the expected value of \\(Y\\) will change by \\(\\beta_1\\) units for every one unit change in \\(X\\). We may then take this relationship to the data that we have to estimate these parameters. The \\(\\epsilon\\) is noise and accounts for deviations away from this relationship. It is this noise that we generally assume to be normally distributed, and it is this that leads to \\(Y \\sim N(\\beta, \\sigma^2)\\).\n\n12.2.1 Simulated example: running times\nTo make this example concrete, we revisit an example from Chapter 9 about the time it takes someone to run five kilometers, compared with the time it takes them to run a marathon (Figure 12.2 (a)). We specified a relationship of 8.4, as that is roughly the ratio between a five-kilometer run and the 42.2-kilometer distance of a marathon. To help the reader, we include the simulation code again here. Notice that it is the noise that is normally distributed, not the variables. We do not require the variables themselves to be normally distributed in order to use linear regression.\n\nset.seed(853)\n\nnum_observations &lt;- 200\nexpected_relationship &lt;- 8.4\nfast_time &lt;- 15\ngood_time &lt;- 30\n\nsim_run_data &lt;-\n  tibble(\n    five_km_time =\n      runif(n = num_observations, min = fast_time, max = good_time),\n    noise = rnorm(n = num_observations, mean = 0, sd = 20),\n    marathon_time = five_km_time * expected_relationship + noise\n  ) |&gt;\n  mutate(\n    five_km_time = round(x = five_km_time, digits = 1),\n    marathon_time = round(x = marathon_time, digits = 1)\n  ) |&gt;\n  select(-noise)\n\n\nbase_plot &lt;- \n  sim_run_data |&gt;\n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Five-kilometer time (minutes)\",\n    y = \"Marathon time (minutes)\"\n  ) +\n  theme_classic()\n\n# Panel (a)\nbase_plot\n\n# Panel (b)\nbase_plot +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE,\n    color = \"black\",\n    linetype = \"dashed\",\n    formula = \"y ~ x\"\n  )\n\n# Panel (c)\nbase_plot +\n  geom_smooth(\n    method = \"lm\",\n    se = TRUE,\n    color = \"black\",\n    linetype = \"dashed\",\n    formula = \"y ~ x\"\n  )\n\n\n\n\n\n\n\n(a) Distribution of simulated data\n\n\n\n\n\n\n\n(b) With one linear best-fit line illustrating the implied relationship\n\n\n\n\n\n\n\n\n\n(c) Including standard errors\n\n\n\n\nFigure 12.2: Simulated data of the relationship between the time to run five kilometers and a marathon\n\n\n\nIn this simulated example, we know the true values of \\(\\beta_0\\) and \\(\\beta_1\\), which are zero and 8.4, respectively. But our challenge is to see if we can use only the data, and simple linear regression, to recover them. That is, can we use \\(x\\), which is the five-kilometer time, to produce estimates of \\(y\\), which is the marathon time, and that we denote by \\(\\hat{y}\\) (by convention, hats are used to indicate that these are, or will be, estimated values) (James et al. [2013] 2021, 61):\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x.\\]\nThis involves estimating values for \\(\\beta_0\\) and \\(\\beta_1\\). But how should we estimate these coefficients? Even if we impose a linear relationship there are many options because many straight lines could be drawn. But some of those lines would fit the data better than others.\nOne way we could define a line as being “better” than another, is if it is as close as possible to each of the known \\(x\\) and \\(y\\) combinations. There are a lot of candidates for how we define as “close as possible”, but one is to minimize the residual sum of squares. To do this we produce estimates for \\(\\hat{y}\\) based on some guesses of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given the \\(x\\). We then work out how wrong, for every observation \\(i\\), we were (James et al. [2013] 2021, 62):\n\\[e_i = y_i - \\hat{y}_i.\\]\nTo compute the residual sum of squares (RSS), we sum the errors across all the points (taking the square to account for negative differences) (James et al. [2013] 2021, 62):\n\\[\\mbox{RSS} = e^2_1+ e^2_2 +\\dots + e^2_n.\\]\nThis results in one linear best-fit line (Figure 12.2 (b)), but it is worth reflecting on all the assumptions and decisions that it took to get us to this point.\nUnderpinning our use of simple linear regression is a belief that there is some “true” relationship between \\(X\\) and \\(Y\\). And that this is a linear function of \\(X\\). We do not, and cannot, know the “true” relationship between \\(X\\) and \\(Y\\). All we can do is use our sample to estimate it. But because our understanding depends on that sample, for every possible sample, we would get a slightly different relationship, as measured by the coefficients.\nThat \\(\\epsilon\\) is a measure of our error—what does the model not know in the small, contained world of the dataset? But it does not tell us whether the model is appropriate outside of the dataset (think, by way of analogy, to the concepts of internal and external validity for experiments introduced in Chapter 8). That requires our judgement and experience.\nWe can conduct simple linear regression with lm() from base R. We specify the outcome variable first, then ~, followed by the predictor. The outcome variable is the variable of interest, while the predictor is the basis on which we consider that variable. Finally, we specify the dataset.\nBefore we run a regression, we may want to include a quick check of the class of the variables, and the number of observations, just to ensure that it corresponds with what we were expecting, although we may have done that earlier in the workflow. And after we run it, we may check that our estimate seems reasonable. For instance, (pretending that we had not ourselves imposed this in the simulation) based on our knowledge of the respective distances of a five-kilometer run and a marathon, we would expect \\(\\beta_1\\) to be somewhere between six and ten.\n\n# Check the class and number of observations are as expected\nstopifnot(\n  class(sim_run_data$marathon_time) == \"numeric\",\n  class(sim_run_data$five_km_time) == \"numeric\",\n  nrow(sim_run_data) == 200\n)\n\nsim_run_data_first_model &lt;-\n  lm(\n    marathon_time ~ five_km_time,\n    data = sim_run_data\n  )\n\nstopifnot(between(\n  sim_run_data_first_model$coefficients[2],\n  6,\n  10\n))\n\nTo quickly see the result of the regression, we can use summary().\n\nsummary(sim_run_data_first_model)\n\n\nCall:\nlm(formula = marathon_time ~ five_km_time, data = sim_run_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.289 -11.948   0.153  11.396  46.511 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.4692     6.7517   0.662    0.509    \nfive_km_time   8.2049     0.3005  27.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.42 on 198 degrees of freedom\nMultiple R-squared:  0.7902,    Adjusted R-squared:  0.7891 \nF-statistic: 745.5 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nBut we can also use modelsummary() from modelsummary (Table 12.2). The advantage of that approach is that we get a nicely formatted table. We focus initially on those results in the “Five km only” column.\n\nmodelsummary(\n  list(\n    \"Five km only\" = sim_run_data_first_model,\n    \"Five km only, centered\" = sim_run_data_centered_model\n  ),\n  fmt = 2\n)\n\n\n\n\n\nTable 12.2:  Explaining marathon times based on five-kilometer run times \n  \n    \n    \n       \n      Five km only\n      Five km only, centered\n    \n  \n  \n    (Intercept)\n4.47\n185.73\n    \n(6.75)\n(1.23)\n    five_km_time\n8.20\n\n    \n(0.30)\n\n    centered_time\n\n8.20\n    \n\n(0.30)\n    Num.Obs.\n200\n200\n    R2\n0.790\n0.790\n    R2 Adj.\n0.789\n0.789\n    AIC\n1714.7\n1714.7\n    BIC\n1724.6\n1724.6\n    Log.Lik.\n-854.341\n-854.341\n    F\n745.549\n745.549\n    RMSE\n17.34\n17.34\n  \n  \n  \n\n\n\n\n\nThe top half of Table 12.2 provides our estimated coefficients and standard errors in brackets. And the second half provides some useful diagnostics, which we will not dwell too much in this book. The intercept in the “Five km only” column is the marathon time associated with a hypothetical five-kilometer time of zero minutes. Hopefully this example illustrates the need to always interpret the intercept coefficient carefully. And to disregard it at times. For instance, in this circumstance, we know that the intercept should be zero, and it is just being set to around four because that is the best fit given all the observations were for five-kilometer times between 15 and 30.\nThe intercept becomes more interpretable when we run the regression using centered five-kilometer time, which we do in the “Five km only, centered” column of Table 12.2. That is, for each of the five-kilometer times we subtract the mean five-kilometer time. In this case, the intercept is interpreted as the expected marathon time for someone who runs five kilometers in the average time. Notice that the slope estimate is unchanged it is only the intercept that has changed.\n\nsim_run_data &lt;-\n  sim_run_data |&gt;\n  mutate(centered_time = five_km_time - mean(sim_run_data$five_km_time))\n\nsim_run_data_centered_model &lt;-\n  lm(\n    marathon_time ~ centered_time,\n    data = sim_run_data\n  )\n\nFollowing Gelman, Hill, and Vehtari (2020, 84) we recommend considering the coefficients as comparisons, rather than effects. And to use language that makes it clear these are comparisons, on average, based on one dataset. For instance, we may consider that the coefficient on the five-kilometer run time shows how different individuals compare. When comparing the marathon times of individuals in our dataset whose five-kilometer run time differed by one minute, on average we find their marathon times differ by about eight minutes. This makes sense seeing as a marathon is roughly that many times longer than a five-kilometer run.\nWe can use augment() from broom to add the fitted values and residuals to our original dataset. This allows us to plot the residuals (Figure 12.3).\n\nsim_run_data &lt;-\n  augment(\n    sim_run_data_first_model,\n    data = sim_run_data\n  )\n\n\n# Plot a)\nggplot(sim_run_data, aes(x = .resid)) +\n  geom_histogram(binwidth = 1) +\n  theme_classic() +\n  labs(y = \"Number of occurrences\", x = \"Residuals\")\n\n# Plot b)\nggplot(sim_run_data, aes(x = five_km_time, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"grey\") +\n  theme_classic() +\n  labs(y = \"Residuals\", x = \"Five-kilometer time (minutes)\")\n\n# Plot c)\nggplot(sim_run_data, aes(x = marathon_time, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dotted\", color = \"grey\") +\n  theme_classic() +\n  labs(y = \"Residuals\", x = \"Marathon time (minutes)\")\n\n# Plot d)\nggplot(sim_run_data, aes(x = marathon_time, y = .fitted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  theme_classic() +\n  labs(y = \"Estimated marathon time\", x = \"Actual marathon time\")\n\n\n\n\n\n\n\n(a) Distribution of residuals\n\n\n\n\n\n\n\n(b) Residuals by five-kilometer time\n\n\n\n\n\n\n\n\n\n(c) Residuals by marathon time\n\n\n\n\n\n\n\n(d) Comparing the estimated time with the actual time\n\n\n\n\nFigure 12.3: Residuals from the simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon\n\n\n\n\nWe want to try to speak to the “true” relationship, so we need to try to capture how much we think our understanding depends on the sample that we have to analyze. And this is where the standard error comes in. It guides us, based on many assumptions, about how to think about the estimates of parameters based on the data that we have (Figure 12.2 (c)). Part of this is captured by the fact that standard errors are a function of sample size \\(n\\), and as the sample size increases, the standard errors decrease.\nThe most common way to summarize the range of uncertainty for a coefficient is to transform its standard error into a confidence interval. These intervals are often misunderstood to represent a statement of probability about a given realization of the coefficients (i.e. \\(\\hat\\beta\\)). In reality, confidence intervals are a statistic whose properties can only be understood “in expectation” (which is equivalent to repeating an experiment many times). A 95 per cent confidence interval is a range, such that there is “approximately a 95 per cent chance that the” range contains the population parameter, which is typically unknown (James et al. [2013] 2021, 66).  \nWhen the coefficient follows a Normal distribution, the lower and upper end of a 95 per cent confidence interval range will be about \\(\\hat{\\beta_1} \\pm 2 \\times \\mbox{SE}\\left(\\hat{\\beta_1}\\right)\\). For instance, in the case of the marathon time example, the lower end is \\(8.2 - 2 \\times 0.3 = 7.6\\) and the upper end is \\(8.2 + 2 \\times 0.3 = 8.8\\), and the true value (which we only know in this case because we simulated it) is 8.4.\nWe can use this machinery to test claims. For instance, we could claim that there is no relationship between \\(X\\) and \\(Y\\), i.e. \\(\\beta_1 = 0\\), as an alternative to a claim that there is some relationship between \\(X\\) and \\(Y\\), i.e. \\(\\beta_1 \\neq 0\\). This is the null hypothesis testing approach mentioned earlier. In Chapter 8 we needed to decide how much evidence it would take to convince us that our tea taster could distinguish whether milk or tea had been added first. In the same way, here we need to decide if the estimate of \\(\\beta_1\\), which we denote as \\(\\hat{\\beta}_1\\), is far enough away from zero for us to be comfortable claiming that \\(\\beta_1 \\neq 0\\). If we were very confident in our estimate of \\(\\beta_1\\) then it would not have to be far, but if we were not, then it would have to be substantial. For instance, if the confidence interval contains zero, then we lack evidence to suggest \\(\\beta_1 \\neq 0\\). The standard error of \\(\\hat{\\beta}_1\\) does an awful lot of work here in accounting for a variety of factors, only some of which it can actually account for, as does our choice as to what it would take to convince us.\nWe use this standard error and \\(\\hat{\\beta}_1\\) to get the “test statistic” or t-statistic:\n\\[t = \\frac{\\hat{\\beta}_1}{\\mbox{SE}(\\hat{\\beta}_1)}.\\]\nAnd we then compare our t-statistic to the t-distribution of Student (1908) to compute the probability of getting this absolute t-statistic or a larger one, if it was the case that \\(\\beta_1 = 0\\). This probability is the p-value. A smaller p-value means there is a smaller “probability of observing something at least as extreme as the observed test statistic” (Gelman, Hill, and Vehtari 2020, 57). Here we use the t-distribution of Student (1908) rather than the Normal distribution, because the t-distribution has slightly larger tails than a standard Normal distribution.\n\nWords! Mere words! How terrible they were! How clear, and vivid, and cruel! One could not escape from them. And yet what a subtle magic there was in them! They seemed to be able to give a plastic form to formless things, and to have a music of their own as sweet as that of viol or of lute. Mere words! Was there anything so real as words?\nThe Picture of Dorian Gray (Wilde 1891).\n\nWe will not make much use of p-values in this book because they are a specific and subtle concept. They are difficult to understand and easy to abuse. Even though they are “little help” for “scientific inference” many disciplines are incorrectly fixated on them (Nelder 1999, 257). One issue is that they embody every assumption of the workflow, including everything that went into gathering and cleaning the data. While p-values have implications if all the assumptions were correct, when we consider the full data science workflow there are usually an awful lot of assumptions. And we do not get guidance from p-values about whether the assumptions are satisfied (Greenland et al. 2016, 339).\nA p-value may reject a null hypothesis because the null hypothesis is false, but it may also be that some data were incorrectly gathered or prepared. We can only be sure that the p-value speaks to the hypothesis we are interested in testing if all the other assumptions are correct. There is nothing wrong about using p-values, but it is important to use them in sophisticated and thoughtful ways. Cox (2018) discusses what this requires.\nOne application where it is easy to see an inappropriate focus on p-values is in power analysis. Power, in a statistical sense, refers to probability of rejecting a null hypothesis that is false. As power relates to hypothesis testing, it also relates to sample size. There is often a worry that a study is “under-powered”, meaning there was not a large enough sample, but rarely a worry that, say, the data were inappropriately cleaned, even though we cannot distinguish between these based only on a p-value. As Meng (2018) and Bradley et al. (2021) demonstrate, a focus on power can blind us from our responsibility to ensure our data are high quality.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Nancy Reid is University Professor in the Department of Statistical Sciences at the University of Toronto. After obtaining a PhD in Statistics from Stanford University in 1979, she took a position as a postdoctoral fellow at Imperial College London. She was appointed as an assistant professor at the University of British Columbia in 1980, and then moved to the University of Toronto in 1986, where she was promoted to full professor in 1988 and served as department chair between 1997 and 2002 (Staicu 2017). Her research focuses on obtaining accurate inference in small-sample regimes and developing inferential procedures for complex models featuring intractable likelihoods. Cox and Reid (1987) examine how re-parameterizing models can simplify inference, Varin, Reid, and Firth (2011) surveys methods for approximating intractable likelihoods, and Reid (2003) overviews inferential procedures in the small-sample regime. Dr Reid was awarded the 1992 COPSS Presidents’ Award, the Royal Statistical Society Guy Medal in Silver in 2016 and in Gold in 2022, and the COPSS Distinguished Achievement Award and Lectureship in 2022."
  },
  {
    "objectID": "12-ijalm.html#multiple-linear-regression",
    "href": "12-ijalm.html#multiple-linear-regression",
    "title": "12  Linear models",
    "section": "12.3 Multiple linear regression",
    "text": "12.3 Multiple linear regression\nTo this point we have just considered one explanatory variable. But we will usually have more than one. One approach would be to run separate regressions for each explanatory variable. But compared with separate linear regressions for each, adding more explanatory variables allows for associations between the outcome variable and the predictor variable of interest to be assessed while adjusting for other explanatory variables. The results can be quite different, especially when the explanatory variables are correlated with each other.\nWe may also like to consider explanatory variables that are not continuous. For instance: pregnant or not; day or night. When there are only two options then we can use a binary variable, which is considered either 0 or 1. If we have a column of character values that only has two values, such as: c(\"Myles\", \"Ruth\", \"Ruth\", \"Myles\", \"Myles\", \"Ruth\"), then using this as an explanatory variable in the usual regression set-up would mean that it is treated as a binary variable. If there are more than two levels, then we can use a combination of binary variables, where some baseline outcome gets integrated into the intercept.\n\n12.3.1 Simulated example: running times with rain and humidity\nAs an example, we add whether it was raining to our simulated relationship between marathon and five-kilometer run times. We then specify that if it was raining, the individual is ten minutes slower than they otherwise would have been.\n\nslow_in_rain &lt;- 10\n\nsim_run_data &lt;-\n  sim_run_data |&gt;\n  mutate(was_raining = sample(\n    c(\"Yes\", \"No\"),\n    size = num_observations,\n    replace = TRUE,\n    prob = c(0.2, 0.8)\n  )) |&gt;\n  mutate(\n    marathon_time = if_else(\n      was_raining == \"Yes\",\n      marathon_time + slow_in_rain,\n      marathon_time\n    )\n  ) |&gt;\n  select(five_km_time, marathon_time, was_raining)\n\nWe can add additional explanatory variables to lm() with +. Again, we will include a variety of quick tests for class and the number of observations, and add another about missing values. We may not have any idea what the coefficient of rain should be, but if we did not expect it to make them faster, then we could also add a test of that with a wide interval of non-negative values.\n\nstopifnot(\n  class(sim_run_data$marathon_time) == \"numeric\",\n  class(sim_run_data$five_km_time) == \"numeric\",\n  class(sim_run_data$was_raining) == \"character\",\n  all(complete.cases(sim_run_data)),\n  nrow(sim_run_data) == 200\n)\n\nsim_run_data_rain_model &lt;-\n  lm(\n    marathon_time ~ five_km_time + was_raining,\n    data = sim_run_data\n  )\n\nstopifnot(\n  between(sim_run_data_rain_model$coefficients[3], 0, 20)\n  )\n\nsummary(sim_run_data_rain_model)\n\n\nCall:\nlm(formula = marathon_time ~ five_km_time + was_raining, data = sim_run_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.592 -11.623   0.591  11.300  47.228 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.4951     6.8225   0.512    0.609    \nfive_km_time     8.2208     0.3009  27.318  &lt; 2e-16 ***\nwas_rainingYes  13.0409     3.0561   4.267 3.07e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.42 on 197 degrees of freedom\nMultiple R-squared:  0.7929,    Adjusted R-squared:  0.7908 \nF-statistic: 377.1 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nThe result, in the second column of Table 12.3, shows that when we compare individuals in our dataset who ran in the rain with those who did not, the ones in the rain tended to have a slower time. And this corresponds with what we expect if we look at a plot of the data (Figure 12.4 (a)).\nWe have included two types of tests here. The ones run before lm() check inputs, and the ones run after lm() check outputs. We may notice that some of the input checks are the same as earlier. One way to avoid having to rewrite tests many times would be to install and load testthat to create a suite of tests of class in say an R file called “class_tests.R”, which are then called using test_file().\nFor instance, we could save the following as “test_class.R” in a dedicated tests folder.\n\ntest_that(\"Check class\", {\n  expect_type(sim_run_data$marathon_time, \"double\")\n  expect_type(sim_run_data$five_km_time, \"double\")\n  expect_type(sim_run_data$was_raining, \"character\")\n})\n\nWe could save the following as “test_observations.R”\n\ntest_that(\"Check number of observations is correct\", {\n  expect_equal(nrow(sim_run_data), 200)\n})\n\ntest_that(\"Check complete\", {\n  expect_true(all(complete.cases(sim_run_data)))\n})\n\nAnd finally, we could save the following as “test_coefficient_estimates.R”.\n\ntest_that(\"Check coefficients\", {\n  expect_gt(sim_run_data_rain_model$coefficients[3], 0)\n  expect_lt(sim_run_data_rain_model$coefficients[3], 20)\n})\n\nWe could then change the regression code to call these test files rather than write them all out.\n\ntest_file(\"tests/test_observations.R\")\ntest_file(\"tests/test_class.R\")\n\nsim_run_data_rain_model &lt;-\n  lm(\n    marathon_time ~ five_km_time + was_raining,\n    data = sim_run_data\n  )\n\ntest_file(\"tests/test_coefficient_estimates.R\")\n\nIt is important to be clear about what we are looking for in the checks of the coefficients. When we simulate data, we put in place reasonable guesses for what the data could look like, and it is similarly reasonable guesses that we test. A failing test is not necessarily a reason to go back and change things, but instead a reminder to look at what is going on in both, and potentially update the test if necessary.\nIn addition to wanting to include additional explanatory variables, we may think that they are related to each another. For instance, maybe rain really matters if it is also humid that day. We are interested in the humidity and temperature, but also how those two variables interact (Figure 12.4 (b)). We can do this by using * instead of + when we specify the model. When we interact variables in this way, then we almost always need to include the individual variables as well and lm() will do this by default. The result is contained in the third column of Table 12.3.\n\nslow_in_humidity &lt;- 15\n\nsim_run_data &lt;- sim_run_data |&gt;\n  mutate(\n    humidity = sample(c(\"High\", \"Low\"), size = num_observations, \n                      replace = TRUE, prob = c(0.2, 0.8)),\n    marathon_time = \n      marathon_time + if_else(humidity == \"High\", slow_in_humidity, 0),\n    weather_conditions = case_when(\n      was_raining == \"No\" & humidity == \"Low\" ~ \"No rain, not humid\",\n      was_raining == \"Yes\" & humidity == \"Low\" ~ \"Rain, not humid\",\n      was_raining == \"No\" & humidity == \"High\" ~ \"No rain, humid\",\n      was_raining == \"Yes\" & humidity == \"High\" ~ \"Rain, humid\"\n    )\n  )\n\n\nbase &lt;-\n  sim_run_data |&gt;\n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  labs(\n    x = \"Five-kilometer time (minutes)\",\n    y = \"Marathon time (minutes)\"\n  ) +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\nbase +\n  geom_point(aes(color = was_raining)) +\n  geom_smooth(\n    aes(color = was_raining),\n    method = \"lm\",\n    alpha = 0.3,\n    linetype = \"dashed\",\n    formula = \"y ~ x\"\n  ) +\n  labs(color = \"Was raining\")\n\nbase +\n  geom_point(aes(color = weather_conditions)) +\n  geom_smooth(\n    aes(color = weather_conditions),\n    method = \"lm\",\n    alpha = 0.3,\n    linetype = \"dashed\",\n    formula = \"y ~ x\"\n  ) +\n  labs(color = \"Conditions\")\n\n\n\n\n\n\n\n(a) Only whether it was raining\n\n\n\n\n\n\n\n\n\n(b) Whether it was raining and the level of humidity\n\n\n\n\nFigure 12.4: Simple linear regression with simulated data on the time someone takes to run five kilometers and a marathon, depending on the weather\n\n\n\n\nsim_run_data_rain_and_humidity_model &lt;-\n  lm(\n    marathon_time ~ five_km_time + was_raining * humidity,\n    data = sim_run_data\n  )\n\nmodelsummary(\n  list(\n    \"Five km only\" = sim_run_data_first_model,\n    \"Add rain\" = sim_run_data_rain_model,\n    \"Add humidity\" = sim_run_data_rain_and_humidity_model\n  ),\n  fmt = 2\n)\n\n\n\n\n\nTable 12.3:  Explaining marathon times based on five-kilometer run times and\nweather features \n  \n    \n    \n       \n      Five km only\n      Add rain\n      Add humidity\n    \n  \n  \n    (Intercept)\n4.47\n3.50\n17.65\n    \n(6.75)\n(6.82)\n(7.18)\n    five_km_time\n8.20\n8.22\n8.23\n    \n(0.30)\n(0.30)\n(0.30)\n    was_rainingYes\n\n13.04\n21.46\n    \n\n(3.06)\n(7.68)\n    humidityLow\n\n\n-14.23\n    \n\n\n(3.28)\n    was_rainingYes × humidityLow\n\n\n-9.93\n    \n\n\n(8.38)\n    Num.Obs.\n200\n200\n200\n    R2\n0.790\n0.793\n0.796\n    R2 Adj.\n0.789\n0.791\n0.792\n    AIC\n1714.7\n1715.7\n1718.2\n    BIC\n1724.6\n1728.9\n1738.0\n    Log.Lik.\n-854.341\n-853.840\n-853.091\n    F\n745.549\n377.132\n190.612\n    RMSE\n17.34\n17.29\n17.23\n  \n  \n  \n\n\n\n\n\nThere are a variety of threats to the validity of linear regression estimates, and aspects to think about, particularly when using an unfamiliar dataset. We need to address these when we use it, and usually graphs and associated text are sufficient to assuage most of them. Aspects of concern include:\n\nLinearity of explanatory variables. We are concerned with whether the predictors enter in a linear way. We can usually be convinced there is enough linearity in our explanatory variables for our purposes by using graphs of the variables.\nHomoscedasticity of errors. We are concerned that the errors are not becoming systematically larger or smaller throughout the sample. If that is happening, then we term it heteroscedasticity. Again, graphs of errors, such as Figure 12.3 (b), are used to convince us of this.\nIndependence of errors. We are concerned that the errors are not correlated with each other. For instance, if we are interested in weather-related measurement such as average daily temperature, then we may find a pattern because the temperature on one day is likely similar to the temperature on another. We can be convinced that we have satisfied this condition by looking at the residuals compared with observed values, such as Figure 12.3 (c), or estimates compared with actual outcomes, such as Figure 12.3 (d).\nOutliers and other high-impact observations. Finally, we might be worried that our results are being driven by a handful of observations. For instance, thinking back to Chapter 5 and Anscombe’s Quartet, we notice that linear regression estimates would be heavily influenced by the inclusion of one or two particular points. We can become comfortable with this by considering our analysis on various subsets. For instance, randomly removing some observation in the way we did to the US states in Chapter 11.\n\nThose aspects are statistical concerns and relate to whether the model is working. The most important threat to validity and hence the aspect that must be addressed at some length, is whether this model is directly relevant to the research question of interest.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Daniela Witten is the Dorothy Gilford Endowed Chair of Mathematical Statistics and Professor of Statistics & Biostatistics at the University of Washington. After taking a PhD in Statistics from Stanford University in 2010, she joined the University of Washington as an assistant professor. She was promoted to full professor in 2018. One active area of her research is double-dipping which is focused on the effect of sample splitting (Gao, Bien, and Witten 2022). She is an author of the influential Introduction to Statistical Learning (James et al. [2013] 2021). Witten was appointed a Fellow of the American Statistical Association in 2020 and awarded the COPSS Presidents’ Award in 2022."
  },
  {
    "objectID": "12-ijalm.html#sec-inferencewithbayesianmethods",
    "href": "12-ijalm.html#sec-inferencewithbayesianmethods",
    "title": "12  Linear models",
    "section": "12.4 Building models",
    "text": "12.4 Building models\nBreiman (2001) describes two cultures of statistical modeling: one focused on inference and the other on prediction. In general, around the time Breiman (2001) was published, various disciplines tended to focus on either inference or prediction. For instance, Jordan (2004) describes how statistics and computer science had been separate for some time, but how the goals of each field were becoming more closely aligned. The rise of data science, and in particular machine learning has meant there is now a need to be comfortable with both (Neufeld and Witten 2021). The two cultures are being brought closer together, and there is an overlap and interaction between prediction and inference. But their separate evolution means there are still considerable cultural differences. As a small example of this, the term “machine learning” tends to be used in computer science, while the term “statistical learning” tends to be used in statistics, even though they usually refer to the same machinery.\nIn this book we will focus on inference using the probabilistic programming language Stan to fit models in a Bayesian framework, and interface with it using rstanarm. Inference and forecasting have different cultures, ecosystems, and priorities. You should try to develop a comfort in both. One way these different cultures manifest is in language choice. The primary language in this book is R, and for the sake of consistency we focus on that here. But there is an extensive culture, especially but not exclusively focused on prediction, that uses Python. We recommend focusing on only one language and approach initially, but after developing that initial familiarity it is important to become multilingual. We introduce prediction based on Python in Online Appendix H.\nWe will again not go into the details, but running a regression in a Bayesian setting is similar to the frequentist approach that underpins lm(). The main difference from a regression point of view, is that the parameters involved in the models (i.e. \\(\\beta_0\\), \\(\\beta_1\\), etc) are considered random variables, and so themselves have associated probability distributions. In contrast, the frequentist paradigm assumes that any randomness from these coefficients comes from a parametric assumption on the distribution of the error term, \\(\\epsilon\\).\nBefore we run a regression in a Bayesian framework, we need to decide on a starting probability distribution for each of these parameters, which we call a “prior”. While the presence of priors adds some additional complication, it has several advantages, and we will discuss the issue of priors in more detail below. This is another reason for the workflow advocated in this book: the simulation stage leads directly to priors. We will again specify the model that we are interested in, but this time we include priors.\n\\[\n\\begin{aligned}\ny_i|\\mu_i, \\sigma &\\sim \\mbox{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 +\\beta_1x_i\\\\\n\\beta_0 &\\sim \\mbox{Normal}(0, 2.5) \\\\\n\\beta_1 &\\sim \\mbox{Normal}(0, 2.5) \\\\\n\\sigma &\\sim \\mbox{Exponential}(1) \\\\\n\\end{aligned}\n\\]\nWe combine information from the data with the priors to obtain posterior distributions for our parameters. Inference is then carried out based on analyzing posterior distributions.\nAnother aspect that is different between Bayesian approaches and the way we have been doing modeling to this point, is that Bayesian models will usually take longer to run. Because of this, it can be useful to run the model in a separate R script and then save it with saveRDS(). With sensible Quarto chunk options for “eval” and “echo” (see Chapter 3), the model can then be read into the Quarto document with readRDS() rather than being run every time the paper is compiled. In this way, the model delay is only imposed once for a given model. It can also be useful to add beep() from beepr to the end of the model, to get an audio notification when the model is done.\n\nsim_run_data_first_model_rstanarm &lt;-\n  stan_glm(\n    formula = marathon_time ~ five_km_time + was_raining,\n    data = sim_run_data,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5),\n    prior_intercept = normal(location = 0, scale = 2.5),\n    prior_aux = exponential(rate = 1),\n    seed = 853\n  )\n\nbeep()\n\nsaveRDS(\n  sim_run_data_first_model_rstanarm,\n  file = \"sim_run_data_first_model_rstanarm.rds\"\n)\n\n\nsim_run_data_first_model_rstanarm &lt;-\n  readRDS(file = \"sim_run_data_first_model_rstanarm.rds\")\n\nWe use stan_glm() with the “gaussian()” family to specify multiple linear regression and the model formula is written in the same way as base R and `rstanarm``. We have explicitly added the default priors, as we see this as good practice, although strictly this is not necessary.\nThe estimation results, which are in the first column of Table 12.4, are not quite what we expect. For instance, the rate of increase in marathon times is estimated to be around three minutes per minute of increase in five-kilometer time, which seems low given the ratio of a five-kilometer run to marathon distance.\nThe issue of picking priors is a challenging one and the subject of extensive research. For the purposes of this book, using the rstanarm defaults is fine. But even if they are just the default, priors should be explicitly specified in the model and included in the function. This is to make it clear to others what has been done. We could use default_prior_intercept() and default_prior_coef() to find the default priors in rstanarm and then explicitly include them in the model.\nIt is normal to find it difficult to know what prior to specify. Getting started by adapting someone else’s rstanarm code is perfectly fine. If they have not specified their priors, then we can use the helper function prior_summary(), to find out which priors were used.\n\nprior_summary(sim_run_data_first_model_rstanarm)\n\nPriors for model 'sim_run_data_first_model_rstanarm' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n ~ normal(location = [0,0], scale = [2.5,2.5])\n\nAuxiliary (sigma)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nWe are interested in understanding what the priors imply before we involve any data. We do this by implementing prior predictive checks. This means simulating from the priors to look at what the model implies about the possible magnitude and direction of the relationships between the explanatory and outcome variables. This process is no different to all the other simulation that we have done to this point.\n\ndraws &lt;- 1000\n\npriors &lt;-\n  tibble(\n    sigma = rep(rexp(n = draws, rate = 1), times = 16),\n    beta_0 = rep(rnorm(n = draws, mean = 0, sd = 2.5), times = 16),\n    beta_1 = rep(rnorm(n = draws, mean = 0, sd = 2.5), times = 16),\n    five_km_time = rep(15:30, each = draws),\n    mu = beta_0 + beta_1 * five_km_time\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    marathon_time = rnorm(n = 1, mean = mu, sd = sigma)\n  )\n\n\npriors |&gt;\n  ggplot(aes(x = marathon_time)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic()\n\npriors |&gt;\n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point(alpha = 0.1) +\n  theme_classic()\n\n\n\n\n\n\n\n(a) Distribution of implied marathon times\n\n\n\n\n\n\n\n(b) Relationship between 5km and marathon times\n\n\n\n\nFigure 12.5: Some implications from the priors that were used\n\n\n\nFigure 12.5 suggests our model has been poorly constructed. Not only are there world record marathon times, but there are also negative marathon times! One issue is that our prior for \\(\\beta_1\\) does not take in all the information that we know. We know that a marathon is about eight times longer than a five-kilometer run and so we could center the prior for \\(\\beta_1\\) around that. Our re-specified model is:\n\\[\n\\begin{aligned}\ny_i|\\mu_i, \\sigma &\\sim \\mbox{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 +\\beta_1x_i\\\\\n\\beta_0 &\\sim \\mbox{Normal}(0, 2.5) \\\\\n\\beta_1 &\\sim \\mbox{Normal}(8, 2.5) \\\\\n\\sigma &\\sim \\mbox{Exponential}(1) \\\\\n\\end{aligned}\n\\] And we can see from prior prediction checks that it seems more reasonable (Figure 12.6).\n\ndraws &lt;- 1000\n\nupdated_priors &lt;-\n  tibble(\n    sigma = rep(rexp(n = draws, rate = 1), times = 16),\n    beta_0 = rep(rnorm(n = draws, mean = 0, sd = 2.5), times = 16),\n    beta_1 = rep(rnorm(n = draws, mean = 8, sd = 2.5), times = 16),\n    five_km_time = rep(15:30, each = draws),\n    mu = beta_0 + beta_1 * five_km_time\n  ) |&gt;\n  rowwise() |&gt;\n  mutate(\n    marathon_time = rnorm(n = 1, mean = mu, sd = sigma)\n  )\n\n\nupdated_priors |&gt;\n  ggplot(aes(x = marathon_time)) +\n  geom_histogram(binwidth = 10) +\n  theme_classic()\n\nupdated_priors |&gt;\n  ggplot(aes(x = five_km_time, y = marathon_time)) +\n  geom_point(alpha = 0.1) +\n  theme_classic()\n\n\n\n\n\n\n\n(a) Distribution of implied marathon times\n\n\n\n\n\n\n\n(b) Relationship between 5km and marathon times\n\n\n\n\nFigure 12.6: Updated priors\n\n\n\nIf we were not sure what to do then rstanarm could help to improve the specified priors, by scaling them based on the data. Specify the prior that you think is reasonable, even if this is just the default, and include it in the function, but also include “autoscale = TRUE”, and rstanarm will adjust the scale. When we re-run our model with these updated priors and allowing auto-scaling we get much better results, which are in the second column of Table 12.4. You can then add those to the written-out model instead.\n\nsim_run_data_second_model_rstanarm &lt;-\n  stan_glm(\n    formula = marathon_time ~ five_km_time + was_raining,\n    data = sim_run_data,\n    family = gaussian(),\n    prior = normal(location = 8, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  sim_run_data_second_model_rstanarm,\n  file = \"sim_run_data_second_model_rstanarm.rds\"\n)\n\n\nmodelsummary(\n  list(\n    \"Non-scaled priors\" = sim_run_data_first_model_rstanarm,\n    \"Auto-scaling priors\" = sim_run_data_second_model_rstanarm\n  ),\n  fmt = 2\n)\n\n\n\n\n\nTable 12.4:  Forecasting and explanatory models of marathon times based on\nfive-kilometer run times \n  \n    \n    \n       \n      Non-scaled priors\n      Auto-scaling priors\n    \n  \n  \n    (Intercept)\n-67.50\n8.66\n    five_km_time\n3.47\n7.90\n    was_rainingYes\n0.12\n9.23\n    Num.Obs.\n100\n100\n    R2\n0.015\n0.797\n    R2 Adj.\n-1.000\n0.790\n    Log.Lik.\n-678.336\n-425.193\n    ELPD\n-679.5\n-429.0\n    ELPD s.e.\n3.3\n8.9\n    LOOIC\n1359.0\n858.0\n    LOOIC s.e.\n6.6\n17.8\n    WAIC\n1359.0\n857.9\n    RMSE\n175.52\n16.85\n  \n  \n  \n\n\n\n\n\nAs we used the “autoscale = TRUE” option, it can be helpful to look at how the priors were updated with prior_summary() from rstanarm.\n\nprior_summary(sim_run_data_second_model_rstanarm)\n\nPriors for model 'sim_run_data_second_model_rstanarm' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 95)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [8,8], scale = [2.5,2.5])\n  Adjusted prior:\n    ~ normal(location = [8,8], scale = [ 22.64,245.52])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.026)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nHaving built a Bayesian model, we may want to look at what it implies (Figure 12.7). One way to do this is to consider the posterior distribution.\nOne way to use the posterior distribution is to consider whether the model is doing a good job of fitting the data. The idea is that if the model is doing a good job of fitting the data, then the posterior should be able to be used to simulate data that are like the actual data (Gelman et al. 2020). We can implement a posterior predictive check with pp_check() from rstanarm (Figure 12.7 (a)). This compares the actual outcome variable with simulations from the posterior distribution. And we can compare the posterior with the prior with posterior_vs_prior() to see how much the estimates change once data are taken into account (Figure 12.7 (b)). Helpfully, pp_check() and posterior_vs_prior() return ggplot2 objects so we can modify the look of them in the normal way we manipulate graphs. These checks and discussion would typically just be briefly mentioned in the main content of a paper, with the detail and graphs added to a dedicated appendix.\n\npp_check(sim_run_data_second_model_rstanarm) +\n  theme_classic() +\n  theme(legend.position = \"bottom\")\n\nposterior_vs_prior(sim_run_data_second_model_rstanarm) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  coord_flip()\n\n\n\n\n\n\n\n(a) Posterior prediction check\n\n\n\n\n\n\n\n(b) Comparing the posterior with the prior\n\n\n\n\nFigure 12.7: Examining how the model fits, and is affected by, the data\n\n\n\nWith a simple model like this, the differences between the prediction and inference approaches are minimal. But as model or data complexity increases these differences can become important.\nWe have already discussed confidence intervals and the Bayesian equivalent to a confidence interval is called a “credibility interval”, and reflects two points where there is a certain probability mass between them, in this case 95 per cent.  Bayesian estimation provides a distribution for each coefficient. This means there is an infinite number of points that we could use to generate this interval.  The entire distribution should be shown graphically (Figure 12.8). This might be using a cross-referenced appendix.\n\nplot(\n  sim_run_data_second_model_rstanarm,\n  \"areas\"\n)\n\n\n\n\nFigure 12.8: Credible intervals\n\n\n\n\nThe final aspect that we would like to check is a practical matter. rstanarm uses a sampling algorithm called Markov chain Monte Carlo (MCMC) to obtain samples from the posterior distributions of interest. We need to quickly check for the existence of signs that the algorithm ran into issues. We consider a trace plot, such as Figure 12.9 (a), and a Rhat plot, such as Figure 12.9 (b). These would typically go in a cross-referenced appendix.\n\nplot(sim_run_data_second_model_rstanarm, \"trace\")\n\nplot(sim_run_data_second_model_rstanarm, \"rhat\")\n\n\n\n\n\n\n\n(a) Trace plot\n\n\n\n\n\n\n\n(b) Rhat plot\n\n\n\n\nFigure 12.9: Checking the convergence of the MCMC algorithm\n\n\n\nIn the trace plot, we are looking for lines that appear to bounce around, but are horizontal, and have a nice overlap between the chains. The trace plot in Figure 12.9 (a) does not suggest anything out of the ordinary. Similarly, with the Rhat plot, we are looking for everything to be close to 1, and ideally no more than 1.1. Again Figure 12.9 (b) is an example that does not suggest any problems. If these diagnostics are not like this, then simplify the model by removing or modifying predictors, change the priors, and re-run it."
  },
  {
    "objectID": "12-ijalm.html#concluding-remarks",
    "href": "12-ijalm.html#concluding-remarks",
    "title": "12  Linear models",
    "section": "12.5 Concluding remarks",
    "text": "12.5 Concluding remarks\nIn this chapter we have considered linear models. We established a foundation for analysis, and described some essential approaches. We have also skimmed over a lot. This chapter and the next should be considered together. These provide you with enough to get started, but for doing more than that, going through the modeling books recommended in Chapter 17."
  },
  {
    "objectID": "12-ijalm.html#exercises",
    "href": "12-ijalm.html#exercises",
    "title": "12  Linear models",
    "section": "12.6 Exercises",
    "text": "12.6 Exercises\n\nScales\n\n(Plan) Consider the following scenario: A person is interested in the heights of all the buildings in London. They walk around the city counting the number of floors for each building, and noting the year of construction. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation, along with three predictors that are associated the count of the number of levels. Please include at least ten tests based on the simulated data. Submit a link to a GitHub Gist that contains your code.\n(Acquire) Please describe one possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Then use rstanarm to build a model with number of floors as an outcome and the year of construction as a predictor. Submit a link to a GitHub Gist that contains your code.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nPlease simulate the situation in which there are two predictors, “race” and “gender”, and one outcome variable, “vote_preference”, which is imperfectly related to them. Submit a link to a GitHub Gist that contains your code.\nPlease write a linear relationship between some outcome variable, Y, and some predictor, X. What is the intercept term? What is the slope term? What would adding a hat to these indicate?\nWhich of the following are examples of linear models (select all that apply)?\n\nlm(y ~ x_1 + x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_2^2 + x_3, data = my_data)\nlm(y ~ x_1 * x_2 + x_3, data = my_data)\nlm(y ~ x_1 + x_1^2 + x_2 + x_3, data = my_data)\n\nWhat is the least squares criterion? Similarly, what is RSS and what are we trying to do when we run least squares regression?\nWhat is bias (in a statistical context)?\nConsider five variables: earth, fire, wind, water, and heart. Please simulate a scenario where heart depends on the other four, which are independent of each other. Then please write R code that would fit a linear regression model to explain heart as a function of the other variables. Submit a link to a GitHub Gist that contains your code.\nAccording to Greenland et al. (2016), p-values test (pick one):\n\nAll the assumptions about how the data were generated (the entire model), not just the targeted hypothesis it is supposed to test (such as a null hypothesis).\nWhether the hypothesis targeted for testing is true or not.\nA dichotomy whereby results can be declared “statistically significant”.\n\nAccording to Greenland et al. (2016), a p-value may be small because (select all that apply):\n\nThe targeted hypothesis is false.\nThe study protocols were violated.\nIt was selected for presentation based on its small size.\n\nPlease explain what a p-value is, using only the term itself (i.e. “p-value”) and words that are amongst the 1,000 most common in the English language according to the XKCD Simple Writer. (Please write one or two paragraphs.)\nWhat is power (in a statistical context)?\nLook at the list of people awarded the COPSS Presidents’ Award or the Guy Medal in Gold and write a short biography of them in the style of the “Shoulders of giants” entries in this book.\nDiscuss, with the help of examples and citations, the quote from McElreath ([2015] 2020, 162) included toward the start of this chapter. Write at least three paragraphs.\n\n\n\nTutorial\nAllow that the true data generating process is a Normal distribution with mean of one, and standard deviation of 1. We obtain a sample of 1,000 observations using some instrument. Simulate the following situation:\n\nUnknown to us, the instrument has a mistake in it, which means that it has a maximum memory of 900 observations, and begins over-writing at that point, so the final 100 observations are actually a repeat of the first 100.\nWe employ a research assistant to clean and prepare the dataset. During the process of doing this, unknown to us, they accidentally change half of the negative draws to be positive.\nThey additionally, accidentally, change the decimal place on any value between 1 and 1.1, so that, for instance 1 becomes 0.1, and 1.1 would become 0.11.\nYou finally get the cleaned dataset and are interested in understanding whether the mean of the true data generating process is greater than 0.\n\nWrite at least two pages about what you did and what you found. Also discuss what effect the issues had, and what steps you can put in place to ensure actual analysis has a chance to flag some of these issues.\nUse Quarto, and include an appropriate title, author, date, link to a GitHub repo, and citations to produce a draft. Following this, please pair with another student and exchange your written work. Update it based on their feedback, and be sure to acknowledge them by name in your paper. Submit a PDF.\n\n\nPaper\nAt about this point the Murrumbidgee Paper from Online Appendix D would be appropriate.\n\n\n\n\nAltman, Douglas, and Martin Bland. 1995. “Statistics notes: The normal distribution.” BMJ 310 (6975): 298–98. https://doi.org/10.1136/bmj.310.6975.298.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nBååth, Rasmus. 2018. beepr: Easily Play Notification Sounds on any Platform. https://CRAN.R-project.org/package=beepr.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic, Xiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big Surveys Significantly Overestimated US Vaccine Uptake.” Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nCox, David. 2018. “In Gentle Praise of Significance Tests.” YouTube, October. https://youtu.be/txLj%5FP9UlCQ.\n\n\nCox, David, and Nancy Reid. 1987. “Parameter Orthogonality and Approximate Conditional Inference.” Journal of the Royal Statistical Society: Series B (Methodological) 49 (1): 1–18. https://doi.org/10.1111/j.2517-6161.1987.tb01422.x.\n\n\nFisher, Ronald. (1925) 1928. Statistical Methods for Research Workers. 2nd ed. London: Oliver; Boyd.\n\n\nGao, Lucy, Jacob Bien, and Daniela Witten. 2022. “Selective Inference for Hierarchical Clustering.” Journal of the American Statistical Association, October, 1–11. https://doi.org/10.1080/01621459.2022.2116331.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. 2020. “Bayesian Workflow.” arXiv. https://doi.org/10.48550/arXiv.2011.01808.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023. “rstanarm: Bayesian applied regression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGreenland, Sander, Stephen Senn, Kenneth Rothman, John Carlin, Charles Poole, Steven Goodman, and Douglas Altman. 2016. “Statistical Tests, P values, Confidence Intervals, and Power: A Guide to Misinterpretations.” European Journal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nIoannidis, John. 2005. “Why Most Published Research Findings Are False.” PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2013) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJordan, Michael. 2004. “Graphical Models.” Statistical Science 19 (1). https://doi.org/10.1214/088342304000000026.\n\n\nKnutson, Victoria, Serge Aleshin-Guendel, Ariel Karlinsky, William Msemburi, and Jon Wakefield. 2022. “Estimating Global and Country-Specific Excess Mortality During the COVID-19 Pandemic,” May. https://cdn.who.int/media/docs/default-source/world-health-data-platform/covid-19-excessmortality/covid-methods-paper-revision.pdf.\n\n\nKuhn, Max, and Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nNelder, John. 1999. “From Statistics to Statistical Science.” Journal of the Royal Statistical Society: Series D (The Statistician) 48 (2): 257–69. https://doi.org/10.1111/1467-9884.00187.\n\n\nNeufeld, Anna, and Daniela Witten. 2021. “Discussion of Breiman’s \"Two Cultures\": From Two Cultures to One.” Observational Studies 7 (1): 171–74. https://doi.org/10.1353/obs.2021.0004.\n\n\nPitman, Jim. 1993. Probability. 1st ed. New York: Springer. https://doi.org/10.1007/978-1-4612-4374-8.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of Inference.” The Annals of Statistics 31 (6): 1695–1731. https://doi.org/10.1214/aos/1074290325.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nStaicu, Ana-Maria. 2017. “Interview with Nancy Reid.” International Statistical Review 85 (3): 381–403. https://doi.org/10.1111/insr.12237.\n\n\nStigler, Stephen. 1986. The History of Statistics. Massachusetts: Belknap Harvard.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nVarin, Cristiano, Nancy Reid, and David Firth. 2011. “An Overview of Composite Likelihood Methods.” Statistica Sinica, 5–42. https://www.jstor.org/stable/24309261.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley. 2011. “testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Lionel Henry. 2022. purrr: Functional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWilde, Oscar. 1891. The Picture of Dorian Gray. https://www.gutenberg.org/files/174/174-h/174-h.htm.\n\n\nWood, Simon. 2015. Core Statistics. Cambridge University Press. https://www.maths.ed.ac.uk/\\%7Eswood34/core-statistics.pdf.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "12-ijalm.html#footnotes",
    "href": "12-ijalm.html#footnotes",
    "title": "12  Linear models",
    "section": "",
    "text": "Given the small sample size in the example of 20, we strictly should use the finite-sample adjustment, but as this is not the focus of this book we will move forward with the general approach.↩︎"
  },
  {
    "objectID": "13-ijaglm.html#introduction",
    "href": "13-ijaglm.html#introduction",
    "title": "13  Generalized linear models",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nLinear models, covered in Chapter 12, have evolved substantially over the past century. Francis Galton, mentioned in Chapter 8, and others of his generation used linear regression in earnest in the late 1800s and early 1900s. Binary outcomes quickly became of interest and needed special treatment, leading to the development and wide adaption of logistic regression and similar methods in the mid-1900s (Cramer 2003). The generalized linear model framework came into being, in a formal sense, in the 1970s with Nelder and Wedderburn (1972). Generalized linear models (GLMs) broaden the types of outcomes that are allowed. We still model outcomes as a linear function, but we are less constrained. The outcome can be anything in the exponential family, and popular choices include the logistic distribution and the Poisson distribution. For the sake of a completed story but turning to approaches that are beyond the scope of this book, a further generalization of GLMs is generalized additive models (GAMs) where we broaden the structure of the explanatory side. We still explain the outcome variable as an additive function of various bits and pieces, but those bits and pieces can be functions. This framework was proposed in the 1990s by Hastie and Tibshirani (1990).\nIn terms of generalized linear models, in this chapter we consider logistic, Poisson, and negative binomial regression. But we also explore a variant that is relevant to both linear models and generalized linear models: multilevel modeling. This is when we take advantage of some type of grouping that exists within our dataset."
  },
  {
    "objectID": "13-ijaglm.html#logistic-regression",
    "href": "13-ijaglm.html#logistic-regression",
    "title": "13  Generalized linear models",
    "section": "13.2 Logistic regression",
    "text": "13.2 Logistic regression\nLinear regression is a useful way to better understand our data. But it assumes a continuous outcome variable that can take any number on the real line. We would like some way to use this same machinery when we cannot satisfy this condition. We turn to logistic and Poisson regression for binary and count outcome variables, respectively. They are still linear models, because the predictor variables enter in a linear fashion.\nLogistic regression, and its close variants, are useful in a variety of settings, from elections (Wang et al. 2015) through to horse racing (Chellel 2018; Bolton and Chapman 1986). We use logistic regression when the outcome variable is a binary outcome, such as 0 or 1, or “yes” or “no”. Although the presence of a binary outcome variable may sound limiting, there are a lot of circumstances in which the outcome either naturally falls into this situation or can be adjusted into it. For instance, win or lose, available or not available, support or not.\nThe foundation of this is the Bernoulli distribution. There is a certain probability, \\(p\\), of outcome “1” and the remainder, \\(1-p\\), for outcome “0”. We can use rbinom() with one trial (“size = 1”) to simulate data from the Bernoulli distribution.\n\nset.seed(853)\n\nbernoulli_example &lt;-\n  tibble(draws = rbinom(n = 20, size = 1, prob = 0.1))\n\nbernoulli_example |&gt; pull(draws)\n\n [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n\n\nOne reason to use logistic regression is that we will be modeling a probability, hence it will be bounded between 0 and 1. With linear regression we may end up with values outside this. The foundation of logistic regression is the logit function:\n\\[\n\\mbox{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right).\n\\] This will transpose values between 0 and 1 onto the real line. For instance, logit(0.1) = -2.2, logit(0.5) = 0, and logit(0.9) = 2.2 (Figure 13.1). We call this the “link function”. It relates the distribution of interest in a generalized linear model to the machinery we use in linear models.\n\n\n\n\n\nFigure 13.1: Example of the logit function for values between 0 and 1\n\n\n\n\n\n13.2.1 Simulated example: day or night\nTo illustrate logistic regression, we will simulate data on whether it is a weekday or weekend, based on the number of cars on the road. We will assume that on weekdays the road is busier.\n\nset.seed(853)\n\nweek_or_weekday &lt;-\n  tibble(\n    num_cars = sample.int(n = 100, size = 1000, replace = TRUE),\n    noise = rnorm(n = 1000, mean = 0, sd = 10),\n    is_weekday = if_else(num_cars + noise &gt; 50, 1, 0)\n  ) |&gt;\n  select(-noise)\n\nweek_or_weekday\n\n# A tibble: 1,000 × 2\n   num_cars is_weekday\n      &lt;int&gt;      &lt;dbl&gt;\n 1        9          0\n 2       64          1\n 3       90          1\n 4       93          1\n 5       17          0\n 6       29          0\n 7       84          1\n 8       83          1\n 9        3          0\n10       33          1\n# ℹ 990 more rows\n\n\nWe can use glm() from base R to do a quick estimation. In this case we will try to work out whether it is a weekday or weekend, based on the number of cars we can see. We are interested in estimating Equation 13.1:\n\\[\n\\mbox{Pr}(y_i=1) = \\mbox{logit}^{-1}\\left(\\beta_0+\\beta_1 x_i\\right)\n\\tag{13.1}\\]\nwhere \\(y_i\\) is whether it is a weekday and \\(x_i\\) is the number of cars on the road.\n\nweek_or_weekday_model &lt;-\n  glm(\n    is_weekday ~ num_cars,\n    data = week_or_weekday,\n    family = \"binomial\"\n  )\n\nsummary(week_or_weekday_model)\n\n\nCall:\nglm(formula = is_weekday ~ num_cars, family = \"binomial\", data = week_or_weekday)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -9.48943    0.74492  -12.74   &lt;2e-16 ***\nnum_cars     0.18980    0.01464   12.96   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1386.26  on 999  degrees of freedom\nResidual deviance:  337.91  on 998  degrees of freedom\nAIC: 341.91\n\nNumber of Fisher Scoring iterations: 7\n\n\nThe estimated coefficient on the number of cars is 0.19. The interpretation of coefficients in logistic regression is more complicated than linear regression as they relate to changes in the log-odds of the binary outcome. For instance, the estimate of 0.19 is the average change in the log-odds of it being a weekday with observing one extra car on the road. The coefficient is positive which means an increase. As it is non-linear, if we want to specify a particular change, then this will be different for different baseline levels of the observation. That is, an increase of 0.19 log-odds has a larger impact when the baseline log-odds are 0, compared to 2.\nWe can translate our estimate into the probability of it being a weekday, for a given number of cars. We can add the implied probability that it is a weekday for each observation using predictions() from marginaleffects.\n\nweek_or_weekday_predictions &lt;-\n  predictions(week_or_weekday_model) |&gt;\n  as_tibble()\n\nweek_or_weekday_predictions\n\n# A tibble: 1,000 × 7\n   rowid estimate  p.value  conf.low conf.high is_weekday num_cars\n   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;int&gt;\n 1     1 0.000417 1.40e-36 0.000125   0.00139           0        9\n 2     2 0.934    9.33e-27 0.898      0.959             1       64\n 3     3 0.999    1.97e-36 0.998      1.00              1       90\n 4     4 1.00     1.10e-36 0.999      1.00              1       93\n 5     5 0.00190  1.22e-35 0.000711   0.00508           0       17\n 6     6 0.0182   3.34e-32 0.00950    0.0348            0       29\n 7     7 0.998    1.00e-35 0.996      0.999             1       84\n 8     8 0.998    1.42e-35 0.995      0.999             1       83\n 9     9 0.000134 5.22e-37 0.0000338  0.000529          0        3\n10    10 0.0382   1.08e-29 0.0222     0.0649            1       33\n# ℹ 990 more rows\n\n\nAnd we can then graph the probability that our model implies, for each observation, of it being a weekday (Figure 13.2). This is a nice opportunity to consider a few different ways of illustrating the fit. While it is common to use a scatterplot (Figure 13.2 (a)), this is also an opportunity to use an ECDF (Figure 13.2 (b)).\n\n# Panel (a)\nweek_or_weekday_predictions |&gt;\n  mutate(is_weekday = factor(is_weekday)) |&gt;\n  ggplot(aes(x = num_cars, y = estimate, color = is_weekday)) +\n  geom_jitter(width = 0.01, height = 0.01, alpha = 0.3) +\n  labs(\n    x = \"Number of cars that were seen\",\n    y = \"Estimated probability it is a weekday\",\n    color = \"Was actually weekday\"\n  ) +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n# Panel (b)\nweek_or_weekday_predictions |&gt;\n  mutate(is_weekday = factor(is_weekday)) |&gt;\n  ggplot(aes(x = num_cars, y = estimate, color = is_weekday)) +\n  stat_ecdf(geom = \"point\", alpha = 0.75) +\n  labs(\n    x = \"Number of cars that were seen\",\n    y = \"Estimated probability it is a weekday\",\n    color = \"Actually weekday\"\n  ) +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) Illustrating the fit with a scatterplot\n\n\n\n\n\n\n\n(b) Illustrating the fit with an ECDF\n\n\n\n\nFigure 13.2: Logistic regression probability results with simulated data of whether it is a weekday or weekend based on the number of cars that are around\n\n\n\nThe marginal effect at each observation is of interest because it provides a sense of how this probability is changing. It enables us to say that at the median (which in this case is if we were to see 50 cars) the probability of it being a weekday increases by almost five per cent if we were to see another car (Table 13.1).\n\nslopes(week_or_weekday_model, newdata = \"median\") |&gt;\n  select(term, estimate, std.error) |&gt;\n  kable(\n    col.names = c(\"Term\", \"Estimate\", \"Standard error\"),\n    digits = 3, booktabs = TRUE\n  )\n\n\n\nTable 13.1: Marginal effect of another car on the probability that it is a weekday, at the median\n\n\nTerm\nEstimate\nStandard error\n\n\n\n\nnum_cars\n0.047\n0.004\n\n\n\n\n\n\nTo more thoroughly examine the situation we might want to build a Bayesian model using rstanarm. As in Chapter 12 we will specify priors for our model, but these will just be the default priors that rstanarm uses:\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+\\beta_1 x_i \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 2.5)\n\\end{aligned}\n\\] where \\(y_i\\) is whether it is a weekday (actually 0 or 1), \\(x_i\\) is the number of cars on the road, and \\(\\pi_i\\) is the probability that observation \\(i\\) is a weekday.\n\nweek_or_weekday_rstanarm &lt;-\n  stan_glm(\n    is_weekday ~ num_cars,\n    data = week_or_weekday,\n    family = binomial(link = \"logit\"),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  week_or_weekday_rstanarm,\n  file = \"week_or_weekday_rstanarm.rds\"\n)\n\nThe results of our Bayesian model are similar to the quick model we built using base (Table 13.2).\n\nmodelsummary(\n  list(\n    \"Day or night\" = week_or_weekday_rstanarm\n  )\n)\n\n\n\n\n\nTable 13.2:  Explaining whether it is day or night, based on the number of cars on\nthe road \n  \n    \n    \n       \n      Day or night\n    \n  \n  \n    (Intercept)\n-9.464\n    number_of_cars\n0.186\n    Num.Obs.\n1000\n    R2\n0.779\n    Log.Lik.\n-177.899\n    ELPD\n-179.8\n    ELPD s.e.\n13.9\n    LOOIC\n359.6\n    LOOIC s.e.\n27.9\n    WAIC\n359.6\n    RMSE\n0.24\n  \n  \n  \n\n\n\n\n\nTable 13.2 makes it clear that each of the approaches is similar in this case. They agree on the direction of the effect of seeing an extra car on the probability of it being a weekday. Even the magnitude of the effect is estimated to be similar.\n\n\n13.2.2 Political support in the United States\nOne area where logistic regression is often used is political polling. In many cases voting implies the need for one preference ranking, and so issues are reduced, whether appropriately or not, to “support” or “not support”.\nAs a reminder, the workflow we advocate in this book is:\n\\[\\mbox{Plan} \\rightarrow \\mbox{Simulate} \\rightarrow \\mbox{Acquire} \\rightarrow \\mbox{Explore} \\rightarrow \\mbox{Share}\\]\nWhile the focus here is the exploration of data using models, we still need to do the other aspects. We begin by planning. In this case, we are interested in US political support. In particular we are interested in whether we can forecast who a respondent is likely to vote for, based only on knowing their highest level of education and gender. That means we are interested in a dataset with variables for who an individual voted for, and some of their characteristics, such as gender and education. A quick sketch of such a dataset is Figure 13.3 (a). We would like our model to average over these points. A quick sketch is Figure 13.3 (b).\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset that could be used to examine US political support\n\n\n\n\n\n\n\n(b) Quick sketch of what we expect from the analysis before finalizing either the data or the analysis\n\n\n\n\nFigure 13.3: Sketches of the expected dataset and analysis focus and clarify our thinking even if they will be updated later\n\n\nWe will simulate a dataset where the chance that a person supports Biden depends on their gender and education.\n\nset.seed(853)\n\nnum_obs &lt;- 1000\n\nus_political_preferences &lt;- tibble(\n  education = sample(0:4, size = num_obs, replace = TRUE),\n  gender = sample(0:1, size = num_obs, replace = TRUE),\n  support_prob = ((education + gender) / 5),\n) |&gt;\n  mutate(\n    supports_biden = if_else(runif(n = num_obs) &lt; support_prob, \"yes\", \"no\"),\n    education = case_when(\n      education == 0 ~ \"&lt; High school\",\n      education == 1 ~ \"High school\",\n      education == 2 ~ \"Some college\",\n      education == 3 ~ \"College\",\n      education == 4 ~ \"Post-grad\"\n    ),\n    gender = if_else(gender == 0, \"Male\", \"Female\")\n  ) |&gt;\n  select(-support_prob, supports_biden, gender, education)\n\nFor the actual data we can use the 2020 Cooperative Election Study (CES) (Schaffner, Ansolabehere, and Luks 2021). This is a long-standing annual survey of US political opinion. In 2020, there were 61,000 respondents who completed the post-election survey. The sampling methodology, detailed in Ansolabehere, Schaffner, and Luks (2021, 13), relies on matching and is an accepted approach that balances sampling concerns and cost.\nWe can access the CES using get_dataframe_by_name() after installing and loading dataverse. This approach was introduced in Chapter 7 and Chapter 10. We save the data that are of interest to us, and then refer to that saved dataset.\n\nces2020 &lt;-\n  get_dataframe_by_name(\n    filename = \"CES20_Common_OUTPUT_vv.csv\",\n    dataset = \"10.7910/DVN/E9N6PH\",\n    server = \"dataverse.harvard.edu\",\n    .f = read_csv\n  ) |&gt;\n  select(votereg, CC20_410, gender, educ)\n\nwrite_csv(ces2020, \"ces2020.csv\")\n\n\nces2020 &lt;-\n  read_csv(\n    \"ces2020.csv\",\n    col_types =\n      cols(\n        \"votereg\" = col_integer(),\n        \"CC20_410\" = col_integer(),\n        \"gender\" = col_integer(),\n        \"educ\" = col_integer()\n      )\n  )\n\nces2020\n\n\n\n# A tibble: 61,000 × 4\n   votereg CC20_410 gender  educ\n     &lt;int&gt;    &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n 1       1        2      1     4\n 2       2       NA      2     6\n 3       1        1      2     5\n 4       1        1      2     5\n 5       1        4      1     5\n 6       1        2      1     3\n 7       2       NA      1     3\n 8       1        2      2     3\n 9       1        2      2     2\n10       1        1      2     5\n# ℹ 60,990 more rows\n\n\nWhen we look at the actual data, there are concerns that we did not anticipate in our sketches. We use the codebook to investigate this more thoroughly. We only want respondents who are registered to vote, and we are only interested in those that voted for either Biden or Trump. We see that when the variable “CC20_410” is 1, then this means the respondent supported Biden, and when it is 2 that means Trump. We can filter to only those respondents and then add more informative labels. Genders of “female” and “male” is what is available from the CES, and when the variable “gender” is 1, then this means “male”, and when it is 2 this means “females”. Finally, the codebook tells us that “educ” is a variable from 1 to 6, in increasing levels of education.\n\nces2020 &lt;-\n  ces2020 |&gt;\n  filter(votereg == 1,\n         CC20_410 %in% c(1, 2)) |&gt;\n  mutate(\n    voted_for = if_else(CC20_410 == 1, \"Biden\", \"Trump\"),\n    voted_for = as_factor(voted_for),\n    gender = if_else(gender == 1, \"Male\", \"Female\"),\n    education = case_when(\n      educ == 1 ~ \"No HS\",\n      educ == 2 ~ \"High school graduate\",\n      educ == 3 ~ \"Some college\",\n      educ == 4 ~ \"2-year\",\n      educ == 5 ~ \"4-year\",\n      educ == 6 ~ \"Post-grad\"\n    ),\n    education = factor(\n      education,\n      levels = c(\n        \"No HS\",\n        \"High school graduate\",\n        \"Some college\",\n        \"2-year\",\n        \"4-year\",\n        \"Post-grad\"\n      )\n    )\n  ) |&gt;\n  select(voted_for, gender, education)\n\nIn the end we are left with 43,554 respondents (Figure 13.4).\n\nces2020 |&gt;\n  ggplot(aes(x = education, fill = voted_for)) +\n  stat_count(position = \"dodge\") +\n  facet_wrap(facets = vars(gender)) +\n  theme_minimal() +\n  labs(\n    x = \"Highest education\",\n    y = \"Number of respondents\",\n    fill = \"Voted for\"\n  ) +\n  coord_flip() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 13.4: The distribution of presidential preferences, by gender, and highest education\n\n\n\n\nThe model that we are interested in is:\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+\\beta_1 \\times \\mbox{gender}_i + \\beta_2 \\times \\mbox{education}_i \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\beta_2 & \\sim \\mbox{Normal}(0, 2.5)\n\\end{aligned}\n\\]\nwhere \\(y_i\\) is the political preference of the respondent and equal to 1 if Biden and 0 if Trump, \\(\\mbox{gender}_i\\) is the gender of the respondent, and \\(\\mbox{education}_i\\) is the education of the respondent. We could estimate the parameters using stan_glm(). Note that the model is a generally accepted short-hand. In practice rstanarm converts categorical variables into a series of indicator variables and there are multiple coefficients estimated. In the interest of run-time we will randomly sample 1,000 observations and fit the model on that, rather than the full dataset.\n\nset.seed(853)\n\nces2020_reduced &lt;- \n  ces2020 |&gt; \n  slice_sample(n = 1000)\n\npolitical_preferences &lt;-\n  stan_glm(\n    voted_for ~ gender + education,\n    data = ces2020_reduced,\n    family = binomial(link = \"logit\"),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = \n      normal(location = 0, scale = 2.5, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  political_preferences,\n  file = \"political_preferences.rds\"\n)\n\n\npolitical_preferences &lt;-\n  readRDS(file = \"political_preferences.rds\")\n\nThe results of our model are interesting. They suggest males were less likely to vote for Biden, and that there is a considerable effect of education (Table 13.3).\n\nmodelsummary(\n  list(\n    \"Support Biden\" = political_preferences\n  ),\n  statistic = \"mad\"\n  )\n\n\n\n\n\nTable 13.3:  Whether a respondent is likely to vote for Biden based on their\ngender and education \n  \n    \n    \n       \n      Support Biden\n    \n  \n  \n    (Intercept)\n-0.745\n    \n(0.517)\n    genderMale\n-0.477\n    \n(0.136)\n    educationHigh school graduate\n0.617\n    \n(0.534)\n    educationSome college\n1.494\n    \n(0.541)\n    education2-year\n0.954\n    \n(0.538)\n    education4-year\n1.801\n    \n(0.532)\n    educationPost-grad\n1.652\n    \n(0.541)\n    Num.Obs.\n1000\n    R2\n0.064\n    Log.Lik.\n-646.335\n    ELPD\n-653.5\n    ELPD s.e.\n9.4\n    LOOIC\n1307.0\n    LOOIC s.e.\n18.8\n    WAIC\n1307.0\n    RMSE\n0.48\n  \n  \n  \n\n\n\n\n\nIt can be useful to plot the credibility intervals of these predictors (Figure 13.5). In particular this might be something that is especially useful in an appendix.\n\nmodelplot(political_preferences, conf_level = 0.9) +\n  labs(x = \"90 per cent credibility interval\")\n\n\n\n\nFigure 13.5: Credible intervals for predictors of support for Biden"
  },
  {
    "objectID": "13-ijaglm.html#poisson-regression",
    "href": "13-ijaglm.html#poisson-regression",
    "title": "13  Generalized linear models",
    "section": "13.3 Poisson regression",
    "text": "13.3 Poisson regression\nWhen we have count data we should initially think to take advantage of the Poisson distribution. One application of Poisson regression is modeling the outcomes of sports. For instance Burch (2023) builds a Poisson model of hockey outcomes, following Baio and Blangiardo (2010) who build a Poisson model of football outcomes.\nThe Poisson distribution is governed by one parameter, \\(\\lambda\\). This distributes probabilities over non-negative integers and hence governs the shape of the distribution. As such, the Poisson distribution has the interesting feature that the mean is also the variance. As the mean increases, so does the variance. The Poisson probability mass function is (Pitman 1993, 121):\n\\[P_{\\lambda}(k) = e^{-\\lambda}\\lambda^k/k!\\mbox{, for }k=0,1,2,\\dots\\] We can simulate \\(n=20\\) draws from the Poisson distribution with rpois(), where \\(\\lambda\\) is equal to three.\n\nrpois(n = 20, lambda = 3)\n\n [1] 3 1 5 9 1 5 2 3 3 2 4 3 3 4 2 2 3 3 3 3\n\n\nWe can also look at what happens to the distribution as we change the value of \\(\\lambda\\) (Figure 13.6).\n\n\n\n\n\nFigure 13.6: The Poisson distribution is governed by the value of the mean, which is the same as its variance\n\n\n\n\n\n13.3.1 Simulated example: number of As by department\nTo illustrate the situation, we could simulate data about the number of As that are awarded in each university course. In this simulated example, we consider three departments, each of which has many courses. Each course will award a different number of As.\n\nset.seed(853)\n\nclass_size &lt;- 26\n\ncount_of_A &lt;-\n  tibble(\n    # From Chris DuBois: https://stackoverflow.com/a/1439843\n    department = \n      c(rep.int(\"1\", 26), rep.int(\"2\", 26), rep.int(\"3\", 26)),\n    course = c(\n      paste0(\"DEP_1_\", letters),\n      paste0(\"DEP_2_\", letters),\n      paste0(\"DEP_3_\", letters)\n    ),\n    number_of_As = c(\n      rpois(n = class_size, lambda = 5),\n      rpois(n = class_size, lambda = 10),\n      rpois(n = class_size, lambda = 20)\n    )\n  )\n\n\ncount_of_A |&gt;\n  ggplot(aes(x = number_of_As)) +\n  geom_histogram(aes(fill = department), position = \"dodge\") +\n  labs(\n    x = \"Number of As awarded\",\n    y = \"Number of classes\",\n    fill = \"Department\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 13.7: Simulated number of As in various classes across three departments\n\n\n\n\nOur simulated dataset has the number of As awarded by courses, which are structured within departments (Figure 13.7). In Chapter 15, we will take advantage of this departmental structure, but for now we just ignore it and focus on differences between departments.\nThe model that we are interested in estimating is:\n\\[\n\\begin{aligned}\ny_i|\\lambda_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) & = \\beta_0 + \\beta_1 \\times \\mbox{department}_i\n\\end{aligned}\n\\] where \\(y_i\\) is the number of A grades awarded, and we are interested in how this differs by department.\nWe can use glm() from base R to get a quick sense of the data. This function is quite general, and we specify Poisson regression by setting the “family” parameter. The estimates are contained in the first column of Table 13.4.\n\ngrades_base &lt;-\n  glm(\n    number_of_As ~ department,\n    data = count_of_A,\n    family = \"poisson\"\n  )\n\nsummary(grades_base)\n\n\nCall:\nglm(formula = number_of_As ~ department, family = \"poisson\", \n    data = count_of_A)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.3269     0.1010  13.135  &lt; 2e-16 ***\ndepartment2   0.8831     0.1201   7.353 1.94e-13 ***\ndepartment3   1.7029     0.1098  15.505  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 426.201  on 77  degrees of freedom\nResidual deviance:  75.574  on 75  degrees of freedom\nAIC: 392.55\n\nNumber of Fisher Scoring iterations: 4\n\n\nAs with logistic regression, the interpretation of the coefficients from Poisson regression can be difficult. The interpretation of the coefficient on “department2” is that it is the log of the expected difference between departments. We expect \\(e^{0.883} \\approx 2.4\\) and \\(e^{1.703} \\approx 5.5\\) as many A grades in departments 2 and 3, respectively, compared with department 1 (Table 13.4).\nWe could build a Bayesian model and estimate it with rstanarm (Table 13.4).\n\\[\n\\begin{aligned}\ny_i|\\lambda_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) & = \\beta_0 + \\beta_1 \\times\\mbox{department}_i\\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 2.5)\n\\end{aligned}\n\\] where \\(y_i\\) is the number of As awarded.\n\ngrades_rstanarm &lt;-\n  stan_glm(\n    number_of_As ~ department,\n    data = count_of_A,\n    family = poisson(link = \"log\"),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  grades_rstanarm,\n  file = \"grades_rstanarm.rds\"\n)\n\nThe results are in Table 13.4.\n\nmodelsummary(\n  list(\n    \"Number of As\" = grades_rstanarm\n  )\n)\n\n\n\n\n\nTable 13.4:  Examining the number of A grades given in different departments \n  \n    \n    \n       \n      Number of As\n    \n  \n  \n    (Intercept)\n1.321\n    department2\n0.884\n    department3\n1.706\n    Num.Obs.\n78\n    Log.Lik.\n-193.355\n    ELPD\n-196.2\n    ELPD s.e.\n7.7\n    LOOIC\n392.4\n    LOOIC s.e.\n15.4\n    WAIC\n392.4\n    RMSE\n3.41\n  \n  \n  \n\n\n\n\n\nAs with logistic regression, we can use slopes() from marginaleffects to help with interpreting these results. It may be useful to consider how we expect the number of A grades to change as we go from one department to another. Table 13.5 suggests that in our dataset, classes in Department 2 tend to have around five additional A grades, compared with Department 1, and that classes in Department 3 tend to have around 17 more A grades, compared with Department 1.\n\nslopes(grades_rstanarm) |&gt;\n  summary() |&gt;\n  select(-term) |&gt;\n  kable(\n    col.names = c(\"Compare department\", \"Estimate\", \"2.5%\", \"97.5%\"),\n    digits = 2, booktabs = TRUE, linesep = \"\"\n  )\n\n\n\nTable 13.5: The estimated difference in the number of A grades awarded at each department\n\n\nCompare department\nEstimate\n2.5%\n97.5%\n\n\n\n\nmean(2) - mean(1)\n5.32\n4.01\n6.70\n\n\nmean(3) - mean(1)\n16.92\n15.10\n18.84\n\n\n\n\n\n\n\n\n13.3.2 Letters used in Jane Eyre\nIn an earlier age, Edgeworth (1885) made counts of the dactyls in Virgil’s Aeneid (Stigler (1978, 301) provides helpful background and the dataset is available using Dactyl from HistData (Friendly 2021)). Inspired by this we could use gutenbergr to get the text of Jane Eyre by Charlotte Brontë. (Recall that in Chapter 7 we converted PDFs of Jane Eyre into a dataset.) We could then consider the first ten lines of each chapter, count the number of words, and count the number of times either “E” or “e” appears. We are interested to see whether the number of e/Es increases as more words are used. If not, it could suggest that the distribution of e/Es is not consistent, which could be of interest to linguists.\nFollowing the workflow advocated in this book, we first sketch our dataset and model. A quick sketch of what the dataset could look like is Figure 13.12 (a), and a quick sketch of our model is Figure 13.12 (b).\n\n\n\n\n\n\n\n(a) Planned counts, by line and chapter, in Jane Eyre\n\n\n\n\n\n\n\n(b) Expected relationship between count of e/Es and number of words in the line\n\n\n\n\nFigure 13.8: Sketches of the expected dataset and analysis force us to consider what we are interested in\n\n\nWe simulate a dataset of how the number of e/Es could be distributed following the Poisson distribution (Figure 13.9).\n\ncount_of_e_simulation &lt;-\n  tibble(\n    chapter = c(rep(1, 10), rep(2, 10), rep(3, 10)),\n    line = rep(1:10, 3),\n    number_words_in_line = runif(min = 0, max = 15, n = 30) |&gt; round(0),\n    number_e = rpois(n = 30, lambda = 10)\n  )\n\ncount_of_e_simulation |&gt;\n  ggplot(aes(y = number_e, x = number_words_in_line)) +\n  geom_point() +\n  labs(\n    x = \"Number of words in line\",\n    y = \"Number of e/Es in the first ten lines\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nFigure 13.9: Simulated counts of e/Es\n\n\n\n\nWe can now gather and prepare our data. We download the text of the book from Project Gutenberg using gutenberg_download() from gutenbergr.\n\ngutenberg_id_of_janeeyre &lt;- 1260\n\njane_eyre &lt;-\n  gutenberg_download(\n    gutenberg_id = gutenberg_id_of_janeeyre,\n    mirror = \"https://gutenberg.pglaf.org/\"\n  )\n\njane_eyre\n\nwrite_csv(jane_eyre, \"jane_eyre.csv\")\n\nWe will download it and then use our local copy to avoid overly imposing on the Project Gutenberg servers.\n\njane_eyre &lt;- read_csv(\n  \"jane_eyre.csv\",\n  col_types = cols(\n    gutenberg_id = col_integer(),\n    text = col_character()\n  )\n)\n\njane_eyre\n\n\n\n# A tibble: 21,001 × 2\n   gutenberg_id text                           \n          &lt;int&gt; &lt;chr&gt;                          \n 1         1260 JANE EYRE                      \n 2         1260 AN AUTOBIOGRAPHY               \n 3         1260 &lt;NA&gt;                           \n 4         1260 by Charlotte Brontë            \n 5         1260 &lt;NA&gt;                           \n 6         1260 _ILLUSTRATED BY F. H. TOWNSEND_\n 7         1260 &lt;NA&gt;                           \n 8         1260 London                         \n 9         1260 SERVICE & PATON                \n10         1260 5 HENRIETTA STREET             \n# ℹ 20,991 more rows\n\n\nWe are interested in only those lines that have content, so we remove those empty lines that are just there for spacing. Then we can create counts of the number of e/Es in that line, for the first ten lines of each chapter. For instance, we can look at the first few lines and see that there are five e/Es in the first line and eight in the second.\n\njane_eyre_reduced &lt;-\n  jane_eyre |&gt;\n  filter(!is.na(text)) |&gt; # Remove empty lines\n  mutate(chapter = if_else(str_detect(text, \"CHAPTER\") == TRUE,\n                           text,\n                           NA_character_)) |&gt; # Find start of chapter\n  fill(chapter, .direction = \"down\") |&gt; \n  mutate(chapter_line = row_number(), \n         .by = chapter) |&gt; # Add line number to each chapter\n  filter(!is.na(chapter), \n         chapter_line %in% c(2:11)) |&gt; # Remove \"CHAPTER I\" etc\n  select(text, chapter) |&gt;\n  mutate(\n    chapter = str_remove(chapter, \"CHAPTER \"),\n    chapter = str_remove(chapter, \"—CONCLUSION\"),\n    chapter = as.integer(as.roman(chapter))\n  ) |&gt; # Change chapters to integers\n  mutate(count_e = str_count(text, \"e|E\"),\n         word_count = str_count(text, \"\\\\w+\")\n         # From: https://stackoverflow.com/a/38058033\n         ) \n\n\njane_eyre_reduced |&gt;\n  select(chapter, word_count, count_e, text) |&gt;\n  head()\n\n# A tibble: 6 × 4\n  chapter word_count count_e text                                               \n    &lt;int&gt;      &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                                              \n1       1         13       5 There was no possibility of taking a walk that day…\n2       1         11       8 wandering, indeed, in the leafless shrubbery an ho…\n3       1         12       9 but since dinner (Mrs. Reed, when there was no com…\n4       1         14       3 the cold winter wind had brought with it clouds so…\n5       1         11       7 so penetrating, that further outdoor exercise was …\n6       1          1       1 question.                                          \n\n\nWe can verify that the mean and variance of the number of e/Es is roughly similar by plotting all of the data (Figure 13.10). The mean, in pink, is 6.7, and the variance, in blue, is 6.2. While they are not entirely the same, they are similar. We include the diagonal in Figure 13.10 (b) to help with thinking about the data. If the data were on the \\(y=x\\) line, then on average there would be one e/E per word. Given the mass of points below that line expect that on average there is less than one per word.\n\nmean_e &lt;- mean(jane_eyre_reduced$count_e)\nvariance_e &lt;- var(jane_eyre_reduced$count_e)\n\njane_eyre_reduced |&gt;\n  ggplot(aes(x = count_e)) +\n  geom_histogram() +\n  geom_vline(xintercept = mean_e, \n             linetype = \"dashed\", \n             color = \"#C64191\") +\n  geom_vline(xintercept = variance_e, \n             linetype = \"dashed\", \n             color = \"#0ABAB5\") +\n  theme_minimal() +\n  labs(\n    y = \"Count\",\n    x = \"Number of e's per line for first ten lines\"\n  )\n\njane_eyre_reduced |&gt;\n  ggplot(aes(x = word_count, y = count_e)) +\n  geom_jitter(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    x = \"Number of words in the line\",\n    y = \"Number of e/Es in the line\"\n  )\n\n\n\n\n\n\n\n(a) Distribution of the number of e/Es\n\n\n\n\n\n\n\n(b) Comparison of the number of e/Es in the line and the number of words in the line\n\n\n\n\nFigure 13.10: Number of e/Es letters in the first ten lines of each chapter in Jane Eyre\n\n\n\nWe could consider the following model:\n\\[\n\\begin{aligned}\ny_i|\\lambda_i &\\sim \\mbox{Poisson}(\\lambda_i)\\\\\n\\log(\\lambda_i) & = \\beta_0 + \\beta_1 \\times \\mbox{Number of words}_i\\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\beta_1 & \\sim \\mbox{Normal}(0, 2.5)\n\\end{aligned}\n\\] where \\(y_i\\) is the number of e/Es in the line and the explanatory variable is the number of words in the line. We could estimate the model using stan_glm().\n\njane_e_counts &lt;-\n  stan_glm(\n    count_e ~ word_count,\n    data = jane_eyre_reduced,\n    family = poisson(link = \"log\"),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  jane_e_counts,\n  file = \"jane_e_counts.rds\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we would normally be interested in the table of estimates, as we have seen that a few times now, rather than again creating a table of the estimates, we introduce plot_cap() from marginaleffects. We can use this to show the number of e/Es predicted by the model, for each line, based on the number of words in that line. Figure 13.11 makes it clear that we expect a positive relationship.\n\nplot_predictions(jane_e_counts, condition = \"word_count\") +\n  labs(x = \"Number of words\",\n       y = \"Average number of e/Es in the first 10 lines\") +\n  theme_classic()\n\n\n\n\nFigure 13.11: The predicted number of e/Es in each line based on the number of words"
  },
  {
    "objectID": "13-ijaglm.html#negative-binomial-regression",
    "href": "13-ijaglm.html#negative-binomial-regression",
    "title": "13  Generalized linear models",
    "section": "13.4 Negative binomial regression",
    "text": "13.4 Negative binomial regression\nOne of the restrictions with Poisson regression is the assumption that the mean and the variance are the same. We can relax this assumption to allow over-dispersion by using a close variant, negative binomial regression.\nPoisson and negative binomial models go hand in hand. It is often the case that we will end up fitting both, and then comparing them. For instance:\n\nMaher (1982) considers both in the context of results from the English Football League and discusses situations in which one may be considered more appropriate than the other.\nSmith (2002) considers the 2000 US presidential election and especially the issue of overdispersion in a Poisson analysis.\nOsgood (2000) compares them in the case of crime data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.4.1 Mortality in Alberta, Canada\nConsider, somewhat morbidly, that every year each individual either dies or does not. From the perspective of a geographic area, we could gather data on the number of people who died each year, by their cause of death. The Canadian province of Alberta has made available the number of deaths, by cause, since 2001, for the top 30 causes each year.\nAs always we first sketch our dataset and model. A quick sketch of what the dataset could look like is Figure 13.12 (a), and a quick sketch of our model is Figure 13.12 (b)\n\n\n\n\n\n\n\n(a) Quick sketch of a dataset that could be used to examine cause of death in Alberta\n\n\n\n\n\n\n\n(b) Quick sketch of what we expect from the analysis of cause of death in Alberta before finalizing either the data or the analysis\n\n\n\n\nFigure 13.12: Sketches of the expected dataset and analysis for cause of death in Alberta\n\n\nWe will simulate a dataset of cause of death distributed following the negative binomial distribution.\n\nalberta_death_simulation &lt;-\n  tibble(\n    cause = rep(x = c(\"Heart\", \"Stroke\", \"Diabetes\"), times = 10),\n    year = rep(x = 2016:2018, times = 10),\n    deaths = rnbinom(n = 30, size = 20, prob = 0.1)\n  )\n\nalberta_death_simulation\n\n# A tibble: 30 × 3\n   cause     year deaths\n   &lt;chr&gt;    &lt;int&gt;  &lt;int&gt;\n 1 Heart     2016    194\n 2 Stroke    2017    177\n 3 Diabetes  2018    205\n 4 Heart     2016    138\n 5 Stroke    2017    222\n 6 Diabetes  2018    166\n 7 Heart     2016    147\n 8 Stroke    2017    151\n 9 Diabetes  2018    148\n10 Heart     2016    192\n# ℹ 20 more rows\n\n\nWe can look at the distribution of these deaths, by year and cause (Figure 13.13). We have truncated the full cause of death because some are quite long. As some causes are not always in the top 30 each year, not all causes have the same number of occurrences.\n\nalberta_cod &lt;-\n  read_csv(\n    \"https://open.alberta.ca/dataset/03339dc5-fb51-4552-97c7-853688fc428d/resource/3e241965-fee3-400e-9652-07cfbf0c0bda/download/deaths-leading-causes.csv\",\n    col_types = cols(\n      `Calendar Year` = col_integer(),\n      Cause = col_character(),\n      Ranking = col_integer(),\n      `Total Deaths` = col_integer()\n    )\n  ) |&gt;\n  clean_names() |&gt;\n  add_count(cause) |&gt;\n  mutate(cause = str_trunc(cause, 30))\n\nIf we were to look at the top-ten causes in 2021, we would notice a variety of interesting aspects (Table 13.6). For instance, we would expect that the most common causes would be present in all 21 years of our data. But we notice that the most common cause, “Other ill-defined and unknown causes of mortality”, is only in three years. “COVID-19, virus identified”, is only in two other years, as there were no known COVID deaths in Canada before 2020.\n\nalberta_cod |&gt;\n  filter(\n    calendar_year == 2021,\n    ranking &lt;= 10\n  ) |&gt;\n  mutate(total_deaths = format(total_deaths, big.mark = \",\")) |&gt;\n  kable(\n    col.names = c(\"Year\", \"Cause\", \"Ranking\", \"Deaths\", \"Years\"),\n    align = c(\"l\", \"r\", \"r\", \"r\", \"r\"),\n    digits = 0, booktabs = TRUE, linesep = \"\"\n  )\n\n\n\nTable 13.6: Top-ten causes of death in Alberta in 2021\n\n\nYear\nCause\nRanking\nDeaths\nYears\n\n\n\n\n2021\nOther ill-defined and unkno…\n1\n3,362\n3\n\n\n2021\nOrganic dementia\n2\n2,135\n21\n\n\n2021\nCOVID-19, virus identified\n3\n1,950\n2\n\n\n2021\nAll other forms of chronic …\n4\n1,939\n21\n\n\n2021\nMalignant neoplasms of trac…\n5\n1,552\n21\n\n\n2021\nAcute myocardial infarction\n6\n1,075\n21\n\n\n2021\nOther chronic obstructive p…\n7\n1,028\n21\n\n\n2021\nDiabetes mellitus\n8\n728\n21\n\n\n2021\nStroke, not specified as he…\n9\n612\n21\n\n\n2021\nAccidental poisoning by and…\n10\n604\n9\n\n\n\n\n\n\nFor simplicity we restrict ourselves to the five most common causes of death in 2021 of those that have been present every year.\n\nalberta_cod_top_five &lt;-\n  alberta_cod |&gt;\n  filter(\n    calendar_year == 2021,\n    n == 21\n  ) |&gt;\n  slice_max(order_by = desc(ranking), n = 5) |&gt;\n  pull(cause)\n\nalberta_cod &lt;-\n  alberta_cod |&gt;\n  filter(cause %in% alberta_cod_top_five)\n\n\nalberta_cod |&gt;\n  ggplot(aes(x = calendar_year, y = total_deaths, color = cause)) +\n  geom_line() +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(x = \"Year\", y = \"Annual number of deaths in Alberta\") +\n  facet_wrap(vars(cause), dir = \"v\", ncol = 1) +\n  theme(legend.position = \"none\")\n\n\n\n\nFigure 13.13: Annual number of deaths for the top-five causes in 2021, since 2001, for Alberta, Canada\n\n\n\n\nOne thing that we notice is that the mean, 1,273, is different to the variance, 182,378 (Table 13.7).\n\n\n\n\n\n\nTable 13.7:  Summary statistics of the number of yearly deaths, by cause, in\nAlberta, Canada \n  \n    \n    \n       \n      Min\n      Mean\n      Max\n      SD\n      Var\n      N\n    \n  \n  \n    total_deaths\n280\n1273\n2135\n427\n182378\n105\n  \n  \n  \n\n\n\n\n\nWe can implement negative binomial regression when using stan_glm() by specifying the negative binomial distribution in “family”. In this case, we run both Poisson and negative binomial.\n\ncause_of_death_alberta_poisson &lt;-\n  stan_glm(\n    total_deaths ~ cause,\n    data = alberta_cod,\n    family = poisson(link = \"log\"),\n    seed = 853\n  )\n\ncause_of_death_alberta_neg_binomial &lt;-\n  stan_glm(\n    total_deaths ~ cause,\n    data = alberta_cod,\n    family = neg_binomial_2(link = \"log\"),\n    seed = 853\n  )\n\nWe can compare our different models (Table 13.8).\n\ncoef_short_names &lt;- \n  c(\"causeAll other forms of chronic ischemic heart disease\"\n    = \"causeAll other forms of...\",\n    \"causeMalignant neoplasms of trachea, bronchus and lung\"\n    = \"causeMalignant neoplas...\",\n    \"causeOrganic dementia\"\n    = \"causeOrganic dementia\",\n    \"causeOther chronic obstructive pulmonary disease\"\n    = \"causeOther chronic obst...\"\n    )\n\nmodelsummary(\n  list(\n    \"Poisson\" = cause_of_death_alberta_poisson,\n    \"Negative binomial\" = cause_of_death_alberta_neg_binomial\n  ),\n  coef_map = coef_short_names\n)\n\n\n\n\n\nTable 13.8:  Modeling the most prevalent cause of deaths in Alberta, 2001-2020 \n  \n    \n    \n       \n      Poisson\n      Negative binomial\n    \n  \n  \n    causeAll other forms of...\n0.442\n0.439\n    \n\n(0.104)\n    causeMalignant neoplas...\n0.224\n0.226\n    \n\n(0.105)\n    causeOrganic dementia\n0.001\n0.002\n    \n\n(0.107)\n    causeOther chronic obst...\n-0.214\n-0.217\n    \n\n(0.104)\n    Num.Obs.\n105\n105\n    Log.Lik.\n-5139.564\n-771.297\n    ELPD\n-5312.5\n-775.8\n    ELPD s.e.\n1099.1\n10.0\n    LOOIC\n10625.0\n1551.6\n    LOOIC s.e.\n2198.3\n20.0\n    WAIC\n10793.7\n1551.6\n    RMSE\n308.24\n308.26\n  \n  \n  \n\n\n\n\n\nThe estimates are similar. We could use posterior predictive checks, introduced in Section 12.4, to show that the negative binomial approach is a better choice for this circumstance (Figure 13.14).\n\npp_check(cause_of_death_alberta_poisson) +\n  theme(legend.position = \"bottom\")\n\npp_check(cause_of_death_alberta_neg_binomial) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) Poisson model\n\n\n\n\n\n\n\n(b) Negative binomial model\n\n\n\n\nFigure 13.14: Comparing posterior prediction checks for Poisson and negative binomial models\n\n\n\nFinally, we can compare between the models using the resampling method leave-one-out (LOO) cross-validation (CV). This is a variant of cross-validation, where the size of each fold is one. That is to say, if there was a dataset with 100 observations, this LOO is equivalent to 100-fold cross validation. We can implement this in rstanarm with loo() for each model, and then compare between them with loo_compare() where the higher the better.1\nWe provide more information on cross-validation in Online Appendix H.\n\npoisson &lt;- loo(cause_of_death_alberta_poisson, cores = 2)\nneg_binomial &lt;- loo(cause_of_death_alberta_neg_binomial, cores = 2)\n\nloo_compare(poisson, neg_binomial)\n\n                                    elpd_diff se_diff\ncause_of_death_alberta_neg_binomial     0.0       0.0\ncause_of_death_alberta_poisson      -4536.7    1089.5\n\n\nIn this case we find that the negative binomial model is a better fit than the Poisson, because ELPD is larger."
  },
  {
    "objectID": "13-ijaglm.html#multilevel-modeling",
    "href": "13-ijaglm.html#multilevel-modeling",
    "title": "13  Generalized linear models",
    "section": "13.5 Multilevel modeling",
    "text": "13.5 Multilevel modeling\nMultilevel modeling goes by a variety of names including “hierarchical”, and “random effects”. While there are sometimes small differences in meaning between disciplines, in general they refer to the same or at least similar ideas. The fundamental insight of multilevel modeling is that a lot of the time our observations are not completely independent of each other, and can instead be grouped. Accounting for that grouping when we model, can provide us with some useful information. For instance, there is a difference in the earnings of professional athletes depending on whether they compete in men’s or women’s events. If we were interested in trying to forecast the earnings of a particular athlete, based on their competition results, then knowing which type of competition the individual competed in would enable the model to make a better forecast.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Fiona Steele is a Professor of Statistics at the London School of Economics (LSE). After earning a PhD in Statistics from University of Southampton in 1996, she was appointed as a Lecturer at the LSE, before moving to the University of London, and the University of Bristol where she was appointed a full professor in 2008. She returned to the LSE in 2013. One area of her research is multilevel modeling and applications in demography, education, family psychology, and health. For instance, Steele (2007) looks at multilevel models for longitudinal data, and Steele, Vignoles, and Jenkins (2007) uses a multilevel model to look at the relationship between school resources and pupil attainment. She was awarded the Royal Statistical Society Guy Medal in Bronze in 2008.\n\n\nWe distinguish between three settings:\n\nComplete pooling, where we treat every observation as being from the same group, which is what we have been doing to this point.\nNo pooling, where we treat every group separately, which might happen if we were to run a separate regression for each group.\nPartial pooling, where we allow group membership to have some influence.\n\nFor instance, consider we are interested in the relationship between GDP and inflation for each of the countries in the world. Complete pooling would have us put all the countries into the one group; no pooling would have us run separate regressions for each continent. We will now illustrate the partial pooling approach.\nIn general there are two ways to go about this:\n\nenable varying intercepts, or\nenable varying slopes.\n\nIn this book we consider only the first, but you should move onto Gelman, Hill, and Vehtari (2020), McElreath ([2015] 2020), and Johnson, Ott, and Dogucu (2022).\n\n13.5.1 Simulated example: political support\nLet us consider a situation in which the probability of support for a particular political party depends on an individual’s gender, and the state that they live in.\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0 + \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{s[i]}^{\\mbox{state}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma_{\\mbox{state}}^2\\right)\\mbox{ for }s=1, 2, \\dots, S\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]\nwhere \\(\\pi_i = \\mbox{Pr}(y_i=1)\\), there are two gender groups, because that is what is going to be available from the survey we will use in Chapter 15, and \\(S\\) is the total number of states. We include this in the function with “(1 | state)” within stan_glmer() from rstanarm (Goodrich et al. 2023). This term indicates that we are looking at a group effect by state, which means that the fitted model’s intercept is allowed to vary according by state.\n\nset.seed(853)\n\npolitical_support &lt;-\n  tibble(\n    state = sample(1:50, size = 1000, replace = TRUE),\n    gender = sample(c(1, 2), size = 1000, replace = TRUE),\n    noise = rnorm(n = 1000, mean = 0, sd = 10) |&gt; round(),\n    supports = if_else(state + gender + noise &gt; 50, 1, 0)\n  )\n\npolitical_support\n\n# A tibble: 1,000 × 4\n   state gender noise supports\n   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1     9      1    11        0\n 2    26      1     3        0\n 3    29      2     7        0\n 4    17      2    13        0\n 5    37      2    11        0\n 6    29      2     9        0\n 7    50      2     3        1\n 8    20      2     3        0\n 9    19      1    -1        0\n10     3      2     7        0\n# ℹ 990 more rows\n\n\n\nvoter_preferences &lt;-\n  stan_glmer(\n    supports ~ gender + (1 | state),\n    data = political_support,\n    family = binomial(link = \"logit\"),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  voter_preferences,\n  file = \"voter_preferences.rds\"\n)\n\n\nvoter_preferences\n\nstan_glmer\n family:       binomial [logit]\n formula:      supports ~ gender + (1 | state)\n observations: 1000\n------\n            Median MAD_SD\n(Intercept) -4.4    0.7  \ngender       0.4    0.3  \n\nError terms:\n Groups Name        Std.Dev.\n state  (Intercept) 2.5     \nNum. levels: state 50 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nIt is worth trying to look for opportunities to use a multilevel model when you come to a new modeling situation, especially one where inference is the primary concern. There is often some grouping that can be taken advantage of to provide the model with more information.\nWhen we move to multilevel modeling, it is possible that some rstanarm models will result in a warning about “divergent transitions”. For the purposes of getting a model working for this book, if there are just a handful of warnings and the Rhat values of the coefficients are all close to one (check this with any(summary(change_this_to_the_model_name)[, \"Rhat\"] &gt; 1.1)), then just ignore it. If there are more than a handful, and/or any of the Rhats are not close to one, then add “adapt_delta = 0.99” as an argument to stan_glmer() and re-run the model (keeping in mind that it will take longer to run). If that does not fix the issue, then simplify the model by removing a variable. We will see an example in Chapter 15 when we apply MRP to the 2020 US election, where the “adapt_delta” strategy fixes the issue.\n\n\n13.5.2 Austen, Brontë, Dickens, and Shakespeare\nAs an example of multilevel modeling, we consider data from Project Gutenberg on the length of books by four authors: Jane Austen, Charlotte Brontë, Charles Dickens, and William Shakespeare. We would expect that Austen, Brontë, and Dickens, as they wrote books, will have longer books than Shakespeare, as he wrote plays. But it is not clear what difference we should expect between the three book authors.\n\nauthors &lt;- c(\"Austen, Jane\", \"Dickens, Charles\", \n             \"Shakespeare, William\", \"Brontë, Charlotte\")\n\n# The document values for duplicates and letters that we do not want\ndont_get_shakespeare &lt;-\n  c(2270, 4774, 5137, 9077, 10606, 12578, 22791, 23041, 23042, 23043, \n    23044, 23045, 23046, 28334, 45128, 47518, 47715, 47960, 49007, \n    49008, 49297, 50095, 50559)\ndont_get_bronte &lt;- c(31100, 42078)\ndont_get_dickens &lt;-\n  c(25852, 25853, 25854, 30368, 32241, 35536, 37121, 40723, 42232, 43111, \n    43207, 46675, 47529, 47530, 47531, 47534, 47535, 49927, 50334)\n\nbooks &lt;-\n  gutenberg_works(\n    author %in% authors,\n    !gutenberg_id %in% \n      c(dont_get_shakespeare, dont_get_bronte, dont_get_dickens)\n    ) |&gt;\n  gutenberg_download(\n    meta_fields = c(\"title\", \"author\"),\n    mirror = \"https://gutenberg.pglaf.org/\"\n  )\n\nwrite_csv(books, \"books-austen_bronte_dickens_shakespeare.csv\")\n\n\nbooks &lt;- read_csv(\n  \"books-austen_bronte_dickens_shakespeare.csv\",\n  col_types = cols(\n    gutenberg_id = col_integer(),\n    text = col_character(),\n    title = col_character(),\n    author = col_character()\n  )\n)\n\n\nlines_by_author_work &lt;-\n  books |&gt;\n  summarise(number_of_lines = n(),\n            .by = c(author, title))\n\nlines_by_author_work\n\n\n\n# A tibble: 125 × 3\n   author            title                       number_of_lines\n   &lt;chr&gt;             &lt;chr&gt;                                 &lt;int&gt;\n 1 Austen, Jane      Emma                                  16488\n 2 Austen, Jane      Lady Susan                             2525\n 3 Austen, Jane      Love and Freindship [sic]              3401\n 4 Austen, Jane      Mansfield Park                        15670\n 5 Austen, Jane      Northanger Abbey                       7991\n 6 Austen, Jane      Persuasion                             8353\n 7 Austen, Jane      Pride and Prejudice                   14199\n 8 Austen, Jane      Sense and Sensibility                 12673\n 9 Brontë, Charlotte Jane Eyre: An Autobiography           21001\n10 Brontë, Charlotte Shirley                               25520\n# ℹ 115 more rows\n\n\n\nauthor_lines_rstanarm &lt;-\n  stan_glm(\n    number_of_lines ~ author,\n    data = lines_by_author_work,\n    family = neg_binomial_2(link = \"log\"),\n    prior = normal(location = 0, scale = 3, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 3, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  author_lines_rstanarm,\n  file = \"author_lines_rstanarm.rds\"\n)\n\nauthor_lines_rstanarm_multilevel &lt;-\n  stan_glmer(\n    number_of_lines ~ (1 | author),\n    data = lines_by_author_work,\n    family = neg_binomial_2(link = \"log\"),\n    prior = normal(location = 0, scale = 3, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 3, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  author_lines_rstanarm_multilevel,\n  file = \"author_lines_rstanarm_multilevel.rds\"\n)\n\n\nmodelsummary(\n  list(\n    \"Neg binomial\" = author_lines_rstanarm,\n    \"Multilevel neg binomial\" = author_lines_rstanarm_multilevel\n  )\n)\n\n\n\n\n\nTable 13.9:  Explaining whether Austen, Brontë, Dickens, or Shakespeare wrote a\nbook based on the number of lines \n  \n    \n    \n       \n      Neg binomial\n      Multilevel neg binomial\n    \n  \n  \n    (Intercept)\n9.245\n8.980\n    \n(0.342)\n(0.371)\n    authorBrontë, Charlotte\n0.686\n\n    \n(0.576)\n\n    authorDickens, Charles\n0.019\n\n    \n(0.364)\n\n    authorShakespeare, William\n-0.875\n\n    \n(0.361)\n\n    Num.Obs.\n125\n125\n    ICC\n\n1.0\n    Log.Lik.\n-1234.105\n-1234.124\n    ELPD\n-1237.3\n-1237.2\n    ELPD s.e.\n11.5\n11.5\n    LOOIC\n2474.7\n2474.4\n    LOOIC s.e.\n23.1\n23.1\n    WAIC\n2474.6\n2474.4\n    RMSE\n8954.42\n8984.71\n  \n  \n  \n\n\n\n\n\nTable 13.9 is a little empty for the multilevel model, and we often use graphs to avoid overwhelming the reader with numbers (we will see examples of this in Chapter 15). For instance, Figure 13.15 shows the distribution of draws for each of the four authors using spread_draws() from tidybayes.\n\nauthor_lines_rstanarm_multilevel |&gt;\n  spread_draws(`(Intercept)`, b[, group]) |&gt;\n  mutate(condition_mean = `(Intercept)` + b) |&gt;\n  ggplot(aes(y = group, x = condition_mean)) +\n  stat_halfeye() +\n  theme_minimal()\n\n\n\n\nFigure 13.15: Examining the distribution of draws for each of the four authors\n\n\n\n\nIn this case, we see that we typically expect Brontë to write the longest books of the three book authors. Shakespeare, as expected, typically wrote works with the fewest lines."
  },
  {
    "objectID": "13-ijaglm.html#concluding-remarks",
    "href": "13-ijaglm.html#concluding-remarks",
    "title": "13  Generalized linear models",
    "section": "13.6 Concluding remarks",
    "text": "13.6 Concluding remarks\nIn this chapter we have considered generalized linear models and introduced multilevel modeling. We built on the foundation established in Chapter 12 and provided some essentials for Bayesian model building. As mentioned in Chapter 12, this is enough to get started. Hopefully you are excited to learn more and to do that you should start with the modeling books recommended in Chapter 17.\nOver the course of Chapter 12 and Chapter 13 we have covered a variety of approaches for Bayesian models. But we have not done everything for every model.\nIt is difficult to be definitive about what is “enough” because it is context specific, but the following checklist, drawn from concepts introduced across Chapter 12 and Chapter 13 would be sufficient for most purposes when you are getting started. In the model section of the paper, write out the model using equations and include a few paragraphs of text explaining the equations. Then justify the model choices, and briefly detail any alternatives that you considered. Finish with a sentence explaining how the model was fit, which in this case is likely to be with rstanarm, and that diagnostics are available in a cross-referenced appendix. In that appendix you should include: prior predictive checks, trace plots, Rhat plots, posterior distributions, and posterior predictive checks.\nIn the results section you should include a table of the estimates, built using modelsummary, and talk through them, likely with the help of marginaleffects. It may also be useful to include a graph of your results, especially if you are using a multilevel model, with the help of tidybayes. The model itself should be run in a separate R script. It should be preceded by tests of class and the number of observations. It should be followed by tests of the coefficients. These should be based on simulation. You should save the model in that R script using saveRDS(). In the Quarto document, you should read in that model using readRDS()."
  },
  {
    "objectID": "13-ijaglm.html#exercises",
    "href": "13-ijaglm.html#exercises",
    "title": "13  Generalized linear models",
    "section": "13.7 Exercises",
    "text": "13.7 Exercises\n\nScales\n\n(Plan) Consider the following scenario: A person is interested in the number of deaths, attributed to cancer, in Sydney, Australia. They collect data from the five largest hospitals, for the past 20 years. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation, along with three predictor variables that are associated with the number of deaths, by cause. Please include at least ten tests based on the simulated data.\n(Acquire) Please describe one possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Then use rstanarm to build a model.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nWhen should we consider logistic regression (pick one)?\n\nContinuous outcome variable.\nBinary outcome variable.\nCount outcome variable.\n\nWe are interested in studying how voting intentions in the 2020 US presidential election vary by an individual’s income. We set up a logistic regression model to study this relationship. In this study, one possible outcome variable would be (pick one)?\n\nWhether the respondent is a US citizen (yes/no)\nThe respondent’s personal income (high/low)\nWhether the respondent is going to vote for Biden (yes/no)\nWho the respondent voted for in 2016 (Trump/Clinton)\n\nWe are interested in studying how voting intentions in the 2020 US presidential election vary by an individual’s income. We set up a logistic regression model to study this relationship. In this study, some possible predictor variables could be (select all that apply)?\n\nThe race of the respondent (white/not white)\nThe respondent’s marital status (married/not)\nWhether the respondent is going to vote for Biden (yes/no)\n\nThe mean of a Poisson distribution is equal to its?\n\nMedian.\nStandard deviation.\nVariance.\n\nPlease redo the rstanarm example of US elections but include additional variables. Which variable did you choose, and how did the performance of the model improve?\nPlease create the graph of the density of the Poisson distribution when \\(\\lambda = 75\\).\nFrom Gelman, Hill, and Vehtari (2020), what is the offset in Poisson regression?\nRedo the Jane Eyre example, but for “A/a”.\nThe twentieth century British statistician George Box, famously said, “[s]ince all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.” (Box 1976, 792). Discuss, with the help of examples and citations.\n\n\n\nTutorial\nPlease consider Maher (1982), Smith (2002), or Cohn (2016). Build a simplified version of their model. Obtain some recent relevant data, estimate the model, and discuss your choice between logistic, Poisson, and negative binomial regression.\n\n\nPaper\nAt about this point the Spadina Paper from Online Appendix D would be appropriate.\n\n\n\n\nAnsolabehere, Stephen, Brian Schaffner, and Sam Luks. 2021. “Guide to the 2020 Cooperative Election Study.” https://doi.org/10.7910/DVN/E9N6PH.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2023. marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests. https://vincentarelbundock.github.io/marginaleffects/.\n\n\nBaio, Gianluca, and Marta Blangiardo. 2010. “Bayesian Hierarchical Model for the Prediction of Football Results.” Journal of Applied Statistics 37 (2): 253–64. https://doi.org/10.1080/02664760802684177.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBolton, Ruth, and Randall Chapman. 1986. “Searching for Positive Returns at the Track.” Management Science 32 (August): 1040–60. https://doi.org/10.1287/mnsc.32.8.1040.\n\n\nBox, George E. P. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356): 791–99. https://doi.org/10.1080/01621459.1976.10480949.\n\n\nBurch, Tyler James. 2023. “2023 NHL Playoff Predictions,” April. https://tylerjamesburch.com/blog/misc/nhl-predictions.\n\n\nCanty, Angelo, and B. D. Ripley. 2021. boot: Bootstrap R (S-Plus) Functions.\n\n\nChellel, Kit. 2018. “The Gambler Who Cracked the Horse-Racing Code.” Bloomberg Businessweek, May. https://www.bloomberg.com/news/features/2018-05-03/the-gambler-who-cracked-the-horse-racing-code.\n\n\nCohn, Nate. 2016. “We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results.” The New York Times, September. https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html.\n\n\nCramer, Jan Salomon. 2003. “The Origins of Logistic Regression.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.360300.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their Applications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.” Journal of the Statistical Society of London, 181–217.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFriendly, Michael. 2021. HistData: Data Sets from the History of Statistics and Data Visualization. https://CRAN.R-project.org/package=HistData.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023. “rstanarm: Bayesian applied regression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nHastie, Trevor, and Robert Tibshirani. 1990. Generalized Additive Models. 1st ed. Boca Raton: Chapman; Hall/CRC.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2013) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJohnson, Alicia, Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with R. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nJohnston, Myfanwy, and David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nKay, Matthew. 2022. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKrantz, Sebastian. 2023. collapse: Advanced and Fast Data Transformation. https://CRAN.R-project.org/package=collapse.\n\n\nKuriwaki, Shiro, Will Beasley, and Thomas Leeper. 2023. dataverse: R Client for Dataverse 4+ Repositories.\n\n\nMaher, Michael. 1982. “Modelling Association Football Scores.” Statistica Neerlandica 36 (3): 109–18. https://doi.org/10.1111/j.1467-9574.1982.tb00782.x.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nNelder, John, and Robert Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society: Series A (General) 135 (3): 370–84. https://doi.org/10.2307/2344614.\n\n\nOsgood, D. Wayne. 2000. “Poisson-Based Regression Analysis of Aggregate Crime Rates.” Journal of Quantitative Criminology 16 (1): 21–43. https://doi.org/10.1023/a:1007521427059.\n\n\nPitman, Jim. 1993. Probability. 1st ed. New York: Springer. https://doi.org/10.1007/978-1-4612-4374-8.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSchaffner, Brian, Stephen Ansolabehere, and Sam Luks. 2021. “Cooperative Election Study Common Content, 2020.” Harvard Dataverse. https://doi.org/10.7910/DVN/E9N6PH.\n\n\nSmith, Richard. 2002. “A Statistical Assessment of Buchanan’s Vote in Palm Beach County.” Statistical Science 17 (4): 441–57. https://doi.org/10.1214/ss/1049993203.\n\n\nSteele, Fiona. 2007. “Multilevel Models for Longitudinal Data.” Journal of the Royal Statistical Society Series A: Statistics in Society 171 (1): 5–19. https://doi.org/10.1111/j.1467-985x.2007.00509.x.\n\n\nSteele, Fiona, Anna Vignoles, and Andrew Jenkins. 2007. “The Effect of School Resources on Pupil Attainment: A Multilevel Simultaneous Equation Modelling Approach.” Journal of the Royal Statistical Society Series A: Statistics in Society 170 (3): 801–24. https://doi.org/10.1111/j.1467-985x.2007.00476.x.\n\n\nStigler, Stephen. 1978. “Francis Ysidro Edgeworth, Statistician.” Journal of the Royal Statistical Society. Series A (General) 141 (3): 287–322. https://doi.org/10.2307/2344804.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015. “Forecasting Elections with Non-Representative Polls.” International Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "13-ijaglm.html#footnotes",
    "href": "13-ijaglm.html#footnotes",
    "title": "13  Generalized linear models",
    "section": "",
    "text": "By way of background, LOO-CV is not done by loo(), because it would be too computationally intensive. Instead an approximation is done which provides the expected log point wise predictive density (ELPD). The rstanarm vignettes provide more detail.↩︎"
  },
  {
    "objectID": "14-causality_from_obs.html#introduction",
    "href": "14-causality_from_obs.html#introduction",
    "title": "14  Causality from observational data",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nLife is grand when we can conduct experiments to be able to speak to causality. But there are circumstances in which we cannot run an experiment, yet nonetheless want to be able to make causal claims. And data from outside experiments have value that experiments do not have. In this chapter we discuss the circumstances and methods that allow us to speak to causality using observational data. We use relatively simple methods, in sophisticated ways, drawing from statistics, but also a variety of social sciences, including economics and political science, as well as epidemiology.\nFor instance, Dagan et al. (2021) use observational data to confirm the effectiveness of the Pfizer-BioNTech vaccine. They discuss how one concern with using observational data in this way is confounding, which is where we are concerned that there is some variable that affects both the predictor and outcome variables and can lead to spurious relationships. Dagan et al. (2021) adjust for this by first making a list of potential confounders, such as age, sex, geographic location, and healthcare usage and then adjusting for each of them, by matching one-to-one between people that were vaccinated and those that were not. The experimental data guided the use of observational data, and the larger size of the latter enabled a focus on specific age-groups and extent of disease.\nThis chapter is about using observational data in sophisticated ways. How we can nonetheless be comfortable making causal statements, even when we cannot run A/B tests or RCTs. Indeed, in what circumstances may we prefer to not run those or to run observational-based approaches in addition to them. We cover three of the major methods: difference-in-differences, regression discontinuity, and instrumental variables."
  },
  {
    "objectID": "14-causality_from_obs.html#directed-acyclic-graphs",
    "href": "14-causality_from_obs.html#directed-acyclic-graphs",
    "title": "14  Causality from observational data",
    "section": "14.2 Directed Acyclic Graphs",
    "text": "14.2 Directed Acyclic Graphs\nWhen we are discussing causality, it can help to be specific about what we mean. It is easy to get caught up in observational data and trick ourselves. We should think hard, and to use all the tools available to us. For instance, in that earlier example, Dagan et al. (2021) were able to use experimental data as a guide. Most of the time, we will not be so lucky as to have both experimental data and observational data available to us. One framework that can help with thinking hard about our data is the use of directed acyclic graphs (DAG). DAGs are a fancy name for a flow diagram and involve drawing arrows and lines between the variables to indicate the relationship between them.\nTo construct them we use Graphviz, which is an open-source package for graph visualization and is built into Quarto. The code needs to be wrapped in a “dot” chunk rather than “R”, and the chunk options are set with “//|” instead of “#|”. Alternatives that do not require this include the use of DiagrammeR (Iannone 2022) and ggdag (Barrett 2021). We provide the whole chunk for the first DAG, but then, only provide the code for the others.\n\n```{dot}\n//| label: fig-dot-firstdag-quarto\n//| fig-cap: \"We expect a causal relationship between x and y, where x influences y\"\n//| fig-width: 4\ndigraph D {\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  {rank=same x y};\n  \n  x -&gt; y;\n}\n```\n\n\n\n\n\n\n\nD\n\n  \n\nx\n\nx   \n\ny\n\ny   \n\nx-&gt;y\n\n   \n\n\nFigure 14.5: We expect a causal relationship between x and y, where x influences y\n\n\n\n\nIn Figure 14.5, we are saying that we think x causes y.\nWe could build another DAG where the situation is less clear. To make the examples a little easier to follow, we will switch to thinking about a hypothetical relationship between income and happiness, with consideration of variables that could affect that relationship. In this first one we consider the relationship between income and happiness, along with education (Figure 14.6).\n\ndigraph D {\n  \n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Education\"];\n  \n  { rank=same a b};\n  \n  a-&gt;b;\n  c-&gt;{a, b};\n}\n\n\n\n\n\n\n\nD\n\n  \n\na\n\nIncome   \n\nb\n\nHappiness   \n\na-&gt;b\n\n    \n\nc\n\nEducation   \n\nc-&gt;a\n\n    \n\nc-&gt;b\n\n   \n\n\nFigure 14.6: Education is a confounder that affects the relationship between income and happiness\n\n\n\n\nIn Figure 14.6, we think income causes happiness. But we also think that education causes happiness, and that education also causes income. That relationship is a “backdoor path”, and failing to adjust for education in a regression could overstate the extent of the relationship, or even create a spurious relationship, between income and happiness in our analysis. That is, we may think that changes in income are causing changes in happiness, but it could be that education is changing them both. That variable, in this case, education, is called a “confounder”.\nHernán and Robins (2023, 83) discuss an interesting case where a researcher was interested in whether one person looking up at the sky makes others look up at the sky also. There was a clear relationship between the responses of both people. But it was also the case that there was noise in the sky. It was unclear whether the second person looked up because the first person looked up, or they both looked up because of the noise. When using experimental data, randomization allows us to avoid this concern, but with observational data we cannot rely on that. It is also not the case that bigger data necessarily get around this problem for us. Instead, we should think carefully about the situation, and DAGs can help with that.\nIf there are confounders, but we are still interested in causal effects, then we need to adjust for them. One way is to include them in the regression. But the validity of this requires several assumptions. In particular, Gelman and Hill (2007, 169) warn that our estimate will only correspond to the average causal effect in the sample if we include all of the confounders and have the right model. Putting the second requirement to one side, and focusing only on the first, if we do not think about and observe a confounder, then it can be difficult to adjust for it. And this is an area where both domain expertise and theory can bring a lot to an analysis.\nIn Figure 14.7 we again consider that income causes happiness. But, if income also causes children, and children also cause happiness, then we have a situation where it would be tricky to understand the effect of income on happiness.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Children\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b, c};\n  c-&gt;b;\n}\n\n\n\n\n\n\n\nD\n\n  \n\na\n\nIncome   \n\nb\n\nHappiness   \n\na-&gt;b\n\n    \n\nc\n\nChildren   \n\na-&gt;c\n\n    \n\nc-&gt;b\n\n   \n\n\nFigure 14.7: Children as a mediator between income and happiness\n\n\n\n\nIn Figure 14.7, children is called a “mediator” and we would not adjust for it if we were interested in the effect of income on happiness. If we were to adjust for it, then some of what we are attributing to income, would be due to children.\nFinally, in Figure 14.8 we have yet another similar situation, where we think that income causes happiness. But this time both income and happiness also cause exercise. For instance, if you have more money then it may be easier to exercise, but also it may be easier to exercise if you are happier.\n\ndigraph D {\n\n  node [shape=plaintext, fontname = \"helvetica\"];\n  \n  a [label = \"Income\"];\n  b [label = \"Happiness\"];\n  c [label = \"Exercise\"];\n  \n  { rank=same a b};\n  \n  a-&gt;{b c};\n  b-&gt;c;\n}\n\n\n\n\n\n\n\nD\n\n  \n\na\n\nIncome   \n\nb\n\nHappiness   \n\na-&gt;b\n\n    \n\nc\n\nExercise   \n\na-&gt;c\n\n    \n\nb-&gt;c\n\n   \n\n\nFigure 14.8: Exercise as a collider affecting the relationship between income and happiness\n\n\n\n\nIn this case, exercise is called a “collider” and if we were to condition on it, then we would create a misleading relationship. Income influences exercise, but a person’s happiness also affects this. Exercise is a collider because both the predictor and outcome variable of interest influence it.\nWe will be clear about this: we must create the DAG ourselves, in the same way that we must put together the model ourselves. There is nothing that will create it for us. This means that we need to think carefully about the situation. Because it is one thing to see something in the DAG and then do something about it, but it is another to not even know that it is there. McElreath ([2015] 2020, 180) describes these as haunted DAGs. DAGs are helpful, but they are just a tool to help us think deeply about our situation.\nWhen we are building models, it can be tempting to include as many predictor variables as possible. DAGs show clearly why we need to be more thoughtful. For instance, if a variable is a confounder, then we would want to adjust for it, whereas if a variable was a collider then we would not. We can never know the truth, and we are informed by aspects such as theory, what we are interested in, research design, limitations of the data, or our own limitations as researchers, to name a few. Knowing the limits is as important as reporting the model. Data and models with flaws are still useful, if you acknowledge those flaws. The work of thinking about a situation is never done, and relies on others, which is why we need to make all our work as reproducible as possible."
  },
  {
    "objectID": "14-causality_from_obs.html#two-common-paradoxes",
    "href": "14-causality_from_obs.html#two-common-paradoxes",
    "title": "14  Causality from observational data",
    "section": "14.3 Two common paradoxes",
    "text": "14.3 Two common paradoxes\nThere are two situations where data can trick us that are so common that we will explicitly go through them. These are:\n\nSimpson’s paradox, and\nBerkson’s paradox.\n\n\n14.3.1 Simpson’s paradox\nSimpson’s paradox occurs when we estimate some relationship for subsets of our data, but a different relationship when we consider the entire dataset (Simpson 1951). It is a particular case of the ecological fallacy, which is when we try to make claims about individuals, based on their group. For instance, it may be that there is a positive relationship between undergraduate grades and performance in graduate school in two departments when considering each department individually. But if undergraduate grades tended to be higher in one department than another while graduate school performance tended to be opposite, we may find a negative relationship between undergraduate grades and performance in graduate school. We can simulate some data to show this more clearly (Figure 14.9).\n\nset.seed(853)\n\nnumber_in_each &lt;- 1000\n\ndepartment_one &lt;-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.7, max = 0.9),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise,\n    type = \"Department 1\"\n  )\n\ndepartment_two &lt;-\n  tibble(\n    undergrad = runif(n = number_in_each, min = 0.6, max = 0.8),\n    noise = rnorm(n = number_in_each, 0, sd = 0.1),\n    grad = undergrad + noise + 0.3,\n    type = \"Department 2\"\n  )\n\nboth_departments &lt;- rbind(department_one, department_two)\n\nboth_departments\n\n# A tibble: 2,000 × 4\n   undergrad   noise  grad type        \n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n 1     0.772 -0.0566 0.715 Department 1\n 2     0.724 -0.0312 0.693 Department 1\n 3     0.797  0.0770 0.874 Department 1\n 4     0.763 -0.0664 0.697 Department 1\n 5     0.707  0.0717 0.779 Department 1\n 6     0.781 -0.0165 0.764 Department 1\n 7     0.726 -0.104  0.623 Department 1\n 8     0.749  0.0527 0.801 Department 1\n 9     0.732 -0.0471 0.684 Department 1\n10     0.738  0.0552 0.793 Department 1\n# ℹ 1,990 more rows\n\n\n\nboth_departments |&gt;\n  ggplot(aes(x = undergrad, y = grad)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = \"lm\", formula = \"y ~ x\") +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", color = \"black\") +\n  labs(\n    x = \"Undergraduate results\",\n    y = \"Graduate results\",\n    color = \"Department\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 14.9: Illustration of simulated data that shows Simpson’s paradox\n\n\n\n\nSimpson’s paradox is often illustrated using real-world data from University of California, Berkeley, on graduate admissions (Bickel, Hammel, and O’Connell 1975). This paper was mentioned in Chapter 4 as having one of the greatest sub-titles ever published. Hernán, Clayton, and Keiding (2011) create DAGs that further illuminate the relationship and the cause of the paradox.\nMore recently, as mentioned in its documentation, the “penguins” dataset from palmerpenguins provides an example of Simpson’s paradox, using real-world data on the relationship between body mass and bill depth in different species of penguins (Figure 14.10). The overall negative trend occurs because Gentoo penguins tend to be heavier but with shorter bills compared to Adelie and Chinstrap penguins.\n\npenguins |&gt;\n  ggplot(aes(x = body_mass_g, y = bill_depth_mm)) +\n  geom_point(aes(color = species), alpha = 0.1) +\n  geom_smooth(aes(color = species), method = \"lm\", formula = \"y ~ x\") +\n  geom_smooth(\n    method = \"lm\",\n    formula = \"y ~ x\",\n    color = \"black\"\n  ) +\n  labs(\n    x = \"Body mass (grams)\",\n    y = \"Bill depth (millimeters)\",\n    color = \"Species\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 14.10: Illustration of Simpson’s paradox in a dataset of penguin bill depth compared with their body mass\n\n\n\n\n\n\n14.3.2 Berkson’s paradox\nBerkson’s paradox occurs when we estimate some relationship based on the dataset that we have, but because the dataset is so selected, the relationship is different in a more general dataset (Berkson 1946). For instance, if we have a dataset of professional cyclists then we might find there is no relationship between their VO2 max and their chance of winning a bike race (Coyle et al. 1988; Podlogar, Leo, and Spragg 2022). But if we had a dataset of the general population then we might find a relationship between these two variables. The professional dataset has just been so selected that the relationship disappears; one cannot become a professional cyclist unless one has a good enough VO2 max, but among professional cyclists everyone has a good enough VO2 max. Again, we can simulate some data to show this more clearly (Figure 14.11).\n\nset.seed(853)\n\nnum_pros &lt;- 100\nnum_public &lt;- 1000\n\nprofessionals &lt;- tibble(\n  VO2 = runif(num_pros, 0.7, 0.9),\n  chance_of_winning = runif(num_pros, 0.7, 0.9),\n  type = \"Professionals\"\n)\n\ngeneral_public &lt;- tibble(\n  VO2 = runif(num_public, 0.6, 0.8),\n  chance_of_winning = VO2 + rnorm(num_public, 0, 0.03) + 0.1,\n  type = \"Public\"\n)\n\nprofessionals_and_public &lt;- bind_rows(professionals, general_public)\n\n\nprofessionals_and_public |&gt;\n  ggplot(aes(x = VO2, y = chance_of_winning)) +\n  geom_point(aes(color = type), alpha = 0.1) +\n  geom_smooth(aes(color = type), method = \"lm\", formula = \"y ~ x\") +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", color = \"black\") +\n  labs(\n    x = \"VO2 max\",\n    y = \"Chance of winning a bike race\",\n    color = \"Type\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 14.11: Illustration of simulated data that shows Berkson’s paradox"
  },
  {
    "objectID": "14-causality_from_obs.html#difference-in-differences",
    "href": "14-causality_from_obs.html#difference-in-differences",
    "title": "14  Causality from observational data",
    "section": "14.4 Difference-in-differences",
    "text": "14.4 Difference-in-differences\nThe ideal situation of being able to conduct an experiment is rarely possible. Can we reasonably expect that Netflix would allow us to change prices? And even if they did once, would they let us do it again, and again, and again? Further, rarely can we explicitly create treatment and control groups. Finally, experiments can be expensive or unethical. Instead, we need to make do with what we have. Rather than our counterfactual coming to us through randomization, and hence us knowing that the two are the same but for the treatment, we try to identify groups that were similar but for the treatment, and hence any differences can be attributed to the treatment.\nWith observational data, sometimes there are differences between our two groups before we treat. Provided those pre-treatment differences satisfy assumptions that essentially amount to the differences being both consistent, and that we expect that consistency to continue in the absence of the treatment—the “parallel trends” assumption—then we can look to any difference in the differences as the effect of the treatment. One of the aspects of difference-in-differences analysis is that we can do it using relatively straight forward methods, for instance Tang (2015). Linear regression with a binary variable is enough to get started and do a convincing job.\nConsider wanting to know the effect of a new tennis racket on serve speed. One way to test this would be to measure the difference between, say, Roger Federer’s serve speed without the tennis racket and the serve speed of an enthusiastic amateur, let us call them Ville, with the tennis racket. Yes, we would find a difference, but would we know how much to attribute to the tennis racket? Another way would be to consider the difference between Ville’s serve speed without the new tennis racket and Ville’s serve speed with the new tennis racket. But what if serves were just getting faster naturally over time? Instead, we combine the two approaches to look at the difference in the differences.\nWe begin by measuring Federer’s serve speed and compare it to Ville’s serve speed, both without the new racket. We then measure Federer’s serve speed again, and measure Ville’s serve speed with the new racket. That difference in the differences would then be the estimate of the effect of the new racket. There are a few key questions we must ask to see if this analysis is appropriate:\n\nIs there something else that may have affected only Ville, and not Federer that could affect Ville’s serve speed?\nIs it likely that Federer and Ville have the same trajectory of serve speed improvement? This is the “parallel trends” assumption, and it dominates many discussions of difference-in-differences analysis.\nFinally, is it likely that the variance of our serve speeds of Federer and Ville are the same?\n\nDespite these requirements, difference-in-differences is a powerful approach because we do not need the treatment and control group to be the same before the treatment. We just need to have a good idea of how they differed.\n\n14.4.1 Simulated example: tennis serve speed\nTo be more specific about the situation, we simulate data. We will simulate a situation in which there is initially a difference of one between the serve speeds of the different people, and then after a new tennis racket, there is a difference of six. We can use a graph to illustrate the situation (Figure 14.12).\n\nset.seed(853)\n\nsimulated_diff_in_diff &lt;-\n  tibble(\n    person = rep(c(1:1000), times = 2),\n    time = c(rep(0, times = 1000), rep(1, times = 1000)),\n    treat_group = rep(sample(x = 0:1, size = 1000, replace = TRUE ), times = 2)\n    ) |&gt;\n  mutate(\n    treat_group = as.factor(treat_group),\n    time = as.factor(time)\n  )\n\nsimulated_diff_in_diff &lt;-\n  simulated_diff_in_diff |&gt;\n  rowwise() |&gt;\n  mutate(\n    serve_speed = case_when(\n      time == 0 & treat_group == 0 ~ rnorm(n = 1, mean = 5, sd = 1),\n      time == 1 & treat_group == 0 ~ rnorm(n = 1, mean = 6, sd = 1),\n      time == 0 & treat_group == 1 ~ rnorm(n = 1, mean = 8, sd = 1),\n      time == 1 & treat_group == 1 ~ rnorm(n = 1, mean = 14, sd = 1)\n    )\n  )\n\nsimulated_diff_in_diff\n\n# A tibble: 2,000 × 4\n# Rowwise: \n   person time  treat_group serve_speed\n    &lt;int&gt; &lt;fct&gt; &lt;fct&gt;             &lt;dbl&gt;\n 1      1 0     0                  4.43\n 2      2 0     1                  6.96\n 3      3 0     1                  7.77\n 4      4 0     0                  5.31\n 5      5 0     0                  4.09\n 6      6 0     0                  4.85\n 7      7 0     0                  6.43\n 8      8 0     0                  5.77\n 9      9 0     1                  6.13\n10     10 0     1                  7.32\n# ℹ 1,990 more rows\n\n\n\nsimulated_diff_in_diff |&gt;\n  ggplot(aes(x = time, y = serve_speed, color = treat_group)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(group = person), alpha = 0.1) +\n  theme_minimal() +\n  labs(x = \"Time period\", y = \"Serve speed\", color = \"Person got a new racket\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 14.12: Illustration of simulated data that shows a difference before and after getting a new tennis racket\n\n\n\n\nWe can obtain our estimate manually, by looking at the average difference of the differences. When we do that, we find that we estimate the effect of the new tennis racket to be 5.06, which is similar to what we simulated.\n\nave_diff &lt;-\n  simulated_diff_in_diff |&gt;\n  pivot_wider(\n    names_from = time,\n    values_from = serve_speed,\n    names_prefix = \"time_\"\n  ) |&gt;\n  mutate(difference = time_1 - time_0) |&gt;\n  # Average difference between old and new racket serve speed within groups\n  summarise(average_difference = mean(difference),\n            .by = treat_group)\n\n# Difference between the average differences of each group\nave_diff$average_difference[2] - ave_diff$average_difference[1]\n\n[1] 5.058414\n\n\nAnd we can use linear regression to get the same result. The model we are interested in is:\n\\[Y_{i,t} = \\beta_0 + \\beta_1\\times\\mbox{Treatment}_i + \\beta_2\\times\\mbox{Time}_t + \\beta_3\\times(\\mbox{Treatment} \\times\\mbox{Time})_{i,t} + \\epsilon_{i,t}\\]\nWhile we should include the separate aspects as well, it is the estimate of the interaction that we are interested in. In this case it is \\(\\beta_3\\). And we find that our estimated effect is 5.06 (Table 14.1).\n\ndiff_in_diff_example_regression &lt;-\n  stan_glm(\n    formula = serve_speed ~ treat_group * time,\n    data = simulated_diff_in_diff,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  diff_in_diff_example_regression,\n  file = \"diff_in_diff_example_regression.rds\"\n)\n\n\ndiff_in_diff_example_regression &lt;-\n  readRDS(file = \"diff_in_diff_example_regression.rds\")\n\n\nmodelsummary(\n  diff_in_diff_example_regression\n)\n\n\n\n\n\nTable 14.1:  Illustration of simulated data that shows a difference before and\nafter getting a new tennis racket \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n4.971\n    treatment_group1\n3.035\n    time1\n1.006\n    treatment_group1 × time1\n5.057\n    Num.Obs.\n2000\n    R2\n0.927\n    R2 Adj.\n0.927\n    Log.Lik.\n-2802.166\n    ELPD\n-2806.3\n    ELPD s.e.\n32.1\n    LOOIC\n5612.5\n    LOOIC s.e.\n64.2\n    WAIC\n5612.5\n    RMSE\n0.98\n  \n  \n  \n\n\n\n\n\n\n\n14.4.2 Assumptions\nIf we want to use difference-in-differences, then we need to satisfy the assumptions. There were three that were touched on earlier, but here we will focus on the “parallel trends” assumption. The parallel trends assumption haunts everything to do with difference-in-differences analysis because we can never prove it; we can just be convinced of it, and try to convince others.\nTo see why we can never prove it, consider an example in which we want to know the effect of a new stadium on a professional sports team’s wins/loses. To do this we consider two professional basketball teams: the Golden State Warriors and the Toronto Raptors. The Warriors changed stadiums at the start of the 2019-20 season, while the Raptors did not, so we will consider four time periods: the 2016-17 season, 2017-18 season, 2018-19 season, and finally we will compare the performance with the one after they moved, so the 2019-20 season. The Raptors here act as our counterfactual. This means that we assume the relationship between the Warriors and the Raptors, in the absence of a new stadium, would have continued to change in a consistent way. But the fundamental problem of causal inference means that we can never know that for certain. We must present sufficient evidence to assuage any concerns that a reader may have.\nThere are four main threats to validity when we use difference-in-differences, and we need to address all of them (Cunningham 2021, 272–77):\n\nNon-parallel trends. The treatment and control groups may be based on differences. As such it can be difficult to convincingly argue for parallel trends. In this case, maybe try to find another factor to consider in your model that may adjust for some of that. This may require triple-differenced approaches. For instance, in the earlier example, we could perhaps add the San Francisco 49ers, a football team, as they are in the same broad geographic area as the Warriors. Or maybe rethink the analysis to see if we can make a different control group. Adding additional earlier time periods may help but may introduce more issues, which we touch on in the third point.\nCompositional differences. This is a concern when working with repeated cross-sections. What if the composition of those cross-sections change? For instance, if we are working at an app that is rapidly growing, and we want to look at the effect of some change. In our initial cross-section, we may have mostly young people, but in a subsequent cross-section, we may have more older people as the demographics of the app usage change. Hence our results may just be an age-effect, not an effect of the change that we are interested in.\nLong-term effects compared with reliability. As we discussed in Chapter 8, there is a trade-off between the length of the analysis that we run. As we run the analysis for longer there is more opportunity for other factors to affect the results. There is also increased chance for someone who was not treated to be treated. But, on the other hand, it can be difficult to convincingly argue that short-term results will continue in the long term.\nFunctional form dependence. This is less of an issue when the outcomes are similar, but if they are different then functional form may be responsible for some aspects of the results.\n\n\n\n14.4.3 French newspaper prices between 1960 and 1974\nIn this case study we introduce Angelucci and Cagé (2019). They are interested in understanding the effect of the introduction of television on French newspapers. We will replicate one of the main findings.\nThe business model of newspapers has been challenged by the internet and many local newspapers have closed. This issue is not new. When television was introduced, there were similar concerns. Angelucci and Cagé (2019) use the introduction of television advertising in France, announced in 1967, to examine the effect of decreased advertising revenue on newspapers. They create a dataset of French newspapers from 1960 to 1974 and then use difference-in-differences to examine the effect of the reduction in advertising revenues on newspapers’ content and prices. The change that they focus on is the introduction of television advertising, which they argue affected national newspapers more than local newspapers. They find that this change results in both less journalism content in the newspapers and lower newspaper prices. Focusing on this change, and analyzing it using difference-in-differences, is important because it allows us to disentangle a few competing effects. For instance, did newspapers become redundant because they could no longer charge high prices for their advertisements, or because consumers preferred to get their news from the television?\nWe can get free access to the data that underpins Angelucci and Cagé (2019) after registration. The dataset is in the Stata data format, “.dta”, which we can read with read_dta() from haven. The file that we are interested in is “Angelucci_Cage_AEJMicro_dataset.dta”, which is the “dta” folder.\n\nnewspapers &lt;- read_dta(\"Angelucci_Cage_AEJMicro_dataset.dta\")\n\nThere are 1,196 observations in the dataset and 52 variables. Angelucci and Cagé (2019) are interested in the 1960-1974 time period which has around 100 newspapers. There are 14 national newspapers at the beginning of the period and 12 at the end. The key period is 1967, when the French government announced it would allow advertising on television. Angelucci and Cagé (2019) argue that national newspapers were affected by this change, but local newspapers were not. The national newspapers are the treatment group and the local newspapers are the control group.\nWe focus just on the headline difference-in-differences result and construct summary statistics.\n\nnewspapers &lt;-\n  newspapers |&gt;\n  select(\n    year, id_news, after_national, local, national, ra_cst, ps_cst, qtotal\n    ) |&gt; \n  mutate(ra_cst_div_qtotal = ra_cst / qtotal, \n         across(c(id_news, after_national, local, national), as.factor),\n         year = as.integer(year))\n\nnewspapers\n\n# A tibble: 1,196 × 9\n    year id_news after_national local national    ra_cst ps_cst  qtotal\n   &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  1960 1       0              1     0         52890272   2.29  94478.\n 2  1961 1       0              1     0         56601060   2.20  96289.\n 3  1962 1       0              1     0         64840752   2.13  97313.\n 4  1963 1       0              1     0         70582944   2.43 101068.\n 5  1964 1       0              1     0         74977888   2.35 102103.\n 6  1965 1       0              1     0         74438248   2.29 105169.\n 7  1966 1       0              1     0         81383000   2.31 126235.\n 8  1967 1       0              1     0         80263152   2.88 128667.\n 9  1968 1       0              1     0         87165704   3.45 131824.\n10  1969 1       0              1     0        102596384   3.28 132417.\n# ℹ 1,186 more rows\n# ℹ 1 more variable: ra_cst_div_qtotal &lt;dbl&gt;\n\n\nWe are interested in what happened from 1967 onward, especially in terms of advertising revenue, and whether that was different for national, compared with local newspapers (Figure 14.13). We use scales to adjust the y-axis.\n\nnewspapers |&gt;\n  mutate(type = if_else(local == 1, \"Local\", \"National\")) |&gt;\n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(\n    labels = dollar_format(\n      prefix = \"$\",\n      suffix = \"M\",\n      scale = 0.000001)) +\n  labs(x = \"Year\", y = \"Advertising revenue\") +\n  facet_wrap(vars(type), nrow = 2) +\n  theme_minimal() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")\n\n\n\n\nFigure 14.13: Revenue of French newspapers (1960-1974), by whether they were local or national\n\n\n\n\nThe model that we are interested in estimating is:\n\\[\\mbox{ln}(y_{n,t}) = \\beta_0 + \\beta_1\\times(\\mbox{National binary}\\times\\mbox{1967 onward binary}) + \\lambda_n + \\gamma_t + \\epsilon\\]\nIt is the \\(\\beta_1\\) coefficient that we are especially interested in. We estimate the models using stan_glm().\n\nad_revenue &lt;-\n  stan_glm(\n    formula = log(ra_cst) ~ after_national + id_news + year,\n    data = newspapers,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  ad_revenue,\n  file = \"ad_revenue.rds\"\n)\n\nad_revenue_div_circulation &lt;-\n  stan_glm(\n    formula = log(ra_cst_div_qtotal) ~ after_national + id_news + year,\n    data = newspapers,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  ad_revenue_div_circulation,\n  file = \"ad_revenue_div_circulation.rds\"\n)\n\n# Consumer side\nsubscription_price &lt;-\n  stan_glm(\n    formula = log(ps_cst) ~ after_national + id_news + year,\n    data = newspapers,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  subscription_price,\n  file = \"subscription_price.rds\"\n)\n\n\nad_revenue &lt;-\n  readRDS(file = \"ad_revenue.rds\")\n\nad_revenue_div_circulation &lt;-\n  readRDS(file = \"ad_revenue_div_circulation\")\n\nsubscription_price &lt;-\n  readRDS(file = \"subscription_price.rds\")\n\nLooking at the advertising-side variables, such as revenue and prices, in Table 14.2 we find consistently negative coefficients.\n\nselected_variables &lt;- c(\"year\" = \"Year\", \"after_national1\" = \"After change\")\n\nmodelsummary(\n  models = list(\n    \"Ad revenue\" = ad_revenue,\n    \"Ad revenue over circulation\" = ad_revenue_div_circulation,\n    \"Subscription price\" = subscription_price\n  ),\n  fmt = 2,\n  coef_map = selected_variables\n)\n\n\n\n\n\nTable 14.2:  Effect of changed television advertising laws on revenue of French\nnewspapers (1960-1974) \n  \n    \n    \n       \n      Ad revenue\n      Ad revenue over circulation\n      Subscription price\n    \n  \n  \n    Year\n0.05\n0.04\n0.05\n    After change\n-0.23\n-0.15\n-0.04\n    Num.Obs.\n1052\n1048\n1044\n    R2\n0.984\n0.896\n0.868\n    R2 Adj.\n0.983\n0.886\n0.852\n    Log.Lik.\n336.539\n441.471\n875.559\n    ELPD\n257.4\n362.3\n793.5\n    ELPD s.e.\n34.4\n45.6\n24.3\n    LOOIC\n-514.8\n-724.6\n-1586.9\n    LOOIC s.e.\n68.9\n91.2\n48.6\n    WAIC\n-515.9\n-725.5\n-1588.9\n    RMSE\n0.17\n0.16\n0.10\n  \n  \n  \n\n\n\n\n\nWe can replicate the main results of Angelucci and Cagé (2019) and find that in many cases there appears to be a difference from 1967 onward. Angelucci and Cagé (2019, 353–58) also include an excellent example of the discussion of interpretation, external validity, and robustness that is required for difference-in-differences models."
  },
  {
    "objectID": "14-causality_from_obs.html#propensity-score-matching",
    "href": "14-causality_from_obs.html#propensity-score-matching",
    "title": "14  Causality from observational data",
    "section": "14.5 Propensity score matching",
    "text": "14.5 Propensity score matching\nDifference-in-differences is a powerful analysis framework. But it can be tough to identify appropriate treatment and control groups. Alexander and Ward (2018) compare migrant brothers, where one brother had most of their education in a different country, and the other brother had most of their education in the United States. Given the data that are available, this match provides a reasonable treatment and control group. But other matches could have given different results, for instance friends or cousins.\nWe can only match based on observable variables. For instance, age-group or education. At two different times we compare smoking rates in 18-year-olds in one city with smoking rates in 18-year-olds in another city. This would be a coarse match because we know that there are many differences between 18-year-olds, even in terms of the variables that we commonly observe, say gender and education. One way to deal with this would be to create sub-groups: 18-year-old males with a high school education, etc. But then the sample sizes quickly become small. We also have the issue of how to deal with continuous variables. And is an 18-year-old really that different to a 19-year-old? Why not also compare with them?\nOne way to proceed is to consider a nearest neighbor approach, but there can be limited concern for uncertainty with this approach. There can also be an issue with having many variables because we end up with a high-dimension graph. This leads to propensity score matching. Here we explain the process of propensity score matching and a few of the concerns that are commonly brought up about it.\nPropensity score matching involves assigning some probability—the “propensity score”—to each observation. We construct that probability based on the observation’s values for the predictors without the treatment. That probability is our best guess at the probability of the observation being treated, regardless of whether it was actually treated. For instance, if 18-year-old males were treated but 19-year-old males were not, then, as there is not much difference between 18-year-old and 19-year-old males in general, our assigned probability would be similar. We then compare the outcomes of observations with similar propensity scores.\n\n14.5.1 Simulated example: free shipping\nOne advantage of propensity score matching is that it allows us to easily consider many predictor variables at once, and it can be constructed using logistic regression. To be more specific we can simulate some data. We will pretend that we work for a large online retailer. We are going to treat some individuals with free shipping to see what happens to their average purchase.\n\nset.seed(853)\n\nsample_size &lt;- 10000\n\npurchase_data &lt;-\n  tibble(\n    unique_person_id = 1:sample_size,\n    age = sample(x = 18:100, size = sample_size, replace = TRUE),\n    gender = sample(\n      x = c(\"Female\", \"Male\", \"Other/decline\"),\n      size = sample_size,\n      replace = TRUE,\n      prob = c(0.49, 0.47, 0.02)\n    ),\n    income = rnorm(n = sample_size, mean = 60000, sd = 15000) |&gt; round(0)\n  ) \n\npurchase_data\n\n# A tibble: 10,000 × 4\n   unique_person_id   age gender income\n              &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1                1    26 Male    68637\n 2                2    81 Female  71486\n 3                3    34 Male    75652\n 4                4    46 Male    68068\n 5                5   100 Female  73206\n 6                6    20 Male    41872\n 7                7    50 Female  75957\n 8                8    36 Female  56566\n 9                9    72 Male    54621\n10               10    52 Female  40722\n# ℹ 9,990 more rows\n\n\nThen we need to add some probability of being treated with free shipping. We will say that it depends on our predictors and that younger, higher-income, male individuals make this treatment more likely. We only know that because we simulated the situation. We would not know it if we were using actual data.\n\npurchase_data &lt;- \n  purchase_data |&gt;\n  mutate(\n    # change characteristics to bounded numbers\n    age_num = rank(1 / age, ties.method = \"random\") %/% 3000,\n    # force it between 0 and 3\n    gender_num = case_when(\n      gender == \"Male\" ~ 3,\n      gender == \"Female\" ~ 2,\n      gender == \"Other/decline\" ~ 1\n    ),\n    income_num = rank(income, ties.method = \"random\") %/% 3000\n  ) |&gt;\n  mutate(\n    sum_num = age_num + gender_num + income_num,\n    softmax_prob = exp(sum_num) / exp(max(sum_num) + 0.5),\n    free_shipping = rbinom(n = sample_size, size = 1, prob = softmax_prob)) |&gt;\n  select(-(age_num:softmax_prob))\n\nFinally, we need to have some measure of a person’s average spend. We will assume that this increases with income. We want those with free shipping to be slightly higher than those without.\n\npurchase_data &lt;-\n  purchase_data |&gt;\n  mutate(\n    noise = rnorm(n = nrow(purchase_data), mean = 5, sd = 2),\n    spend = income / 1000 + noise,\n    spend = if_else(free_shipping == 1, spend + 10, spend),\n    spend = as.integer(spend)\n    ) |&gt;\n  select(-noise) |&gt;\n  mutate(across(c(gender, free_shipping), as.factor))\n\npurchase_data\n\n# A tibble: 10,000 × 6\n   unique_person_id   age gender income free_shipping spend\n              &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;int&gt;\n 1                1    26 Male    68637 0                72\n 2                2    81 Female  71486 0                73\n 3                3    34 Male    75652 0                80\n 4                4    46 Male    68068 0                75\n 5                5   100 Female  73206 0                78\n 6                6    20 Male    41872 0                45\n 7                7    50 Female  75957 0                78\n 8                8    36 Female  56566 0                62\n 9                9    72 Male    54621 0                55\n10               10    52 Female  40722 0                47\n# ℹ 9,990 more rows\n\n\nNaively we can see that there is a difference in the average spend between those with free shipping and those without (Table 14.3). But the fundamental concern is what would have the spend have been of those with free shipping if they have not had free shipping. Table 14.3 shows an average comparison but not everyone had the same chance of getting free shipping. So we question the validity of that use of an average comparison. Instead we use propensity score matching to “link” each observation that actually got free shipping with their most similar observation, based on the observable variables, that did not get free shipping.\n\npurchase_data |&gt;\n  summarise(average_spend = round(mean(spend), 2), .by = free_shipping) |&gt;\n  mutate(free_shipping = if_else(free_shipping == 0, \"No\", \"Yes\")) |&gt;\n  kable(\n    col.names = c(\"Received free shipping?\", \"Average spend\"),\n    booktabs = TRUE, linesep = \"\"\n  )\n\n\n\nTable 14.3: Difference in average spend by whether had free shipping\n\n\nReceived free shipping?\nAverage spend\n\n\n\n\nNo\n64.44\n\n\nYes\n86.71\n\n\n\n\n\n\nWe use matchit() from MatchIt to implement logistic regression and create matched groups. We then use match.data() to get the data of matches containing both all 254 people who were actually treated with free shipping and the untreated person who is considered as similar to them, based on propensity score, as possible. The result is a dataset of 508 observations.\n\nmatched_groups &lt;- \n  matchit(\n  free_shipping ~ age + gender + income,\n  data = purchase_data,\n  method = \"nearest\",\n  distance = \"glm\"\n)\n\nmatched_groups\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 10000 (original), 508 (matched)\n - target estimand: ATT\n - covariates: age, gender, income\n\nmatched_dataset &lt;- match.data(matched_groups)\n\nmatched_dataset\n\n# A tibble: 508 × 9\n   unique_person_id   age gender     income free_shipping spend distance weights\n              &lt;int&gt; &lt;int&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;         &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1               23    28 Female      65685 1                79  0.0334        1\n 2               24    67 Male        71150 0                76  0.0220        1\n 3               32    22 Female      86071 0                92  0.131         1\n 4               48    66 Female     100105 0               108  0.0473        1\n 5               59    25 Male        55548 1                68  0.0541        1\n 6               82    66 Male        70721 0                75  0.0224        1\n 7               83    58 Male        83443 0                88  0.0651        1\n 8               87    46 Male        59073 1                73  0.0271        1\n 9              119    89 Other/dec…  72284 0                74  0.00301       1\n10              125    51 Female      81164 1                96  0.0303        1\n# ℹ 498 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFinally, we can estimate the effect of being treated on average spend using linear regression (Table 14.4). We are particularly interested in the coefficient associated with the treatment variable, in this case free shipping.\n\npropensity_score_regression &lt;- lm(\n  spend ~ age + gender + income + free_shipping,\n  data = matched_dataset\n)\n\nmodelsummary(propensity_score_regression)\n\n\n\n\n\nTable 14.4:  Effect of being treated, using simulated data \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n3.862\n    \n(0.506)\n    age\n0.007\n    \n(0.005)\n    genderMale\n0.013\n    \n(0.202)\n    genderOther/decline\n-0.509\n    \n(0.847)\n    income\n0.001\n    \n(0.000)\n    free_shipping1\n10.073\n    \n(0.180)\n    Num.Obs.\n508\n    R2\n0.983\n    R2 Adj.\n0.983\n    AIC\n2167.6\n    BIC\n2197.2\n    Log.Lik.\n-1076.811\n    F\n5911.747\n    RMSE\n2.02\n  \n  \n  \n\n\n\n\n\nIn Table 14.4, which was based on only the matched sample, we find that the effect is what we simulated. That is, there is a difference of ten between the average spend of those who received free shipping and those that did not. That is in contrast to Table 14.3 which was based on the entire sample.\nWe cover propensity score matching because it is widely used. But there are tradeoffs. Transparency is needed when it is being used (Greifer 2021). These concerns include (King and Nielsen 2019):\n\nUnobservables. Propensity score matching cannot match on unobserved variables. This may be fine in a classroom setting, but in more realistic settings it will likely cause issues. It is difficult to understand why individuals that appear to be so similar, would have received different treatments, unless there is something unobserved that causes the difference. As propensity score matching cannot account for these, it is difficult to know which features are actually being brought together.\nModeling. The results of propensity score matching tend to be specific to the model that is used. As there is considerable flexibility as to which model is used, this enables researchers to pick through matches to find one that suits. Additionally, because the two regression steps (the matching and the analysis) are conducted separately, there is no propagation of uncertainty.\n\nThe fundamental problem of unobservables can never be shown to be inconsequential because that would require the unobserved data. Those who want to use propensity score matching, and other matching methods, need to be able to argue convincingly that it is appropriate. McKenzie (2021) presents a few cases where this is possible, for instance, when there are capacity limits. As is the common theme of this book, such cases will require focusing on the data and a deep understanding of the situation that produced it."
  },
  {
    "objectID": "14-causality_from_obs.html#regression-discontinuity-design",
    "href": "14-causality_from_obs.html#regression-discontinuity-design",
    "title": "14  Causality from observational data",
    "section": "14.6 Regression discontinuity design",
    "text": "14.6 Regression discontinuity design\nRegression discontinuity design (RDD) was established by Thistlethwaite and Campbell (1960) and is a popular way to get causality when there is a continuous variable with cut-offs that determine treatment. Is there a difference between a student who gets 79 per cent and a student who gets 80 per cent? Probably not much, but one may get a A-, while the other may get a B+. Seeing that on a transcript could affect who gets a job which could affect income. In this case the percentage is a “forcing variable” or “forcing function” and the cut-off for an A- is a “threshold”. As the treatment is determined by the forcing variable we need to control for that variable. These seemingly arbitrary cut-offs can be seen all the time. Hence, there has been a great deal of work using RDD.\nThere is sometimes slightly different terminology used when it comes to RDD. For instance, Cunningham (2021) refers to the forcing function as a running variable. The exact terminology that is used does not matter provided we use it consistently.\n\n14.6.1 Simulated example: income and grades\nTo be more specific about the situation, we simulate data. We will consider the relationship between income and grades, and simulate there to be a change if a student gets at least 80 (Figure 14.14).\n\nset.seed(853)\n\nnum_observations &lt;- 1000\n\nrdd_example_data &lt;- tibble(\n  person = c(1:num_observations),\n  mark = runif(num_observations, min = 78, max = 82),\n  income = rnorm(num_observations, 10, 1)\n)\n\n## Make income more likely to be higher if mark at least 80\nrdd_example_data &lt;-\n  rdd_example_data |&gt;\n  mutate(\n    noise = rnorm(n = num_observations, mean = 2, sd = 1),\n    income = if_else(mark &gt;= 80, income + noise, income)\n  )\n\nrdd_example_data\n\n# A tibble: 1,000 × 4\n   person  mark income noise\n    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1      1  79.4   9.43 1.87 \n 2      2  78.5   9.69 2.26 \n 3      3  79.9  10.8  1.14 \n 4      4  79.3   9.34 2.50 \n 5      5  78.1  10.7  2.21 \n 6      6  79.6   9.83 2.47 \n 7      7  78.5   8.96 4.22 \n 8      8  79.0  10.5  3.11 \n 9      9  78.6   9.53 0.671\n10     10  78.8  10.6  2.46 \n# ℹ 990 more rows\n\n\n\nrdd_example_data |&gt;\n  ggplot(aes(\n    x = mark,\n    y = income\n  )) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(\n    data = rdd_example_data |&gt; filter(mark &lt; 80),\n    method = \"lm\",\n    color = \"black\",\n    formula = \"y ~ x\"\n  ) +\n  geom_smooth(\n    data = rdd_example_data |&gt; filter(mark &gt;= 80),\n    method = \"lm\",\n    color = \"black\",\n    formula = \"y ~ x\"\n  ) +\n  theme_minimal() +\n  labs(\n    x = \"Mark\",\n    y = \"Income ($)\"\n  )\n\n\n\n\nFigure 14.14: Illustration of simulated data that shows an effect on income from getting a mark that is 80, compared with 79\n\n\n\n\nWe can use a binary variable with linear regression to estimate the effect of getting a mark over 80 on income. We expect the coefficient to be around two, which is what we simulated, and what we find (Table 14.5).\n\nrdd_example_data &lt;-\n  rdd_example_data |&gt;\n  mutate(mark_80_and_over = if_else(mark &lt; 80, 0, 1))\n\nrdd_example &lt;-\n  stan_glm(\n    formula = income ~ mark + mark_80_and_over,\n    data = rdd_example_data,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  rdd_example,\n  file = \"rdd_example.rds\"\n)\n\n\nrdd_example &lt;-\n  readRDS(file = \"rdd_example.rds\")\n\n\nmodelsummary(\n  models = rdd_example,\n  fmt = 2\n)\n\n\n\n\n\nTable 14.5:  Example of regression discontinuity with simulated data \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n5.22\n    mark\n0.06\n    mark_80_and_over\n1.89\n    Num.Obs.\n1000\n    R2\n0.417\n    R2 Adj.\n0.415\n    Log.Lik.\n-1591.847\n    ELPD\n-1595.1\n    ELPD s.e.\n25.4\n    LOOIC\n3190.3\n    LOOIC s.e.\n50.9\n    WAIC\n3190.3\n    RMSE\n1.19\n  \n  \n  \n\n\n\n\n\nThere are various caveats to this estimate that we will discuss, but the essentials of RDD are here. Given an appropriate set-up, and model, RDD can compare favorably to randomized trials (Bloom, Bell, and Reiman 2020).\nWe could also implement RDD using rdrobust. The advantage of this approach is that many common extensions are easily available.\n\nrdrobust(\n  y = rdd_example_data$income,\n  x = rdd_example_data$mark,\n  c = 80,\n  h = 2,\n  all = TRUE\n) |&gt;\n  summary()\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  497          503\nEff. Number of Obs.             497          503\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.000        2.000\nBW bias (b)                   2.000        2.000\nrho (h/b)                     1.000        1.000\nUnique Obs.                     497          503\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     1.913     0.161    11.876     0.000     [1.597 , 2.229]     \nBias-Corrected     1.966     0.161    12.207     0.000     [1.650 , 2.282]     \n        Robust     1.966     0.232     8.461     0.000     [1.511 , 2.422]     \n=============================================================================\n\n\n\n\n14.6.2 Assumptions\nThe key assumptions of RDD are (Cunningham 2021, 163):\n\nThe cut-off is specific, fixed, and known to all.\nThe forcing function is continuous.\n\nThe first assumption is largely about being unable to manipulate the cut-off, and ensures that the cut-off has meaning. The second assumption enables us to be confident that people on either side of the threshold are similar, apart from just happening to just fall on either side of the threshold.\nWhen we discussed randomized control trials and A/B testing in Chapter 8 the randomized assignment of the treatment meant that the control and treatment groups were the same, but for the treatment. Then we moved to difference-in-differences, and we assumed that there was a common trend between the treated and control groups. We allowed that the groups could be different, but that we could “difference out” their differences. Finally, we considered matching, and we said that even if the control and treatment groups seemed different, we were able to match, to some extent, those who were treated with a group that were like them in all ways, apart from the fact that they were not treated.\nIn regression discontinuity we consider a slightly different setting. The two groups are completely different in terms of the forcing variable. They are on either side of the threshold. There is no overlap at all. But we know the threshold and believe that those on either side are essentially matched. Let us consider the 2019 NBA Eastern Conference Semifinals—Toronto and Philadelphia:\n\nGame 1: Raptors win 108-95;\nGame 2: 76ers win 94-89;\nGame 3: 76ers win 116-95;\nGame 4: Raptors win 101-96;\nGame 5: Raptors win 125-89;\nGame 6: 76ers win 112-101; and finally,\nGame 7: Raptors win 92-90, because of a ball that went in after bouncing on the rim four times.\n\nWas there really that much difference between the teams?\nThe continuity assumption is important, but we cannot test this as it is based on a counterfactual. Instead, we need to convince people of it. Ways to do this include:\n\nUsing a test/train set-up.\nTrying different specifications. We are especially concerned if results do not broadly persist with just linear or quadratic functions.\nConsidering different subsets of the data.\nConsidering different windows, which is the term we give to how far each side of the cutoff we examine.\nBeing clear about uncertainty intervals, especially in graphs.\nDiscuss and assuaging concerns about the possibility of omitted variables.\n\nThe threshold is also important. For instance, is there an actual shift or is there a non-linear relationship?\nThere are a variety of weaknesses of RDD, including:\n\nExternal validity may be difficult. For instance, when we think about the A-/B+ example, it is hard to see those generalizing to also B-/C+ students.\nThe important responses are those that are close to the cut-off. This means that even if we have many A and B students, they do not help much. Hence, we need a lot of data or we may have concerns about our ability to support our claims (Green et al. 2009).\nAs the researcher, we have a lot of freedom to implement different options. This means that open science best practice becomes vital.\n\nTo this point we have considered “sharp” RDD. That is, the threshold is strict. But, in reality, often the boundary is a little less strict. In a sharp RDD setting, if we know the value of the forcing function then we know the outcome. For instance, if a student gets a mark of 80 then we know that they got an A-, but if they got a mark of 79 then we know that they got a B+. But with fuzzy RDD it is only known with some probability.\nWe want as “sharp” an effect as possible, but if the thresholds are known, then they will be gamed. For instance, there is a lot of evidence that people run for certain marathon times, and we know that people aim for certain grades. Similarly, from the other side, it is a lot easier for an instructor to just give out As than it is to have to justify Bs. One way to look at this is to consider how “balanced” the sample is on either side of the threshold. We can do this using histograms with appropriate bins. For instance, think of the age-heaping that we found in the cleaned Kenyan census data in Chapter 9.\nAnother key factor for RDD is the possible effect of the decision around the choice of model. For instance, Figure 14.15 illustrates the difference between linear (Figure 14.15 (a)) and polynomial (Figure 14.15 (b)).\n\nsome_data &lt;-\n  tibble(\n    outcome = rnorm(n = 100, mean = 1, sd = 1),\n    running_variable = c(1:100),\n    location = \"before\"\n  )\n\nsome_more_data &lt;-\n  tibble(\n    outcome = rnorm(n = 100, mean = 2, sd = 1),\n    running_variable = c(101:200),\n    location = \"after\"\n  )\n\nboth &lt;-\n  rbind(some_data, some_more_data)\n\nboth |&gt;\n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nboth |&gt;\n  ggplot(aes(x = running_variable, y = outcome, color = location)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = y ~ poly(x, 3), method = \"lm\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n(b) Polynomial\n\n\n\n\nFigure 14.15: Comparing the result of considering the same situation with different functions\n\n\n\nThe result is that our estimate of the difference in outcome is dependent on the choice of model. We see this issue occur often in RDD (Gelman 2019) and it is especially recommended that higher order polynomials not be used, and instead the choice of models be either linear, quadratic, or some other smooth function (Gelman and Imbens 2019).\nRDD is a popular approach, but meta-analysis suggests that standard errors are often inappropriately small and this could result in spurious results (Stommes, Aronow, and Sävje 2021). If you use RDD it is critical that you discuss the possibility of much wider standard errors than are reported by software packages, and what effect this would have on your conclusions.\n\n\n14.6.3 Alcohol and crime in California\nThere are many opportunities to use regression discontinuity design. For instance, we often see it used in elections where one candidate barely wins. Caughey and Sekhon (2011) examine US House elections between 1942 and 2008 and showed that there is considerable difference between bare winners and bare losers. They highlight that one of the advantages of regression discontinuity is the fact that the assumptions can be tested. Another common use is when there is a somewhat arbitrary cut-off. For instance, in much of the USA the legal drinking age is 21. Carpenter and Dobkin (2015) consider the possible effect of alcohol on crime by comparing arrests and other records of those who are either side of 21 in California. They find those who are slightly over 21 are slightly more likely to be arrested than those slightly under 21. We will revisit Carpenter and Dobkin (2015) in the context of crime in California.\nWe can obtain their replication data (Carpenter and Dobkin 2014) from here. Carpenter and Dobkin (2015) consider a large number of variables and construct a rate, and average this rate over a fortnight, but for simplicity, we will just consider numbers for a few variables: assault, aggravated assault, DUI, and traffic violations (Figure 14.16).\n\ncarpenter_dobkin &lt;-\n  read_dta(\n    \"P01 Age Profile of Arrest Rates 1979-2006.dta\"\n  )\n\n\ncarpenter_dobkin_prepared &lt;-\n  carpenter_dobkin |&gt;\n  mutate(age = 21 + days_to_21 / 365) |&gt;\n  select(age, assault, aggravated_assault, dui, traffic_violations) |&gt;\n  pivot_longer(\n    cols = c(assault, aggravated_assault, dui, traffic_violations),\n    names_to = \"arrested_for\",\n    values_to = \"number\"\n  )\n\ncarpenter_dobkin_prepared |&gt;\n  mutate(\n    arrested_for =\n      case_when(\n        arrested_for == \"assault\" ~ \"Assault\",\n        arrested_for == \"aggravated_assault\" ~ \"Aggravated assault\",\n        arrested_for == \"dui\" ~ \"DUI\",\n        arrested_for == \"traffic_violations\" ~ \"Traffic violations\"\n      )\n  ) |&gt;\n  ggplot(aes(x = age, y = number)) +\n  geom_point(alpha = 0.05) +\n  facet_wrap(facets = vars(arrested_for), scales = \"free_y\") +\n  theme_minimal()\n\n\n\n\nFigure 14.16: Comparing the number of arrests either side of turning 21 for selected reasons\n\n\n\n\n\ncarpenter_dobkin_aggravated_assault_only &lt;-\n  carpenter_dobkin_prepared |&gt;\n  filter(\n    arrested_for == \"aggravated_assault\",\n    abs(age - 21) &lt; 2\n  ) |&gt;\n  mutate(is_21_or_more = if_else(age &lt; 21, 0, 1))\n\n\nrdd_carpenter_dobkin &lt;-\n  stan_glm(\n    formula = number ~ age + is_21_or_more,\n    data = carpenter_dobkin_aggravated_assault_only,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(0, 2.5, autoscale = TRUE),\n    prior_aux = exponential(rate = 1, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  rdd_example,\n  file = \"rdd_example.rds\"\n)\n\n\nrdd_carpenter_dobkin &lt;-\n  readRDS(file = \"rdd_carpenter_dobkin.rds\")\n\n\nmodelsummary(\n  models = rdd_carpenter_dobkin,\n  fmt = 2\n)\n\n\n\n\n\nTable 14.6:  Examining the effect of alcohol on crime in California \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n145.54\n    age\n3.87\n    is_21_or_more\n13.24\n    Num.Obs.\n1459\n    R2\n0.299\n    R2 Adj.\n0.297\n    Log.Lik.\n-6153.757\n    ELPD\n-6157.3\n    ELPD s.e.\n32.9\n    LOOIC\n12314.6\n    LOOIC s.e.\n65.7\n    WAIC\n12314.6\n    RMSE\n16.42\n  \n  \n  \n\n\n\n\n\nAnd the results are similar if we use rdrobust.\n\nrdrobust(\n  y = carpenter_dobkin_aggravated_assault_only$number,\n  x = carpenter_dobkin_aggravated_assault_only$age,\n  c = 21,\n  h = 2,\n  all = TRUE\n) |&gt;\n  summary()\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1459\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  729          730\nEff. Number of Obs.             729          730\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.000        2.000\nBW bias (b)                   2.000        2.000\nrho (h/b)                     1.000        1.000\nUnique Obs.                     729          730\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    14.126     1.918     7.364     0.000    [10.366 , 17.886]    \nBias-Corrected    16.708     1.918     8.709     0.000    [12.948 , 20.468]    \n        Robust    16.708     2.879     5.804     0.000    [11.066 , 22.350]    \n============================================================================="
  },
  {
    "objectID": "14-causality_from_obs.html#instrumental-variables",
    "href": "14-causality_from_obs.html#instrumental-variables",
    "title": "14  Causality from observational data",
    "section": "14.7 Instrumental variables",
    "text": "14.7 Instrumental variables\nInstrumental variables (IV) is an approach that can be handy when we have some type of treatment and control going on, but we have a lot of correlation with other variables and we possibly do not have a variable that actually measures what we are interested in. Adjusting for observables will not be enough to create a good estimate. Instead we find some variable—the eponymous instrumental variable—that is:\n\ncorrelated with the treatment variable, but\nnot correlated with the outcome.\n\nThis solves our problem because the only way the instrumental variable can have an effect is through the treatment variable, and so we can adjust our understanding of the effect of the treatment variable appropriately. The trade-off is that instrumental variables must satisfy a bunch of different assumptions, and that, frankly, they are difficult to identify ex ante. Nonetheless, when we are able to use them, they are a powerful tool for speaking about causality.\nThe canonical instrumental variables example is smoking. These days we know that smoking causes cancer. But because smoking is correlated with a lot of other variables, for instance, education, it could be that it was actually education that causes cancer. RCTs may be possible, but they are likely to be troublesome in terms of speed and ethics, and so instead we look for some other variable that is correlated with smoking, but not, in and of itself, with lung cancer. In this case, we look to tax rates, and other policy responses, on cigarettes. As the tax rates on cigarettes are correlated with the number of cigarettes that are smoked, but not correlated with lung cancer, other than through their impact on cigarette smoking, through them we can assess the effect of cigarettes smoked on lung cancer.\nTo implement instrumental variables we first regress tax rates on cigarette smoking to get some coefficient on the instrumental variable, and then (in a separate regression) regress tax rates on lung cancer to again, get some coefficient on the instrumental variable. Our estimate is then the ratio of these coefficients, which is described as a “Wald estimate” (Gelman and Hill 2007, 219).\nSometimes instrumental variables are used in the context of random allocation of treatment, such as the Oregon Health Insurance Experiment introduced in Chapter 8. Recall the issue was that a lottery was used to select individuals who were allocated to apply for health insurance, but there was nothing forcing them to do this. Our approach would then be to consider the relationship between being selected and taking up health insurance, and then between various health outcomes and taking up insurance. Our instrumental variable estimate, which would be the ratio, would estimate only those that took up health insurance because they were selected.\nFollowing the language of Gelman and Hill (2007, 216), when we use instrumental variables we make a variety of assumptions including:\n\nIgnorability of the instrument.\nCorrelation between the instrumental variable and the treatment variable.\nMonotonicity.\nExclusion restriction.\n\nAs an aside, the history of instrumental variables is intriguing, and Stock and Trebbi (2003), via Cunningham (2021), provide a brief overview. The method was first published in Wright (1928). This is a book about the effect of tariffs on animal and vegetable oil. Why might instrumental variables be important in a book about tariffs on animal and vegetable oil? The fundamental problem is that the effect of tariffs depends on both supply and demand. But we only know prices and quantities, so we do not know what is driving the effect. We can use instrumental variables to pin down causality. The intriguing aspect is that the instrumental variables discussion is only in “Appendix B” of that book. It would seem odd to relegate a major statistical break through to an appendix. Further, Philip G. Wright, the book’s author, had a son Sewall Wright, who had considerable expertise in statistics and the specific method used in “Appendix B”. Hence the mystery of “Appendix B”: did Philip or Sewall write it? Cunningham (2021), Stock and Trebbi (2003), and Angrist and Krueger (2001) all go into more detail, but on balance feel that it is likely that Philip authored the work.\n\n14.7.1 Simulated example: health status, smoking, and tax rates\nLet us generate some data. We will explore a simulation related to the canonical example of health status, smoking, and tax rates. We are looking to explain how healthy someone is based on the amount they smoke, via the tax rate on smoking. We are going to generate different tax rates by provinces. The tax rate on cigarettes is now similar across the Canadian provinces but that this is fairly recent. Let us assume Alberta had a low tax, and Nova Scotia had a high tax.\nWe are simulating data for illustrative purposes, so we need to impose the answer that we want. When you actually use instrumental variables you will be reversing the process.\n\nset.seed(853)\n\nnum_observations &lt;- 10000\n\niv_example_data &lt;- tibble(\n  person = c(1:num_observations),\n  smoker = \n    sample(x = c(0:1), size = num_observations, replace = TRUE)\n  )\n\nNow we need to relate the number of cigarettes that someone smoked to their health. We will model health status as a draw from the Normal distribution, with either a high or low mean depending on whether the person smokes.\n\niv_example_data &lt;-\n  iv_example_data |&gt;\n  mutate(health = if_else(\n    smoker == 0,\n    rnorm(n = n(), mean = 1, sd = 1),\n    rnorm(n = n(), mean = 0, sd = 1)\n  ))\n\nNow we need a relationship between cigarettes and the province (because in this illustration, the provinces have different tax rates).\n\niv_example_data &lt;- iv_example_data |&gt;\n  mutate(\n    province = case_when(\n      smoker == 0 ~ sample(\n        c(\"Nova Scotia\", \"Alberta\"),\n        size = n(),\n        replace = TRUE,\n        prob = c(1/2, 1/2)\n      ),\n      smoker == 1 ~ sample(\n        c(\"Nova Scotia\", \"Alberta\"),\n        size = n(),\n        replace = TRUE,\n        prob = c(1/4, 3/4)\n      )\n    ),\n    tax = case_when(province == \"Alberta\" ~ 0.3, \n                    province == \"Nova Scotia\" ~ 0.5,\n                    TRUE ~ 9999999\n                    )\n    )\n\niv_example_data\n\n# A tibble: 10,000 × 5\n   person smoker  health province      tax\n    &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1      1      0  1.11   Alberta       0.3\n 2      2      1 -0.0831 Alberta       0.3\n 3      3      1 -0.0363 Alberta       0.3\n 4      4      0  2.48   Alberta       0.3\n 5      5      0  0.617  Nova Scotia   0.5\n 6      6      0  0.748  Alberta       0.3\n 7      7      0  0.499  Alberta       0.3\n 8      8      0  1.05   Nova Scotia   0.5\n 9      9      1  0.113  Alberta       0.3\n10     10      1 -0.0105 Alberta       0.3\n# ℹ 9,990 more rows\n\n\nNow we can look at our data.\n\niv_example_data |&gt;\n  mutate(smoker = as_factor(smoker)) |&gt;\n  ggplot(aes(x = health, fill = smoker)) +\n  geom_histogram(position = \"dodge\", binwidth = 0.2) +\n  theme_minimal() +\n  labs(\n    x = \"Health rating\",\n    y = \"Number of people\",\n    fill = \"Smoker\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(vars(province))\n\n\n\n\nFinally, we can use the tax rate as an instrumental variable to estimate the effect of smoking on health.\n\nhealth_on_tax &lt;- lm(health ~ tax, data = iv_example_data)\nsmoker_on_tax &lt;- lm(smoker ~ tax, data = iv_example_data)\n\ntibble(\n  coefficient = c(\"health ~ tax\", \"smoker ~ tax\", \"ratio\"),\n  value = c(\n    coef(health_on_tax)[\"tax\"],\n    coef(smoker_on_tax)[\"tax\"],\n    coef(health_on_tax)[\"tax\"] / coef(smoker_on_tax)[\"tax\"]\n  )\n)\n\n# A tibble: 3 × 2\n  coefficient   value\n  &lt;chr&gt;         &lt;dbl&gt;\n1 health ~ tax  1.24 \n2 smoker ~ tax -1.27 \n3 ratio        -0.980\n\n\nBy understanding the effect of tax rates on both smoking and health, we find that if you smoke then your health is likely to be worse than if you do not smoke.\nWe can use iv_robust() from estimatr to estimate IV (Table 14.7). One nice reason for doing this is that it can help to keep everything organized and adjust the standard errors.\n\niv_robust(health ~ smoker | tax, data = iv_example_data) |&gt;\n  modelsummary()\n\n\n\n\n\nTable 14.7:  Instrumental variable example using simulated data \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n0.977\n    \n(0.041)\n    smoker\n-0.980\n    \n(0.081)\n    Num.Obs.\n10000\n    R2\n0.201\n    R2 Adj.\n0.201\n    AIC\n28342.1\n    BIC\n28363.7\n    RMSE\n1.00\n  \n  \n  \n\n\n\n\n\n\n\n14.7.2 Assumptions\nThe set-up of instrumental variables is described in Figure 14.18, which shows education as a confounder between income and happiness. A tax rebate likely only affects income, not education, and could be used as an instrumental variable.\n\ndigraph D {\n  \n  node [shape=plaintext, fontname = \"helvetica\"];\n  a [label = \"Income\"]\n  b [label = \"Happiness\"]\n  c [label = \"Education\"]\n  d [label = \"Tax rebate\"]\n  { rank=same a b};\n  \n  a-&gt;b\n  c-&gt;a\n  c-&gt;b\n  d-&gt;a\n}\n\n\n\n\n\n\n\nD\n\n  \n\na\n\nIncome   \n\nb\n\nHappiness   \n\na-&gt;b\n\n    \n\nc\n\nEducation   \n\nc-&gt;a\n\n    \n\nc-&gt;b\n\n    \n\nd\n\nTax rebate   \n\nd-&gt;a\n\n   \n\n\nFigure 14.18: Education as a confounder for the relationship between income and happiness with tax rebate as an instrumental variable\n\n\n\n\nAs discussed earlier, there are a variety of assumptions that are made when using instrumental variables. The two most important are:\n\nExclusion Restriction. This assumption is that the instrumental variable only affects the outcome variable through the predictor variable of interest.\nRelevance. There must actually be a relationship between the instrumental variable and the predictor variable.\n\nThere is typically a trade-off between these two. There are plenty of variables that satisfy one, precisely because they do not satisfy the other. Cunningham (2021, 211) describes how one test of a good instrument is if people are initially confused before you explain it to them, only to think it obvious in hindsight.\nRelevance can be tested using regression and other tests for correlation. The exclusion restriction cannot be tested. We need to present evidence and convincing arguments. The difficult aspect is that the instrument needs to seem irrelevant because that is the implication of the exclusion restriction (Cunningham 2021, 225).\nInstrumental variables is a useful approach because one can obtain causal estimates even without explicit randomization. Finding instrumental variables used to be a bit of a white whale, especially in academia. But there has been increased use of IV approaches downstream of A/B tests (Taddy 2019, 162).\nFor a long time, the canonical instrumental variable was rainfall, or more generally, the weather. However, the issue is that if the instrumental variable is correlated with other, potentially unobserved, variables, then they could be correlated with the variable of interest. This is a similar criticism to that above of propensity score matching. Mellon (2023) found a large number of variables have been linked to weather in instrumental variable papers. It would seem that the likelihood of incorrectly estimated effects in some of them is quite high.\nWhen considering an instrumental variable approach, you should spend considerable amounts of time on both of these assumptions. Mellon (2023) shows that we are especially concerned that this particular tax rebate only affects income and no other variable, which could itself be linked with our variables of interest. Approaches based on instrumental variables provide extensive freedom for the researcher, and Brodeur, Cook, and Heyes (2020) find they are more associated with p-hacking and selection reporting compared with RCTs and RDD. As with multiple-imputation, and propensity score matching, we recommend caution when using IV, and that it is never naively turned to. Indeed, Betz, Cook, and Hollenbach (2018) go further and say that spatial instruments are rarely valid."
  },
  {
    "objectID": "14-causality_from_obs.html#exercises",
    "href": "14-causality_from_obs.html#exercises",
    "title": "14  Causality from observational data",
    "section": "14.8 Exercises",
    "text": "14.8 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Two children will both look when an ambulance passes, but only the older one will look if a street car passes, and only the younger one will look when a bike passes. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include at least ten tests based on the simulated data.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Then use rstanarm to build a model.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nWhat is the key assumption when using difference-in-differences?\nPlease read Varner and Sankin (2020) and discuss: i) two statistical aspects; and ii) two ethical aspects.\nPlease go to the GitHub page related to Varner and Sankin (2020). Please list two points about what is good, and another two points about what could be improved.\nWhat are the fundamental features of regression discontinuity design and what are the conditions that are needed in order for regression discontinuity design to be able to be used?\nWhat are some threats to the validity of regression discontinuity design estimates?\nAccording to Meng (2021) “Data science can persuade via\\(\\dots\\)” (pick all that apply):\n\nthe careful establishment of evidence from fair-minded and high-quality data collection\nprocessing and analysis\nthe honest interpretation and communication of findings\nlarge sample sizes\n\nAccording to Riederer (2021) if we have “disjoint treated and untreated groups partitioned by a sharp cut-off” then which method should we use to measure the local treatment effect at the juncture between groups (pick one)?\n\nregression discontinuity\nmatching\ndifference-in-differences\nevent study methods\n\nWhat does causal inference require according to Riederer (2021) (pick all that apply)?\n\ndata management\ndomain knowledge\nprobabilistic reasoning\n\nConsider an Australian 30-39 year old male living in Toronto with two children and a PhD. Which of the following do you think they would match most closely with and why (please explain in a paragraph or two)?\n\nAn Australian 30-39 year old male living in Toronto with one child and a bachelors degree\nA Canadian 30-39 year old male living in Toronto with one child and a PhD\nAn Australian 30-39 year old male living in Ottawa with one child and a PhD\nA Canadian 18-29 year old male living in Toronto with one child and a PhD\n\nWhat is propensity score matching? If you were matching people, then what are some of the features that you would like to match on? What sort of ethical questions does collecting and storing such information raise for you? (Please write at least one paragraph for each question.)\nDraw a DAG illustrating the collider bias described by Bronner (2020).\nKahneman, Sibony, and Sunstein (2021) say “\\(\\dots\\)while correlation does not imply causation, causation does imply correlation. Where there is a causal link, we should find a correlation”. With reference to Cunningham (2021, chap. 1), are they right or wrong, and why?\n\n\n\nTutorial\nYou are interested in the characteristics of people’s friendship groups and how those characteristics relate to individual-level outcomes, particularly economic measures.\nYou have access to individual-level data from a social media website, which contains information about social interactions (comments on posts, tags, etc) on the website, as well as a wide variety of individual-level characteristics.\n\nWhile the social media website is very popular, not everyone in the population you are interested in has an account, and not everyone that has an account is active on the website. Given you are interested in economic measures, what are some possible issues with using these data to make inferences about the broader population?\nThe data do not contain information on individual-level income. But for around 20 per cent of the sample you have information on the “census block” of the individual. By way of background, a census block contains no more than 3,000 individuals. The median income of each census block is known. As such, you decide to estimate individual-level income as follows:\n\nRegress the median income of each census block on a series of individual level characteristics (such as age, education, marital status, gender, \\(\\dots\\)).\nUse these estimates to predict the income of individuals that do not have location information. Briefly discuss the advantages and disadvantages of this approach, particularly in how it could affect the study of income characteristics of friendship groups. Ensure that you address the ecological fallacy.\n\nUnderstandably, the social media website will not allow the unfettered distribution of individual-level data. What are some ways in which you might nonetheless enhance the reproducibility of your work?\n\nThis should take at least two pages.\n\n\n\n\nAlexander, Rohan, and Zachary Ward. 2018. “Age at Arrival and Assimilation During the Age of Mass Migration.” The Journal of Economic History 78 (3): 904–37. https://doi.org/10.1017/S0022050718000335.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of Low Advertising Revenues.” American Economic Journal: Microeconomics 11 (3): 319–64. https://doi.org/10.1257/mic.20170306.\n\n\nAngrist, Joshua, and Alan Krueger. 2001. “Instrumental Variables and the Search for Identification: From Supply and Demand to Natural Experiments.” Journal of Economic Perspectives 15 (4): 69–85. https://doi.org/10.1257/jep.15.4.69.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nBarrett, Malcolm. 2021. ggdag: Analyze and Create Elegant Directed Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold Table Analysis to Hospital Data.” Biometrics Bulletin 2 (3): 47–53. https://doi.org/10.2307/3002000.\n\n\nBetz, Timm, Scott Cook, and Florian Hollenbach. 2018. “On the Use and Abuse of Spatial Instruments.” Political Analysis 26 (4): 474–79. https://doi.org/10.1017/pan.2018.10.\n\n\nBickel, Peter, Eugene Hammel, and William O’Connell. 1975. “Sex Bias in Graduate Admissions: Data from Berkeley: Measuring Bias Is Harder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary to Expectation.” Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and Luke Sonnet. 2021. estimatr: Fast Estimators for Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nBloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data from Randomized Trials to Assess the Likely Generalizability of Educational Treatment-Effect Estimates from Regression Discontinuity Designs.” Journal of Research on Educational Effectiveness 13 (3): 488–517. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBrodeur, Abel, Nikolai Cook, and Anthony Heyes. 2020. “Methods Matter: p-Hacking and Publication Bias in Causal Analysis in Economics.” American Economic Review 110 (11): 3634–60. https://doi.org/10.1257/aer.20190687.\n\n\nBronner, Laura. 2020. “Why Statistics Don’t Capture the Full Extent of the Systemic Bias in Policing.” FiveThirtyEight, June. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\nCalonico, Sebastian, Matias Cattaneo, Max Farrell, and Rocio Titiunik. 2021. rdrobust: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCarpenter, Christopher, and Carlos Dobkin. 2014. “Replication data for: The Minimum Legal Drinking Age and Crime.” https://doi.org/10.7910/DVN/27070.\n\n\n———. 2015. “The Minimum Legal Drinking Age and Crime.” The Review of Economics and Statistics 97 (2): 521–24. https://doi.org/10.1162/REST_a_00489.\n\n\nCaughey, Devin, and Jasjeet Sekhon. 2011. “Elections and the Regression Discontinuity Design: Lessons from Close U.S. House Races, 1942–2008.” Political Analysis 19 (4): 385–408. https://doi.org/10.1093/pan/mpr032.\n\n\nCoyle, Edward, Andrew Coggan, Mari Hopper, and Thomas Walters. 1988. “Determinants of Endurance in Well-Trained Cyclists.” Journal of Applied Physiology 64 (6): 2622–30. https://doi.org/10.1152/jappl.1988.64.6.2622.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark Katz, Miguel Hernán, Marc Lipsitch, Ben Reis, and Ran Balicer. 2021. “BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination Setting.” New England Journal of Medicine 384 (15): 1412–23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nGelman, Andrew. 2019. “Another Regression Discontinuity Disaster and What Can We Learn from It,” June. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order Polynomials Should Not Be Used in Regression Discontinuity Designs.” Journal of Business & Economic Statistics 37 (3): 447–56. https://doi.org/10.1080/07350015.2017.1366909.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023. “rstanarm: Bayesian applied regression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGreen, Donald, Terence Leong, Holger Kern, Alan Gerber, and Christopher Larimer. 2009. “Testing the Accuracy of Regression Discontinuity Analysis Using Experimental Benchmarks.” Political Analysis 17 (4): 400–417. https://doi.org/10.1093/pan/mpp018.\n\n\nGreifer, Noah. 2021. “Why Do We Do Matching for Causal Inference Vs Regressing on Confounders?” Cross Validated, September. https://stats.stackexchange.com/q/544958.\n\n\nHernán, Miguel, David Clayton, and Niels Keiding. 2011. “The Simpson’s Paradox Unraveled.” International Journal of Epidemiology 40 (3): 780–85. https://doi.org/10.1093/ije/dyr041.\n\n\nHernán, Miguel, and James Robins. 2023. What If. 1st ed. Boca Raton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHo, Daniel, Kosuke Imai, Gary King, and Elizabeth Stuart. 2011. “MatchIt: Nonparametric Preprocessing for Parametric Causal Inference.” Journal of Statistical Software 42 (8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) penguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nIannone, Richard. 2022. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nKahneman, Daniel, Olivier Sibony, and Cass Sunstein. 2021. Noise: A Flaw in Human Judgment. William Collins.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis 27 (4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\nMcKenzie, David. 2021. “What Do You Need To Do To Make A Matching Estimator Convincing? Rhetorical vs Statistical Checks.” World Bank Blogs—Development Impact, February. https://blogs.worldbank.org/impactevaluations/what-do-you-need-do-make-matching-estimator-convincing-rhetorical-vs-statistical.\n\n\nMellon, Jonathan. 2023. “Rain, Rain, Go Away: 195 Potential Exclusion-Restriction Violations for Studies Using Weather as an Instrumental Variable.” SocArXiv. https://doi.org/10.31235/osf.io/9qj4f.\n\n\nMeng, Xiao-Li. 2021. “What Are the Values of Data, Data Science, or Data Scientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nPodlogar, Tim, Peter Leo, and James Spragg. 2022. “Using VO2max as a marker of training status in athletes—Can we do better?” Journal of Applied Physiology 133 (6): 144–47. https://doi.org/10.1152/japplphysiol.00723.2021.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRiederer, Emily. 2021. “Causal Design Patterns for Data Analysts,” January. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSekhon, Jasjeet, and Rocío Titiunik. 2017. “Understanding Regression Discontinuity Designs as Observational Studies.” Observational Studies 3 (2): 174–82. https://doi.org/10.1353/obs.2017.0005.\n\n\nSimpson, Edward. 1951. “The Interpretation of Interaction in Contingency Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41. https://doi.org/10.1111/j.2517-6161.1951.tb00088.x.\n\n\nStock, James, and Francesco Trebbi. 2003. “Retrospectives: Who Invented Instrumental Variable Regression?” Journal of Economic Perspectives 17 (3): 177–94. https://doi.org/10.1257/089533003769204416.\n\n\nStommes, Drew, P. M. Aronow, and Fredrik Sävje. 2021. “On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science.” arXiv. https://doi.org/10.48550/ARXIV.2109.14526.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nTang, John. 2015. “Pollution havens and the trade in toxic chemicals: Evidence from U.S. trade flows.” Ecological Economics 112 (April): 150–60. https://doi.org/10.1016/j.ecolecon.2015.02.022.\n\n\nThistlethwaite, Donald, and Donald Campbell. 1960. “Regression-Discontinuity Analysis: An Alternative to the Ex Post Facto Experiment.” Journal of Educational Psychology 51 (6): 309–17. https://doi.org/10.1037/h0044319.\n\n\nVarner, Maddy, and Aaron Sankin. 2020. “Suckers List: How Allstate’s Secret Auto Insurance Algorithm Squeezes Big Spenders.” The Markup, February. https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, and Dana Seidel. 2022. scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWright, Philip. 1928. The Tariff on Animal and Vegetable Oils. New York: Macmillan Company.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "15-mrp.html#introduction",
    "href": "15-mrp.html#introduction",
    "title": "15  Multilevel regression with post-stratification",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\n\n[The Presidential election of] 2016 was the largest analytics failure in US political history.\nDavid Shor, 13 August 2020\n\nMultilevel regression with post-stratification (MRP) is a popular way to adjust non-representative surveys to analyze opinion and other responses. It uses a regression model to relate individual-level survey responses to various characteristics and then rebuilds the sample to better match the population. In this way MRP can not only allow a better understanding of responses, but also allow us to analyze data that may otherwise be unusable. However, it can be a challenge to get started with MRP as the terminology may be unfamiliar, and the data requirements can be onerous.\nConsider a biased survey. For instance, perhaps we conducted a survey about computer preferences at an academic conference, so people with post-graduate degrees are likely over-represented. We are nonetheless interested in making claims about the broader population. Let us say that we found 37.5 per cent of respondents prefer Macs. One way forward is to just ignore the bias and conclude that “37.5 per cent of people prefer Macs”. Another way is to adjust using information that we know. For instance, say 50 per cent of our respondents with a post-graduate degree prefer Macs, and of those without a post-graduate degree, 25 per cent prefer Macs. Then if we knew what proportion of the broader population had a post-graduate degree, say 10 per cent, then we could conduct re-weighting, or post-stratification, to create an estimate: \\(0.5 \\times 0.1 + 0.25 \\times 0.9 = 0.275\\). Our estimate would be that 27.5 per cent of people prefer Macs. MRP is a third approach and uses a model to help do that re-weighting. Here we could use logistic regression to estimate the relationship between computer preferences and highest educational attainment in our survey. We then apply that relationship to a dataset that is representative, in terms of education, of our population. One advantage of this is that we can better account for uncertainty. In terms of a real-world example, Clinton, Lapinski, and Trussler (2022) find a substantial difference in telephone response rates between Democrats and Republicans in the 2020 US Presidential election and that when corrected this reduces average polling error.\nMRP is a handy approach when dealing with survey data. Hanretty (2020) describes how we use MRP because the alternatives either do badly or are expensive. Essentially, MRP trains a model based on the survey, and then applies that trained model to another dataset. There are two main, related, advantages:\n\nIt can allow us to “re-weight” in a way that brings uncertainty front-of-mind and is not as hamstrung by small samples. The alternative way to deal with having a small sample is to either gather more data or throw it away.\nIt can allow us to use broad surveys to speak to subsets in a way that remains representative in certain aspects. For instance, say we gathered a sample that was representative of age, gender, and education across the country. If we were interested in state/provincial-specific estimates there is no guarantee that representativeness would hold at that disaggregated level.\n\nFrom a practical perspective, it tends to be less expensive to collect non-probability samples and so there are benefits of being able to use these types of data. That said, MRP is not a magic bullet and the laws of statistics still apply. We will have larger uncertainty around our estimates than when using probability samples and they will still be subject to all the usual biases. It is an exciting area of research in both academia and industry.\nThe workflow that we need for MRP is straight forward, but the details and decisions that have to be made at each step can become overwhelming. The point to keep in mind is that we are trying to create a relationship between two datasets using a statistical model, and so we need to establish similarity between the two datasets in terms of their variables and levels. The steps are:\n\ngather and prepare the survey dataset, thinking about what is needed for coherence with the post-stratification dataset;\ngather and prepare the post-stratification dataset thinking about what is needed for coherence with the survey dataset;\nmodel the variable of interest from the survey using predictors and levels that are available in both the survey and the post-stratification datasets;\napply the model to the post-stratification data.\n\nOne famous MRP example is Wang et al. (2015). They used data from the Xbox gaming platform to forecast the 2012 US Presidential Election. Wang et al. (2015) were able to implement an opt-in poll through the Xbox gaming platform during the 45 days leading up to the 2012 US presidential election, which was between Barack Obama and Mitt Romney. Each day there were three to five questions, including voter intention: “If the election were held today, who would you vote for?”. Respondents were allowed to answer at most once per day. And first-time respondents were asked to provide information about themselves, including their sex, race, age, education, state, party ID, political ideology, and who they voted for in the 2008 presidential election.\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Andrew Gelman is Higgins Professor of Statistics and Political Science at Columbia University. After earning a PhD in Statistics from Harvard University in 1990, he was appointed as an assistant professor at the University of California, Berkeley, and then moved to Columbia in 1996, where he was promoted to full professor in 2000. His research focuses on statistics, social sciences, and their intersection. For instance, Wang et al. (2015) showed that biased surveys can still have value. He was the principal investigator for Stan, a probabilistic programming language, that is widely used for Bayesian modeling. And he has written many books, with Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill 2007) and Bayesian Data Analysis (Gelman et al. [1995] 2014) having been especially influential on a generation of researchers. He was appointed a Fellow of the American Statistical Association in 1998 and awarded the COPSS Presidents’ Award in 2003.\n\n\nIn total, 750,148 interviews were conducted, with 345,858 unique respondents, over 30,000 of whom completed five or more polls. As may be expected, young men dominate the Xbox population: 18-to-29-year-olds comprise 65 per cent of the Xbox dataset, compared to 19 per cent in the exit poll; and men make up 93 per cent of the Xbox sample but only 47 per cent of the electorate.\nThe details do not matter, but essentially they model how likely a respondent is to vote for Obama, given various information such as state, education, sex, etc. Having a trained model that considers the effect of these various predictors on support for the candidates, they now post-stratify, where each of these “cell-level estimates are weighted by the proportion of the electorate in each cell and aggregated to the appropriate level (i.e., state or national).”\nThey need cross-tabulated population data which counts the number of people in each combination of variables. In general, the census would have worked, or one of the other large surveys available in the US, such as the ACS, which we introduced in Chapter 6. The difficulty is that the variables need to be available on a cross-tabulated basis. As such, they use exit polls, although these are not as widely available in other countries.\nThey make state-specific estimates by post-stratifying to the features of each state. And they similarly examine demographic-differences. Finally, they convert their estimates into electoral college estimates.\nIn general, MRP is a good way to accomplish specific aims, but it is not without trade-offs. If we have a good quality survey, then it may be a way to speak to disaggregated aspects of it. Or if we are concerned about uncertainty then it is a good way to think about that. If we have a biased survey, then it is a great place to start, but it is not a panacea. There is plenty of scope for exciting work from a variety of approaches. For instance, from a more statistical perspective, there is a lot of work to do in terms of thinking through how survey design and modeling approaches interact and the extent to which we are underestimating uncertainty. It is also interesting to think through the implications of small samples and uncertainty in the post-stratification dataset. There is an awful lot to do in terms of thinking through what the appropriate model is to use, and how do we even evaluate what “appropriate” means here, for instance, based on Si (2020). More generally, we have little idea of the conditions under which we will have the stable preferences and relationships that are required for MRP to be accurate. A great deal of work is needed to understand how this relates to uncertainty in survey design, for instance, based on Lauderdale et al. (2020) or Ghitza and Gelman (2020).\nIn this chapter, we begin with simulating a situation in which we pretend that we know the features of the population. We then consider the US 2020 presidential election."
  },
  {
    "objectID": "15-mrp.html#simulated-example-coffee-or-tea",
    "href": "15-mrp.html#simulated-example-coffee-or-tea",
    "title": "15  Multilevel regression with post-stratification",
    "section": "15.2 Simulated example: coffee or tea?",
    "text": "15.2 Simulated example: coffee or tea?\n\n15.2.1 Construct a population and biased sample\nTo get started we will harken back to the tea-tasting experiment in Chapter 8 and simulate a population about whether someone prefers coffee or tea. We will then take a biased sample in favor of tea, and use MRP get those population-level preferences back. We will have two explanatory variables. Age-group will be either “young” or “old”, and nationality will be either “United States” or “England”. The simulation will impose an increased chance of preferring tea of the individual is English and/or old. Everything in our population will be roughly balanced, (that is half and half between each of the variables). But our survey will skew older and English. To be clear, in this example we will “know” the “true” features of the population, but this is not something that occurs when we use real data—it is just to help you understand what is happening in MRP.\n\nset.seed(853)\n\npop_size &lt;- 1000000\n\nsim_population &lt;-\n  tibble(\n    age = rbinom(n = pop_size, size = 1, prob = 0.5),\n    nationality = rbinom(n = pop_size, size = 1, prob = 0.5),\n    probability = (age + nationality + 0.1) / 2.2, # prevent certainty\n    prefers_tea = rbinom(n = pop_size, 1, prob = probability)\n  ) \n\nsim_population\n\n# A tibble: 1,000,000 × 4\n     age nationality probability prefers_tea\n   &lt;int&gt;       &lt;int&gt;       &lt;dbl&gt;       &lt;int&gt;\n 1     0           1      0.5              0\n 2     0           0      0.0455           0\n 3     0           1      0.5              1\n 4     0           0      0.0455           0\n 5     0           0      0.0455           0\n 6     0           0      0.0455           0\n 7     0           1      0.5              0\n 8     0           0      0.0455           0\n 9     0           1      0.5              1\n10     0           0      0.0455           0\n# ℹ 999,990 more rows\n\n\nWe can see that the counts, by group, are fairly similar (Table 15.1).\n\nsim_population |&gt;\n  count(age, nationality, prefers_tea) |&gt; \n  kable(\n    col.names = c(\"Age\", \"Nationality\", \"Prefers tea\", \"Number\"),\n    format.args = list(big.mark = \",\"),\n    booktabs = TRUE,\n    linesep = \"\"\n    )\n\n\n\nTable 15.1: Preference for tea, by age and nationality\n\n\nAge\nNationality\nPrefers tea\nNumber\n\n\n\n\n0\n0\n0\n238,568\n\n\n0\n0\n1\n11,319\n\n\n0\n1\n0\n125,371\n\n\n0\n1\n1\n124,730\n\n\n1\n0\n0\n125,438\n\n\n1\n0\n1\n124,723\n\n\n1\n1\n0\n11,421\n\n\n1\n1\n1\n238,430\n\n\n\n\n\n\nOn average, 50 per cent of the population prefers tea, but this preference depends on the population sub-groups.\nNow we want to pretend that we have some survey that has a biased sample. We will allow that it over-samples older respondents and English respondents. We are interested in looking at what proportion of our biased sample prefers tea to coffee, and expect, by construction, that it will lean toward tea.\n\nset.seed(853)\n\ntea_sample &lt;- \n  sim_population |&gt; \n  slice_sample(n = 1000, weight_by = probability)\n\n\ntea_sample |&gt;\n  count(age, nationality, prefers_tea) |&gt; \n  kable(\n    col.names = c(\"Age\", \"Nationality\", \"Prefers tea\", \"Number\"),\n    format.args = list(big.mark = \",\"),\n    booktabs = TRUE,\n    linesep = \"\"\n    )\n\n\n\nTable 15.2: Biased sample of preferences for tea, by age and nationality, oversampling those who like tea\n\n\nAge\nNationality\nPrefers tea\nNumber\n\n\n\n\n0\n0\n0\n18\n\n\n0\n0\n1\n3\n\n\n0\n1\n0\n119\n\n\n0\n1\n1\n128\n\n\n1\n0\n0\n133\n\n\n1\n0\n1\n126\n\n\n1\n1\n0\n25\n\n\n1\n1\n1\n448\n\n\n\n\n\n\nIt is clear that our sample has a different average tea preference than the overall population (Table 15.2).\n\n\n15.2.2 Model the sample\nWe now train a model based on the biased survey. We explain tea preferences based on age and national origin. There is nothing that says you have to use a multilevel model, but a lot of situations will have circumstances such that it is not likely to do any worse. To be clear, this means that although we have individual-level data, there is some grouping of the individuals that we will take advantage of.\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0 + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{n[i]}^{\\mbox{nat}} \\\\\n\\alpha_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{n}^{\\mbox{nat}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{nat}}\\right)\\mbox{ for }n = 1, 2, \\dots, N\\\\\n\\sigma_{\\mbox{age}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{nat}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]\nwhere \\(y_i\\) is the tea preference of the respondent, \\(\\pi_i = \\mbox{Pr}(y_i=1)\\), and \\(\\alpha^{\\mbox{age}}\\) and \\(\\alpha^{\\mbox{nat}}\\) are the effect of age and national origin, respectively. The \\(a[i]\\) and \\(n[i]\\) refer to which age-group and nationality, respectively, the respondent belongs to. \\(A\\) and \\(N\\) are the total number of age-groups and nationalities, respectively. We will estimate the model with stan_glm().\n\ntea_preference_model &lt;-\n  stan_glmer(\n    prefers_tea ~ (1 | age) + (1 | nationality),\n    data = tea_sample,\n    family = binomial(link = \"logit\"),\n    prior = normal(location = 0, scale = 0.5, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 0.5, autoscale = TRUE),\n    seed = 853\n  )\n\nsaveRDS(\n  tea_preference_model,\n  file = \"tea_preference_model.rds\"\n)\n\n\ntea_preference_model &lt;-\n  readRDS(file = \"tea_preference_model.rds\")\n\n\nmodelsummary(\n  list(\n    \"Tea preferences\" = tea_preference_model\n  )\n)\n\n\n\n\n\nTable 15.3:  Model trained on biased sample that oversamples a preference for\ntea \n  \n    \n    \n       \n      Tea preferences\n    \n  \n  \n    (Intercept)\n0.001\n    Sigma[age × (Intercept),(Intercept)]\n2.171\n    Sigma[nationality × (Intercept),(Intercept)]\n2.337\n    Num.Obs.\n1000\n    R2\n0.261\n    R2 Marg.\n0.000\n    ICC\n0.7\n    Log.Lik.\n-457.970\n    ELPD\n-461.0\n    ELPD s.e.\n16.2\n    LOOIC\n921.9\n    LOOIC s.e.\n32.4\n    WAIC\n921.9\n    RMSE\n0.39\n  \n  \n  \n\n\n\n\n\nFigure 15.1 shows the distribution of draws for each of the different groups.\n\ntea_preference_model |&gt;\n  spread_draws(`(Intercept)`, b[, group]) |&gt;\n  mutate(condition_mean = `(Intercept)` + b) |&gt;\n  ggplot(aes(y = group, x = condition_mean)) +\n  stat_halfeye() +\n  theme_minimal()\n\n\n\n\nFigure 15.1: Examining the distribution of draws for each of the groups\n\n\n\n\n\n\n15.2.3 Post-stratification dataset\nNow we will use a post-stratification dataset to get some estimates of the number in each cell. We typically use a larger dataset that may more closely reflect the population. In the US a popular choice is the American Community Survey (ACS) which we covered in Chapter 6, while in other countries we typically use the census.\nIn this simulated example, we could use the population as our post-stratification dataset. The issue is that at one million observations it is unwieldy, so we take a 10,000 person sample from it. We also remove the tea preferences variable because that is what we are pretending we do not know.\n\nset.seed(853)\n\ntea_poststrat_dataset &lt;- \n  sim_population |&gt; \n  slice_sample(n = 10000) |&gt; \n  select(-prefers_tea)\n\ntea_poststrat_dataset\n\n# A tibble: 10,000 × 3\n     age nationality probability\n   &lt;int&gt;       &lt;int&gt;       &lt;dbl&gt;\n 1     0           1      0.5   \n 2     0           1      0.5   \n 3     0           1      0.5   \n 4     0           1      0.5   \n 5     1           0      0.5   \n 6     1           0      0.5   \n 7     0           0      0.0455\n 8     1           0      0.5   \n 9     1           0      0.5   \n10     1           0      0.5   \n# ℹ 9,990 more rows\n\n\nThis is an idealized example where we assume individual-level data in our post-stratification dataset. In that world we can apply our model to each individual.\n\npredicted_tea_preference &lt;-\n  tea_preference_model |&gt;\n  add_epred_draws(newdata = tea_poststrat_dataset,\n                             value = \"preference\") |&gt;\n  ungroup() |&gt;\n  summarise(\n    average_preference = mean(preference),\n    lower = quantile(preference, 0.025),\n    upper = quantile(preference, 0.975),\n    .by = c(age, nationality, .row)\n  )\n\npredicted_tea_preference |&gt;\n  count(age, nationality, average_preference)\n\n# A tibble: 4 × 4\n    age nationality average_preference     n\n  &lt;int&gt;       &lt;int&gt;              &lt;dbl&gt; &lt;int&gt;\n1     0           0             0.0657  2416\n2     0           1             0.528   2505\n3     1           0             0.496   2544\n4     1           1             0.941   2535\n\n\nTable 15.4 compares the MRP estimates, with the raw estimates from the biased sample. In this case, because we know the truth, we can also compare them to the known truth, but that is not something we can do normally.\n\ncomparison &lt;- tibble(\n  Type = c(\"Truth\", \"Biased sample\", \"MRP estimate\"),\n  Estimate = c(\n    mean(sim_population$prefers_tea),\n    mean(tea_sample$prefers_tea),\n    mean(predicted_tea_preference$average_preference)\n  )\n)\n\ncomparison |&gt; \n  kable(digits = 2,\n    booktabs = TRUE,\n    linesep = \"\")\n\n\n\nTable 15.4: MRP estimates compared with the truth and the biased sample\n\n\nType\nEstimate\n\n\n\n\nTruth\n0.50\n\n\nBiased sample\n0.70\n\n\nMRP estimate\n0.51\n\n\n\n\n\n\nIn this case, the MRP approach has done a good job of taking a biased sample and resulting in an estimate of tea preferences that did reflect the truth."
  },
  {
    "objectID": "15-mrp.html#forecasting-the-2020-united-states-election",
    "href": "15-mrp.html#forecasting-the-2020-united-states-election",
    "title": "15  Multilevel regression with post-stratification",
    "section": "15.3 Forecasting the 2020 United States election",
    "text": "15.3 Forecasting the 2020 United States election\nPresidential elections in the United States have many features that are unique to the United States, but the model that we are going to build here will be generalizable to a variety of settings. We will use survey data from the Democracy Fund Voter Study Group introduced in Chapter 8. They conducted polling in the lead-up to the US election and make this publicly available after registration. We will use IPUMS, introduced in Chapter 6, to access the 2019 American Community Survey (ACS) as a post-stratification dataset. We will use state, age-group, gender, and education as explanatory variables.\n\n15.3.1 Survey data\nWe will use the Democracy Fund Voter Study Group Nationscape survey dataset. One tricky aspect of MRP is ensuring consistency between the survey dataset and the post-stratification dataset. In this case, after reading in the dataset that we cleaned up in Chapter 8 we need to do some work to make the variables consistent.\n\nnationscape_data &lt;- \n  read_csv(file = \"nationscape_data.csv\")\n\n\nnationscape_data\n\n# A tibble: 5,200 × 5\n   gender state vote_biden age_group education_level    \n * &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;              \n 1 female WI             0 45-59     Post sec +         \n 2 female VA             0 45-59     Post sec +         \n 3 female TX             0 60+       High school or less\n 4 female WA             0 45-59     High school or less\n 5 female MA             1 18-29     Some post sec      \n 6 female TX             1 30-44     Some post sec      \n 7 female CA             0 60+       Some post sec      \n 8 female NC             0 45-59     Post sec +         \n 9 female MD             0 60+       Post sec +         \n10 female FL             1 45-59     Some post sec      \n# ℹ 5,190 more rows\n\n\n\n# Format state names to match IPUMS\nstates_names_and_abbrevs &lt;-\n  tibble(stateicp = state.name, state = state.abb)\n\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  left_join(states_names_and_abbrevs, by = \"state\")\n\nrm(states_names_and_abbrevs)\n\n# Make lowercase to match IPUMS data\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  mutate(stateicp = tolower(stateicp))\n\n# Replace NAs with DC\nnationscape_data$stateicp &lt;-\n  replace_na(nationscape_data$stateicp, \"district of columbia\")\n\n# Tidy the class\nnationscape_data &lt;-\n  nationscape_data |&gt;\n  mutate(across(c(gender, stateicp, education_level, age_group), \n                as_factor))\n\nFinally, we save the prepared dataset as a parquet file.\n\nwrite_parquet(x = nationscape_data,\n              sink = \"nationscape_data_cleaned.parquet\")\n\n\n\n15.3.2 Post-stratification data\nWe have many options for a dataset to post-stratify by and there are various considerations. We are after a dataset that is good quality (however that is to be defined), and likely larger. From a strictly data perspective, the best choice would probably be something like the Cooperative Election Study (CES), as used in Chapter 12, but it is only publicly released after the election, which limits the reasonableness of using it for forecasting the election. Wang et al. (2015) use exit poll data, but again that is only available after the election.\nWe will use the 2019 American Community Survey (ACS) dataset that we gathered in Chapter 6.\n\npoststrat_data\n\n# A tibble: 407,354 × 4\n   gender age_group education_level     stateicp\n * &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;               &lt;fct&gt;   \n 1 male   60+       High school or less alabama \n 2 male   60+       Some post sec       alabama \n 3 male   18-29     High school or less alabama \n 4 female 18-29     Some post sec       alabama \n 5 male   30-44     Some post sec       alabama \n 6 female 18-29     High school or less alabama \n 7 female 60+       High school or less alabama \n 8 female 18-29     Some post sec       alabama \n 9 male   60+       High school or less alabama \n10 male   45-59     High school or less alabama \n# ℹ 407,344 more rows\n\n\nThis dataset is on an individual level. We will create counts of each sub-cell, and then proportions by state.\n\npoststrat_data_cells &lt;-\n  poststrat_data |&gt;\n  count(stateicp, gender, age_group, education_level)\n\nAnd finally we add proportions for each of these cells.\n\npoststrat_data_cells &lt;-\n  poststrat_data_cells |&gt;\n  mutate(prop = n / sum(n),\n         .by = stateicp)\n\npoststrat_data_cells\n\n# A tibble: 1,627 × 6\n   stateicp    gender age_group education_level         n    prop\n   &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;     &lt;fct&gt;               &lt;int&gt;   &lt;dbl&gt;\n 1 connecticut male   18-29     High school or less   194 0.0419 \n 2 connecticut male   18-29     Some post sec         128 0.0276 \n 3 connecticut male   18-29     Post sec +             72 0.0156 \n 4 connecticut male   18-29     Grad degree            14 0.00302\n 5 connecticut male   30-44     High school or less   132 0.0285 \n 6 connecticut male   30-44     Some post sec          93 0.0201 \n 7 connecticut male   30-44     Post sec +            147 0.0317 \n 8 connecticut male   30-44     Grad degree            88 0.0190 \n 9 connecticut male   45-59     High school or less   187 0.0404 \n10 connecticut male   45-59     Some post sec          88 0.0190 \n# ℹ 1,617 more rows\n\n\n\n\n15.3.3 Model the sample\nWe are going to use logistic regression to estimate a model where the binary of support for Biden versus Trump is explained by gender, age-group, education, and state.\n\\[\n\\begin{aligned}\ny_i|\\pi_i & \\sim \\mbox{Bern}(\\pi_i) \\\\\n\\mbox{logit}(\\pi_i) & = \\beta_0+ \\alpha_{g[i]}^{\\mbox{gender}} + \\alpha_{a[i]}^{\\mbox{age}} + \\alpha_{s[i]}^{\\mbox{state}} + \\alpha_{e[i]}^{\\mbox{edu}} \\\\\n\\beta_0 & \\sim \\mbox{Normal}(0, 2.5)\\\\\n\\alpha_{g}^{\\mbox{gender}} & \\sim \\mbox{Normal}(0, 2.5)\\mbox{ for }g=1, 2\\\\\n\\alpha_{a}^{\\mbox{age}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{age}}\\right)\\mbox{ for }a = 1, 2, \\dots, A\\\\\n\\alpha_{s}^{\\mbox{state}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{state}}\\right)\\mbox{ for }s = 1, 2, \\dots, S\\\\\n\\alpha_{e}^{\\mbox{edu}} & \\sim \\mbox{Normal}\\left(0, \\sigma^2_{\\mbox{edu}}\\right)\\mbox{ for }e = 1, 2, \\dots, E\\\\\n\\sigma_{\\mbox{gender}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{state}} & \\sim \\mbox{Exponential}(1)\\\\\n\\sigma_{\\mbox{edu}} & \\sim \\mbox{Exponential}(1)\n\\end{aligned}\n\\]\nwhere \\(y_i\\) is whether a respondent supports Biden and \\(\\pi_i = \\mbox{Pr}(y = 1)\\). Then \\(\\alpha^{\\mbox{gender}}\\), \\(\\alpha^{\\mbox{age}}\\), \\(\\alpha^{\\mbox{state}}\\), and \\(\\alpha^{\\mbox{edu}}\\) are the effect of gender, age, state, and education, respectively. The \\(g[i]\\), \\(a[i]\\), \\(s[i]\\), and \\(e[i]\\) refer to which gender, age-group, state, and education level, respectively, the respondent belongs to. \\(A\\), \\(S\\), and \\(E\\) are the total number of age-groups, states, and education levels, respectively.\nAfter reading in the data that we cleaned earlier, following Kennedy and Gabry (2020) we use stan_glmer() from rstanarm to estimate the model.\n\nnationscape_data &lt;- \n  read_parquet(file = \"nationscape_data_cleaned.parquet\")\n\n\nus_election_model &lt;-\n  stan_glmer(\n    vote_biden ~ gender + (1|age_group) + (1|stateicp) + (1|education_level),\n    data = nationscape_data,\n    family = binomial(link = \"logit\"),\n    prior = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    prior_intercept = normal(location = 0, scale = 2.5, autoscale = TRUE),\n    cores = 4,\n    adapt_delta = 0.99,\n    seed = 853\n  )\n\n\nsaveRDS(\n  us_election_model,\n  file = \"us_election_model_mrp.rds\"\n)\n\nThis model will take about 15 minutes to run, so you should be careful to save it afterwards with saveRDS(). And you can load it with readRDS().\n\nus_election_model &lt;-\n  readRDS(file = \"us_election_model_mrp.rds\")\n\nWe might be interested to look at the coefficient estimates (Table 15.5).\n\nmodelsummary(\n  us_election_model\n)\n\n\n\n\n\nTable 15.5:  Estimating a model of choice between Biden and Trump in the 2020 US\nelection \n  \n    \n    \n       \n      (1)\n    \n  \n  \n    (Intercept)\n0.373\n    gendermale\n-0.542\n    Sigma[stateicp × (Intercept),(Intercept)]\n0.081\n    Sigma[education_level × (Intercept),(Intercept)]\n0.036\n    Sigma[age_group × (Intercept),(Intercept)]\n0.241\n    Num.Obs.\n5200\n    R2\n0.056\n    R2 Marg.\n0.018\n    ICC\n0.2\n    Log.Lik.\n-3434.075\n    ELPD\n-3468.0\n    ELPD s.e.\n16.2\n    LOOIC\n6936.1\n    LOOIC s.e.\n32.4\n    WAIC\n6936.0\n    RMSE\n0.48\n  \n  \n  \n\n\n\n\n\nFigure 15.2 shows the distribution of draws for age groups, and education. We plot some selected states separately for reasons of space (Figure 15.3).\n\nus_election_model |&gt;\n  spread_draws(`(Intercept)`, b[, group]) |&gt;\n  mutate(condition_mean = `(Intercept)` + b) |&gt;\n  separate(col = group, \n           into = c(\"type\", \"instance\"), \n           sep = \":\", remove = FALSE) |&gt; \n  filter(type != \"stateicp\") |&gt; \n  ggplot(aes(y = group, x = condition_mean)) +\n  stat_halfeye() +\n  theme_minimal()\n\n\n\n\nFigure 15.2: Examining the distribution of draws for each of the groups\n\n\n\n\n\nus_election_model |&gt;\n  spread_draws(`(Intercept)`, b[, group]) |&gt;\n  mutate(condition_mean = `(Intercept)` + b) |&gt;\n  separate(col = group, into = c(\"type\", \"instance\"), sep = \":\", remove = FALSE) |&gt; \n  filter(type == \"stateicp\") |&gt; \n  filter(instance %in% \n           c(\"california\", \"florida\", \"michigan\", \"new_york\", \"pennsylvania\", \n             \"vermont\", \"west_virginia\", \"wisconsin\")\n         ) |&gt; \n  ggplot(aes(y = group, x = condition_mean)) +\n  stat_halfeye() +\n  theme_minimal()\n\n\n\n\nFigure 15.3: Examining the distribution of draws for selected states\n\n\n\n\n\n\n15.3.4 Post-stratify\nWe now post-stratify according to the population proportions calculated previously, and calculate credible intervals for each state as well.\n\nbiden_support_by_state &lt;-\n  us_election_model |&gt;\n  add_epred_draws(newdata = poststrat_data_cells) |&gt;\n  rename(support_biden_predict = .epred) |&gt;\n  mutate(support_biden_predict_prop = support_biden_predict * prop) |&gt;\n  ungroup() |&gt; \n  summarise(support_biden_predict = sum(support_biden_predict_prop),\n            .by = c(stateicp, .draw)) |&gt;\n  summarise(\n    mean = mean(support_biden_predict),\n    lower = quantile(support_biden_predict, 0.025),\n    upper = quantile(support_biden_predict, 0.975),\n    .by = stateicp\n  )\n\nhead(biden_support_by_state)\n\n# A tibble: 6 × 4\n  stateicp       mean lower upper\n  &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 connecticut   0.624 0.284 0.884\n2 maine         0.535 0.211 0.840\n3 massachusetts 0.623 0.292 0.884\n4 new hampshire 0.531 0.201 0.834\n5 rhode island  0.540 0.206 0.841\n6 vermont       0.582 0.256 0.872\n\n\nAnd we can have a look at our estimates graphically (Figure 15.4).\n\nbiden_support_by_state |&gt;\n  ggplot(aes(y = mean, x = fct_reorder(stateicp, mean), \n             color = \"MRP estimate\")) +\n  geom_point() +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0) +\n  geom_point(\n    data = nationscape_data |&gt;\n      summarise(n = n(),\n                .by = c(stateicp, vote_biden)) |&gt;\n      mutate(prop = n / sum(n),\n             .by = stateicp) |&gt;\n      filter(vote_biden == 1),\n    aes(y = prop, x = stateicp, color = \"Nationscape raw data\")\n    ) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\") +\n  labs(\n    x = \"State\",\n    y = \"Estimated proportion support for Biden\",\n    color = \"Source\"\n  ) +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\") +\n  coord_flip() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 15.4: Comparing the MRP estimates with the Nationscape raw data\n\n\n\n\nThe Nationscape dataset is a high-quality survey. But it was weighted to major census region—the West, the Midwest, the Northeast, and the South—rather than state, which could be one reason we see a difference between the MRP estimates and the raw data."
  },
  {
    "objectID": "15-mrp.html#exercises",
    "href": "15-mrp.html#exercises",
    "title": "15  Multilevel regression with post-stratification",
    "section": "15.4 Exercises",
    "text": "15.4 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Support for a political party is a binary (yes/no), and is related to age-group, gender, income group, and highest education. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include at least ten tests based on the simulated data.\n(Acquire) Please describe one possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Use rstanarm to estimate a model.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nPlease explain what MRP is, and the strengths and weaknesses, being sure to explain any technical terms that you use (write at least three paragraphs).\nWhat are three aspects that you like about Wang et al. (2015)? What are three aspects that do you not? To what extent is it possible to reproduce the paper?\nWith respect to Wang et al. (2015), what is a weakness of MRP (pick one)?\n\nDetailed data requirement.\nAllows use of biased data.\nExpensive to conduct.\n\nWith respect to Wang et al. (2015), what is concerning about the Xbox sample (pick one)?\n\nNon-representative.\nSmall sample size.\nMultiple responses from the same respondent.\n\nWe are interested in studying how voting intentions in the 2020 US presidential election vary by an individual’s income. We set up a logistic regression model to study this relationship. In this study, what are some possible predictors (select all that apply)?\n\nWhether the respondent is registered to vote (yes/no).\nWhether the respondent is going to vote for Biden (yes/no).\nThe race of the respondent (white/not white).\nThe respondent’s marital status (married/not).\n\nPlease think about Cohn (2016). Why is this type of exercise not carried out more? Why do you think that different groups, even with the same background and level of quantitative sophistication, could have such different estimates even when they use the same data?\nWe train a model based on a survey, and then post-stratify it using the ACS dataset. What are some of the practical considerations that we may have to contend with when doing this?\nConsider a situation in which you have a survey dataset with these age-groups: 18-29; 30-44; 45-60; and 60+. And a post-stratification dataset with these age-groups: 18-34; 35-49; 50-64; and 65+. Please write about the approach you would take to bringing these together.\n\n\n\nTutorial\nIn a similar manner to Ghitza and Gelman (2020) pretend you have access to a US voter file record from a private company. You train a model on the 2020 US Cooperative Election Study, and post-stratify it, on an individual basis, based on that voter file.\n\nPut together a datasheet for the voter file dataset following Gebru et al. (2021)? As a reminder, datasheets accompany datasets and document “motivation, composition, collection process, recommended uses,” among other aspects.\nCreate a model card for your model, following Mitchell et al. (2019)? As a reminder, model cards are deliberately straight forward one- or two-page documents that report aspects such as: model details; intended use; metrics; training data; ethical considerations; as well as caveats and recommendations (Mitchell et al. 2019).\nDiscuss three ethical aspects around the features that you are using in your model? [Please write a paragraph or two for each point.]\nDetail the protections that you would put in place in terms of the dataset, the model, and the predictions?\n\n\n\nPaper\nAt about this point the Spofforth Paper from Online Appendix D would be appropriate.\n\n\n\n\nAlexander, Monica. 2019. “Analyzing Name Changes After Marriage Using a Non-Representative Survey,” August. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\nArel-Bundock, Vincent. 2022. “modelsummary: Data and Model Summaries in R.” Journal of Statistical Software 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed Models. https://CRAN.R-project.org/package=broom.mixed.\n\n\nClinton, Joshua, John Lapinski, and Marc Trussler. 2022. “Reluctant Republicans, Eager Democrats?” Public Opinion Quarterly 86 (2): 247–69. https://doi.org/10.1093/poq/nfac011.\n\n\nCohn, Nate. 2016. “We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results.” The New York Times, September. https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html.\n\n\nGalef, Julia. 2020. “Episode 248: Are Democrats Being Irrational? (David Shor).” Rationally Speaking, December. http://rationallyspeakingpodcast.org/248-are-democrats-being-irrational-david-shor/.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGelman, Andrew. 2020. “Statistical Models of Election Outcomes.” YouTube, August. https://youtu.be/7gjDnrbLQ4k.\n\n\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. (1995) 2014. Bayesian Data Analysis. 3rd ed. Chapman; Hall/CRC.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. 1st ed. Cambridge University Press.\n\n\nGhitza, Yair, and Andrew Gelman. 2020. “Voter Registration Databases and MRP: Toward the Use of Large-Scale Databases in Public Opinion Research.” Political Analysis 28 (4): 507–31. https://doi.org/10.1017/pan.2020.3.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023. “rstanarm: Bayesian applied regression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister P helps us understand vaccine hesitancy,” December. https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nHanretty, Chris. 2020. “An Introduction to Multilevel Regression and Post-Stratification for Estimating Constituency Opinion.” Political Studies Review 18 (4): 630–45. https://doi.org/10.1177/1478929919864773.\n\n\nJohnston, Myfanwy, and David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nKay, Matthew. 2022. tidybayes: Tidy Data and Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKennedy, Lauren, and Jonah Gabry. 2020. “MRP with rstanarm,” July. https://mc-stan.org/rstanarm/articles/mrp.html.\n\n\nLarmarange, Joseph. 2023. labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLauderdale, Benjamin, Delia Bailey, Jack Blumenau, and Douglas Rivers. 2020. “Model-Based Pre-Election Polling for National and Sub-National Outcomes in the US and UK.” International Journal of Forecasting 36 (2): 399–413. https://doi.org/10.1016/j.ijforecast.2019.05.012.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” Proceedings of the Conference on Fairness, Accountability, and Transparency, January. https://doi.org/10.1145/3287560.3287596.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nSi, Yajuan. 2020. “On the Use of Auxiliary Variables in Multilevel Regression and Poststratification.” https://arxiv.org/abs/2011.00360.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015. “Forecasting Elections with Non-Representative Polls.” International Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "16-text.html#introduction",
    "href": "16-text.html#introduction",
    "title": "16  Text as data",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nText is all around us. In many cases, text is the earliest type of data that we are exposed to. Increases in computational power, the development of new methods, and the enormous availability of text, mean that there has been a great deal of interest in using text as data. Using text as data provides opportunities for unique analyses. For instance:\n\ntext analysis of state-run newspapers in African countries can identify manipulation by governments (Hassan 2022);\nthe text from UK daily newspapers can be used to generate better forecasts of GDP and inflation (Kalamara et al. 2022), and similarly, The New York Times can be used to create an uncertainty index which correlates with US economic activity (Alexopoulos and Cohen 2015);\nthe analysis of notes in Electronic Health Records (EHR) can improve the efficiency of disease prediction (Gronsbell et al. 2019); and\nanalysis of US congressional records indicates just how often women legislators are interrupted by men (Miller and Sutherland 2022).\n\nEarlier approaches to the analysis of text tend to convert words into numbers, divorced of context. They could then be analyzed using traditional approaches, such as variants of logistic regression. More recent methods try to take advantage of the structure inherent in text, which can bring additional meaning. The difference is perhaps like a child who can group similar colors, compared with a child who knows what objects are; although both crocodiles and trees are green, and you can do something with that knowledge, it is useful to know that a crocodile could eat you while a tree probably would not.\nText can be considered an unwieldy, yet similar, version of the datasets that we have used throughout this book. The main difference is that we will typically begin with wide data, where each variable is a word, or token more generally. Often each entry is then a count. We would then typically transform this into rather long data, with one variable of words and another of the counts. Considering text as data naturally requires some abstraction from its context. But this should not be entirely separated as this can perpetuate historical inequities. For instance, Koenecke et al. (2020) find that automated speech recognition systems perform much worse for Black compared with White speakers, and Davidson, Bhattacharya, and Weber (2019) find that tweets that use Black American English, which is a specifically defined technical term, are classified at hate speech at higher rates than similar tweets in Standard American English, which again is a technical term.\nOne exciting aspect of text data is that it is typically not generated for the purposes of our analysis. The trade-off is that we typically must do a bunch more work to get it into a form that we can work with. There are a lot of decisions to be made in the data cleaning and preparation stages.\nThe larger size of text datasets means that it is especially important to simulate, and start small, when it comes to their analysis. Using text as data is exciting because of the quantity and variety of text that is available to us. But in general, dealing with text datasets is messy. There is a lot of cleaning and preparation that is typically required. Often, text datasets are large. As such having a reproducible workflow in place and then clearly communicating your findings, becomes critical. Nonetheless, it is an exciting area.\nIn this chapter we first consider preparing text datasets. We then consider Term Frequency-Inverse Document Frequency (TF-IDF) and topic models."
  },
  {
    "objectID": "16-text.html#text-cleaning-and-preparation",
    "href": "16-text.html#text-cleaning-and-preparation",
    "title": "16  Text as data",
    "section": "16.2 Text cleaning and preparation",
    "text": "16.2 Text cleaning and preparation\nText modeling is an exciting area of research. But, and this is true more generally, the cleaning and preparation aspect is often at least as difficult as the modeling. We will cover some essentials and provide a foundation that can be built on.\nThe first step is to get some data. We discussed data gathering in Chapter 7 and mentioned in passing many sources including:\n\nUsing Inside Airbnb, which provides text from reviews.\nProject Gutenberg which provides the text from out-of-copyright books.\nScraping Wikipedia or other websites.\n\nThe workhorse packages that we need for text cleaning and preparation are stringr, which is part of the tidyverse, and quanteda.\nFor illustrative purposes we construct a corpus of the first sentence or two, from three books: Beloved by Toni Morrison, The Last Samurai by Helen DeWitt, and Jane Eyre by Charlotte Brontë.\n\nlast_samurai &lt;-\"My father's father was a Methodist minister.\"\n\nbeloved &lt;- \"124 was spiteful. Full of Baby's venom.\"\n\njane_eyre &lt;- \"There was no possibility of taking a walk that day.\"\n\nbookshelf &lt;-\n  tibble(\n    book = c(\"Last Samurai\", \"Beloved\", \"Jane Eyre\"),\n    first_sentence = c(last_samurai, beloved, jane_eyre)\n  )\n\nbookshelf\n\n# A tibble: 3 × 2\n  book         first_sentence                                     \n  &lt;chr&gt;        &lt;chr&gt;                                              \n1 Last Samurai My father's father was a Methodist minister.       \n2 Beloved      124 was spiteful. Full of Baby's venom.            \n3 Jane Eyre    There was no possibility of taking a walk that day.\n\n\nWe typically want to construct a document-feature matrix, which has documents in each observation, words in each column, and a count for each combination, along with associated metadata. For instance, if our corpus was the text from Airbnb reviews, then each document may be a review, and typical features could include: “The”, “Airbnb”, “was”, “great”. Notice here that the sentence has been split into different words. We typically talk of “tokens” to generalize away from words, because of the variety of aspects we may be interested in, but words are commonly used.\n\nbooks_corpus &lt;-\n  corpus(bookshelf, \n         docid_field = \"book\", \n         text_field = \"first_sentence\")\n\nbooks_corpus\n\nCorpus consisting of 3 documents.\nLast Samurai :\n\"My father's father was a Methodist minister.\"\n\nBeloved :\n\"124 was spiteful. Full of Baby's venom.\"\n\nJane Eyre :\n\"There was no possibility of taking a walk that day.\"\n\n\nWe use the tokens in the corpus to construct a document-feature matrix (DFM) using dfm() from quanteda (Benoit et al. 2018).\n\nbooks_dfm &lt;-\n  books_corpus |&gt;\n  tokens() |&gt;\n  dfm()\n\nbooks_dfm\n\nDocument-feature matrix of: 3 documents, 21 features (57.14% sparse) and 0 docvars.\n              features\ndocs           my father's father was a methodist minister . 124 spiteful\n  Last Samurai  1        1      1   1 1         1        1 1   0        0\n  Beloved       0        0      0   1 0         0        0 2   1        1\n  Jane Eyre     0        0      0   1 1         0        0 1   0        0\n[ reached max_nfeat ... 11 more features ]\n\n\nWe now consider some of the many decisions that need to be made as part of this process. There is no definitive right or wrong answer. Instead, we make those decisions based on what we will be using the dataset for.\n\n16.2.1 Stop words\nStop words are words such as “the”, “and”, and “a”. For a long time stop words were not thought to convey much meaning, and there were concerns around memory-constrained computation. A common step of preparing a text dataset was to remove stop words. We now know that stop words can have a great deal of meaning (Schofield, Magnusson, and Mimno 2017). The decision to remove them is a nuanced one that depends on circumstances.\nWe can get a list of stop words using stopwords() from quanteda.\n\nstopwords(source = \"snowball\")[1:10]\n\n [1] \"i\"         \"me\"        \"my\"        \"myself\"    \"we\"        \"our\"      \n [7] \"ours\"      \"ourselves\" \"you\"       \"your\"     \n\n\nWe could then look for all instances of words in that list and crudely remove them with str_replace_all().\n\nstop_word_list &lt;-\n  paste(stopwords(source = \"snowball\"), collapse = \" | \")\n\nbookshelf |&gt;\n  mutate(no_stops = str_replace_all(\n    string = first_sentence,\n    pattern = stop_word_list,\n    replacement = \" \")\n  ) |&gt;\n  select(no_stops, first_sentence)\n\n# A tibble: 3 × 2\n  no_stops                                 first_sentence                       \n  &lt;chr&gt;                                    &lt;chr&gt;                                \n1 My father's father a Methodist minister. My father's father was a Methodist m…\n2 124 spiteful. Full Baby's venom.         124 was spiteful. Full of Baby's ven…\n3 There no possibility taking walk day.    There was no possibility of taking a…\n\n\nThere are many different lists of stop words that have been put together by others. For instance, stopwords() can use lists including: “snowball”, “stopwords-iso”, “smart”, “marimo”, “ancient”, and “nltk”. More generally, if we decide to use stop words then we often need to augment such lists with project-specific words. We can do this by creating a count of individual words in the corpus, and then sorting by the most common and adding those to the stop words list as appropriate.\n\nstop_word_list_updated &lt;-\n  paste(\n    \"Methodist |\",\n    \"spiteful |\",\n    \"possibility |\",\n    stop_word_list,\n    collapse = \" | \"\n  )\n\nbookshelf |&gt;\n  mutate(no_stops = str_replace_all(\n    string = first_sentence,\n    pattern = stop_word_list_updated,\n    replacement = \" \")\n  ) |&gt;\n  select(no_stops)\n\n# A tibble: 3 × 1\n  no_stops                        \n  &lt;chr&gt;                           \n1 My father's father a  minister. \n2 124 spiteful. Full Baby's venom.\n3 There no of taking walk day.    \n\n\nWe can integrate the removal of stop words into our construction of the DFM with dfm_remove() from quanteda.\n\nbooks_dfm |&gt;\n  dfm_remove(stopwords(source = \"snowball\"))\n\nDocument-feature matrix of: 3 documents, 14 features (61.90% sparse) and 0 docvars.\n              features\ndocs           father's father methodist minister . 124 spiteful full baby's\n  Last Samurai        1      1         1        1 1   0        0    0      0\n  Beloved             0      0         0        0 2   1        1    1      1\n  Jane Eyre           0      0         0        0 1   0        0    0      0\n              features\ndocs           venom\n  Last Samurai     0\n  Beloved          1\n  Jane Eyre        0\n[ reached max_nfeat ... 4 more features ]\n\n\nWhen we remove stop words we artificially adjust our dataset. Sometimes there may be a good reason to do that. But it must not be done unthinkingly. For instance, in Chapter 6 and Chapter 10 we discussed how sometimes datasets may need to be censored, truncated, or manipulated in other similar ways, to preserve the privacy of respondents. It is possible that the integration of the removal of stop words as a default step in natural language processing was due to computational power, which may have been more limited when these methods were developed. In any case, Jurafsky and Martin ([2000] 2023, 62) conclude that removing stop words does not improve performance for text classification. Relatedly, Schofield, Magnusson, and Mimno (2017) find that inference from topic models is not improved by the removal of anything other than the most frequent words. If stop words are to be removed, then they recommend doing this after topics are constructed.\n\n\n16.2.2 Case, numbers, and punctuation\nThere are times when all we care about is the word, not the case or punctuation. For instance, if the text corpus was particularly messy or the existence of particular words was informative. We trade-off the loss of information for the benefit of making things simpler. We can convert to lower case with str_to_lower(), and use str_replace_all() to remove punctuation with “[:punct:]”, and numbers with “[:digit:]”.\n\nbookshelf |&gt;\n  mutate(lower_sentence = str_to_lower(string = first_sentence)) |&gt;\n  select(lower_sentence)\n\n# A tibble: 3 × 1\n  lower_sentence                                     \n  &lt;chr&gt;                                              \n1 my father's father was a methodist minister.       \n2 124 was spiteful. full of baby's venom.            \n3 there was no possibility of taking a walk that day.\n\n\n\nbookshelf |&gt;\n  mutate(no_punctuation_numbers = str_replace_all(\n    string = first_sentence,\n    pattern = \"[:punct:]|[:digit:]\",\n    replacement = \" \"\n  )) |&gt;\n  select(no_punctuation_numbers)\n\n# A tibble: 3 × 1\n  no_punctuation_numbers                               \n  &lt;chr&gt;                                                \n1 \"My father s father was a Methodist minister \"       \n2 \"    was spiteful  Full of Baby s venom \"            \n3 \"There was no possibility of taking a walk that day \"\n\n\nAs an aside, we can remove letters, numbers, and punctuation with “[:graph:]” in str_replace_all(). While this is rarely needed in textbook examples, it is especially useful with real datasets, because they will typically have a small number of unexpected symbols that we need to identify and then remove. We use it to remove everything that we are used to, leaving only that which we are not.\nMore generally, we can use arguments in tokens() from quanteda() to do this.\n\nbooks_corpus |&gt;\n  tokens(remove_numbers = TRUE, remove_punct = TRUE)\n\nTokens consisting of 3 documents.\nLast Samurai :\n[1] \"My\"        \"father's\"  \"father\"    \"was\"       \"a\"         \"Methodist\"\n[7] \"minister\" \n\nBeloved :\n[1] \"was\"      \"spiteful\" \"Full\"     \"of\"       \"Baby's\"   \"venom\"   \n\nJane Eyre :\n [1] \"There\"       \"was\"         \"no\"          \"possibility\" \"of\"         \n [6] \"taking\"      \"a\"           \"walk\"        \"that\"        \"day\"        \n\n\n\n\n16.2.3 Typos and uncommon words\nThen we need to decide what to do about typos and other minor issues. Every real-world text has typos. Sometimes these should clearly be fixed. But if they are made in a systematic way, for instance, a certain writer always makes the same mistakes, then they could have value if we were interested in grouping by the writer. The use of OCR will introduce common issues as well, as was seen in Chapter 7. For instance, “the” is commonly incorrectly recognized as “thc”.\nWe could fix typos in the same way that we fixed stop words, i.e. with lists of corrections. When it comes to uncommon words, we can build this into our document-feature matrix creation with dfm_trim(). For instance, we could use “min_termfreq = 2” to remove any word that does not occur at least twice, or “min_docfreq = 0.05” to remove any word that is not in at least five per cent of documents or “max_docfreq = 0.90” to remove any word that is in at least 90 per cent of documents.\n\nbooks_corpus |&gt;\n  tokens(remove_numbers = TRUE, remove_punct = TRUE) |&gt;\n  dfm(tolower = TRUE) |&gt;\n  dfm_trim(min_termfreq = 2)\n\nDocument-feature matrix of: 3 documents, 3 features (22.22% sparse) and 0 docvars.\n              features\ndocs           was a of\n  Last Samurai   1 1  0\n  Beloved        1 0  1\n  Jane Eyre      1 1  1\n\n\n\n\n16.2.4 Tuples\nA tuple is an ordered list of elements. In the context of text it is a series of words. If the tuple comprises two words, then we term this a “bi-gram”, three words is a “tri-gram”, etc. These are an issue when it comes to text cleaning and preparation because we often separate terms based on a space. This would result in an inappropriate separation.\nThis is a clear issue when it comes to place names. For instance, consider “British Columbia”, “New Hampshire”, “United Kingdom”, and “Port Hedland”. One way forward is to create a list of such places and then use str_replace_all() to add an underscore, for instance, “British_Columbia”, “New_Hampshire”, “United_Kingdom”, and “Port_Hedland”. Another option is to use tokens_compound() from quanteda.\n\nsome_places &lt;- c(\"British Columbia\", \n                 \"New Hampshire\", \n                 \"United Kingdom\", \n                 \"Port Hedland\")\na_sentence &lt;-\nc(\"Vancouver is in British Columbia and New Hampshire is not\")\n\ntokens(a_sentence) |&gt;\n  tokens_compound(pattern = phrase(some_places))\n\nTokens consisting of 1 document.\ntext1 :\n[1] \"Vancouver\"        \"is\"               \"in\"               \"British_Columbia\"\n[5] \"and\"              \"New_Hampshire\"    \"is\"               \"not\"             \n\n\nIn that case, we knew what the tuples were. But it might be that we were not sure what the common tuples were in the corpus. We could use tokens_ngrams() to identify them. We could ask for, say, all bi-grams in an excerpt from Jane Eyre. We showed how to download the text of this book from Project Gutenberg in Chapter 13 and so here we load the local version that we saved earlier.\n\njane_eyre &lt;- read_csv(\n  \"jane_eyre.csv\",\n  col_types = cols(\n    gutenberg_id = col_integer(),\n    text = col_character()\n  )\n)\n\njane_eyre\n\n\n\n# A tibble: 21,001 × 2\n   gutenberg_id text                           \n          &lt;int&gt; &lt;chr&gt;                          \n 1         1260 JANE EYRE                      \n 2         1260 AN AUTOBIOGRAPHY               \n 3         1260 &lt;NA&gt;                           \n 4         1260 by Charlotte Brontë            \n 5         1260 &lt;NA&gt;                           \n 6         1260 _ILLUSTRATED BY F. H. TOWNSEND_\n 7         1260 &lt;NA&gt;                           \n 8         1260 London                         \n 9         1260 SERVICE & PATON                \n10         1260 5 HENRIETTA STREET             \n# ℹ 20,991 more rows\n\n\nAs there are many blank lines we will remove them.\n\njane_eyre &lt;- \n  jane_eyre |&gt; \n  filter(!is.na(text))\n\n\njane_eyre_text &lt;- tibble(\n  book = \"Jane Eyre\",\n  text = paste(jane_eyre$text, collapse = \" \") |&gt;\n    str_replace_all(pattern = \"[:punct:]\",\n                    replacement = \" \") |&gt;\n    str_replace_all(pattern = stop_word_list,\n                    replacement = \" \")\n)\n\njane_eyre_corpus &lt;-\n  corpus(jane_eyre_text, docid_field = \"book\", text_field = \"text\")\nngrams &lt;- tokens_ngrams(tokens(jane_eyre_corpus), n = 2)\nngram_counts &lt;-\n  tibble(ngrams = unlist(ngrams)) |&gt;\n  count(ngrams, sort = TRUE)\n\nhead(ngram_counts)\n\n# A tibble: 6 × 2\n  ngrams           n\n  &lt;chr&gt;        &lt;int&gt;\n1 I_not          344\n2 Mr_Rochester   332\n3 I_thought      136\n4 St_John        132\n5 don_t          126\n6 I_saw          122\n\n\nHaving identified some common bi-grams, we could add them to the list to be changed. This example includes names like “Mr Rochester” and “St John” which would need to remain together for analysis.\n\n\n16.2.5 Stemming and lemmatizing\nStemming and lemmatizing words is another common approach for reducing the dimensionality of a text dataset. Stemming means to remove the last part of the word, in the expectation that this will result in more general words. For instance, “Canadians”, “Canadian”, and “Canada” all stem to “Canad”. Lemmatizing is similar, but is more involved. It means changing words, not just on their spelling, but on their canonical form (Grimmer, Roberts, and Stewart 2022, 54). For instance, “Canadians”, “Canadian”, “Canucks”, and “Canuck” may all be changed to “Canada”.\nWe can do this with dfm_wordstem(). We notice, that, say, “minister”, has been changed to “minist”.\n\nchar_wordstem(c(\"Canadians\", \"Canadian\", \"Canada\"))\n\n[1] \"Canadian\" \"Canadian\" \"Canada\"  \n\nbooks_corpus |&gt;\n  tokens(remove_numbers = TRUE, remove_punct = TRUE) |&gt;\n  dfm(tolower = TRUE) |&gt;\n  dfm_wordstem()\n\nDocument-feature matrix of: 3 documents, 18 features (59.26% sparse) and 0 docvars.\n              features\ndocs           my father was a methodist minist spite full of babi\n  Last Samurai  1      2   1 1         1      1     0    0  0    0\n  Beloved       0      0   1 0         0      0     1    1  1    1\n  Jane Eyre     0      0   1 1         0      0     0    0  1    0\n[ reached max_nfeat ... 8 more features ]\n\n\nWhile this is a common step in using text as data, Schofield et al. (2017) find that in the context of topic modeling, which we cover later, stemming has little effect and there is little need to do it.\n\n\n16.2.6 Duplication\nDuplication is a major concern with text datasets because of their size. For instance, Bandy and Vincent (2021) showed that around 30 per cent of the data were inappropriately duplicated in the BookCorpus dataset, and Schofield, Thompson, and Mimno (2017) show that this is a major concern and could substantially affect results. However, it can be a subtle and difficult to diagnose problem. For instance, in Chapter 13 when we considered counts of page numbers for various authors in the context of Poisson regression, we could easily have accidentally included each Shakespeare entry twice because not only are there entries for each play, but also many anthologies that contained all of them. Careful consideration of our dataset identified the issue, but that would be difficult at scale."
  },
  {
    "objectID": "16-text.html#term-frequency-inverse-document-frequency-tf-idf",
    "href": "16-text.html#term-frequency-inverse-document-frequency-tf-idf",
    "title": "16  Text as data",
    "section": "16.3 Term Frequency-Inverse Document Frequency (TF-IDF)",
    "text": "16.3 Term Frequency-Inverse Document Frequency (TF-IDF)\n\n16.3.1 Distinguishing horoscopes\nInstall and load astrologer, which is a dataset of horoscopes to explore a real dataset.\nWe can then access the “horoscopes” dataset.\n\nhoroscopes\n\n# A tibble: 1,272 × 4\n   startdate  zodiacsign  horoscope                                        url  \n   &lt;date&gt;     &lt;fct&gt;       &lt;chr&gt;                                            &lt;chr&gt;\n 1 2015-01-05 Aries       Considering the fact that this past week (espec… http…\n 2 2015-01-05 Taurus      It's time Taurus. You aren't one to be rushed a… http…\n 3 2015-01-05 Gemini      Soon it will be time to review what you know, t… http…\n 4 2015-01-05 Cancer      Feeling  feelings and being full of flavorful s… http…\n 5 2015-01-05 Leo         Look, listen, watch, meditate and engage in pra… http…\n 6 2015-01-05 Virgo       Last week's astrology is still reverberating th… http…\n 7 2015-01-05 Libra       Get out your markers and your glue sticks. Get … http…\n 8 2015-01-05 Scorpio     Time to pay extra attention to the needs of you… http…\n 9 2015-01-05 Sagittarius Everything right now is about how you say it, h… http…\n10 2015-01-05 Capricorn   The full moon on January 4th/5th was a healthy … http…\n# ℹ 1,262 more rows\n\n\nThere are four variables: “startdate”, “zodiacsign”, “horoscope”, and “url” (note that URL is out-of-date because the website has been updated, for instance, the first one refers to here). We are interested in the words that are used to distinguish the horoscope of each zodiac sign.\n\nhoroscopes |&gt;\n  count(zodiacsign)\n\n# A tibble: 12 × 2\n   zodiacsign      n\n   &lt;fct&gt;       &lt;int&gt;\n 1 Aries         106\n 2 Taurus        106\n 3 Gemini        106\n 4 Cancer        106\n 5 Leo           106\n 6 Virgo         106\n 7 Libra         106\n 8 Scorpio       106\n 9 Sagittarius   106\n10 Capricorn     106\n11 Aquarius      106\n12 Pisces        106\n\n\nThere are 106 horoscopes for each zodiac sign. In this example we first tokenize by word, and then create counts based on zodiac sign only, not date. We use tidytext because it is used extensively in Hvitfeldt and Silge (2021).\n\nhoroscopes_by_word &lt;-\n  horoscopes |&gt;\n  select(-startdate,-url) |&gt;\n  unnest_tokens(output = word,\n                input = horoscope,\n                token = \"words\")\n\nhoroscopes_counts_by_word &lt;-\n  horoscopes_by_word |&gt;\n  count(zodiacsign, word, sort = TRUE)\n\nhoroscopes_counts_by_word\n\n# A tibble: 41,850 × 3\n   zodiacsign  word      n\n   &lt;fct&gt;       &lt;chr&gt; &lt;int&gt;\n 1 Cancer      to     1440\n 2 Sagittarius to     1377\n 3 Aquarius    to     1357\n 4 Aries       to     1335\n 5 Pisces      to     1313\n 6 Leo         to     1302\n 7 Libra       to     1270\n 8 Sagittarius you    1264\n 9 Virgo       to     1262\n10 Scorpio     to     1260\n# ℹ 41,840 more rows\n\n\nWe can see that the most popular words appear to be similar for the different zodiacs. At this point, we could use the data in a variety of ways.\nWe might be interested to know which words characterize each group—that is to say, which words are commonly used only in each group. We can do that by first looking at a word’s term frequency (TF), which is how many times a word is used in the horoscopes for each zodiac sign. The issue is that there are a lot of words that are commonly used regardless of context. As such, we may also like to look at the inverse document frequency (IDF) in which we “penalize” words that occur in the horoscopes for many zodiac signs. A word that occurs in the horoscopes of many zodiac signs would have a lower IDF than a word that only occurs in the horoscopes of one. The term frequency–inverse document frequency (tf-idf) is then the product of these.\nWe can create this value using bind_tf_idf() from tidytext. It will create new variables for each of these measures.\n\nhoroscopes_counts_by_word_tf_idf &lt;-\n  horoscopes_counts_by_word |&gt;\n  bind_tf_idf(\n    term = word,\n    document = zodiacsign,\n    n = n\n  ) |&gt;\n  arrange(-tf_idf)\n\nhoroscopes_counts_by_word_tf_idf\n\n# A tibble: 41,850 × 6\n   zodiacsign  word            n       tf   idf   tf_idf\n   &lt;fct&gt;       &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Capricorn   goat            6 0.000236  2.48 0.000585\n 2 Pisces      pisces         14 0.000531  1.10 0.000584\n 3 Sagittarius sagittarius    10 0.000357  1.39 0.000495\n 4 Cancer      cancer         10 0.000348  1.39 0.000483\n 5 Gemini      gemini          7 0.000263  1.79 0.000472\n 6 Taurus      bulls           5 0.000188  2.48 0.000467\n 7 Aries       warns           5 0.000186  2.48 0.000463\n 8 Cancer      organize        7 0.000244  1.79 0.000437\n 9 Cancer      overwork        5 0.000174  2.48 0.000433\n10 Taurus      let's          10 0.000376  1.10 0.000413\n# ℹ 41,840 more rows\n\n\nIn Table 16.1 we look at the words that distinguish the horoscopes of each zodiac sign. The first thing to notice is that some of them have their own zodiac sign. On the one hand, there is an argument for removing this, but on the other hand, the fact that it does not happen for all of them is perhaps informative of the nature of the horoscopes for each sign.\n\nhoroscopes_counts_by_word_tf_idf |&gt;\n  slice(1:5,\n        .by = zodiacsign) |&gt;\n  select(zodiacsign, word) |&gt;\n  summarise(all = paste0(word, collapse = \"; \"),\n            .by = zodiacsign) |&gt;\n  knitr::kable(col.names = c(\"Zodiac sign\",\n                             \"Most common words unique to that sign\"))\n\n\n\nTable 16.1: Most common words in horoscopes that are unique to a particular zodiac sign\n\n\nZodiac sign\nMost common words unique to that sign\n\n\n\n\nCapricorn\ngoat; capricorn; capricorns; signify; neighborhood\n\n\nPisces\npisces; wasted; missteps; node; shoes\n\n\nSagittarius\nsagittarius; rolodex; distorted; coat; reinvest\n\n\nCancer\ncancer; organize; overwork; procrastinate; scuttle\n\n\nGemini\ngemini; mood; output; admit; faces\n\n\nTaurus\nbulls; let’s; painfully; virgin; taurus\n\n\nAries\nwarns; vesta; aries; fearful; chase\n\n\nVirgo\ndigesting; trace; liberate; someone’s; final\n\n\nLibra\nproof; inevitably; recognizable; reference; disguise\n\n\nScorpio\nskate; advocate; knots; bottle; meditating\n\n\nAquarius\nsaves; consult; yearnings; sexy; athene\n\n\nLeo\ntrines; blessed; regrets; leo; agree"
  },
  {
    "objectID": "16-text.html#topic-models",
    "href": "16-text.html#topic-models",
    "title": "16  Text as data",
    "section": "16.4 Topic models",
    "text": "16.4 Topic models\nTopic models are useful when we have many statements and we want to create groups based on which sentences that use similar words. We consider those groups of similar words to define topics. One way to get consistent estimates of the topics of each statement is to use topic models. While there are many variants, one way is to use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan (2003), as implemented by stm. For clarity, in the context of this chapter, LDA refers to latent Dirichlet allocation and not Linear Discriminant Analysis, although this is another common subject associated with the acronym LDA.\nThe key assumption behind the LDA method is that for each statement, a document, is made by a person who decides the topics they would like to talk about in that document, and who then chooses words, terms, that are appropriate to those topics. A topic could be thought of as a collection of terms, and a document as a collection of topics. The topics are not specified ex ante; they are an outcome of the method. Terms are not necessarily unique to a particular topic, and a document could be about more than one topic. This provides more flexibility than other approaches such as a strict word count method. The goal is to have the words found in documents group themselves to define topics.\nLDA considers each statement to be a result of a process where a person first chooses the topics they want to speak about. After choosing the topics, the person then chooses appropriate words to use for each of those topics. More generally, the LDA topic model works by considering each document as having been generated by some probability distribution over topics. For instance, if there were five topics and two documents, then the first document may be comprised mostly of the first few topics; the other document may be mostly about the final few topics (Figure 16.1).\n\n\n\n\n\n\n\n(a) Distribution for Document 1\n\n\n\n\n\n\n\n(b) Distribution for Document 2\n\n\n\n\nFigure 16.1: Probability distributions over topics\n\n\nSimilarly, each topic could be considered a probability distribution over terms. To choose the terms used in each document the speaker picks terms from each topic in the appropriate proportion. For instance, if there were ten terms, then one topic could be defined by giving more weight to terms related to immigration; and some other topic may give more weight to terms related to the economy (Figure 16.2).\n\n\n\n\n\n\n\n(a) Distribution for Topic 1\n\n\n\n\n\n\n\n(b) Distribution for Topic 2\n\n\n\n\nFigure 16.2: Probability distributions over terms\n\n\n\n\n\n\n\n\nBy way of background, the Dirichlet distribution is a variation of the beta distribution that is commonly used as a prior for categorical and multinomial variables. If there are just two categories, then the Dirichlet and the beta distributions are the same. In the special case of a symmetric Dirichlet distribution, \\(\\eta=1\\), it is equivalent to a uniform distribution. If \\(\\eta&lt;1\\), then the distribution is sparse and concentrated on a smaller number of the values, and this number decreases as \\(\\eta\\) decreases. A hyperparameter, in this usage, is a parameter of a prior distribution.\n\n\n\n\nAfter the documents are created, they are all that we can analyze. The term usage in each document is observed, but the topics are hidden, or “latent”. We do not know the topics of each document, nor how terms defined the topics. That is, we do not know the probability distributions of Figure 16.1 or Figure 16.2. In a sense we are trying to reverse the document generation process—we have the terms, and we would like to discover the topics.\nIf we observe the terms in each document, then we can obtain estimates of the topics (Steyvers and Griffiths 2006). The outcomes of the LDA process are probability distributions. It is these distributions that define the topics. Each term will be given a probability of being a member of a particular topic, and each document will be given a probability of being about a particular topic.\n\n\nThe initial practical step when implementing LDA given a corpus of documents is usually to remove stop words. Although, as mentioned earlier, this is not necessary, and may be better done after the groups are created. We often also remove punctuation and capitalization. We then construct our document-feature matrix using dfm() from quanteda.\nAfter the dataset is ready, stm can be used to implement LDA and approximate the posterior.  The process attempts to find a topic for a particular term in a particular document, given the topics of all other terms for all other documents. Broadly, it does this by first assigning every term in every document to a random topic, specified by Dirichlet priors.  It then selects a particular term in a particular document and assigns it to a new topic based on the conditional distribution where the topics for all other terms in all documents are taken as given (Grün and Hornik 2011, 6).   Once this has been estimated, then estimates for the distribution of words into topics and topics into documents can be backed out.\nThe conditional distribution assigns topics depending on how often a term has been assigned to that topic previously, and how common the topic is in that document (Steyvers and Griffiths 2006). The initial random allocation of topics means that the results of early passes through the corpus of document are poor, but given enough time the algorithm converges to an appropriate estimate.\nThe choice of the number of topics, \\(k\\), affects the results, and must be specified a priori. If there is a strong reason for a particular number, then this can be used. Otherwise, one way to choose an appropriate number is to use a test and training set process. Essentially, this means running the process on a variety of possible values for k and then picking an appropriate value that performs well.\nOne weakness of the LDA method is that it considers a “bag of words” where the order of those words does not matter (Blei 2012). It is possible to extend the model to reduce the impact of the bag-of-words assumption and add conditionality to word order. Additionally, alternatives to the Dirichlet distribution can be used to extend the model to allow for correlation.\n\n16.4.1 What is talked about in the Canadian parliament?\nFollowing the example of the British, the written record of what is said in the Canadian parliament is called “Hansard”. It is not completely verbatim, but is very close. It is available in CSV format from LiPaD, which was constructed by Beelen et al. (2017).\nWe are interested in what was talked about in the Canadian parliament in 2018. To get started we can download the entire corpus from here, and then discard all of the years apart from 2018. If the datasets are in a folder called “2018”, we can use read_csv() to read and combine all the CSVs.\n\nfiles_of_interest &lt;-\n  dir_ls(path = \"2018/\", glob = \"*.csv\", recurse = 2)\n\nhansard_canada_2018 &lt;-\n  read_csv(\n    files_of_interest,\n    col_types = cols(\n      basepk = col_integer(),\n      speechdate = col_date(),\n      speechtext = col_character(),\n      speakerparty = col_character(),\n      speakerriding = col_character(),\n      speakername = col_character()\n    ),\n    col_select = \n      c(basepk, speechdate, speechtext, speakername, speakerparty, \n        speakerriding)) |&gt;\n  filter(!is.na(speakername))\n\nhansard_canada_2018\n\n\n\n# A tibble: 33,105 × 6\n    basepk speechdate speechtext          speakername speakerparty speakerriding\n     &lt;int&gt; &lt;date&gt;     &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        \n 1 4732776 2018-01-29 \"Mr. Speaker, I wo… Julie Dabr… Liberal      Toronto—Danf…\n 2 4732777 2018-01-29 \"Mr. Speaker, I wa… Matthew Du… New Democra… Beloeil—Cham…\n 3 4732778 2018-01-29 \"Mr. Speaker, I am… Stephanie … Conservative Calgary Midn…\n 4 4732779 2018-01-29 \"Resuming debate.\\… Anthony Ro… Liberal      Nipissing—Ti…\n 5 4732780 2018-01-29 \"Mr. Speaker, we a… Alain Rayes Conservative Richmond—Art…\n 6 4732781 2018-01-29 \"The question is o… Anthony Ro… Liberal      Nipissing—Ti…\n 7 4732782 2018-01-29 \"Agreed.\\n No.\"     Some hon. … &lt;NA&gt;         &lt;NA&gt;         \n 8 4732783 2018-01-29 \"All those in favo… Anthony Ro… Liberal      Nipissing—Ti…\n 9 4732784 2018-01-29 \"Yea.\"              Some hon. … &lt;NA&gt;         &lt;NA&gt;         \n10 4732785 2018-01-29 \"All those opposed… Anthony Ro… Liberal      Nipissing—Ti…\n# ℹ 33,095 more rows\n\n\nThe use of filter() at the end is needed because sometimes aspects such as “directions” and similar non-speech aspects are included in the Hansard. For instance, if we do not include that filter() then the first line is “The House resumed from November 9, 2017, consideration of the motion.” We can then construct a corpus.\n\nhansard_canada_2018_corpus &lt;-\n  corpus(hansard_canada_2018, \n         docid_field = \"basepk\", \n         text_field = \"speechtext\")\n\nWarning: NA is replaced by empty string\n\nhansard_canada_2018_corpus\n\nCorpus consisting of 33,105 documents and 4 docvars.\n4732776 :\n\"Mr. Speaker, I would like to wish everyone in this place a h...\"\n\n4732777 :\n\"Mr. Speaker, I want to thank my colleague from Richmond—Arth...\"\n\n4732778 :\n\"Mr. Speaker, I am here today to discuss a motion that asks t...\"\n\n4732779 :\n\"Resuming debate. There being no further debate, the hon. mem...\"\n\n4732780 :\n\"Mr. Speaker, we are nearing the end of the discussion and de...\"\n\n4732781 :\n\"The question is on the motion. Is the pleasure of the House ...\"\n\n[ reached max_ndoc ... 33,099 more documents ]\n\n\nWe use the tokens in the corpus to construct a document-feature matrix. To make our life a little easier, computationally, we remove any word that does not occur at least twice, and any word that does not occur in at least two documents.\n\nhansard_dfm &lt;-\n  hansard_canada_2018_corpus |&gt;\n  tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE\n  ) |&gt;\n  dfm() |&gt;\n  dfm_trim(min_termfreq = 2, min_docfreq = 2) |&gt;\n  dfm_remove(stopwords(source = \"snowball\"))\n\nhansard_dfm\n\nDocument-feature matrix of: 33,105 documents, 29,595 features (99.77% sparse) and 4 docvars.\n         features\ndocs      mr speaker like wish everyone place happy new year great\n  4732776  1       1    2    1        1     4     2   3    5     1\n  4732777  1       1    5    0        1     1     0   0    0     1\n  4732778  1       1    2    0        0     1     0   0    4     1\n  4732779  0       0    0    0        0     0     0   0    0     0\n  4732780  1       1    4    0        1     1     0   0    2     0\n  4732781  0       0    0    0        0     0     0   0    0     0\n[ reached max_ndoc ... 33,099 more documents, reached max_nfeat ... 29,585 more features ]\n\n\nAt this point we can use stm() from stm to implement a LDA model. We need to specify a document-feature matrix and the number of topics. Topic models are essentially just summaries. Instead of a document becoming a collection of words, they become a collection of topics with some probability associated with each topic. But because it is just providing a collection of words that tend to be used at similar times, rather than actual underlying meaning, we need to specify the number of topics that we are interested in. This decision will have a big impact, and we should consider a few different numbers.\n\nhansard_topics &lt;- stm(documents = hansard_dfm, K = 10)\n\nbeepr::beep()\n\nwrite_rds(\n  hansard_topics,\n  file = \"hansard_topics.rda\"\n)\n\nThis will take some time, likely 15-30 minutes, so it is useful to save the model when it is done using write_rds(), and use beep to get a notification when it is done. We could then read the results back in with read_rds().\n\nhansard_topics &lt;- read_rds(\n  file = \"hansard_topics.rda\"\n)\n\nWe can look at the words in each topic with labelTopics().\n\nlabelTopics(hansard_topics)\n\nTopic 1 Top Words:\n     Highest Prob: indigenous, government, communities, bill, first, development, nations \n     FREX: fisheries, fish, oceans, marine, habitat, coastal, plastic \n     Lift: 10-knot, 13-storey, 167.4, 1885, 2,560, 200-mile, 2016-19 \n     Score: indigenous, fisheries, oceans, fish, marine, environmental, peoples \nTopic 2 Top Words:\n     Highest Prob: community, people, canada, mr, housing, speaker, many \n     FREX: latin, celebrate, filipino, sikh, anniversary, cancer, celebrating \n     Lift: #myfeminism, 1897, 1904, 1915, 1925, 1941, 1943 \n     Score: latin, housing, filipino, sikh, celebrate, heritage, enverga \nTopic 3 Top Words:\n     Highest Prob: bill, justice, system, criminal, victims, court, violence \n     FREX: justice, criminal, crime, firearms, offences, c-75, gun \n     Lift: 1608, 2,465, 202, 273, 273.1, 3.01, 302-page \n     Score: criminal, justice, c-75, firearms, offences, correctional, crime \nTopic 4 Top Words:\n     Highest Prob: carbon, energy, pipeline, climate, government, oil, canada \n     FREX: carbon, emissions, pipelines, greenhouse, pricing, fuel, fossil \n     Lift: emitters, ipcc, pricing, -30, 0822, 1.3-billion, 1.62 \n     Score: carbon, pipeline, oil, energy, emissions, climate, pollution \nTopic 5 Top Words:\n     Highest Prob: canada, government, canadians, workers, canadian, new, tax \n     FREX: tariffs, steel, postal, cptpp, cra, nafta, aluminum \n     Lift: ip, wages, 0.6, 0.9, 1,090, 1.26-billion, 1.84 \n     Score: tax, workers, economy, cptpp, tariffs, postal, growth \nTopic 6 Top Words:\n     Highest Prob: member, speaker, mr, minister, house, hon, question \n     FREX: ethics, yea, resuming, hon, briefing, fundraisers, secretary \n     Lift: khan, norman, 1,525, 100-plus, 13.1, 1316, 13198 \n     Score: hon, member, prime, question, ethics, nay, minister \nTopic 7 Top Words:\n     Highest Prob: government, minister, liberals, liberal, prime, canadians, veterans \n     FREX: deficit, deficits, phoenix, veterans, promises, promised, davie \n     Lift: 0.23, 12.50, 14-billion, 148.6, 20-billion, 2045, 2051 \n     Score: liberals, veterans, prime, tax, liberal, budget, deficit \nTopic 8 Top Words:\n     Highest Prob: canada, motion, government, rights, mr, house, speaker \n     FREX: petition, refugees, pursuant, asylum, refugee, iran, israel \n     Lift: cfia, pre-clearance, terror, c-350, consular, crossers, deferred \n     Score: iran, immigration, pursuant, asylum, refugees, petition, petitioners \nTopic 9 Top Words:\n     Highest Prob: bill, committee, act, legislation, canadians, canada, information \n     FREX: amendment, harassment, organ, cannabis, tobacco, c-59, c-65 \n     Lift: 240.1, a36, anti-tobacco, appropriations, beadonor.ca, calibrated, carvin \n     Score: bill, amendments, elections, harassment, organ, amendment, c-76 \nTopic 10 Top Words:\n     Highest Prob: people, one, can, want, us, get, going \n     FREX: things, think, lot, something, see, really, look \n     Lift: 22-year-old, 980,000, backtracking, balloting, carolina, disenfranchise, enfranchisement \n     Score: people, going, forward, legislation, things, voter, think"
  },
  {
    "objectID": "16-text.html#exercises",
    "href": "16-text.html#exercises",
    "title": "16  Text as data",
    "section": "16.5 Exercises",
    "text": "16.5 Exercises\n\nScales\n\n(Plan) Consider the following scenario: You run a news website and are trying to understand whether to allow anonymous comments. You decide to do a A/B test, where we keep everything the same, but only allow anonymous comments on one version of the site. All you will have to decide is the text data that you obtain from the test. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Please include at least ten tests based on the simulated data.\n(Acquire) Please describe one possible source of such a dataset.\n(Explore) Please use ggplot2 to build the graph that you sketched. Use rstanarm to build a model.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nWhich argument to str_replace_all() would remove punctuation?\n\n“[:punct:]”\n“[:digit:]”\n“[:alpha:]”\n“[:lower:]”\n\nChange stopwords(source = \"snowball\")[1:10] to find the ninth stopword in the “nltk” list.\n\n“her”\n“my”\n“you”\n“i”\n\nWhich function from quanteda() will tokenize a corpus?\n\ntokenizer()\ntoken()\ntokenize()\ntokens()\n\nWhich argument to dfm_trim() should be used if we want to only include terms that occur at least twice? = 2)\n\n“min_wordfreq”\n“min_termfreq”\n“min_term_occur”\n“min_ occurrence”\n\nWhat is your favorite example of a tri-gram?\nWhat is the second-most common word used in the zodiac signs for Cancer?\n\nto\nyour\nthe\nyou\n\nWhat is the sixth-most common word used in the zodiac signs for Pisces, that is unique to that sign?\n\nshoes\nprayer\nfishes\npisces\n\nRe-run the Canadian topic model, but only including five topics. Looking at the words in each topic, how would you describe what each of them is about?\n\n\n\nTutorial\nPlease follow the code of Hvitfeldt and Silge (2021) in Supervised Machine Learning for Text Analysis in R, Chapter 5.2 “Understand word embeddings by finding them yourself”, freely available here, to implement your own word embeddings for one year’s worth of data from LiPaD.\n\n\n\n\nAlexopoulos, Michelle, and Jon Cohen. 2015. “The power of print: Uncertainty shocks, markets, and the economy.” International Review of Economics & Finance 40 (November): 8–28. https://doi.org/10.1016/j.iref.2015.02.002.\n\n\nAmaka, Ofunne, and Amber Thomas. 2021. “The Naked Truth: How the Names of 6,816 Complexion Products Can Reveal Bias in Beauty.” The Pudding, March. https://pudding.cool/2021/03/foundation-names/.\n\n\nBååth, Rasmus. 2018. beepr: Easily Play Notification Sounds on any Platform. https://CRAN.R-project.org/package=beepr.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing ‘Documentation Debt’ in Machine Learning Research: A Retrospective Datasheet for BookCorpus.” arXiv. https://doi.org/10.48550/arXiv.2105.05241.\n\n\nBeelen, Kaspar, Timothy Alberdingk Thim, Christopher Cochrane, Kees Halvemaan, Graeme Hirst, Michael Kimmins, Sander Lijbrink, et al. 2017. “Digitization of the Canadian Parliamentary Debates.” Canadian Journal of Political Science 50 (3): 849–64.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In The SAGE Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese, 461–97. London: SAGE Publishing. https://doi.org/10.4135/9781526486387.n29.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of textual data.” Journal of Open Source Software 3 (30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBlei, David. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nBlei, David, Andrew Ng, and Michael Jordan. 2003. “Latent Dirichlet Allocation.” Journal of Machine Learning Research 3 (Jan): 993–1022. https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf.\n\n\nDavidson, Thomas, Debasmita Bhattacharya, and Ingmar Weber. 2019. “Racial Bias in Hate Speech and Abusive Language Detection Datasets.” In Proceedings of the Third Workshop on Abusive Language Online, 25–35.\n\n\nGelfand, Sharla. 2022. Astrologer: Chani Nicholas Weekly Horoscopes (2013-2017). http://github.com/sharlagelfand/astrologer.\n\n\nGrimmer, Justin, Margaret Roberts, and Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and the Social Sciences. New Jersey: Princeton University Press.\n\n\nGronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, and Tianxi Cai. 2019. “Automated Feature Selection of Predictors in Electronic Medical Records Data.” Biometrics 75 (1): 268–77. https://doi.org/10.1111/biom.12987.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nHassan, Mai. 2022. “New Insights on Africa’s Autocratic Past.” African Affairs 121 (483): 321–33. https://doi.org/10.1093/afraf/adac002.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2021. fs: Cross-Platform File System Operations Based on “libuv”. https://CRAN.R-project.org/package=fs.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nJohnston, Myfanwy, and David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from Project Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nJurafsky, Dan, and James Martin. (2000) 2023. Speech and Language Processing. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nKalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. 2022. “Making text count: Economic forecasting using newspaper text.” Journal of Applied Econometrics 37 (5): 896–919. https://doi.org/10.1002/jae.2907.\n\n\nKoenecke, Allison, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John Rickford, Dan Jurafsky, and Sharad Goel. 2020. “Racial Disparities in Automated Speech Recognition.” Proceedings of the National Academy of Sciences 117 (14): 7684–89. https://doi.org/10.1073/pnas.1915768117.\n\n\nMiller, Michael, and Joseph Sutherland. 2022. “The Effect of Gender on Interruptions at Congressional Hearings.” American Political Science Review, 1–19. https://doi.org/10.1017/S0003055422000260.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRoberts, Margaret, Brandon Stewart, and Dustin Tingley. 2019. “stm: An R Package for Structural Topic Models.” Journal of Statistical Software 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nSchofield, Alexandra, Måns Magnusson, and David Mimno. 2017. “Pulling Out the Stops: Rethinking Stopword Removal for Topic Models.” In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 432–36. Valencia, Spain: Association for Computational Linguistics. https://aclanthology.org/E17-2069.\n\n\nSchofield, Alexandra, Måns Magnusson, Laure Thompson, and David Mimno. 2017. “Understanding Text Pre-Processing for Latent Dirichlet Allocation.” In ACL Workshop for Women in NLP (WiNLP). https://www.cs.cornell.edu/~xanda/winlp2017.pdf.\n\n\nSchofield, Alexandra, Laure Thompson, and David Mimno. 2017. “Quantifying the Effects of Text Duplication on Semantic Models.” In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2737–47. Copenhagen, Denmark: Association for Computational Linguistics. https://doi.org/10.18653/v1/D17-1290.\n\n\nSilge, Julia, and David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” The Journal of Open Source Software 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic Models.” In Latent Semantic Analysis: A Road to Meaning, edited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "17-concluding.html#concluding-remarks",
    "href": "17-concluding.html#concluding-remarks",
    "title": "17  Concluding remarks",
    "section": "17.1 Concluding remarks",
    "text": "17.1 Concluding remarks\nThere is an old saying, something along the lines of “may you live in interesting times”. Maybe every generation feels this way, but we sure do live in interesting times. In this book, we have covered some essential skills for telling stories with data. And this is just the start.\nIn less than a generation, data science has gone from something that barely existed, to a defining part of academia and industry. The extent and pace of this change has many implications for those learning data science. For instance, it may imply that one should not just make decisions that optimize for what data science looks like right now, but also what could happen. While that is a little difficult, that is also one of the things that makes data science so exciting. That might mean choices like:\n\ntaking courses on fundamentals, not just fashionable applications;\nreading core texts, not just whatever is trending; and\ntrying to be at the intersection of at least a few different areas, rather than hyper-specialized.\n\nOne of the most exciting times when you learn data science is realizing that you just love playing with data. A decade ago, this did not fit into any particular department or company. These days, it fits into almost any of them.\nData science needs to insist on diversity, both in terms of approaches and applications. It is increasingly the most important work in the world, and hegemonic approaches have no place. It is just such an exciting time to be enthusiastic about data and able to build things.\nThe central thesis of this book has been that a revolution is needed in data science, and we have proposed one view of what it could look like. This revolution builds on the long history of statistics, borrows heavily from computer science, and draws on other disciplines as needed, but is centered around reproducibility, workflows, and respect. When data science began it was nebulous and ill-defined. As it has matured, we now come to see it as able to stand on its own.\nThis book has been a reimagining of what data science is, and what it could be. In Chapter 1 we provided an informal definition of data science. We now revisit it. We consider data science to be the process of developing and applying a principled, tested, reproducible, end-to-end workflow that focuses on quantitative measures in and of themselves, and as a foundation to explore questions. We have known for a long-time what rigor looks like in mathematical and statistical theory: theorems are accompanied by proofs (Horton et al. 2022). And we increasingly know what rigor looks like in data science: claims that are accompanied by verified, tested, reproducible, code and data. Rigorous data science creates lasting understanding of the world."
  },
  {
    "objectID": "17-concluding.html#some-outstanding-issues",
    "href": "17-concluding.html#some-outstanding-issues",
    "title": "17  Concluding remarks",
    "section": "17.2 Some outstanding issues",
    "text": "17.2 Some outstanding issues\nThere are many issues that are outstanding as we think about data science. They are not the type of issues with a definitive answer. Instead, they are questions to be explored and played with. This work will move data science forward and, more importantly, help us tell better stories about the world. Here we detail some of them.\n1. How do we write effective tests?\nComputer science has built a thorough foundation around testing and the importance of unit and functional tests is broadly accepted. One of the innovations of this book has been to integrate testing throughout the data science workflow, but this, like the first iteration of anything, needs considerable improvement and development.\nWe need to thoroughly integrate testing through data science. But it is unclear what this should look like, how we should do it, and what is the end-state. What does it mean to have well-tested code in data science? Code coverage, which is a measure of the percentage of lines of code that have tests, is not especially meaningful in data science, but what should we use instead? What do tests look like in data science? How are they written? The extensive use of simulation in statistics, which data science has adopted, provides groundwork, but there is a significant amount of work and investment that is needed.\n2. What is happening at the data cleaning and preparation stage?\nWe do not have a good understanding how much data cleaning and preparation is driving estimates. Huntington-Klein et al. (2021), and Breznau et al. (2022), among others, have begun this work. They show that hidden research decisions have a big effect on subsequent estimates, sometimes greater than the standard errors. Statistics provides a good understanding of how modeling affects estimates, but we need more investigation of the influence of the earlier stages of the data science workflow. More specifically, we need to look for key points of failure and understand the ways in which failure can happen.\nThis is especially concerning as we scale to larger datasets. For instance, ImageNet is a dataset of 14 million images, which were hand-annotated. The cost, in both time and money, makes it prohibitively difficult to go through every image to ensure the label is consistent with the needs of each user of the dataset. Yet without undertaking this it is difficult to have much faith in subsequent model forecasts, especially in non-obvious cases.\n3. How do we create effective names?\nOne of the crowning achievements of biology is the binomial nomenclature. This is the formal systematic approach to names, established by Carolus Linnaeus, the eighteenth century physician (Morange 2016, 81). Each species is referred to by two words with Latin grammatical form: the first is its genus, and the second is an adjective to characterize the species. Ensuring standardized nomenclature is given active consideration in biology. For instance, the use of nomenclature committees by researchers is recommended (McCarthy et al. 2023). As discussed in Chapter 9, names are a large source of friction in data science, and a standardized approach is similarly needed in data science.\nThe reason this is so pressing is that it affects understanding, which impacts efficiency. The binomial nomenclature provides diagnostic information, not just a casual reference (Koerner 2000, 45). This is particularly the case when data science is conducted in a team, rather than just one individual. A thorough understanding of what makes an effective name and then infrastructure to encourage them would bring significant dividends.\n\n\n\n\n\n4. What is the appropriate relationship for data science with the constituent parts?\nWe have described the origins of data science as being various disciplines. Moving forward we need to consider what role these constituent parts, especially statistics and computer science, should play. More generally, we also need to establish how data science relates to, and interacts with, econometrics, applied mathematics, and computational social science. These draw on data science to answer questions in their own discipline, but like statistics and computer science, they also contribute back to data science. For instance, applications of machine learning in computational social science need to focus on transparency, interpretability, uncertainty, and ethics, and this all advances the more theoretical machine learning research done in other disciplines (Wallach 2018).\nWe must be careful to continue to learn statistics from statisticians, computer science from computer scientists, etc. An example of the danger of not doing this is clear in the case of p-values, which we have not made much of in this book, but which dominate quantitative analysis even though statisticians have warned about their misapplication for decades. One issue with not learning statistics from statisticians is that statistical practice can become a recipe that is naively followed, because that is the easiest way to teach it, even though that is not how statisticians do statistics.\nData science must remain deeply connected to these disciplines. How we continue to ensure that data science has the best aspects, without also bringing bad practice, is an especially significant challenge. And this is not just technical, but also cultural (Meng 2021). It is particularly important to ensure that data science maintains an inclusive culture of excellence.\n5. How do we teach data science?\nWe are beginning to have agreement on what the foundations of data science are. It involves developing comfort with: computational thinking, sampling, statistics, graphs, Git and GitHub, SQL, command line, cleaning messy data, a few languages including R and Python, ethics, and writing. But we have very little agreement on how best to teach it. Partly this is because data science instructors often come from different fields, but it is also partly a difference in resources and priorities.\nComplicating matters is that given the demand for data science skills we cannot limit data science education to graduate students because undergraduate students need those skills when they enter the workforce. If data science is to be taught at the undergraduate level, then it needs to be robust enough to be taught in large classes. Developing teaching tools that scale is critical. For instance, GitHub Actions could be used to run checks of student code and suggest improvements without instructor involvement. However, it is especially difficult to scale case studies style classes, which students often find so useful. Substantial innovation is needed.\n6. What does the relationship between industry and academia look like?\nConsiderable innovation in data science occurs in industry, but sometimes this knowledge cannot be shared, and when it can it tends to be done slowly. The term data science has been used in academia since the 1960s, but it is because of industry that it has become popular in the past decade or so (Irizarry 2020).\nBringing academia and industry together is both a key challenge for data science and one of the easiest to overlook. The nature of the problems faced in industry, for instance scoping the needs of a client, and operating at scale, are removed from typical academic concerns. There is a danger that academic research could be rendered moot unless academics establish and maintain one foot in industry, and enable industry to actively participate in academia. From the industry side, ensuring that best practice is quickly adopted can be challenging if there is no immediate payoff. Ensuring that industry experience is valued in academic hiring and grant evaluation would help, as would encouraging entrepreneurship in academia."
  },
  {
    "objectID": "17-concluding.html#next-steps",
    "href": "17-concluding.html#next-steps",
    "title": "17  Concluding remarks",
    "section": "17.3 Next steps",
    "text": "17.3 Next steps\nThis book has covered much ground, and while we are toward the end of it, as the butler Stevens is told in the novel The Remains of the Day by Kazuo Ishiguro:\n\nThe evening’s the best part of the day. You’ve done your day’s work. Now you can put your feet up and enjoy it.\nIshiguro (1989)\n\nChances are there are aspects that you want to explore further, building on the foundation that you have established. If so, then the book has accomplished its aim.\nIf you were new to data science at the start of this book, then the next step would be to backfill what we skipped over. Begin with Data Science: A First Introduction (Timbers, Campbell, and Lee 2022). After that go through R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund [2016] 2023). We used R in this book and only mentioned SQL and Python in passing, but it is important to develop comfort in these languages. Start with SQL for Data Scientists (Teate 2022), Python for Data Analysis (McKinney [2011] 2022), and the free Replit “100 Days of Code” Python course.\nSampling is a critical, but easy to overlook, aspect of data science. It would be sensible to go through Sampling: Design and Analysis (Lohr [1999] 2022). To deepen your understanding of surveys and experiments, go next to Field Experiments: Design, Analysis, and Interpretation (Gerber and Green 2012) and Trustworthy online controlled experiments (Kohavi, Tang, and Xu 2020).\nFor developing better data visualization skills, begin by turning to Data Sketches (Bremer and Wu 2021) and Data Visualization (Healy 2018). After that, develop strong foundations, such as The Grammar of Graphics (Wilkinson 2005).\nIf you are interested to learn more about modeling, then the next steps are Statistical Rethinking: A Bayesian Course with Examples in R and Stan (McElreath [2015] 2020), which additionally has an excellent series of accompanying videos, Bayes Rules! An Introduction to Bayesian Modeling with R (Johnson, Ott, and Dogucu 2022), and Regression and Other Stories (Gelman, Hill, and Vehtari 2020). It would also be worthwhile to establish a foundation of probability with All of Statistics (Wasserman 2005).\nThere is only one next natural step if you are interested in machine learning and that is An Introduction to Statistical Learning (James et al. [2013] 2021) followed by The Elements of Statistical Learning (Friedman, Tibshirani, and Hastie 2009).\nTo learn more about causality start with the economics perspective by going through Causal Inference: The Mixtape (Cunningham 2021) and The Effect: An Introduction to Research Design and Causality (Huntington-Klein 2021). Then turn to the health sciences perspective by going through What If (Hernán and Robins 2023).\nFor text as data, start with Text As Data (Grimmer, Roberts, and Stewart 2022). Then turn to Supervised Machine Learning for Text Analysis in R (Hvitfeldt and Silge 2021).\nIn terms of ethics, there are a variety of books. We have covered many chapters of it, throughout this book, but going through Data Feminism (D’Ignazio and Klein 2020) end-to-end would be useful, as would Atlas of AI (Crawford 2021).\nAnd finally, for writing, it would be best to turn inward. Force yourself to write every day for a month. Then do it again and again. You will get better. That said, there are some useful books, including Working (Caro 2019) and On Writing: A Memoir of the Craft (King 2000).\nWe often hear the phrase “let the data speak”. Hopefully it is clear this never happens. All that we can do is to acknowledge that we are the ones using data to tell stories, and strive and seek to make them worthy.\n\nIt was her voice that made\nThe sky acutest at its vanishing.\nShe measured to the hour its solitude.\nShe was the single artificer of the world\nIn which she sang. And when she sang, the sea,\nWhatever self it had, became the self\nThat was her song, for she was the maker.\nExtract from “The Idea of Order at Key West”, (Stevens 1934)"
  },
  {
    "objectID": "17-concluding.html#exercises",
    "href": "17-concluding.html#exercises",
    "title": "17  Concluding remarks",
    "section": "17.4 Exercises",
    "text": "17.4 Exercises\n\nQuestions\n\nWhat is data science?\nWho does data affect, and what affects data?\nDiscuss the inclusion of “race” and/or “sexuality” in a model.\nWhat makes a story more or less convincing?\nWhat is the role of ethics when dealing with data?\n\n\n\n\n\nBremer, Nadieh, and Shirley Wu. 2021. Data Sketches. A K Peters/CRC Press. https://doi.org/10.1201/9780429445019.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung HV Nguyen, Muna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, et al. 2022. “Observing Many Researchers Using the Same Data and Hypothesis Reveals a Hidden Universe of Uncertainty.” Proceedings of the National Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCrawford, Kate. 2021. Atlas of AI. 1st ed. New Haven: Yale University Press.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed. New Haven: Yale Press. https://mixtape.scunning.com.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2009. The Elements of Statistical Learning. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGerber, Alan, and Donald Green. 2012. Field Experiments: Design, Analysis, and Interpretation. New York: WW Norton.\n\n\nGray, Charles T., and Ben Marwick. 2019. “Truth, Proof, and Reproducibility: There’s No Counter-Attack for the Codeless.” In Communications in Computer and Information Science, 111–29. Springer Singapore. https://doi.org/10.1007/978-981-15-1960-4_8.\n\n\nGrimmer, Justin, Margaret Roberts, and Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and the Social Sciences. New Jersey: Princeton University Press.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton University Press. https://socviz.co.\n\n\nHernán, Miguel, and James Robins. 2023. What If. 1st ed. Boca Raton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHorton, Nicholas, Rohan Alexander, Micaela Parker, Aneta Piekut, and Colin Rundel. 2022. “The Growing Importance of Reproducibility and Responsible Workflow in the Data Science and Statistics Curriculum.” Journal of Statistics and Data Science Education 30 (3): 207–8. https://doi.org/10.1080/26939169.2022.2141001.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni, Jeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The Influence of Hidden Researcher Decisions in Applied Microeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nIrizarry, Rafael. 2020. “The Role of Academia in Data Science Education.” Harvard Data Science Review 2 (1). https://doi.org/10.1162/99608f92.dd363929.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. 1st ed. Faber; Faber.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2013) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJohnson, Alicia, Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with R. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed. Scribner.\n\n\nKoerner, Lisbet. 2000. Linnaeus: Nature and Nation. Cambridge: Harvard University Press.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.\n\n\nLeek, Jeff, Blakeley McShane, Andrew Gelman, David Colquhoun, Michèle Nuijten, and Steven Goodman. 2017. “Five Ways to Fix Statistics.” Nature 551 (7682): 557–59. https://doi.org/10.1038/d41586-017-07522-z.\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys.” In Data Journeys in the Sciences, 1–24. Springer International Publishing. https://doi.org/10.1007/978-3-030-37177-7_1.\n\n\nLohr, Sharon. (1999) 2022. Sampling: Design and Analysis. 3rd ed. Chapman; Hall/CRC.\n\n\nMcCarthy, Fiona M., Tamsin E. M. Jones, Anne E. Kwitek, Cynthia L. Smith, Peter D. Vize, Monte Westerfield, and Elspeth A. Bruford. 2023. “The Case for Standardizing Gene Nomenclature in Vertebrates.” Nature 614 (7948): E31–32. https://doi.org/10.1038/s41586-022-05633-w.\n\n\nMcElreath, Richard. (2015) 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Chapman; Hall/CRC.\n\n\n———. 2020. “Science as Amateur Software Development.” YouTube, September. https://youtu.be/zwRdO9%5FGGhY.\n\n\nMcKinney, Wes. (2011) 2022. Python for Data Analysis. 3rd ed. https://wesmckinney.com/book/.\n\n\nMeng, Xiao-Li. 2021. “What Are the Values of Data, Data Science, or Data Scientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey: Princeton University Press.\n\n\nPerkel, Jeffrey. 2021. “Ten Computer Codes That Transformed Science.” Nature 589 (7842): 344–48. https://doi.org/10.1038/d41586-021-00075-2.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nTeate, Renée. 2022. SQL for Data Scientists. Wiley.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data Science: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nWallach, Hanna. 2018. “Computational Social Science \\(\\ne\\) Computer Science + Social Data.” Communications of the ACM 61 (3): 42–44. https://doi.org/10.1145/3132698.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed. Springer."
  },
  {
    "objectID": "20-r_essentials.html#introduction",
    "href": "20-r_essentials.html#introduction",
    "title": "Online Appendix A — R essentials",
    "section": "A.1 Introduction",
    "text": "A.1 Introduction\nIn this chapter we focus on foundational skills needed to use the statistical programming language R (R Core Team 2023) to tell stories with data. Some of it may not make sense at first, but these are skills and approaches that we will often use. You should initially go through this chapter quickly, noting aspects that you do not understand. Then come back to this chapter from time to time as you continue through the rest of the book. That way you will see how the various bits fit into context.\nR is an open-source language for statistical programming. You can download R for free from the Comprehensive R Archive Network (CRAN). RStudio is an Integrated Development Environment (IDE) for R which makes the language easier to use and can be downloaded for free from Posit here.\nThe past ten years or so have been characterized by the increased use of the tidyverse. This is “…an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures” (Wickham 2020). There are three distinctions to be clear about: the original R language, typically referred to as “base”; the tidyverse which is a coherent collection of packages that build on top of base, and other packages.\nEssentially everything that we can do in the tidyverse, we can also do in base. But, as the tidyverse was built especially for data science it is often easier to use, especially when learning. Additionally, most everything that we can do in the tidyverse, we can also do with other packages. But, as the tidyverse is a coherent collection of packages, it is often easier to use, again, especially when learning. Eventually there are cases where it makes sense to trade-off the convenience and coherence of the tidyverse for some features of base, other packages, or languages. Indeed, we introduce SQL in Chapter 10 as one source of considerable efficiency gain when working with data. For instance, the tidyverse can be slow, and so if one needs to import thousands of CSVs then it can make sense to switch away from read_csv(). The appropriate use of base and non-tidyverse packages, or even other languages, rather than dogmatic insistence on a particular solution, is a sign of intellectual maturity.\nCentral to our use of the statistical programming language R is data, and most of the data that we use will have humans at the heart of it. Sometimes, dealing with human-centered data in this way can have a numbing effect, resulting in over-generalization, and potentially problematic work. Another sign of intellectual maturity is when it has the opposite effect, increasing our awareness of our decision-making processes and their consequences.\n\nIn practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture—what they mean, and for whom.\nHealy (2020)"
  },
  {
    "objectID": "20-r_essentials.html#broader-impacts",
    "href": "20-r_essentials.html#broader-impacts",
    "title": "Online Appendix A — R essentials",
    "section": "A.2 Broader impacts",
    "text": "A.2 Broader impacts\n\n“We shouldn’t have to think about the societal impact of our work because it’s hard and other people can do it for us” is a really bad argument. I stopped doing CV [computer vision] research because I saw the impact my work was having. I loved the work but the military applications and privacy concerns eventually became impossible to ignore. But basically all facial recognition work would not get published if we took Broader Impacts sections seriously. There is almost no upside and enormous downside risk. To be fair though I should have a lot of humility here. For most of grad school I bought in to the myth that science is apolitical and research is objectively moral and good no matter what the subject is.\nJoe Redmon, 20 February 2020\n\nAlthough the term “data science” is ubiquitous in academia, industry, and even more generally, as we have seen, it is difficult to define. One deliberately antagonistic definition of data science is “[t]he inhumane reduction of humanity down to what can be counted” (Keyes 2019). While purposefully controversial, this definition highlights one reason for the increased demand for data science and quantitative methods over the past decade—individuals and their behavior are now at the heart of it. Many of the techniques have been around for many decades, but what makes them popular now is this human focus.\nUnfortunately, even though much of the work may be focused on individuals, the issues of privacy and consent, and ethical concerns more broadly, rarely seem front of mind. While there are some exceptions, in general, even at the same time as claiming that AI, machine learning, and data science are going to revolutionize society, consideration of these types of issues appears to have been largely treated as something that would be nice to have, rather than something that we may like to think of before we embrace the revolution.\nFor the most part, these types of issues are not new. In the sciences, there has been extensive ethical consideration around CRISPR technology and gene editing (Brokowski and Adli 2019; Marchese 2022). And in an earlier time similar conversations were had, for instance, about Wernher von Braun being allowed to build rockets for the US despite having done the same for Nazi Germany (Neufeld 2002; Wilford 1977). In medicine these concerns have been front-of-mind for some time (American Medical Association and New York Academy of Medicine 1848). Data science seems determined to have its own Tuskegee-moment rather than think about, and proactively address, these issues based on the experiences of other fields.\nThat said, there is some evidence that some data scientists are beginning to be more concerned about the ethics surrounding the practice. For instance, NeurIPS, a prestigious machine learning conference, has required a statement on ethics to accompany all submissions since 2020.\n\nIn order to provide a balanced perspective, authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. Authors should take care to discuss both positive and negative outcomes.\nNeurIPS 2020 Conference Call For Papers\n\nThe purpose of ethical consideration and concern for the broader impact of data science is not to prescriptively rule things in or out, but to provide an opportunity to raise some issues that should be paramount. The variety of data science applications, the relative youth of the field, and the speed of change, mean that such considerations are sometimes knowingly set aside, and this is acceptable to the rest of the field. This contrasts with fields such as science, medicine, engineering, and accounting. Possibly those fields are more self-aware (Figure A.1).\n\n\n\nFigure A.1: Probability, from Randall Munroe’s XKCD."
  },
  {
    "objectID": "20-r_essentials.html#r-rstudio-and-posit-cloud",
    "href": "20-r_essentials.html#r-rstudio-and-posit-cloud",
    "title": "Online Appendix A — R essentials",
    "section": "A.3 R, RStudio, and Posit Cloud",
    "text": "A.3 R, RStudio, and Posit Cloud\nR and RStudio are complementary, but they are not the same thing. Müller, Schieferdecker, and Schratz (2019) explain their relationship by analogy, where R is like an engine and RStudio is like a car—we can use engines in a lot of different situations, and they are not limited to being used in cars, but the combination is especially useful.\n\nA.3.1 R\nR is an open-source and free programming language that is focused on general statistics. Free in this context does not refer to a price of zero, but instead to the freedom that the creators give users to largely do what they want with it (although it also does have a price of zero). This is in contrast with an open-source programming language that is designed for general purpose, such as Python, or an open-source programming language that is focused on probability, such as Stan. It was created by Ross Ihaka and Robert Gentleman at the University of Auckland in the 1990s, and traces its provenance to S, which was developed at Bell Labs in the 1970s. It is maintained by the R Core Team and changes to this “base” of code occur methodically and with concern given to a variety of different priorities.\nMany people build on this stable base, to extend the capabilities of R to better and more quickly suit their needs. They do this by creating packages. Typically, although not always, a package is a collection of R code, mostly functions, and this allows us to more easily do things that we want to do. These packages are managed by repositories such as CRAN and Bioconductor.\nIf you want to use a package, then you first need to install it on your computer, and then you need to load it when you want to use it. Dr Di Cook, Professor of Business Analytics at Monash University, describes this as analogous to a lightbulb. If you want light in your house, first you need to fit a lightbulb, and then you need to turn the switch on. Installing a package, say, install.packages(\"tidyverse\"), is akin to fitting a lightbulb into a socket—you only need to do this once for each lightbulb. But then each time you want light you need to turn on the switch to the lightbulb, which in the R packages case, means drawing on your library, say, library(tidyverse).\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Di Cook is Professor of Business Analytics at Monash University. After earning a PhD in statistics from Rutgers University in 1993 where she focused on statistical graphics, she was appointed as an assistant professor at Iowa State University, being promoted to full professor in 2005, and in 2015 she moved to Monash. One area of her research is data visualization, especially interactive and dynamic graphics. One particularly important paper is Buja, Cook, and Swayne (1996) which proposes a taxonomy of interactive data visualization and associated software XGobi.\n\n\nTo install a package on your computer (again, we will need to do this only once per computer) we use install.packages().\n\ninstall.packages(\"tidyverse\")\n\nAnd then when we want to use the package, we use library().\n\nlibrary(tidyverse)\n\nHaving downloaded it, we can open R and use it directly. It is primarily designed to be interacted with through the command line. While this is functional, it can be useful to have a richer environment than the command line provides. In particular, it can be useful to install an Integrated Development Environment (IDE), which is an application that brings together various bits and pieces that will be used often. One common IDE for R is RStudio, although others such as Visual Studio are also used.\n\n\nA.3.2 RStudio\nRStudio is distinct to R, and they are different entities. RStudio builds on top of R to make it easier to use R. This is in the same way that one could use the internet from the command line, but most people use a browser such as Chrome, Firefox, or Safari.\nRStudio is free in the sense that we do not pay for it. It is also free in the sense of being able to take the code, modify it, and distribute that code. But the maker of RStudio, Posit, is a company, albeit it a B Corp, and so it is possible that the current situation could change. It can be downloaded from Posit here.\nWhen we open RStudio it will look like Figure A.2.\n\n\n\nFigure A.2: Opening RStudio for the first time\n\n\nThe left pane is a console in which you can type and execute R code line by line. Try it with 2+2 by clicking next to the prompt “&gt;”, typing “2+2”, and then pressing “return/enter”.\n\n2 + 2\n\n[1] 4\n\n\nThe pane on the top right has information about the environment. For instance, when we create variables a list of their names and some properties will appear there. Next to the prompt type the following code, replacing Rohan with your name, and again press enter.\n\nmy_name &lt;- \"Rohan\"\n\nAs mentioned in Chapter 2 the &lt;-, or “assignment operator”, allocates \"Rohan\" to an object called “my_name”. You should notice a new value in the environment pane with the variable name and its value.\nThe pane in the bottom right is a file manager. At the moment it should just have two files: an R History file and a R Project file. We will get to what these are later, but for now we will create and save a file.\nRun the following code, without worrying too much about the details for now. And you should see a new “.rds” file in your list of files.\n\nsaveRDS(object = my_name, file = \"my_first_file.rds\")\n\n\n\nA.3.3 Posit Cloud\nWhile you can and should download RStudio to your own computer, initially we recommend using Posit Cloud. This is an online version of RStudio that is provided by Posit. We will use this so that you can focus on getting comfortable with R and RStudio in an environment that is consistent. This way you do not have to worry about what computer you have or installation permissions, amongst other things.\nThe free version of Posit Cloud is free as is no financial cost. The trade-off is that it is not very powerful, and it is sometimes slow, but for the purposes of getting started it is enough."
  },
  {
    "objectID": "20-r_essentials.html#getting-started",
    "href": "20-r_essentials.html#getting-started",
    "title": "Online Appendix A — R essentials",
    "section": "A.4 Getting started",
    "text": "A.4 Getting started\nWe will now start going through some code. Actively write this all out yourself.\nWhile working line-by-line in the console is fine, it is easier to write out a whole script that can then be run. We will do this by making an R Script (“File” \\(\\rightarrow\\) “New File” \\(\\rightarrow\\) “R Script”). The console pane will fall to the bottom left and an R Script will open in the top left. We will write some code that will get all of the Australian federal politicians and then construct a small table about the genders of the prime ministers. Some of this code will not make sense at this stage, but just type it all out to get into the habit and then run it. To run the whole script, we can click “Run” or we can highlight certain lines and then click “Run” to just run those lines.\n\n# Install the packages that we need\ninstall.packages(\"tidyverse\")\ninstall.packages(\"AustralianPoliticians\")\n\n\n# Load the packages that we need to use this time\nlibrary(tidyverse)\nlibrary(AustralianPoliticians)\n\n# Make a table of the counts of genders of the prime ministers\nget_auspol(\"all\") |&gt; # Imports data from GitHub\n  as_tibble() |&gt;\n  filter(wasPrimeMinister == 1) |&gt;\n  count(gender)\n\n# A tibble: 2 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female     1\n2 male      29\n\n\nWe can see that, as at the end of 2021, one female has been prime minister (Julia Gillard), while the other 29 prime ministers were male.\nOne critical operator when programming is the “pipe”: |&gt;. We read this as “and then”. This takes the output of a line of code and uses it as the first input to the next line of code. It makes code easier to read. By way of background, for many years R users used %&gt;% as the pipe, which is from magrittr (Bache and Wickham 2022) and part of the tidyverse. Base R added the pipe that we use in this book, |&gt;, in 2021, and so if you look at older code, you may see the earlier pipe being used. For the most part, they are interchangeable.\nThe idea of the pipe is that we take a dataset, and then do something to it. We used this in the earlier example. Another example follows where we will look at the first six lines of a dataset by piping it to head(). Notice that head() does not explicitly take any arguments in this example. It knows which data to display because the pipe tells it implicitly.\n\nget_auspol(\"all\") |&gt; # Imports data from GitHub\n  head()\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames          firstName commonName displayName    \n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;          \n1 Abbott1859 Abbott  Richard Hartley Smith  Richard   &lt;NA&gt;       Abbott, Richard\n2 Abbott1869 Abbott  Percy Phipps           Percy     &lt;NA&gt;       Abbott, Percy  \n3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    \n4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey \n5 Abbott1891 Abbott  Joseph Palmer          Joseph    &lt;NA&gt;       Abbott, Joseph \n6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   \n# ℹ 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, gender &lt;chr&gt;,\n#   birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;,\n#   member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;,\n#   wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt;\n\n\nWe can save this R Script as “my_first_r_script.R” (“File” \\(\\rightarrow\\) “Save As”). At this point, our workspace should look something like Figure A.3.\n\n\n\nFigure A.3: After running an R Script\n\n\nOne thing to be aware of is that each Posit Cloud workspace is essentially a new computer. Because of this, we need to install any package that we want to use for each workspace. For instance, before we can use the tidyverse, we need to install it with install.packages(\"tidyverse\"). This contrasts with using one’s own computer.\nA few final notes on Posit Cloud:\n\nIn the Australian politician’s example, we got our data from the website GitHub using an R package, but we can get data into a workspace from a local computer in a variety of ways. One way is to use the “upload” button in the “Files” panel. Another is to use readr (Wickham, Hester, and Bryan 2022), which is part of the tidyverse (Wickham et al. 2019).\nPosit Cloud allows some degree of collaboration. For instance, you can give someone else access to a workspace that you create and even both be in the same workspace at the one time. This could be useful for collaboration.\nThere are a variety of weaknesses of Posit Cloud, in particular the RAM limits. Additionally, like any web application, things break from time to time or go down."
  },
  {
    "objectID": "20-r_essentials.html#the-dplyr-verbs",
    "href": "20-r_essentials.html#the-dplyr-verbs",
    "title": "Online Appendix A — R essentials",
    "section": "A.5 The dplyr verbs",
    "text": "A.5 The dplyr verbs\nOne of the key packages that we will use is the tidyverse (Wickham et al. 2019). The tidyverse is actually a package of packages, which means when we install the tidyverse, we actually install a whole bunch of different packages. The key package in the tidyverse in terms of manipulating data is dplyr (Wickham et al. 2022).\nThere are five dplyr functions that are regularly used, and we will now go through each of these. These are commonly referred to as the dplyr verbs.\n\nselect()\nfilter()\narrange()\nmutate()\nsummarise() or equally summarize()\n\nWe will also cover .by, and count() here as they are closely related.\nAs we have already installed the tidyverse, we just need to load it.\n\nlibrary(tidyverse)\n\nAnd we will begin by again using some data about Australian politicians from the AustralianPoliticians package (Alexander and Hodgetts 2021).\n\nlibrary(AustralianPoliticians)\n\naustralian_politicians &lt;-\n  get_auspol(\"all\")\n\nhead(australian_politicians)\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames          firstName commonName displayName    \n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;          \n1 Abbott1859 Abbott  Richard Hartley Smith  Richard   &lt;NA&gt;       Abbott, Richard\n2 Abbott1869 Abbott  Percy Phipps           Percy     &lt;NA&gt;       Abbott, Percy  \n3 Abbott1877 Abbott  Macartney              Macartney Mac        Abbott, Mac    \n4 Abbott1886 Abbott  Charles Lydiard Aubrey Charles   Aubrey     Abbott, Aubrey \n5 Abbott1891 Abbott  Joseph Palmer          Joseph    &lt;NA&gt;       Abbott, Joseph \n6 Abbott1957 Abbott  Anthony John           Anthony   Tony       Abbott, Tony   \n# ℹ 14 more variables: earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, gender &lt;chr&gt;,\n#   birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;,\n#   member &lt;dbl&gt;, senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;,\n#   wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt;\n\n\n\nA.5.1 select()\nWe use select() to pick particular columns of a dataset. For instance, we might like to select the “firstName” column.\n\naustralian_politicians |&gt;\n  select(firstName)\n\n# A tibble: 1,783 × 1\n   firstName\n   &lt;chr&gt;    \n 1 Richard  \n 2 Percy    \n 3 Macartney\n 4 Charles  \n 5 Joseph   \n 6 Anthony  \n 7 John     \n 8 Eric     \n 9 Judith   \n10 Dick     \n# ℹ 1,773 more rows\n\n\nIn R, there are many ways to do things. Sometimes these are different ways to do the same thing, and other times they are different ways to do almost the same thing. For instance, another way to pick a particular column of a dataset is to use the “extract” operator $. This is from base, as opposed to select() which is from the tidyverse.\n\naustralian_politicians$firstName |&gt;\n  head()\n\n[1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"   \"Joseph\"    \"Anthony\"  \n\n\nThe two appear similar—both pick the “firstName” column—but they differ in the class of what they return, with select() returning a tibble and $ returning a vector. For the sake of completeness, if we combine select() with pull() then we get the same class of output, a vector, as if we had used the extract operator.\n\naustralian_politicians |&gt;\n  select(firstName) |&gt;\n  pull() |&gt;\n  head()\n\n[1] \"Richard\"   \"Percy\"     \"Macartney\" \"Charles\"   \"Joseph\"    \"Anthony\"  \n\n\nWe can also use select() to remove columns, by negating the column name.\n\naustralian_politicians |&gt;\n  select(-firstName)\n\n# A tibble: 1,783 × 19\n   uniqueID   surname allOtherNames   commonName displayName earlierOrLaterNames\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;              \n 1 Abbott1859 Abbott  Richard Hartle… &lt;NA&gt;       Abbott, Ri… &lt;NA&gt;               \n 2 Abbott1869 Abbott  Percy Phipps    &lt;NA&gt;       Abbott, Pe… &lt;NA&gt;               \n 3 Abbott1877 Abbott  Macartney       Mac        Abbott, Mac &lt;NA&gt;               \n 4 Abbott1886 Abbott  Charles Lydiar… Aubrey     Abbott, Au… &lt;NA&gt;               \n 5 Abbott1891 Abbott  Joseph Palmer   &lt;NA&gt;       Abbott, Jo… &lt;NA&gt;               \n 6 Abbott1957 Abbott  Anthony John    Tony       Abbott, To… &lt;NA&gt;               \n 7 Abel1939   Abel    John Arthur     &lt;NA&gt;       Abel, John  &lt;NA&gt;               \n 8 Abetz1958  Abetz   Eric            &lt;NA&gt;       Abetz, Eric &lt;NA&gt;               \n 9 Adams1943  Adams   Judith Anne     &lt;NA&gt;       Adams, Jud… nee Bird           \n10 Adams1951  Adams   Dick Godfrey H… &lt;NA&gt;       Adams, Dick &lt;NA&gt;               \n# ℹ 1,773 more rows\n# ℹ 13 more variables: title &lt;chr&gt;, gender &lt;chr&gt;, birthDate &lt;date&gt;,\n#   birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;, deathDate &lt;date&gt;, member &lt;dbl&gt;,\n#   senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;, wikipedia &lt;chr&gt;,\n#   adb &lt;chr&gt;, comments &lt;chr&gt;\n\n\nFinally, we can select() based on conditions. For instance, we can select() all of the columns that start with, say, “birth”.\n\naustralian_politicians |&gt;\n  select(starts_with(\"birth\"))\n\n# A tibble: 1,783 × 3\n   birthDate  birthYear birthPlace  \n   &lt;date&gt;         &lt;dbl&gt; &lt;chr&gt;       \n 1 NA              1859 Bendigo     \n 2 1869-05-14        NA Hobart      \n 3 1877-07-03        NA Murrurundi  \n 4 1886-01-04        NA St Leonards \n 5 1891-10-18        NA North Sydney\n 6 1957-11-04        NA London      \n 7 1939-06-25        NA Sydney      \n 8 1958-01-25        NA Stuttgart   \n 9 1943-04-11        NA Picton      \n10 1951-04-29        NA Launceston  \n# ℹ 1,773 more rows\n\n\nThere are a variety of similar “selection helpers” including starts_with(), ends_with(), and contains(). More information about these is available in the help page for select() which can be accessed by running ?select().\nAt this point, we will use select() to reduce the width of our dataset.\n\naustralian_politicians &lt;-\n  australian_politicians |&gt;\n  select(\n    uniqueID,\n    surname,\n    firstName,\n    gender,\n    birthDate,\n    birthYear,\n    deathDate,\n    member,\n    senator,\n    wasPrimeMinister\n  )\n\naustralian_politicians\n\n# A tibble: 1,783 × 10\n   uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 4 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 5 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 6 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1\n 7 Abel1939   Abel    John      male   1939-06-25        NA NA              1\n 8 Abetz1958  Abetz   Eric      male   1958-01-25        NA NA              0\n 9 Adams1943  Adams   Judith    female 1943-04-11        NA 2012-03-31      0\n10 Adams1951  Adams   Dick      male   1951-04-29        NA NA              1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nOne thing that sometimes confuses people who are new to R, is that the output is not “saved”, unless you assign it to an object. For instance, here the first lines are australian_politicians &lt;- australian_politicians |&gt; and then select() is used, compared with australian_politicians |&gt;. This ensures that the changes brought about by select() are applied to the object, and so it is that modified version that would be used at any point later in the code.\n\n\nA.5.2 filter()\nWe use filter() to pick particular rows of a dataset. For instance, we might be only interested in politicians that became prime minister.\n\naustralian_politicians |&gt;\n  filter(wasPrimeMinister == 1)\n\n# A tibble: 30 × 10\n   uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1\n 2 Barton1849  Barton  Edmund    male   1849-01-18        NA 1920-01-07      1\n 3 Bruce1883   Bruce   Stanley   male   1883-04-15        NA 1967-08-25      1\n 4 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n 5 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n 6 Curtin1885  Curtin  John      male   1885-01-08        NA 1945-07-05      1\n 7 Deakin1856  Deakin  Alfred    male   1856-08-03        NA 1919-10-07      1\n 8 Fadden1894  Fadden  Arthur    male   1894-04-13        NA 1973-04-21      1\n 9 Fisher1862  Fisher  Andrew    male   1862-08-29        NA 1928-10-22      1\n10 Forde1890   Forde   Francis   male   1890-07-18        NA 1983-01-28      1\n# ℹ 20 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWe could also give filter() two conditions. For instance, we could look at politicians that become prime minister and were named Joseph, using the “and” operator &.\n\naustralian_politicians |&gt;\n  filter(wasPrimeMinister == 1 & firstName == \"Joseph\")\n\n# A tibble: 3 × 10\n  uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWe get the same result if we use a comma instead of an ampersand.\n\naustralian_politicians |&gt;\n  filter(wasPrimeMinister == 1, firstName == \"Joseph\")\n\n# A tibble: 3 × 10\n  uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Chifley1885 Chifley Joseph    male   1885-09-22        NA 1951-06-13      1\n2 Cook1860    Cook    Joseph    male   1860-12-07        NA 1947-07-30      1\n3 Lyons1879   Lyons   Joseph    male   1879-09-15        NA 1939-04-07      1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nSimilarly, we could look at politicians who were named, say, Myles or Ruth using the “or” operator |.\n\naustralian_politicians |&gt;\n  filter(firstName == \"Myles\" | firstName == \"Ruth\")\n\n# A tibble: 3 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Coleman1931  Coleman  Ruth      female 1931-09-27        NA 2008-03-27      0\n2 Ferricks1875 Ferricks Myles     male   1875-11-12        NA 1932-08-20      0\n3 Webber1965   Webber   Ruth      female 1965-03-24        NA NA              0\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWe could also pipe the result. For instance we could pipe from filter() to select().\n\naustralian_politicians |&gt;\n  filter(firstName == \"Ruth\" | firstName == \"Myles\") |&gt;\n  select(firstName, surname)\n\n# A tibble: 3 × 2\n  firstName surname \n  &lt;chr&gt;     &lt;chr&gt;   \n1 Ruth      Coleman \n2 Myles     Ferricks\n3 Ruth      Webber  \n\n\nIf we happen to know the particular row number that is of interest then we could filter() to only that particular row. For instance, say the row 853 was of interest.\n\naustralian_politicians |&gt;\n  filter(row_number() == 853)\n\n# A tibble: 1 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate member\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nThere is also a dedicated function to do this, which is slice().\n\naustralian_politicians |&gt;\n  slice(853)\n\n# A tibble: 1 × 10\n  uniqueID     surname  firstName gender birthDate  birthYear deathDate member\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n1 Jakobsen1947 Jakobsen Carolyn   female 1947-09-11        NA NA             1\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWhile this may seem somewhat esoteric, it is especially useful if we would like to remove a particular row using negation, or duplicate specific rows. For instance, we could remove the first row.\n\naustralian_politicians |&gt;\n  slice(-1)\n\n# A tibble: 1,782 × 10\n   uniqueID    surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1869  Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 2 Abbott1877  Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 3 Abbott1886  Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 4 Abbott1891  Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 5 Abbott1957  Abbott  Anthony   male   1957-11-04        NA NA              1\n 6 Abel1939    Abel    John      male   1939-06-25        NA NA              1\n 7 Abetz1958   Abetz   Eric      male   1958-01-25        NA NA              0\n 8 Adams1943   Adams   Judith    female 1943-04-11        NA 2012-03-31      0\n 9 Adams1951   Adams   Dick      male   1951-04-29        NA NA              1\n10 Adamson1857 Adamson John      male   1857-02-18        NA 1922-05-02      0\n# ℹ 1,772 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWe could also only, say, only keep the first three rows.\n\naustralian_politicians |&gt;\n  slice(1:3)\n\n# A tibble: 3 × 10\n  uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n3 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nFinally, we could duplicate the first two rows and this takes advantage of n() which provides the current group size.\n\naustralian_politicians |&gt;\n  slice(1:2, 1:n())\n\n# A tibble: 1,785 × 10\n   uniqueID   surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 2 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 3 Abbott1859 Abbott  Richard   male   NA              1859 1940-02-28      0\n 4 Abbott1869 Abbott  Percy     male   1869-05-14        NA 1940-09-09      1\n 5 Abbott1877 Abbott  Macartney male   1877-07-03        NA 1960-12-30      0\n 6 Abbott1886 Abbott  Charles   male   1886-01-04        NA 1975-04-30      1\n 7 Abbott1891 Abbott  Joseph    male   1891-10-18        NA 1965-05-07      1\n 8 Abbott1957 Abbott  Anthony   male   1957-11-04        NA NA              1\n 9 Abel1939   Abel    John      male   1939-06-25        NA NA              1\n10 Abetz1958  Abetz   Eric      male   1958-01-25        NA NA              0\n# ℹ 1,775 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n\n\nA.5.3 arrange()\nWe use arrange() to change the order of the dataset based on the values of particular columns. For instance, we could arrange the politicians by their birthday.\n\naustralian_politicians |&gt;\n  arrange(birthYear)\n\n# A tibble: 1,783 × 10\n   uniqueID    surname firstName gender birthDate birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Edwards1842 Edwards Richard   male   NA             1842 1915-10-29      1\n 2 Sawers1844  Sawers  William   male   NA             1844 1916-05-19      1\n 3 Barker1846  Barker  Stephen   male   NA             1846 1924-06-21      0\n 4 Corser1852  Corser  Edward    male   NA             1852 1928-07-31      1\n 5 Lee1856     Lee     Henry     male   NA             1856 1927-08-12      1\n 6 Grant1857   Grant   John      male   NA             1857 1928-05-19      0\n 7 Abbott1859  Abbott  Richard   male   NA             1859 1940-02-28      0\n 8 Palmer1859  Palmer  Albert    male   NA             1859 1919-08-14      1\n 9 Riley1859   Riley   Edward    male   NA             1859 1943-07-21      1\n10 Kennedy1860 Kennedy Thomas    male   NA             1860 1929-02-16      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWe could modify arrange() with desc() to change from ascending to descending order.\n\naustralian_politicians |&gt;\n  arrange(desc(birthYear))\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 McBain1982    McBain  Kristy    female 1982-09-29      1982 NA              1\n 2 Cox1977       Cox     Dorinda   female NA              1977 NA              0\n 3 Thorpe1973    Thorpe  Lidia     female 1973-08-18      1973 NA              0\n 4 McLachlan1966 McLach… Andrew    male   1966-01-14      1966 NA              0\n 5 Wortley1959   Wortley Dana      female NA              1959 NA              0\n 6 Baker1903     Baker   Francis   male   NA              1903 1939-03-28      1\n 7 Clay1900      Clay    Lionel    male   NA              1900 1965-04-16      1\n 8 Breen1898     Breen   John      male   NA              1898 1966-02-05      1\n 9 Clasby1891    Clasby  John      male   NA              1891 1932-01-15      1\n10 Gander1888    Gander  Joseph    male   NA              1888 1954-11-22      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nThis could also be achieved with the minus sign.\n\naustralian_politicians |&gt;\n  arrange(-birthYear)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 McBain1982    McBain  Kristy    female 1982-09-29      1982 NA              1\n 2 Cox1977       Cox     Dorinda   female NA              1977 NA              0\n 3 Thorpe1973    Thorpe  Lidia     female 1973-08-18      1973 NA              0\n 4 McLachlan1966 McLach… Andrew    male   1966-01-14      1966 NA              0\n 5 Wortley1959   Wortley Dana      female NA              1959 NA              0\n 6 Baker1903     Baker   Francis   male   NA              1903 1939-03-28      1\n 7 Clay1900      Clay    Lionel    male   NA              1900 1965-04-16      1\n 8 Breen1898     Breen   John      male   NA              1898 1966-02-05      1\n 9 Clasby1891    Clasby  John      male   NA              1891 1932-01-15      1\n10 Gander1888    Gander  Joseph    male   NA              1888 1954-11-22      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nAnd we could arrange based on more than one column. For instance, if two politicians have the same first name, then we could also arrange based on their birthday.\n\naustralian_politicians |&gt;\n  arrange(firstName, birthYear)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 3 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 4 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Bird1906      Bird    Alan      male   1906-09-28        NA 1962-07-21      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWe could achieve the same result by piping between two instances of arrange().\n\naustralian_politicians |&gt;\n  arrange(birthYear) |&gt;\n  arrange(firstName)\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 3 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 4 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Bird1906      Bird    Alan      male   1906-09-28        NA 1962-07-21      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nWhen we use arrange() we should be clear about precedence. For instance, changing to birthday and then first name would give a different arrangement.\n\naustralian_politicians |&gt;\n  arrange(birthYear, firstName)\n\n# A tibble: 1,783 × 10\n   uniqueID    surname firstName gender birthDate birthYear deathDate  member\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;        &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Edwards1842 Edwards Richard   male   NA             1842 1915-10-29      1\n 2 Sawers1844  Sawers  William   male   NA             1844 1916-05-19      1\n 3 Barker1846  Barker  Stephen   male   NA             1846 1924-06-21      0\n 4 Corser1852  Corser  Edward    male   NA             1852 1928-07-31      1\n 5 Lee1856     Lee     Henry     male   NA             1856 1927-08-12      1\n 6 Grant1857   Grant   John      male   NA             1857 1928-05-19      0\n 7 Palmer1859  Palmer  Albert    male   NA             1859 1919-08-14      1\n 8 Riley1859   Riley   Edward    male   NA             1859 1943-07-21      1\n 9 Abbott1859  Abbott  Richard   male   NA             1859 1940-02-28      0\n10 Kennedy1860 Kennedy Thomas    male   NA             1860 1929-02-16      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\nA nice way to arrange by a variety of columns is to use across(). It enables us to use the “selection helpers” such as starts_with() that were mentioned in association with select().\n\naustralian_politicians |&gt;\n  arrange(across(c(firstName, birthYear)))\n\n# A tibble: 1,783 × 10\n   uniqueID      surname firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Blain1894     Blain   Adair     male   1894-11-21        NA 1983-04-28      1\n 2 Armstrong1909 Armstr… Adam      male   1909-07-01        NA 1982-02-22      1\n 3 Bandt1972     Bandt   Adam      male   1972-03-11        NA NA              1\n 4 Dein1889      Dein    Adam      male   1889-03-04        NA 1969-05-09      1\n 5 Ridgeway1962  Ridgew… Aden      male   1962-09-18        NA NA              0\n 6 Bennett1933   Bennett Adrian    male   1933-01-21        NA 2006-05-09      1\n 7 Gibson1935    Gibson  Adrian    male   1935-11-03        NA 2015-04-30      1\n 8 Wynne1850     Wynne   Agar      male   1850-01-15        NA 1934-05-12      1\n 9 Robertson1882 Robert… Agnes     female 1882-07-31        NA 1968-01-29      0\n10 Bird1906      Bird    Alan      male   1906-09-28        NA 1962-07-21      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\naustralian_politicians |&gt;\n  arrange(across(starts_with(\"birth\")))\n\n# A tibble: 1,783 × 10\n   uniqueID     surname  firstName gender birthDate  birthYear deathDate  member\n   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;date&gt;         &lt;dbl&gt; &lt;date&gt;      &lt;dbl&gt;\n 1 Braddon1829  Braddon  Edward    male   1829-06-11        NA 1904-02-02      1\n 2 Ferguson1830 Ferguson John      male   1830-03-15        NA 1906-03-30      0\n 3 Zeal1830     Zeal     William   male   1830-12-05        NA 1912-03-11      0\n 4 Fraser1832   Fraser   Simon     male   1832-08-21        NA 1919-07-30      0\n 5 Groom1833    Groom    William   male   1833-03-09        NA 1901-08-08      1\n 6 Sargood1834  Sargood  Frederick male   1834-05-30        NA 1903-01-02      0\n 7 Fysh1835     Fysh     Philip    male   1835-03-01        NA 1919-12-20      1\n 8 Playford1837 Playford Thomas    male   1837-11-26        NA 1915-04-19      0\n 9 Solomon1839  Solomon  Elias     male   1839-09-02        NA 1909-05-23      1\n10 McLean1840   McLean   Allan     male   1840-02-03        NA 1911-07-13      1\n# ℹ 1,773 more rows\n# ℹ 2 more variables: senator &lt;dbl&gt;, wasPrimeMinister &lt;dbl&gt;\n\n\n\n\nA.5.4 mutate()\nWe use mutate() when we want to make a new column. For instance, perhaps we want to make a new column that is 1 if a person was both a member and a senator and 0 otherwise. That is to say that our new column would denote politicians that served in both the upper and the lower house.\n\naustralian_politicians &lt;-\n  australian_politicians |&gt;\n  mutate(was_both = if_else(member == 1 & senator == 1, 1, 0))\n\naustralian_politicians |&gt;\n  select(member, senator, was_both)\n\n# A tibble: 1,783 × 3\n   member senator was_both\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1      0       1        0\n 2      1       1        1\n 3      0       1        0\n 4      1       0        0\n 5      1       0        0\n 6      1       0        0\n 7      1       0        0\n 8      0       1        0\n 9      0       1        0\n10      1       0        0\n# ℹ 1,773 more rows\n\n\nWe could use mutate() with math, such as addition and subtraction. For instance, we could calculate the age that the politicians are (or would have been) in 2022.\n\nlibrary(lubridate)\n\naustralian_politicians &lt;-\n  australian_politicians |&gt;\n  mutate(age = 2022 - year(birthDate))\n\naustralian_politicians |&gt;\n  select(uniqueID, age)\n\n# A tibble: 1,783 × 2\n   uniqueID     age\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Abbott1859    NA\n 2 Abbott1869   153\n 3 Abbott1877   145\n 4 Abbott1886   136\n 5 Abbott1891   131\n 6 Abbott1957    65\n 7 Abel1939      83\n 8 Abetz1958     64\n 9 Adams1943     79\n10 Adams1951     71\n# ℹ 1,773 more rows\n\n\nThere are a variety of functions that are especially useful when constructing new columns. These include log() which will compute the natural logarithm, lead() which will bring values up by one row, lag() which will push values down by one row, and cumsum() which creates a cumulative sum of the column.\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  mutate(log_age = log(age))\n\n# A tibble: 1,783 × 3\n   uniqueID     age log_age\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Abbott1859    NA   NA   \n 2 Abbott1869   153    5.03\n 3 Abbott1877   145    4.98\n 4 Abbott1886   136    4.91\n 5 Abbott1891   131    4.88\n 6 Abbott1957    65    4.17\n 7 Abel1939      83    4.42\n 8 Abetz1958     64    4.16\n 9 Adams1943     79    4.37\n10 Adams1951     71    4.26\n# ℹ 1,773 more rows\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  mutate(lead_age = lead(age))\n\n# A tibble: 1,783 × 3\n   uniqueID     age lead_age\n   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 Abbott1859    NA      153\n 2 Abbott1869   153      145\n 3 Abbott1877   145      136\n 4 Abbott1886   136      131\n 5 Abbott1891   131       65\n 6 Abbott1957    65       83\n 7 Abel1939      83       64\n 8 Abetz1958     64       79\n 9 Adams1943     79       71\n10 Adams1951     71      165\n# ℹ 1,773 more rows\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  mutate(lag_age = lag(age))\n\n# A tibble: 1,783 × 3\n   uniqueID     age lag_age\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1 Abbott1859    NA      NA\n 2 Abbott1869   153      NA\n 3 Abbott1877   145     153\n 4 Abbott1886   136     145\n 5 Abbott1891   131     136\n 6 Abbott1957    65     131\n 7 Abel1939      83      65\n 8 Abetz1958     64      83\n 9 Adams1943     79      64\n10 Adams1951     71      79\n# ℹ 1,773 more rows\n\naustralian_politicians |&gt;\n  select(uniqueID, age) |&gt;\n  drop_na(age) |&gt;\n  mutate(cumulative_age = cumsum(age))\n\n# A tibble: 1,718 × 3\n   uniqueID      age cumulative_age\n   &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n 1 Abbott1869    153            153\n 2 Abbott1877    145            298\n 3 Abbott1886    136            434\n 4 Abbott1891    131            565\n 5 Abbott1957     65            630\n 6 Abel1939       83            713\n 7 Abetz1958      64            777\n 8 Adams1943      79            856\n 9 Adams1951      71            927\n10 Adamson1857   165           1092\n# ℹ 1,708 more rows\n\n\nAs we have in earlier examples, we can also use mutate() in combination with across(). This includes the potential use of the selection helpers. For instance, we could count the number of characters in both the first and last names at the same time.\n\naustralian_politicians |&gt;\n  mutate(across(c(firstName, surname), str_count)) |&gt;\n  select(uniqueID, firstName, surname)\n\n# A tibble: 1,783 × 3\n   uniqueID   firstName surname\n   &lt;chr&gt;          &lt;int&gt;   &lt;int&gt;\n 1 Abbott1859         7       6\n 2 Abbott1869         5       6\n 3 Abbott1877         9       6\n 4 Abbott1886         7       6\n 5 Abbott1891         6       6\n 6 Abbott1957         7       6\n 7 Abel1939           4       4\n 8 Abetz1958          4       5\n 9 Adams1943          6       5\n10 Adams1951          4       5\n# ℹ 1,773 more rows\n\n\nFinally, we use case_when() when we need to make a new column on the basis of more than two conditional statements (in contrast to if_else() from our first mutate() example). For instance, we may have some years and want to group them into decades.\n\nlibrary(lubridate)\n\naustralian_politicians |&gt;\n  mutate(\n    year_of_birth = year(birthDate),\n    decade_of_birth =\n      case_when(\n        year_of_birth &lt;= 1929 ~ \"pre-1930\",\n        year_of_birth &lt;= 1939 ~ \"1930s\",\n        year_of_birth &lt;= 1949 ~ \"1940s\",\n        year_of_birth &lt;= 1959 ~ \"1950s\",\n        year_of_birth &lt;= 1969 ~ \"1960s\",\n        year_of_birth &lt;= 1979 ~ \"1970s\",\n        year_of_birth &lt;= 1989 ~ \"1980s\",\n        year_of_birth &lt;= 1999 ~ \"1990s\",\n        TRUE ~ \"Unknown or error\"\n      )\n  ) |&gt;\n  select(uniqueID, year_of_birth, decade_of_birth)\n\n# A tibble: 1,783 × 3\n   uniqueID   year_of_birth decade_of_birth \n   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;           \n 1 Abbott1859            NA Unknown or error\n 2 Abbott1869          1869 pre-1930        \n 3 Abbott1877          1877 pre-1930        \n 4 Abbott1886          1886 pre-1930        \n 5 Abbott1891          1891 pre-1930        \n 6 Abbott1957          1957 1950s           \n 7 Abel1939            1939 1930s           \n 8 Abetz1958           1958 1950s           \n 9 Adams1943           1943 1940s           \n10 Adams1951           1951 1950s           \n# ℹ 1,773 more rows\n\n\nWe could accomplish this with a series of nested if_else() statements, but case_when() is more clear. The cases are evaluated in order and as soon as there is a match case_when() does not continue to the remainder of the cases. It can be useful to have a catch-all at the end that will signal if there is a potential issue that we might like to know about if the code were to ever get there.\n\n\nA.5.5 summarise()\nWe use summarise() when we would like to make new, condensed, summary variables. For instance, perhaps we would like to know the minimum, average, and maximum of some column.\n\naustralian_politicians |&gt;\n  summarise(\n    youngest = min(age, na.rm = TRUE),\n    oldest = max(age, na.rm = TRUE),\n    average = mean(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  youngest oldest average\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1       28    193    101.\n\n\nAs an aside, summarise() and summarize() are equivalent and we can use either. In this book we use summarise().\n\naustralian_politicians |&gt;\n  summarize(\n    youngest = min(age, na.rm = TRUE),\n    oldest = max(age, na.rm = TRUE),\n    average = mean(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 3\n  youngest oldest average\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1       28    193    101.\n\n\nBy default, summarise() will provide one row of output for a whole dataset. For instance, in the earlier example we found the youngest, oldest, and average across all politicians. However, we can create more groups in our dataset using .by within the function. We can use many functions on the basis of groups, but the summarise() function is particularly powerful in conjunction with .by. For instance, we could group by gender, and then get age-based summary statistics.\n\naustralian_politicians |&gt;\n  summarise(\n    youngest = min(age, na.rm = TRUE),\n    oldest = max(age, na.rm = TRUE),\n    average = mean(age, na.rm = TRUE),\n    .by = gender\n  )\n\n# A tibble: 2 × 4\n  gender youngest oldest average\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 male         28    193   106. \n2 female       32    140    66.0\n\n\nSimilarly, we could look at youngest, oldest, and mean age at death by gender.\n\naustralian_politicians |&gt;\n  mutate(days_lived = deathDate - birthDate) |&gt;\n  drop_na(days_lived) |&gt;\n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |&gt; round(),\n    max_days = max(days_lived),\n    .by = gender\n  )\n\n# A tibble: 2 × 4\n  gender min_days   mean_days  max_days  \n  &lt;chr&gt;  &lt;drtn&gt;     &lt;drtn&gt;     &lt;drtn&gt;    \n1 male   12380 days 27376 days 36416 days\n2 female 14856 days 28857 days 35560 days\n\n\nAnd so we learn that female members of parliament on average lived slightly longer than male members of parliament.\nWe can use .by on the basis of more than one group. For instance, we could look at the average number of days lived by gender and by they were in the House of Representatives or the Senate.\n\naustralian_politicians |&gt;\n  mutate(days_lived = deathDate - birthDate) |&gt;\n  drop_na(days_lived) |&gt;\n  summarise(\n    min_days = min(days_lived),\n    mean_days = mean(days_lived) |&gt; round(),\n    max_days = max(days_lived),\n    .by = c(gender, member)\n  )\n\n# A tibble: 4 × 5\n  gender member min_days   mean_days  max_days  \n  &lt;chr&gt;   &lt;dbl&gt; &lt;drtn&gt;     &lt;drtn&gt;     &lt;drtn&gt;    \n1 male        1 12380 days 27496 days 36328 days\n2 male        0 13619 days 27133 days 36416 days\n3 female      0 21746 days 29517 days 35560 days\n4 female      1 14856 days 27538 days 33442 days\n\n\nWe can use count() to create counts by groups. For instance, the number of politicians by gender.\n\naustralian_politicians |&gt;\n  count(gender)\n\n# A tibble: 2 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 female   240\n2 male    1543\n\n\nIn addition to the count(), we could calculate a proportion.\n\naustralian_politicians |&gt;\n  count(gender) |&gt;\n  mutate(proportion = n / (sum(n)))\n\n# A tibble: 2 × 3\n  gender     n proportion\n  &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 female   240      0.135\n2 male    1543      0.865\n\n\nUsing count() is essentially the same as using .by within summarise() with n(), and we get the same result in that way.\n\naustralian_politicians |&gt;\n  summarise(n = n(),\n            .by = gender)\n\n# A tibble: 2 × 2\n  gender     n\n  &lt;chr&gt;  &lt;int&gt;\n1 male    1543\n2 female   240\n\n\nAnd there is a comparably helpful function that acts similarly tomutate(), which is add_count(). The difference is that the number will be added in a new column.\n\naustralian_politicians |&gt;\n  add_count(gender) |&gt;\n  select(uniqueID, gender, n)\n\n# A tibble: 1,783 × 3\n   uniqueID   gender     n\n   &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;\n 1 Abbott1859 male    1543\n 2 Abbott1869 male    1543\n 3 Abbott1877 male    1543\n 4 Abbott1886 male    1543\n 5 Abbott1891 male    1543\n 6 Abbott1957 male    1543\n 7 Abel1939   male    1543\n 8 Abetz1958  male    1543\n 9 Adams1943  female   240\n10 Adams1951  male    1543\n# ℹ 1,773 more rows"
  },
  {
    "objectID": "20-r_essentials.html#base",
    "href": "20-r_essentials.html#base",
    "title": "Online Appendix A — R essentials",
    "section": "A.6 Base",
    "text": "A.6 Base\nWhile the tidyverse was established relatively recently to help with data science, R existed long before this. There is a host of functionality that is built into R especially around the core needs of programming and statisticians.\nIn particular, we will cover:\n\nclass().\nData simulation.\nfunction(), for(), and apply().\n\nThere is no need to install or load any additional packages, as this functionality comes with R.\n\nA.6.1 class()\nIn everyday usage “a, b, c, …” are letters and “1, 2, 3,…” are numbers. And we use letters and numbers differently; for instance, we do not add or subtract letters. Similarly, R needs to have some way of distinguishing different classes of content and to define the properties that each class has, “how it behaves, and how it relates to other types of objects” (Wickham 2019).\nClasses have a hierarchy. For instance, we are “human”, which is itself “animal”. All “humans” are “animals”, but not all “animals” are “humans”. Similarly, all integers are numbers, but not all numbers are integers. We can find out the class of an object in R with class().\n\na_number &lt;- 8\nclass(a_number)\n\n[1] \"numeric\"\n\na_letter &lt;- \"a\"\nclass(a_letter)\n\n[1] \"character\"\n\n\nThe classes that we cover here are “numeric”, “character”, “factor”, “date”, and “data.frame”.\nThe first thing to know is that, in the same way that a frog can become a prince, we can sometimes change the class of an object in R. This is called “casting”. For instance, we could start with a “numeric”, change it to a “character” with as.character(), and then a “factor” with as.factor(). But if we tried to make it into a “date” with as.Date() we would get an error because no all numbers have the properties that are needed to be a date.\n\na_number &lt;- 8\na_number\n\n[1] 8\n\nclass(a_number)\n\n[1] \"numeric\"\n\na_number &lt;- as.character(a_number)\na_number\n\n[1] \"8\"\n\nclass(a_number)\n\n[1] \"character\"\n\na_number &lt;- as.factor(a_number)\na_number\n\n[1] 8\nLevels: 8\n\nclass(a_number)\n\n[1] \"factor\"\n\n\nCompared with “numeric” and “character” classes, the “factor” class might be less familiar. A “factor” is used for categorical data that can only take certain values (Wickham 2019). For instance, typical usage of a “factor” variable would be a binary, such as “day” or “night”. It is also often used for age-groups, such as “18-29”, “30-44”, “45-60”, “60+” (as opposed to age, which would often be a “numeric”); and sometimes for level of education: “less than high school”, “high school”, “college”, “undergraduate degree”, “postgraduate degree”. We can find the allowed levels for a “factor” using levels().\n\nage_groups &lt;- factor(\n  c(\"18-29\", \"30-44\", \"45-60\", \"60+\")\n)\nage_groups\n\n[1] 18-29 30-44 45-60 60+  \nLevels: 18-29 30-44 45-60 60+\n\nclass(age_groups)\n\n[1] \"factor\"\n\nlevels(age_groups)\n\n[1] \"18-29\" \"30-44\" \"45-60\" \"60+\"  \n\n\nDates are an especially tricky class and quickly become complicated. Nonetheless, at a foundational level, we can use as.Date() to convert a character that looks like a “date” into an actual “date”. This enables us to, say, perform addition and subtraction, when we would not be able to do that with a “character”.\n\nlooks_like_a_date_but_is_not &lt;- \"2022-01-01\"\nlooks_like_a_date_but_is_not\n\n[1] \"2022-01-01\"\n\nclass(looks_like_a_date_but_is_not)\n\n[1] \"character\"\n\nis_a_date &lt;- as.Date(looks_like_a_date_but_is_not)\nis_a_date\n\n[1] \"2022-01-01\"\n\nclass(is_a_date)\n\n[1] \"Date\"\n\nis_a_date + 3\n\n[1] \"2022-01-04\"\n\n\nThe final class that we discuss here is “data.frame”. This looks like a spreadsheet and is commonly used to store the data that we will analyze. Formally, “a data frame is a list of equal-length vectors” (Wickham 2019). It will have column and row names which we can see using colnames() and rownames(), although often the names of the rows are just numbers.\nTo illustrate this, we use the “ResumeNames” dataset from AER (Kleiber and Zeileis 2008). This package can be installed in the same way as any other package from CRAN. This dataset comprises cross-sectional data about resume content, especially the name used on the resume, and associated information about whether the candidate received a call-back for 4,870 fictitious resumes. The dataset was created by Bertrand and Mullainathan (2004) who sent fictitious resumes in response to job advertisements in Boston and Chicago that differed in whether the resume was assigned a “very African American sounding name or a very White sounding name”. They found considerable discrimination whereby “White names receive 50 per cent more callbacks for interviews”. Hangartner, Kopp, and Siegenthaler (2021) generalize this using an online Swiss platform and find that immigrants and minority ethnic groups are contacted less by recruiters, as are women when the profession is men-dominated, and vice versa.\n\ninstall.packages(\"AER\")\n\n\nlibrary(AER)\ndata(\"ResumeNames\", package = \"AER\")\n\n\nResumeNames |&gt;\n  head()\n\n     name gender ethnicity quality call    city jobs experience honors\n1 Allison female      cauc     low   no chicago    2          6     no\n2 Kristen female      cauc    high   no chicago    3          6     no\n3 Lakisha female      afam     low   no chicago    1          6     no\n4 Latonya female      afam    high   no chicago    4          6     no\n5  Carrie female      cauc    high   no chicago    3         22     no\n6     Jay   male      cauc     low   no chicago    2          6    yes\n  volunteer military holes school email computer special college minimum equal\n1        no       no   yes     no    no      yes      no     yes       5   yes\n2       yes      yes    no    yes   yes      yes      no      no       5   yes\n3        no       no    no    yes    no      yes      no     yes       5   yes\n4       yes       no   yes     no   yes      yes     yes      no       5   yes\n5        no       no    no    yes   yes      yes      no      no    some   yes\n6        no       no    no     no    no       no     yes     yes    none   yes\n      wanted requirements reqexp reqcomm reqeduc reqcomp reqorg\n1 supervisor          yes    yes      no      no     yes     no\n2 supervisor          yes    yes      no      no     yes     no\n3 supervisor          yes    yes      no      no     yes     no\n4 supervisor          yes    yes      no      no     yes     no\n5  secretary          yes    yes      no      no     yes    yes\n6      other           no     no      no      no      no     no\n                          industry\n1                    manufacturing\n2                    manufacturing\n3                    manufacturing\n4                    manufacturing\n5 health/education/social services\n6                            trade\n\nclass(ResumeNames)\n\n[1] \"data.frame\"\n\ncolnames(ResumeNames)\n\n [1] \"name\"         \"gender\"       \"ethnicity\"    \"quality\"      \"call\"        \n [6] \"city\"         \"jobs\"         \"experience\"   \"honors\"       \"volunteer\"   \n[11] \"military\"     \"holes\"        \"school\"       \"email\"        \"computer\"    \n[16] \"special\"      \"college\"      \"minimum\"      \"equal\"        \"wanted\"      \n[21] \"requirements\" \"reqexp\"       \"reqcomm\"      \"reqeduc\"      \"reqcomp\"     \n[26] \"reqorg\"       \"industry\"    \n\n\nWe can examine the class of the vectors, i.e. the columns, that make-up a data frame by specifying the column name.\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$jobs)\n\n[1] \"integer\"\n\n\nSometimes it is helpful to be able to change the classes of many columns at once. We can do this by using mutate() and across().\n\nclass(ResumeNames$name)\n\n[1] \"factor\"\n\nclass(ResumeNames$gender)\n\n[1] \"factor\"\n\nclass(ResumeNames$ethnicity)\n\n[1] \"factor\"\n\nResumeNames &lt;- ResumeNames |&gt;\n  mutate(across(c(name, gender, ethnicity), as.character)) |&gt;\n  head()\n\nclass(ResumeNames$name)\n\n[1] \"character\"\n\nclass(ResumeNames$gender)\n\n[1] \"character\"\n\nclass(ResumeNames$ethnicity)\n\n[1] \"character\"\n\n\nThere are many ways for code to not run but having an issue with the class is always among the first things to check. Common issues are variables that we think should be “character” or “numeric” actually being “factor”. And variables that we think should be “numeric” actually being “character”.\nFinally, it is worth pointing out that the class of a vector is whatever the class of the content. In Python and other languages, a similar data structure to a vector is a “list”. A “list” is a class of its own, and the objects in a “list” have their own classes (for instance, [\"a\", 1] is an object of class “list” with entries of class “string” and “int”). This may be counter-intuitive to see that a vector is not its own class if you are coming to R from another language.\n\n\nA.6.2 Simulating data\nSimulating data is a key skill for telling believable stories with data. In order to simulate data, we need to be able to randomly draw from statistical distributions and other collections. R has a variety of functions to make this easier, including: the normal distribution, rnorm(); the uniform distribution, runif(); the Poisson distribution, rpois(); the binomial distribution, rbinom(); and many others. To randomly sample from a collection of items, we can use sample().\nWhen dealing with randomness, the need for reproducibility makes it important, paradoxically, that the randomness is repeatable. That is to say, another person needs to be able to draw the random numbers that we draw. We do this by setting a seed for our random draws using set.seed().\nWe could get observations from the standard normal distribution and put the those into a data frame.\n\nset.seed(853)\n\nnumber_of_observations &lt;- 5\n\nsimulated_data &lt;-\n  data.frame(\n    person = c(1:number_of_observations),\n    std_normal_observations = rnorm(\n      n = number_of_observations,\n      mean = 0,\n      sd = 1\n    )\n  )\n\nsimulated_data\n\n  person std_normal_observations\n1      1             -0.35980342\n2      2             -0.04064753\n3      3             -1.78216227\n4      4             -1.12242282\n5      5             -1.00278400\n\n\nWe could then add draws from the uniform, Poisson, and binomial distributions, using cbind() to bring the columns of the original dataset and the new one together.\n\nsimulated_data &lt;-\n  simulated_data |&gt;\n  cbind() |&gt;\n  data.frame(\n    uniform_observations =\n      runif(n = number_of_observations, min = 0, max = 10),\n    poisson_observations =\n      rpois(n = number_of_observations, lambda = 100),\n    binomial_observations =\n      rbinom(n = number_of_observations, size = 2, prob = 0.5)\n  )\n\nsimulated_data\n\n  person std_normal_observations uniform_observations poisson_observations\n1      1             -0.35980342            9.6219155                   81\n2      2             -0.04064753            7.2269016                   91\n3      3             -1.78216227            0.8252921                   84\n4      4             -1.12242282            1.0379810                  100\n5      5             -1.00278400            3.0942004                   97\n  binomial_observations\n1                     2\n2                     1\n3                     1\n4                     1\n5                     1\n\n\nFinally, we will add a favorite color to each observation with sample().\n\nsimulated_data &lt;-\n  data.frame(\n    favorite_color = sample(\n      x = c(\"blue\", \"white\"),\n      size = number_of_observations,\n      replace = TRUE\n    )\n  ) |&gt;\n  cbind(simulated_data)\n\nsimulated_data\n\n  favorite_color person std_normal_observations uniform_observations\n1           blue      1             -0.35980342            9.6219155\n2           blue      2             -0.04064753            7.2269016\n3           blue      3             -1.78216227            0.8252921\n4          white      4             -1.12242282            1.0379810\n5           blue      5             -1.00278400            3.0942004\n  poisson_observations binomial_observations\n1                   81                     2\n2                   91                     1\n3                   84                     1\n4                  100                     1\n5                   97                     1\n\n\nWe set the option “replace” to “TRUE” because we are only choosing between two items, but each time we choose we want the possibility that either are chosen. Depending on the simulation we may need to think about whether “replace” should be “TRUE” or “FALSE”. Another useful optional argument in sample() is to adjust the probability with which each item is drawn. The default is that all options are equally likely, but we could specify particular probabilities if we wanted to with “prob”. As always with functions, we can find more in the help file, for instance ?sample.\n\n\nA.6.3 function(), for(), and apply()\nR “is a functional programming language” (Wickham 2019). This means that we foundationally write, use, and compose functions, which are collections of code that accomplish something specific.\nThere are a lot of functions in R that other people have written, and we can use. Almost any common statistical or data science task that we might need to accomplish likely already has a function that has been written by someone else and made available to us, either as part of the base R installation or a package. But we will need to write our own functions from time to time, especially for more-specific tasks. We define a function using function(), and then assign a name. We will likely need to include some inputs and outputs for the function. Inputs are specified between round brackets. The specific task that the function is to accomplish goes between braces.\n\nprint_names &lt;- function(some_names) {\n  print(some_names)\n}\n\nprint_names(c(\"rohan\", \"monica\"))\n\n[1] \"rohan\"  \"monica\"\n\n\nWe can specify defaults for the inputs in case the person using the function does not supply them.\n\nprint_names &lt;- function(some_names = c(\"edward\", \"hugo\")) {\n  print(some_names)\n}\n\nprint_names()\n\n[1] \"edward\" \"hugo\"  \n\n\nOne common scenario is that we want to apply a function multiple times. Like many programming languages, we can use a for() loop for this. The look of a for() loop in R is similar to function(), in that we define what we are iterating over in the round brackets, and the function to apply in braces.\n\n\n\n\n\n\n\n\n\n\n\nBecause R is a programming language that is focused on statistics, we are often interested in arrays or matrices. We use apply() to apply a function to rows (“MARGIN = 1”) or columns (“MARGIN = 2”).\n\nsimulated_data\n\n  favorite_color person std_normal_observations uniform_observations\n1           blue      1             -0.35980342            9.6219155\n2           blue      2             -0.04064753            7.2269016\n3           blue      3             -1.78216227            0.8252921\n4          white      4             -1.12242282            1.0379810\n5           blue      5             -1.00278400            3.0942004\n  poisson_observations binomial_observations\n1                   81                     2\n2                   91                     1\n3                   84                     1\n4                  100                     1\n5                   97                     1\n\napply(X = simulated_data, MARGIN = 2, FUN = unique)\n\n$favorite_color\n[1] \"blue\"  \"white\"\n\n$person\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n$std_normal_observations\n[1] \"-0.35980342\" \"-0.04064753\" \"-1.78216227\" \"-1.12242282\" \"-1.00278400\"\n\n$uniform_observations\n[1] \"9.6219155\" \"7.2269016\" \"0.8252921\" \"1.0379810\" \"3.0942004\"\n\n$poisson_observations\n[1] \" 81\" \" 91\" \" 84\" \"100\" \" 97\"\n\n$binomial_observations\n[1] \"2\" \"1\""
  },
  {
    "objectID": "20-r_essentials.html#making-graphs-with-ggplot2",
    "href": "20-r_essentials.html#making-graphs-with-ggplot2",
    "title": "Online Appendix A — R essentials",
    "section": "A.7 Making graphs with ggplot2",
    "text": "A.7 Making graphs with ggplot2\nIf the key package in the tidyverse in terms of manipulating data is dplyr (Wickham et al. 2022), then the key package in the tidyverse in terms of creating graphs is ggplot2 (Wickham 2016). We will have more to say about graphing in Chapter 5, but here we provide a quick tour of some essentials. ggplot2 works by defining layers which build to form a graph, based around the “grammar of graphics” (hence, the “gg”). Instead of the pipe operator (|&gt;) ggplot2 uses the add operator +. As part of the tidyverse collection of packages, ggplot2 does not need to be explicitly installed or loaded if the tidyverse has been loaded.\nThere are three key aspects that need to be specified to build a graph with ggplot2:\n\nData;\nAesthetics / mapping; and\nType.\n\nTo get started we will obtain some GDP data for countries in the Organisation for Economic Co-operation and Development (OECD) (OECD 2022).\n\nlibrary(tidyverse)\n\noecd_gdp &lt;-\n  read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.QGDP.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n\nwrite_csv(oecd_gdp, \"inputs/data/oecd_gdp.csv\")\n\n\n\n# A tibble: 6 × 8\n  LOCATION INDICATOR SUBJECT MEASURE  FREQUENCY TIME  Value `Flag Codes`\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 OECD     QGDP      TOT     PC_CHGPP A         1962   5.70 &lt;NA&gt;        \n2 OECD     QGDP      TOT     PC_CHGPP A         1963   5.20 &lt;NA&gt;        \n3 OECD     QGDP      TOT     PC_CHGPP A         1964   6.38 &lt;NA&gt;        \n4 OECD     QGDP      TOT     PC_CHGPP A         1965   5.35 &lt;NA&gt;        \n5 OECD     QGDP      TOT     PC_CHGPP A         1966   5.75 &lt;NA&gt;        \n6 OECD     QGDP      TOT     PC_CHGPP A         1967   3.96 &lt;NA&gt;        \n\n\nWe are interested, firstly, in making a bar chart of GDP change in the third quarter of 2021 for ten countries: Australia, Canada, Chile, Indonesia, Germany, Great Britain, New Zealand, South Africa, Spain, and the US.\n\noecd_gdp_2021_q3 &lt;-\n  oecd_gdp |&gt;\n  filter(\n    TIME == \"2021-Q3\",\n    SUBJECT == \"TOT\",\n    LOCATION %in% c(\n      \"AUS\",\n      \"CAN\",\n      \"CHL\",\n      \"DEU\",\n      \"GBR\",\n      \"IDN\",\n      \"ESP\",\n      \"NZL\",\n      \"USA\",\n      \"ZAF\"\n    ),\n    MEASURE == \"PC_CHGPY\"\n  ) |&gt;\n  mutate(\n    european = if_else(\n      LOCATION %in% c(\"DEU\", \"GBR\", \"ESP\"),\n      \"European\",\n      \"Not european\"\n    ),\n    hemisphere = if_else(\n      LOCATION %in% c(\"CAN\", \"DEU\", \"GBR\", \"ESP\", \"USA\"),\n      \"Northern Hemisphere\",\n      \"Southern Hemisphere\"\n    ),\n  )\n\nWe start with ggplot() and specify a mapping/aesthetic, which in this case means specifying the x-axis and the y-axis. The first argument in ggplot() is the data we want to visualize, so we can use the pipe operator at this stage as usual.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value))\n\n\n\n\nNow we need to specify the type of graph that we are interested in. In this case we want a bar chart and we do this by adding geom_bar() using +.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nWe can color the bars by whether the country is European by adding another aesthetic, “fill”.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\nFinally, we could make it look nicer by: adding labels, labs(); changing the color, scale_fill_brewer(); and the background, theme_classic().\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\",\n    x = \"Countries\",\n    y = \"Change (%)\",\n    fill = \"Is European?\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nFacets enable us to that we create subplots that focus on specific aspects of our data. They are invaluable because they allow us to add another variable to a graph without having to make a 3D graph. We use facet_wrap() to add facets and specify the variable that we would like to facet by. In this case, we facet by hemisphere.\n\noecd_gdp_2021_q3 |&gt;\n  ggplot(mapping = aes(x = LOCATION, y = Value, fill = european)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Quarterly change in GDP for ten OECD countries in 2021Q3\",\n    x = \"Countries\",\n    y = \"Change (%)\",\n    fill = \"Is European?\"\n  ) +\n  theme_classic() +\n  scale_fill_brewer(palette = \"Set1\") +\n  facet_wrap(\n    ~hemisphere,\n    scales = \"free_x\"\n  )"
  },
  {
    "objectID": "20-r_essentials.html#exploring-the-tidyverse",
    "href": "20-r_essentials.html#exploring-the-tidyverse",
    "title": "Online Appendix A — R essentials",
    "section": "A.8 Exploring the tidyverse",
    "text": "A.8 Exploring the tidyverse\nWe have focused on two aspects of the tidyverse: dplyr, and ggplot2. However, the tidyverse comprises a variety of different packages and functions. We will now go through four common aspects:\n\nImporting data and tibble();\nJoining and pivoting datasets;\nString manipulation and stringr;\nFactor variables and forcats.\n\nHowever, the first task is to deal with the nomenclature, and in particular to be specific about what is “tidy” about the “tidyverse”. The name refers to tidy data, and the benefit of that is that while there are a variety of ways for data to be messy, tidy data satisfy three rules. This means the structure of the datasets is consistent regardless of the specifics, and makes it easier to apply functions that expect certain types of input. Tidy data refers to a dataset where (Wickham, Çetinkaya-Rundel, and Grolemund [2016] 2023; Wickham 2014, 4):\n\nEvery variable is in a column of its own.\nEvery observation is in its own row.\nEvery value is in its own cell.\n\nTable A.1 is not tidy because age and hair share a column. Table A.2 is its tidy counterpart.\n\n\n\n\nTable A.1: Example of data that are not tidy\n\n\nPerson\nVariable\nValue\n\n\n\n\nRohan\nAge\n35\n\n\nRohan\nHair\nBlack\n\n\nMonica\nAge\n35\n\n\nMonica\nHair\nBlonde\n\n\nEdward\nAge\n3\n\n\nEdward\nHair\nBrown\n\n\nHugo\nAge\n1\n\n\nHugo\nHair\nBlonde\n\n\n\n\n\n\n\n\n\n\nTable A.2: Example of tidy data\n\n\nPerson\nAge\nHair\n\n\n\n\nRohan\n35\nBlack\n\n\nMonica\n35\nBlonde\n\n\nEdward\n3\nBrown\n\n\nHugo\n1\nBlonde\n\n\n\n\n\n\n\nA.8.1 Importing data and tibble()\nThere are a variety of ways to get data into R so that we can use it. For CSV files, there is read_csv() from readr (Wickham, Hester, and Bryan 2022), and for Stata files, there is read_dta() from haven (Wickham, Miller, and Smith 2023).\nCSVs are a common format and have many advantages including the fact that they typically do not modify the data. Each column is separated by a comma, and each row is a record. We can provide read_csv() with a URL or a local file to read. There are a variety of different options that can be passed to read_csv() including the ability to specify whether the dataset has column names, the types of the columns, and how many lines to skip. If we do not specify the types of the columns then read_csv() will make a guess by looking at the dataset.\nWe use read_dta() to read .dta files, which are commonly produced by the statistical program Stata. This means that they are common in fields such as sociology, political science, and economics. This format separates the data from its labels and so we typically reunite these using to_factor() from labelled (Larmarange 2023). haven is part of the tidyverse, but is not automatically loaded by default, in contrast to a package such as ggplot2, and so we would need to run library(haven).\nTypically a dataset enters R as a “data.frame”. While this can be useful, another helpful class for a dataset is “tibble”. These can be created using tibble() from tibble, which is part of the tidyverse. A tibble is a data frame, with some particular changes that make it easier to work with, including not converting strings to factors by default, showing the class of columns, and printing nicely.\nWe can make a tibble manually, if need be, for instance, when we simulate data. But we typically import data directly as a tibble, for instance, when we use read_csv().\n\npeople_as_dataframe &lt;-\n  data.frame(\n    names = c(\"rohan\", \"monica\"),\n    website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n    fav_color = c(\"blue\", \"white\")\n  )\nclass(people_as_dataframe)\n\n[1] \"data.frame\"\n\npeople_as_dataframe\n\n   names             website fav_color\n1  rohan  rohanalexander.com      blue\n2 monica monicaalexander.com     white\n\npeople_as_tibble &lt;-\n  tibble(\n    names = c(\"rohan\", \"monica\"),\n    website = c(\"rohanalexander.com\", \"monicaalexander.com\"),\n    fav_color = c(\"blue\", \"white\")\n  )\npeople_as_tibble\n\n# A tibble: 2 × 3\n  names  website             fav_color\n  &lt;chr&gt;  &lt;chr&gt;               &lt;chr&gt;    \n1 rohan  rohanalexander.com  blue     \n2 monica monicaalexander.com white    \n\nclass(people_as_tibble)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nA.8.2 Dataset manipulation with joins and pivots\nThere are two dataset manipulations that are often needed: joins and pivots.\nWe often have a situation where we have two, or more, datasets and we are interested in combining them. We can join datasets together in a variety of ways. A common way is to use left_join() from dplyr (Wickham et al. 2022). This is most useful where there is one main dataset that we are using and there is another dataset with some useful variables that we want to add to that. The critical aspect is that we have a column or columns that we can use to link the two datasets. Here we will create two tibbles and then join them on the basis of names.\n\nmain_dataset &lt;-\n  tibble(\n    names = c(\"rohan\", \"monica\", \"edward\", \"hugo\"),\n    status = c(\"adult\", \"adult\", \"child\", \"infant\")\n  )\nmain_dataset\n\n# A tibble: 4 × 2\n  names  status\n  &lt;chr&gt;  &lt;chr&gt; \n1 rohan  adult \n2 monica adult \n3 edward child \n4 hugo   infant\n\nsupplementary_dataset &lt;-\n  tibble(\n    names = c(\"rohan\", \"monica\", \"edward\", \"hugo\"),\n    favorite_food = c(\"pasta\", \"salmon\", \"pizza\", \"milk\")\n  )\nsupplementary_dataset\n\n# A tibble: 4 × 2\n  names  favorite_food\n  &lt;chr&gt;  &lt;chr&gt;        \n1 rohan  pasta        \n2 monica salmon       \n3 edward pizza        \n4 hugo   milk         \n\nmain_dataset &lt;-\n  main_dataset |&gt;\n  left_join(supplementary_dataset, by = \"names\")\n\nmain_dataset\n\n# A tibble: 4 × 3\n  names  status favorite_food\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        \n1 rohan  adult  pasta        \n2 monica adult  salmon       \n3 edward child  pizza        \n4 hugo   infant milk         \n\n\nThere are a variety of other options to join datasets, including inner_join(), right_join(), and full_join().\nAnother common dataset manipulation task is pivoting them. Datasets tend to be either long or wide. Long data means that each variable is on a row, and so possibly there is repetition, whereas with wide data each variable is a column and so there is typically little repetition (Wickham 2009). For instance, “anscombe” is wide, and “anscombe_long” is long.\n\nanscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\n\nanscombe_long\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n\nGenerally, in the tidyverse, and certainly for ggplot2, we need long data. To go from one to the other we use pivot_longer() and pivot_wider() from tidyr (Wickham, Vaughan, and Girlich 2023).\nWe will create some wide data on whether “mark” or “lauren” won a running race in each of three years.\n\npivot_example_data &lt;-\n  tibble(\n    year = c(2019, 2020, 2021),\n    mark = c(\"first\", \"second\", \"first\"),\n    lauren = c(\"second\", \"first\", \"second\")\n  )\n\npivot_example_data\n\n# A tibble: 3 × 3\n   year mark   lauren\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n1  2019 first  second\n2  2020 second first \n3  2021 first  second\n\n\nThis dataset is in wide format at the moment. To get it into long format, we need a column that specifies the person, and another that specifies the result. We use pivot_longer() to achieve this.\n\ndata_pivoted_longer &lt;-\n  pivot_example_data |&gt;\n  pivot_longer(\n    cols = c(\"mark\", \"lauren\"),\n    names_to = \"person\",\n    values_to = \"position\"\n  )\n\nhead(data_pivoted_longer)\n\n# A tibble: 6 × 3\n   year person position\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;   \n1  2019 mark   first   \n2  2019 lauren second  \n3  2020 mark   second  \n4  2020 lauren first   \n5  2021 mark   first   \n6  2021 lauren second  \n\n\nOccasionally, we need to go from long data to wide data. We use pivot_wider() to do this.\n\ndata_pivoted_wider &lt;-\n  data_pivoted_longer |&gt;\n  pivot_wider(\n    names_from = \"person\",\n    values_from = \"position\"\n  )\n\nhead(data_pivoted_wider)\n\n# A tibble: 3 × 3\n   year mark   lauren\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n1  2019 first  second\n2  2020 second first \n3  2021 first  second\n\n\n\n\nA.8.3 String manipulation and stringr\nIn R we often create a string with double quotes, although using single quotes works too. For instance c(\"a\", \"b\") consists of two strings “a” and “b”, that are contained in a character vector. There are a variety of ways to manipulate strings in R and we focus on stringr (Wickham 2022). This is automatically loaded when we load the tidyverse.\nIf we want to look for whether a string contains certain content, then we can use str_detect(). And if we want to remove or change some particular content then we can use str_remove() or str_replace().\n\ndataset_of_strings &lt;-\n  tibble(\n    names = c(\n      \"rohan alexander\",\n      \"monica alexander\",\n      \"edward alexander\",\n      \"hugo alexander\"\n    )\n  )\n\ndataset_of_strings |&gt;\n  mutate(\n    is_rohan = str_detect(names, \"rohan\"),\n    make_howlett = str_replace(names, \"alexander\", \"howlett\"),\n    remove_rohan = str_remove(names, \"rohan\")\n  )\n\n# A tibble: 4 × 4\n  names            is_rohan make_howlett   remove_rohan      \n  &lt;chr&gt;            &lt;lgl&gt;    &lt;chr&gt;          &lt;chr&gt;             \n1 rohan alexander  TRUE     rohan howlett  \" alexander\"      \n2 monica alexander FALSE    monica howlett \"monica alexander\"\n3 edward alexander FALSE    edward howlett \"edward alexander\"\n4 hugo alexander   FALSE    hugo howlett   \"hugo alexander\"  \n\n\nThere are a variety of other functions that are often especially useful in data cleaning. For instance, we can use str_length() to find out how long a string is, and str_c() to bring strings together.\n\ndataset_of_strings |&gt;\n  mutate(\n    length_is = str_length(string = names),\n    name_and_length = str_c(names, length_is, sep = \" - \")\n  )\n\n# A tibble: 4 × 3\n  names            length_is name_and_length      \n  &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;                \n1 rohan alexander         15 rohan alexander - 15 \n2 monica alexander        16 monica alexander - 16\n3 edward alexander        16 edward alexander - 16\n4 hugo alexander          14 hugo alexander - 14  \n\n\nFinally, separate() from tidyr, although not part of stringr, is indispensable for string manipulation. It turns one character column into many.\n\ndataset_of_strings |&gt;\n  separate(\n    col = names,\n    into = c(\"first\", \"last\"),\n    sep = \" \",\n    remove = FALSE\n  )\n\n# A tibble: 4 × 3\n  names            first  last     \n  &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;    \n1 rohan alexander  rohan  alexander\n2 monica alexander monica alexander\n3 edward alexander edward alexander\n4 hugo alexander   hugo   alexander\n\n\n\n\nA.8.4 Factor variables and forcats\nA factor is a collection of strings that are categories. Sometimes there will be an inherent ordering. For instance, the days of the week have an order – Monday, Tuesday, Wednesday, … – which is not alphabetical. But there is no requirement for that to be the case, for instance pregnancy status: pregnant or not pregnant. Factors feature prominently in base R. They can be useful because they ensure that only appropriate strings are allowed. For instance, if “days_of_the_week” was a factor variable then “January” would not be allowed. But they can add a great deal of complication, and so they have a less prominent role in the tidyverse. Nonetheless taking advantage of factors is useful in certain circumstances. For instance, when plotting the days of the week we probably want them in the usual ordering than in the alphabetical ordering that would result if we had them as a character variable. While factors are built into base R, one tidyverse package that is especially useful when using factors is forcats (Wickham 2023).\nSometimes we have a character vector, and we will want it ordered in a particular way. The default is that a character vector is ordered alphabetically, but we may not want that. For instance, the days of the week would look strange on a graph if they were alphabetically ordered: Friday, Monday, Saturday, Sunday, Thursday, Tuesday, and Wednesday!\nThe way to change the ordering is to change the variable from a character to a factor. We can use fct_relevel() from forcats (Wickham 2023) to specify an ordering.\n\nset.seed(853)\n\ndays_data &lt;-\n  tibble(\n    days =\n      c(\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n      ),\n    some_value = c(sample.int(100, 7))\n  )\n\ndays_data &lt;-\n  days_data |&gt;\n  mutate(\n    days_as_factor = factor(days),\n    days_as_factor = fct_relevel(\n      days,\n      \"Monday\",\n      \"Tuesday\",\n      \"Wednesday\",\n      \"Thursday\",\n      \"Friday\",\n      \"Saturday\",\n      \"Sunday\"\n    )\n  )\n\nAnd we can compare the results by graphing first with the original character vector on the x-axis, and then another graph with the factor vector on the x-axis.\n\ndays_data |&gt;\n  ggplot(aes(x = days, y = some_value)) +\n  geom_point()\n\ndays_data |&gt;\n  ggplot(aes(x = days_as_factor, y = some_value)) +\n  geom_point()"
  },
  {
    "objectID": "20-r_essentials.html#exercises",
    "href": "20-r_essentials.html#exercises",
    "title": "Online Appendix A — R essentials",
    "section": "A.9 Exercises",
    "text": "A.9 Exercises\n\nScales\n\n(Plan) Consider the following scenario: A person is interested in whether kings or queens live longer, and for every monarch they gather data about how long they lived. Please sketch what that dataset could look like, and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described, assume there are 1,000 monarchs, and decide which of the following could be used to simulate the situation (select all that apply)?\n\nrunif(n = 1000, min = 1, max = 110) |&gt; floor()\nrpois(n = 1000, lambda = 65)\nrnorm(n = 1000) |&gt; floor()\nsample(x = sunspot.month, size = 1000, replace = TRUE)) |&gt; floor()\n\n(Acquire) Please identify one possible source of actual data about how long monarchs lived for.\n(Explore) Assume that tidyverse is loaded and the dataset “monarchs” has the column “years”. Which of the following would result only those monarchs that lived longer than 35 years (pick one)?\n\nmonarchs |&gt; arrange(years &gt; 35)\nmonarchs |&gt; select(years &gt; 35)\nmonarchs |&gt; filter(years &gt; 35)\nmonarchs |&gt; mutate(years &gt; 35)\n\n(Communicate) Please write two paragraphs as if you had gathered data from that source, and had built a graph. The exact details contained in the paragraphs do not have to be factual (i.e. you do not actually have to get the data nor create the graphs).\n\n\n\nQuestions\n\nWhat is R (pick one)?\n\nA open-source statistical programming language\nA programming language created by Guido van Rossum\nA closed source statistical programming language\nAn integrated development environment (IDE)\n\nWhat are three advantages of R? What are three disadvantages?\nWhat is RStudio?\n\nAn integrated development environment (IDE).\nA closed source paid program.\nA programming language created by Guido van Rossum\nA statistical programming language.\n\nWhat is the class of the output of 2 + 2 (pick one)?\n\ncharacter\nfactor\nnumeric\ndate\n\nSay we had run: my_name &lt;- \"rohan\". What would be the result of running print(my_name) (pick one)?\n\n“edward”\n“monica”\n“hugo”\n“rohan”\n\nSay we had a dataset with two columns: “name”, and “age”. Which verb should we use to pick just “name” (pick one)?\n\nselect()\nmutate()\nfilter()\nrename()\n\nSay we had loaded AustralianPoliticians and tidyverse and then run the following code: australian_politicians &lt;- get_auspol(\"all\"). How could we select all of the columns that end with “Name” (pick one)?\n\naustralian_politicians |&gt; select(contains(\"Name\"))\naustralian_politicians |&gt; select(starts_with(\"Name\"))\naustralian_politicians |&gt; select(matches(\"Name\"))\naustralian_politicians |&gt; select(ends_with(\"Name\"))\n\nUnder what circumstances, in terms of the names of the columns, would the use of contains() potentially give different answers to using ends_with() in the above question?\nWhich of the following are not tidyverse verbs (pick one)?\n\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\n\nWhich function would make a new column (pick one)?\n\nselect()\nfilter()\narrange()\nmutate()\nvisualize()\n\nWhich function would focus on particular rows (pick one)?\n\nselect()\nfilter()\narrange()\nmutate()\nsummarise()\n\nWhich combination of two could provide a mean of a dataset, by sex (pick two)?\n\nsummarise()\nfilter()\narrange()\nmutate()\n.by\n\nAssume a variable called “age” is an integer. Which line of code would create a column that is its exponential (pick one)?\n\ngenerate(exp_age = exp(age))\nchange(exp_age = exp(age))\nmake(exp_age = exp(age))\nmutate(exp_age = exp(age))\n\nAssume a column called “age”. Which line of code could create a column that contains the value from five rows above?\n\nmutate(five_before = lag(age))\nmutate(five_before = lead(age))\nmutate(five_before = lag(age, n = 5))\nmutate(five_before = lead(age, n = 5))\n\nWhat would be the output of class(\"edward\") (pick one)?\n\n“numeric”\n“character”\n“data.frame”\n“vector”\n\nWhich function would enable us to draw once from three options “blue, white, red”, with 10 per cent probability on “blue” and “white”, and the remainder on “red”?\n\nsample(c(\"blue\", \"white\", \"red\"), prob = c(0.1, 0.1, 0.8))\nsample(c(\"blue\", \"white\", \"red\"), size = 1)\nsample(c(\"blue\", \"white\", \"red\"), size = 1, prob = c(0.8, 0.1, 0.1))\nsample(c(\"blue\", \"white\", \"red\"), size = 1, prob = c(0.1, 0.1, 0.8))\n\nWhich code simulates 10,000 draws from a normal distribution with a mean of 27 and a standard deviation of 3 (pick one)?\n\nrnorm(10000, mean = 27, sd = 3)\nrnorm(27, mean = 10000, sd = 3)\nrnorm(3, mean = 10000, sd = 27)\nrnorm(27, mean = 3, sd = 1000)\n\nWhat are the three key aspects of the grammar of graphics (select all)?\n\ndata\naesthetics\ntype\ngeom_histogram()\n\n\n\n\nTutorial\n\nI think we should be suspicious when we find ourselves attracted to data—very, very thin and weak data—that seem to justify beliefs that have held great currency in lots of societies throughout history, in a way that is conducive to the oppression of large segments of the population.\nAmia Srinivasan (Cowen 2021)\n\nReflect on the quote from Amia Srinivasan, Chichele Professor of Social and Political Theory, All Souls College, Oxford, and D’Ignazio and Klein (2020), especially Chapter 6, and spend at least two pages discussing them in relation to a dataset that you are familiar with.\n\n\n\n\nAlexander, Rohan, and Paul Hodgetts. 2021. AustralianPoliticians: Provides Datasets About Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAmerican Medical Association and New York Academy of Medicine. 1848. Code of Medical Ethics. Academy of Medicine. https://hdl.handle.net/2027/chi.57108026.\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. magrittr: A Forward-Pipe Operator for R. https://CRAN.R-project.org/package=magrittr.\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review 94 (4): 991–1013. https://doi.org/10.1257/0002828042002561.\n\n\nBrokowski, Carolyn, and Mazhar Adli. 2019. “CRISPR Ethics: Moral Considerations for Applications of a Powerful Tool.” Journal of Molecular Biology 431 (1): 88–101. https://doi.org/10.1016/j.jmb.2018.05.044.\n\n\nBuja, Andreas, Dianne Cook, and Deborah Swayne. 1996. “Interactive High-Dimensional Data Visualization.” Journal of Computational and Graphical Statistics 5 (1): 78–99. https://doi.org/10.2307/1390754.\n\n\nCowen, Tyler. 2021. “Episode 132: Amia Srinivasan on Utopian Feminism.” Conversations with Tyler, September. https://conversationswithtyler.com/episodes/amia-srinivasan/.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism. Massachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nHangartner, Dominik, Daniel Kopp, and Michael Siegenthaler. 2021. “Monitoring Hiring Discrimination Through Online Recruitment Platforms.” Nature 589 (7843): 572–76. https://doi.org/10.1038/s41586-020-03136-0.\n\n\nHealy, Kieran. 2020. “The Kitchen Counter Observatory,” May. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real Life. https://reallifemag.com/counting-the-countless/.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics with R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nLarmarange, Joseph. 2023. labelled: Manipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nMarchese, David. 2022. “Her Discovery Changed the World. How Does She Think We Should Use It?” The New York Times, August. https://www.nytimes.com/interactive/2022/08/15/magazine/jennifer-doudna-crispr-interview.html.\n\n\nMüller, Kirill, Tobias Schieferdecker, and Patrick Schratz. 2019. Visualization, Transformation and Reporting with the Tidyverse. https://krlmlr.github.io/vistransrep/.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nNeufeld, Michael. 2002. “Wernher von Braun, the SS, and Concentration Camp Labor: Questions of Moral, Political, and Criminal Responsibility.” German Studies Review 25 (1): 57–78. https://doi.org/10.2307/1433245.\n\n\nOECD. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThieme, Nick. 2018. “R Generation.” Significance 15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nWickham, Hadley. 2009. “Manipulating Data.” In ggplot2, 157–75. Springer New York. https://doi.org/10.1007/978-0-387-98141-3_9.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC. https://adv-r.hadley.nz.\n\n\n———. 2020. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2022. stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023. forcats: Tools for Working with Categorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS” “Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWilford, John Noble. 1977. “Wernher von Braun, Rocket Pioneer, Dies.” The New York Times, June. https://www.nytimes.com/1977/06/18/archives/wernher-von-braun-rocket-pioneer-dies-wernher-von-braun-pioneer-in.html."
  },
  {
    "objectID": "21-datasets.html",
    "href": "21-datasets.html",
    "title": "Online Appendix B — Datasets",
    "section": "",
    "text": "One thing students often struggle with is picking a dataset. In general, it is better to stay away from datasets on Kaggle, the UCI Machine Learning Repository, and other commonly used options. From a data science perspective, using a dataset as it is available from such a source means that almost all the important decisions have been already made, and are potentially undocumented. And from a career perspective, it does not set your portfolio apart because everyone else just uses these datasets. Some alternatives include:\n\nAidData provides a large number of datasets related to research on development and foreign aid.\nAlex Cookson’s datasets.\nAndrews and Herzberg (2012) provide a variety of datasets, which are available here.\nAPIs for social scientists provides a variety of APIs that could be used to gather data.\nBombieri et al. (2023) provide a dataset about more than 5,000 large carnivore attacks on humans.\nThe British Library’s catalogue of world newspapers contains information about the start and end years of publication, the places of publication, variant titles and editions, and the language of publication.\nBuzzFeed News provides access to many datasets underpinning their articles.\nThe Canadian Municipal Elections Database contains complete municipal election results for municipalities across Canada (Lucas et al. 2020).\nCongressindata provides datasets about US Congress Members from 2005 to 2015.\nThe Congress.gov API is an especially useful source of data about the US Congress especially bills and other text data.\nCOVerAGE-DB is a global demographic database of COVID-19 cases and deaths (Riffe et al. 2021).\ncricketdata (Hyndman et al. 2022) provides functions for downloading data about international and other major cricket matches\nThe Data And Story Library provides access to hundreds of datasets.\nData Is Plural provides a weekly newsletter of interesting datasets with archives back to 2015.\nThe Data Liberation Project focuses on using FOI requests to build US government datasets.\nThe Demographic and Health Surveys (DHS) Program provides survey data for 90 countries beginning in 1984.\nDuolingo provides access to datasets that underpin its research papers.\nThe Economist provides access to many datasets underpinning their articles.\nEH.net provides a variety of interesting historical economic datasets.\nEuropean NUTS-Level Election Database (EU-NED) provides national and European parliamentary election results from 1990 to 2020.\nFederal Reserve Economic Data (FRED) provides US economic data, and there is an R package fredr (Boysel and Vaughan 2021) for accessing the API.\nFiveThirtyEight provides access to many datasets underpinning their articles.\nGoodreads Datasets are a scrape from 2017 of public data about more than two million books including meta-data and reviews (Wan and McAuley 2018; Wan et al. 2019).\nHistorical Social Conflict Database provide data about more than 20,000 conflicts, largely focused on Europe (Chambru and Maneuvrier-Hervieu 2022).\nHistorical Statistics provides links to historical statistics.\nHuman Mortality Database provides detailed mortality and population data for a variety of countries.\nICANN’s Centralized Zone Data Service provides access to all domain names, after an application and approval process that can take a few days.\nIPCC Data Distribution Centre.\nThe Irish Social Science Data Archive has a wide variety of datasets available.\nJ-PAL (Abdul Latif Jameel Poverty Action Lab) maintains a catalog of administrative data.\nNFL Savant provides team-specific data about the NFL, including play-by-play data since 2013, combine data since 1999, and weather data.\nThe Markup’s Show Your Work series often include links to GitHub repos with the data that underpin the article. A few notable ones include: The Secret Bias Hidden in Mortgage-Approval Algorithms.\nThe Massachusetts Water Resources Authority makes its Wastewater COVID-19 Tracking data available here, with the raw data available in a PDF that could be parsed.\nThe Museum of Modern Art (MoMA) makes datasets about their collection and exhibitions available.\nNASA’s Planetary Data System.\nProPublica Data Store provides an extensive number of datasets about the US, some of which are quite large. For instance, the Open Payments Data (2016) is 6 GB.\nThe Notable People dataset of Laouenan et al. (2022) provides a cross-verified database of notable people from 3500BC to 2018AD.\nThe OECD provides economic data.\nThe ParlEE dataset contains annotated full-text of millions of speeches in the EU legislative chambers (Sylvester et al. 2023).\nThe Prison Policy Initiative provides many datasets about US prisons and jails.\nThe Pudding makes many of the datasets underpinning their articles available. A few notable ones include: The Naked Truth, and The Evolution of the American Census.\nThe Pushshift Reddit Dataset is a collection of Reddit posts since 2015 (Baumgartner et al. 2020).\nThe Refugee Law Lab provides the full text of full text of Supreme Court of Canada decisions in JSON format (Rehaag 2023).\nThe Rijksmuseum provides a variety of data about their collections.\nThe Socioeconomic High-resolution Rural-Urban Geographic Platform (SHRUG) is an open data platform provides data about socioeconomic development across 600,000 villages and towns in India (Asher et al. 2021).\nTom Cardoso’s Bias behind bars provides data about Black and Indigenous inmates in Canada.\nTracking (In)Justice is a dataset that tracks police-involved deaths in Canada (Data and Justice Criminology Lab, Institute of Criminology and Criminal Justice, Carleton University; The Centre for Research & Innovation for Black Survivors of Homicide Victims (The CRIB), at the Factor-Inwentash Faculty of Social Work, University of Toronto; Canadian Civil Liberties Association; Ethics and Technology Lab, Queen’s University 2022).\nThe US Centers for Disease Control and Prevention (CDC) National Vital Statistics System provides a variety of datasets, including Linked Birth and Infant Death Data.\nThe United States Sentencing Commission Individual Offender Data Sets as cleaned and prepared by Kevin Wilson.\nWomen’s Activities in Armed Rebellion provides access to measures of women’s participation in rebel organizations between 1946-2015 (Loken and Matfess 2023).\nThe Washington Post provides access to many datasets underpinning their articles. Especially of interest may be congress slaveowners, fatal force shooting, school shootings, and Why FEMA is denying aid to Black disaster survivors in the Deep South.\nThe Wordbank database is an open database of children’s vocabulary growth. Access is additionally available using wordbankr (Braginsky 2020), and Alison Presmanes Hill provides useful background and cleaning code.\nThe World Bank provides an extensive range of global development data and a Microdata Library.\nYale’s International Center for Finance datasets: Historical Financial Research Data, and Stock Market Confidence Indices.\n\n\n\n\n\nAndrews, David, and Agnes Herzberg. 2012. Data: A Collection of Problems from Many Fields for the Student and Research Worker. New York: Springer Science & Business Media.\n\n\nAsher, Sam, Tobias Lunt, Ryu Matsuura, and Paul Novosad. 2021. “Development Research at High Geographic Resolution: An Analysis of Night Lights, Firms, and Poverty in India Using the SHRUG Open Data Platform.” World Bank Economic Review 35 (4). https://shrug-assets-ddl.s3.amazonaws.com/static/main/assets/other/almn-shrug.pdf.\n\n\nBaumgartner, Jason, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. “The Pushshift Reddit Dataset.” arXiv. https://doi.org/10.48550/arxiv.2001.08435.\n\n\nBombieri, Giulia, Vincenzo Penteriani, Kamran Almasieh, Hüseyin Ambarlı, Mohammad Reza Ashrafzadeh, Chandan Surabhi Das, Nishith Dharaiya, et al. 2023. “A Worldwide Perspective on Large Carnivore Attacks on Humans.” PLOS Biology 21 (1): e3001946. https://doi.org/10.1371/journal.pbio.3001946.\n\n\nBoysel, Sam, and Davis Vaughan. 2021. fredr: An R Client for the “FRED” API. https://CRAN.R-project.org/package=fredr.\n\n\nBraginsky, Mika. 2020. wordbankr: Accessing the Wordbank Database. https://CRAN.R-project.org/package=wordbankr.\n\n\nChambru, Cédric, and Paul Maneuvrier-Hervieu. 2022. “Introducing HiSCoD: A new gateway for the study of historical social conflict.” Working Paper Series, Department of Economics, University of Zurich. https://doi.org/10.5167/uzh-217109.\n\n\nData and Justice Criminology Lab, Institute of Criminology and Criminal Justice, Carleton University; The Centre for Research & Innovation for Black Survivors of Homicide Victims (The CRIB), at the Factor-Inwentash Faculty of Social Work, University of Toronto; Canadian Civil Liberties Association; Ethics and Technology Lab, Queen’s University. 2022. “Tracking (in)justice: A Living Data Set Tracking Canadian Police-Involved Deaths.” https://trackinginjustice.ca.\n\n\nHyndman, Rob, Timothy Hyndman, Charles Gray, Sayani Gupta, and Jacquie Tran. 2022. cricketdata: International Cricket Data. https://CRAN.R-project.org/package=cricketdata.\n\n\nLaouenan, Morgane, Palaash Bhargava, Jean-Benoı̂t Eyméoud, Olivier Gergaud, Guillaume Plique, and Etienne Wasmer. 2022. “A Cross-Verified Database of Notable People, 3500BC–2018AD.” Scientific Data 9 (290). https://doi.org/10.1038/s41597-022-01369-4.\n\n\nLoken, Meredith, and Hilary Matfess. 2023. “Introducing the Women’s Activities in Armed Rebellion (WAAR) Project, 1946-2015.” Journal of Peace Research.\n\n\nLucas, Jack, Reed Merrill, Kelly Blidook, Sandra Breux, Laura Conrad, Gabriel Eidelman, Royce Koop, et al. 2020. “Canadian Municipal Elections Database.” Scholars Portal Dataverse. https://doi.org/10.5683/sp2/4mzjpq.\n\n\nRehaag, Sean. 2023. “Supreme Court of Canada Bulk Decisions Dataset.” Refugee Law Laboratory. https://refugeelab.ca/bulk-data/scc.\n\n\nRiffe, Tim, Enrique Acosta, Enrique José Acosta, Diego Manuel Aburto, Anna Alburez-Gutierrez, Ainhoa Altová, Ugofilippo Alustiza, et al. 2021. “Data Resource Profile: COVerAGE-DB: A Global Demographic Database of COVID-19 Cases and Deaths.” International Journal of Epidemiology 50 (2): 390–390f. https://doi.org/10.1093/ije/dyab027.\n\n\nSylvester, Christine, Anastasia Ershova, Aleksandra Khokhlova, Nikoleta Yordanova, and Zachary Greene. 2023. “ParlEE plenary speeches V2 data set: Annotated full-text of 15.1 million sentence-level plenary speeches of six EU legislative chambers.” Harvard Dataverse. https://doi.org/10.7910/DVN/VOPK0E.\n\n\nWan, Mengting, and Julian J. McAuley. 2018. “Item Recommendation on Monotonic Behavior Chains.” In Proceedings of the 12th ACM Conference on Recommender Systems, RecSys 2018, Vancouver, BC, Canada, October 2-7, 2018, edited by Sole Pera, Michael D. Ekstrand, Xavier Amatriain, and John O’Donovan, 86–94. ACM. https://doi.org/10.1145/3240323.3240369.\n\n\nWan, Mengting, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley. 2019. “Fine-Grained Spoiler Detection from Large-Scale Review Corpora.” In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, edited by Anna Korhonen, David R. Traum, and Lluı́s Màrquez, 2605–10. Association for Computational Linguistics. https://doi.org/10.18653/v1/p19-1248."
  },
  {
    "objectID": "22-rmarkdown.html#r-chunks",
    "href": "22-rmarkdown.html#r-chunks",
    "title": "Online Appendix C — R Markdown",
    "section": "C.1 R chunks",
    "text": "C.1 R chunks\nWe can include code for R and many other languages in code chunks within an R Markdown document. Then when we knit the document, the code will run and be included in the document.\nTo create an R chunk, we start with three backticks and then within curly braces we tell R Markdown that this is an R chunk. Anything inside this chunk will be considered R code and run as such. For instance, we could load the tidyverse and AER and make a graph of the number of times a survey respondent visited the doctor in the past two weeks.\n\n\n```{r}\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n```\n\n\nThe output of that code is Figure @ref(fig:doctervisits).\n\n\n\n\n\nNumber of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\nNotice that in contrast to Quarto, all the options go between curly braces in the top line. It is not possible to use the comment notation of Quarto."
  },
  {
    "objectID": "22-rmarkdown.html#cross-references",
    "href": "22-rmarkdown.html#cross-references",
    "title": "Online Appendix C — R Markdown",
    "section": "C.2 Cross-references",
    "text": "C.2 Cross-references\nIt can be useful to cross-reference figures, tables, and equations. This makes it easier to refer to them in the text. To do this for a figure we refer to the name of the R chunk that creates or contains the figure. For instance, (Figure \\@ref(fig:uniquename)) will produce: (Figure @ref(fig:uniquename)) as the name of the R chunk is uniquename. We also need to add ‘fig’ in front of the chunk name so that R Markdown knows that this is a figure. We then include a ‘fig.cap’ in the R chunk that specifies a caption.\n```{r uniquename, fig.cap = \"Number of illnesses in the past two weeks, based on the 1977--1978 Australian Health Survey\", echo = TRUE}\n\nlibrary(tidyverse)\nlibrary(AER)\n\ndata(\"DoctorVisits\", package = \"AER\")\n\nDoctorVisits |&gt;\n  ggplot(aes(x = illness)) +\n  geom_histogram(stat = \"count\")\n\n\n\n\nNumber of illnesses in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\n\n\nWe can take a similar, but slightly different, approach to cross-reference tables. For instance, (Table \\@ref(tab:docvisittablermarkdown)) will produce: (Table C.1). In this case we specify ‘tab’ before the unique reference to the table, so that R Markdown knows that it is a table. For tables we need to include the caption in the main content, as a ‘caption’, rather than in a ‘fig.cap’ chunk option as is the case for figures.\n\n\n\n\nTable C.1: Number of visits to the doctor in the past two weeks, based on the 1977–1978 Australian Health Survey\n\n\nvisits\nn\n\n\n\n\n0\n4141\n\n\n1\n782\n\n\n2\n174\n\n\n3\n30\n\n\n4\n24\n\n\n5\n9\n\n\n6\n12\n\n\n7\n12\n\n\n8\n5\n\n\n9\n1\n\n\n\n\n\n\nFinally, we can also cross-reference equations. To that we need to add a tag (\\#eq:macroidentity) which we then reference. For instance, use Equation \\@ref(eq:macroidentity). to produce Equation @ref(eq:macroidentity).\n\\begin{equation}\nY = C + I + G + (X - M) (\\#eq:macroidentity)\n\\end{equation}\n\\[\\begin{equation}\nY = C + I + G + (X - M) (\\#eq:macroidentity)\n\\end{equation}\\]\nWhen using cross-references ensure that the R chunks have simple labels. In general, try to keep the names simple but unique, and if possible, avoid punctuation and stick to letters. Do not use underbars in the labels because that will cause an error."
  },
  {
    "objectID": "23-assessment.html#sec-paper-one",
    "href": "23-assessment.html#sec-paper-one",
    "title": "Online Appendix D — Papers",
    "section": "D.1 Donaldson Paper",
    "text": "D.1 Donaldson Paper\n\nD.1.1 Task\n\nWorking individually and in an entirely reproducible way, please find a dataset of interest on Open Data Toronto and write a short paper telling a story about the data.\n\nCreate a well-organized folder with appropriate sub-folders, and add it to GitHub. You are welcome to use this starter folder.\nFind a dataset of interest on Open Data Toronto.\n\nPut together an R script, “scripts/00-simulation.R”, that simulates the dataset of interest. Push to GitHub and include an informative commit message\nWrite an R script, “scripts/00-download_data.R” to download the actual data in a reproducible way using opendatatoronto (Gelfand 2022). Save the data: “inputs/data/unedited_data.csv” (or whatever file type the file is). Push to GitHub and include an informative commit message\n\nPrepare a PDF using Quarto “outputs/paper/paper.qmd” with these sections: title, author, date, abstract, introduction, data, and references.\n\nThe title should be descriptive, informative, and specific.\nThe date should be in an unambiguous format. Add a link to the GitHub repo in the acknowledgments.\nThe abstract should be three or four sentences. The abstract must tell the reader the top-level finding. What is the one thing that we learn about the world because of this paper?\nThe introduction should be two or three paragraphs of content. And there should be an additional final paragraph that sets out the remainder of the paper.\nThe data section should thoroughly and precisely discuss the source of the data and the bias this brings (ethical, statistical, and otherwise). Comprehensively describe and summarize the data using text, graphs, and tables. Graphs must be made with ggplot2 (Wickham 2016) and tables must be made with knitr (Xie 2023) or gt (Iannone et al. 2022). Graphs must show the actual data, or as close to it as possible, not summary statistics. Graphs and tables should be cross-referenced in the text e.g. “Table 1 shows…”).\nReferences should be added using BibTeX. Be sure to reference R and any R packages you use, as well as the dataset. Strong submissions will draw on related literature and reference those.\nThe paper should be well-written, draw on relevant literature, and explain all technical concepts. Pitch it at an educated, but non-specialist, audience.\nUse appendices for supporting, but not critical, material.\nPush to GitHub and include an informative commit message\n\n\nSubmit a PDF of your paper.\nThere should be no evidence that this is a class assignment.\n\n\n\nD.1.2 Checks\n\nThere should be no R code or raw R output in the final PDF.\nCode should be entirely reproducible, well-documented, commented, and readable.\nThe paper should knit directly to PDF i.e. use “Knit to PDF”.\n\nDo not use “Knit to html” and then save as a PDF.\nDo not use “Knit to Word” and then save as a PDF\n\nGraphs, tables, and text should be clear, and of comparable quality to those of FiveThirtyEight.\nThe date should be up-to-date and unambiguous (e.g. 2/3/2022 is ambiguous, 2 March 2022 is not).\nThe entire workflow should be entirely reproducible.\nThere should not be any typos.\nThere should be no sign this is a school paper.\nThere must be a link to the paper’s GitHub repo using a footnote.\nThe GitHub repo should be well-organized, and contain an informative README.\nThe paper should be well-written and able to be understood by the average reader of, say, FiveThirtyEight. This means that you are allowed to use mathematical notation, but you must explain all of it in plain language. All statistical concepts and terminology must be explained. Your reader is someone with a university education, but not necessarily someone who understands what a p-value is.\n\n\n\nD.1.3 FAQ\n\nCan I use a dataset from Kaggle instead? No, because they have done the hard work for you.\nI cannot use code to download my dataset, can I just manually download it? No, because your entire workflow needs to be reproducible. Please fix the download problem or pick a different dataset.\nHow much should I write? Most students submit something in the two-to-six-page range, but it is up to you. Be precise and thorough.\nMy data is about apartment blocks/NBA/League of Legends so there’s no ethical or bias aspect, what do I do? Please re-read the relevant chapter and readings to better understand bias and ethics. If you really cannot think of something, then it might be worth picking a different dataset.\nCan I use Python? No. If you already know Python then it does not hurt to learn another language.\nWhy do I need to cite R, when I don’t need to cite Word? R is a free statistical programming language with academic origins, so it is appropriate to acknowledge the work of others. It is also important for reproducibility.\nWhat reference style should I use? Any major reference style is fine (APA, Harvard, Chicago, etc); just pick one that you are used to.\nThe paper in the starter folder has a model section, so do I need to put together a model? No. The starter folder is designed to be applicable to all of the papers; just delete the aspects that you do not need.\nThe paper in the starter folder has a data sheets appendix, so do I need to put together a data sheet? No. The starter folder is designed to be applicable to all of the papers; just delete the aspects that you do not need.\nWhat does “graph the actual data” mean? If you have, say 5,000 observations in the dataset and three variables, then for every variable there should be a graph that has 5,000 points in the case of dots, or adds up to 5,000 in the case of bar charts and histograms.\n\n\n\nD.1.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.1.5 Previous examples\n\n2023: Christina Wei, and Inessa De Angelis.\n2022: Adam Labas, Alicia Yang, Alyssa Schleifer, Ethan Sansom, Hudson Yuen, Jack McKay, Roy Chan, Thomas D’Onofrio, and William Gerecke.\n2021: Amy Farrow, Morgaine Westin, and Rachael Lam."
  },
  {
    "objectID": "23-assessment.html#sec-paper-two",
    "href": "23-assessment.html#sec-paper-two",
    "title": "Online Appendix D — Papers",
    "section": "D.2 Mawson Paper",
    "text": "D.2 Mawson Paper\n\nD.2.1 Task\n\nWorking as part of a team of one to three people, please pick a paper of interest to you, with code and data that are available, published anytime since 2019, in an American Economic Association journal. These journals are: “American Economic Review”, “AER: Insights”, “AEJ: Applied Economics”, “AEJ: Economic Policy”, “AEJ: Macroeconomics”, “AEJ: Microeconomics”, “Journal of Economic Literature”, “Journal of Economic Perspectives”, “AEA Papers & Proceedings”. Alternatively, you may choose any article from the Institute for Replication list available here that has a replicability status of “Looking for replicator”.\nFollowing the Guide for Accelerating Computational Reproducibility in the Social Sciences, please complete a replication1 of at least three graphs, tables, or a combination, from that paper, using the Social Science Reproduction Platform. Note the DOI of your replication.\nWorking in an entirely reproducible way then conduct a reproduction based on two or three aspects of the paper, and write a short paper about that.\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using Quarto with these sections (you are welcome to use this starter folder): title, author, date, abstract, introduction, data, results, discussion, and references.\nThe aspects that you focus on in your paper could be the same aspects that you replicated, but they do not need to be. Follow the direction of the paper, but make it your own. That means you should ask a slightly different question, or answer the same question in a slightly different way, but still use the same dataset.\nInclude the DOI of your replication in your paper and a link to the GitHub repo that underpins your paper.\nThe results section should convey findings.\nThe discussion should include three or four sub-sections that each focus on an interesting point, and there should be another sub-section on the weaknesses of your paper, and another on potential next steps for it.\nIn the discussion section, and any other relevant section, please be sure to discuss ethics and bias, with reference to relevant literature.\nThe paper should be well-written, draw on relevant literature, and explain all technical concepts. Pitch it at an educated, but non-specialist, audience.\nUse appendices for supporting, but not critical, material.\nCode should be entirely reproducible, well-documented, and readable.\n\nSubmit a PDF of your paper.\nThere should be no evidence that this is a class assignment.\n\n\n\nD.2.2 Checks\n\nThe paper should not just copy/paste the code from the original paper, but have instead used that as a foundation to work from.\nYour paper should have a link to the associated GitHub repository and the DOI of the Social Science Reproduction Platform replication that you conducted.\nMake sure you have referenced everything, including R. Strong submissions will draw on related literature in the discussion (and other sections) and would be sure to also reference those. The style of references does not matter, provided it is consistent.\n\n\n\nD.2.3 FAQ\n\nHow much should I write? Most students submit something in the 10-to-15-page range, but it is up to you. Be precise and thorough.\nDo I have to focus on a model result? No, it is likely best to stay away from that at this point, and instead focus on tables or graphs of summary or explanatory statistics.\nWhat if the paper I choose is in a language other than R? Both your replication and reproduction code should be in R. So you will need to translate the code into R for the replication. And the reproduction should be your own work, so that also should be in R. One common language is Stata, and Huntington-Klein (2022) might be useful as a “Rosetta Stone” of sorts, for R, Python, and Stata, or just use a LLM to help.\nCan I work by myself? Yes.\nDo the graphs/tables have to look identical to the original? No, you are welcome to, and should, make them look better as part of the reproduction. And even as part of the replication, they do not have to be identical, just similar enough.\nOne of my graphs has four panels, do I have to do all of them for this to be counted as one element? No, for the purpose of this paper, every panel counts as a separate element, so all you would need to do is three panels and that would be enough.\nHow do I automatically download the data if they are behind a sign-in? If the data are behind a sign-in, just add commented documentation for how to download it into the download_data.R R file, rather than code.\nDo we need to commit our original, unedited data data to Github if it is really big? No, you do not necessarily need to commit the original, unedited data data to GitHub if it is too large, just add a note explaining the situation in the README and how to obtain the data.\nWhat should the abstract and introduction be about? The abstract and introduction should reflect your own work and findings, rather than those of the original paper (even though those will necessarily nonetheless have some role). You are (almost surely) not replicating their entire paper, and so your abstract should be different. See the examples for guidance.\n\n\n\nD.2.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Replication\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSSRP submission needs to be filled out completely for three elements.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.2.5 Previous examples\n\n2023: Jayden Jung, Finn Korol, and Sofia_Sellitto.\n2022: Alyssa Schleifer, Hudson Yuen, Tamsen Yau; Olaedo Okpareke, Arsh Lakhanpal, Swarnadeep Chattopadhyay; and Kimlin Chin."
  },
  {
    "objectID": "23-assessment.html#sec-paper-three",
    "href": "23-assessment.html#sec-paper-three",
    "title": "Online Appendix D — Papers",
    "section": "D.3 Howrah Paper",
    "text": "D.3 Howrah Paper\n\nD.3.1 Task\n\nWorking as part of a team of one to three people, and in an entirely reproducible way, please obtain data from the US General Social Survey2. (You are welcome to use a different government-run survey, but please obtain permission before starting.)\nObtain the data, focus on one aspect of the survey, and then use it to tell a story.\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then use Quarto to prepare a PDF with these sections (you are welcome to use this starter folder): title, author, date, abstract, introduction, data, results, discussion, an appendix that will, at least, contain a survey, and references.\nIn addition to conveying a sense of the dataset of interest, the data section should include, but not be limited to:\n\nA discussion of the survey’s methodology, and its key features, strengths, and weaknesses. For instance: what is the population, frame, and sample; how is the sample recruited; what sampling approach is taken, and what are some of the trade-offs of this; how is non-response handled.\nA discussion of the questionnaire: what is good and bad about it?\nIf this becomes too detailed, then use appendices for supporting but not essential aspects.\n\nIn an appendix, please put together a supplementary survey that could be used to augment the general social survey the paper focuses on. The purpose of the supplementary survey is to gain additional information on the topic that is the focus of the paper, beyond that gathered by the general social survey. The survey would be distributed in the same manner as the general social survey but needs to stand independently. The supplementary survey should be put together using a survey platform. A link to this should be included in the appendix. Additionally, a copy of the survey should be included in the appendix.\nPlease be sure to discuss ethics and bias, with reference to relevant literature.\nCode should be entirely reproducible, well-documented, and readable.\n\nSubmit a PDF of the paper.\nThe paper should be well-written, draw on relevant literature, and explain all technical concepts. Pitch it at a university-educated, but non-specialist, audience. Use survey, sampling, and statistical terminology, but be sure to explain it. The paper should flow, and be easy to follow and understand.\nThere should be no evidence that this is a class paper.\n\n\n\nD.3.2 Checks\n\nAn appendix should contain both a link to the supplementary survey and the details of it, including questions (in case the link fails, and to make the paper self-contained).\n\n\n\nD.3.3 FAQ\n\nWhat should I focus on? You may focus on any year, aspect, or geography that is reasonable given the focus and constraints of the general social survey that you are interested in. Please consider the year and topics that you are interested in together, as some surveys focus on particular topics in some years.\nDo I need to include the raw GSS data in the repo? For most of the general social surveys you will not have permission to share the GSS data. If that is the case, then you should add clear details in the README explaining how the data could be obtained.\nHow many graphs do I need? In general, you need at least as many graphs as you have variables, because you need to show all the observations for all variables. But you may be able to combine a few; or, vice versa, you may be interested in looking at different aspects or relationships.\n\n\n\nD.3.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Survey\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe survey should have an introductory section and include the details of a contact person. The survey questions should be well constructed and appropriate to the task. The questions should have an appropriate ordering. A final section should thank the respondent.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.3.5 Previous examples\n\n2023: Christina Wei and Michaela Drouillard; and Inessa De Angelis.\n2022: Anna Li and Mohammad Sardar Sheikh; Chyna Hui and Marco Chau; Ethan Sansom; Luckyna Laurent, Samita Prabhasavat, and Zoie So; Pascal Lee Slew, and Yunkyung_Park; and Ray Wen, Isfandyar Virani, and Rayhan Walia."
  },
  {
    "objectID": "23-assessment.html#sec-paper-four",
    "href": "23-assessment.html#sec-paper-four",
    "title": "Online Appendix D — Papers",
    "section": "D.4 Dysart Paper",
    "text": "D.4 Dysart Paper\n\nD.4.1 Task\n\nWorking as part of a team of one to three people, and in an entirely reproducible way, please convert at least one full-page table from one DHS Program “Final Report”, from the 1980s or 1990s, as available here, into a usable dataset, then write a short paper telling a story with the data.\nCreate a well-organized folder with appropriate sub-folders, and add it to GitHub. You are welcome to use this starter folder.\nCreate and document a dataset:\n\nSave the PDF to “inputs”.\nPut together a simulation of your plan for the usable dataset and save the script to “scripts/00-simulation.R”.\nWrite R code, saved as “scripts/01-gather_data.R”, to either OCR or parse the PDF, as appropriate, and save the output to “outputs/data/first_parse.csv”.\nWrite R code, saved as “scripts/02-clean_and_prepare_data.R”, that draws on “first_parse.csv” to clean and prepare the dataset. Use pointblank to put together tests that the dataset passes (at a minimum, every variable should have a test for class and another for content). Save the dataset to “outputs/data/cleaned_data.parquet”.\nFollowing Gebru et al. (2021), put together a data sheet for the dataset you put together (put this in the appendix of your paper). You are welcome to start from the template “inputs/data/datasheet_template.qmd” in the starter folder, although, again, you should add it to the appendix of your paper, rather than a stand-alone document.\n\nUse the dataset to tell a story by using Quarto to prepare a PDF with these sections: title, author, date, abstract, introduction, data, results, discussion, an appendix that will, at least, contain a datasheet for the dataset, and references.\n\nIn addition to conveying a sense of the dataset of interest, the data section should include details of the methodology used by the DHS you used, and its key features, strengths, and weaknesses.\n\nSubmit a PDF of the paper.\nThere should be no evidence that this is a class paper.\n\n\n\nD.4.2 Checks\n\nUse GitHub in a well-developed way by making at least a few commits and using descriptive commit messages.\n\n\n\n\nD.4.3 FAQ\n\n\n\n\n\n\nD.4.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Parquet\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe analysis dataset is saved as a parquet file (optionally also as a CSV).\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    Datasheet\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA thorough datasheet for the dataset that was constructed is included.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.4.5 Previous examples\n\n2022: Bilal Haq and Ritvik Puri; Charles Lu, Mahak Jain, and Yujun Jiao; Jacob Yoke Hong Si; and Pascal Lee Slew and Yunkyung Park."
  },
  {
    "objectID": "23-assessment.html#sec-murrumbidgee",
    "href": "23-assessment.html#sec-murrumbidgee",
    "title": "Online Appendix D — Papers",
    "section": "D.5 Murrumbidgee Paper",
    "text": "D.5 Murrumbidgee Paper\n\nD.5.1 Task\n\nWorking as part of a team of one to three people, and in an entirely reproducible way, please revisit the dataset that you used in Section D.1. Build a linear model for one of the variables, and consider the results. Then write a short paper telling a story with the data.\nCreate a well-organized folder with appropriate sub-folders, and add it to GitHub. You are welcome to use this starter folder.\nUse the model to tell a story by using Quarto to prepare a PDF with these sections: title, author, date, abstract, introduction, data, model, results, discussion, and references.\nSubmit a PDF of the paper.\nThere should be no evidence that this is a class paper.\n\n\n\nD.5.2 Checks\n\nBe careful to thoroughly explain the model. Also consider the assumptions of the model and the threats to its validity.\n\n\n\nD.5.3 FAQ\n\nCan we use aspects of the data and other sections that were submitted in Section D.1? Yes, it is fine to re-use aspects of Section D.1, but chances are you have developed since then and it would make sense to re-write much of that.\n\n\n\nD.5.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Model\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe model should be nicely written out, well-explained, justified, and appropriate.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Parquet\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe analysis dataset is saved as a parquet file (optionally also as a CSV).\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    Datasheet\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA thorough datasheet for the dataset that was constructed is included.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.5.5 Previous examples"
  },
  {
    "objectID": "23-assessment.html#sec-spadina",
    "href": "23-assessment.html#sec-spadina",
    "title": "Online Appendix D — Papers",
    "section": "D.6 Spadina Paper",
    "text": "D.6 Spadina Paper\n\nD.6.1 Task\n\nWorking as part of a team of one to three people, and in an entirely reproducible way, please pick one of the examples in Chapter 13. Change the situation slightly, and then build a generalized linear model. Then write a short paper telling a story with the data.\nCreate a well-organized folder with appropriate sub-folders, and add it to GitHub. You are welcome to use this starter folder.\nUse the model to tell a story by using Quarto to prepare a PDF with these sections: title, author, date, abstract, introduction, data, model, results, discussion, and references.\nSubmit a PDF of the paper.\nThere should be no evidence that this is a class paper.\n\n\n\nD.6.2 Checks\n\nBe careful to thoroughly explain the model. Also consider the assumptions of the model and the threats to its validity.\n\n\n\nD.6.3 FAQ\n\nWhat does “change the situation slightly” mean? You are welcome to use the same, or similar, data, but consider a different aspect. For instance:\n\nIn the logistic regression example of US political support, you may use the CES from a different year, and/or with slightly different explanatory variables.\nIn the Poisson regression example of the letters used in Jane Eyre, you may consider a different novel.\nIn the negative binomial regression of mortality in Alberta, you may consider a different geographic area.\n\n\n\n\nD.6.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Model\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe model should be nicely written out, well-explained, justified, and appropriate.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Parquet\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe analysis dataset is saved as a parquet file (optionally also as a CSV).\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    Datasheet\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA thorough datasheet for the dataset that was constructed is included.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.6.5 Previous examples"
  },
  {
    "objectID": "23-assessment.html#sec-paper-five",
    "href": "23-assessment.html#sec-paper-five",
    "title": "Online Appendix D — Papers",
    "section": "D.7 Spofforth Paper",
    "text": "D.7 Spofforth Paper\n\nD.7.1 Task\n\nWorking as part of a team of one to three people, please forecast the popular vote of the 2020 US election using multilevel regression with post-stratification and then write a short paper telling a story. This requires individual-level survey data, post-stratification data, and a model that brings them together. Given the expense of collecting these data, and the privilege of having access to them, please be sure to properly cite all datasets that you use.\nIndividual-level survey data:\n\nRequest access to the Democracy Fund + UCLA Nationscape “Full Data Set”. This could take a day or two. Please start early.\nSimulate the survey dataset that you will use, and save the script to “scripts/00-simulation-survey.R”.\nOnce you have access then pick one survey of interest (they were conducted at different times).\nThis will be a large file and is not yours to share. Do not push it to GitHub. Use a .gitignore file to accomplish this. Instead document how to get the original, unedited data in the README.\nClean and prepare the dataset based on what you need.\n\nPost-stratification data:\n\nCreate an account with IPUMS and then use this to access the American Community Surveys (ACS).\nSimulate the post-stratification dataset that you will use, and save the script to “scripts/00-simulation-poststratification.R”.\nPick an appropriate 1-year ACS (there is one every year). Then select some variables. This will depend on what you want to model and the survey data, but some options include: REGION, STATEICP, AGE, SEX, MARST, RACE, HISPAN, BPL, CITIZEN, EDUC, LABFORCE, or INCTOT. Have a look around and see what you are interested in, remembering that you will need to establish a correspondence to the survey.\nDownload the relevant post-stratification data (it is probably easiest to change the data format to .dta).\nAgain, this will be a large file and is not yours to share. Do not push it to GitHub. Use a .gitignore file to accomplish this. Instead document how to get the original, unedited data in the README.\nClean and prepare the post-stratification dataset. Remember that you need cell counts for the sub-populations in the model.\n\nModelling:\n\nYou will want to explain vote intention based on a variety of explanatory variables. The decision is yours, but you should probably use logistic regression. In that case, construct the vote intention variable so that it is binary (either “supports Trump” or “supports Biden”). Then build a model.\nThink about model fit, diagnostics, and other similar aspects that you need to convince someone that the model is appropriate.\nYou have flexibility of the model that you use, (and hence the cells that you will need to create). In general, the more cells the better, but you may want fewer cells for simplicity in the writing process and to ensure a decent sample in each cell. It would be best to start with a simple model and then complicate it, rather than vice versa.\nApply the trained model to the post-stratification dataset to forecast the election result. The specifics will depend on your modelling approach but will likely involve predict(), add_predicted_draws(), or similar. The primary aspect of interest is the forecast distribution of the popular vote, and how the explanatory variables affect this. Strong submissions would go beyond that.\n\nWrite-up:\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using Quarto with these sections (you are welcome to use this starter folder): title, author, date, abstract, introduction, data, model, results, discussion, and references. Use appendices for supporting, but not critical, material.\n\nIn the model section, you should carefully spell out the statistical model that you are using, being sure to define and explain each aspect and why it is important. The model should be appropriately complex; that is, not inappropriately simple, but not unnecessarily complicated. The model should have well-defined variables and these should correspond to what is discussed in the data section. You should explain how the aspects discussed in the data section assert themselves in the modelling decisions that you made. The model should be written out in appropriate mathematical notation but also in plain English. Every aspect of that notation should be defined. The model should make sense based on the substantive area, and the form of the model. If the model is Bayesian, then priors should be defined and sensible. There should be explanation of how features enter the model and why. For instance, why use age rather than age-groups, why does province have a levels effect, why is gender categorical, etc? In general, there should be a clear justification that this is the model for the situation. The assumptions underpinning the model should be clearly discussed. Alternative models, or variants, should be discussed, and strengths and weaknesses made clear. Why was this model chosen? You should mention the software that you used to run the model. There should be evidence of thought about the circumstances in which the model may not be appropriate. There should be evidence of model validation and checking, whether that is out-of-sample, RMSE, a test/training split, or appropriate sensitivity checks. You should be clear about model convergence, model checks, and diagnostic issues.\n\n\nSubmit a PDF of your paper.\nThere should be no evidence that this is a class assignment.\n\n\n\nD.7.2 Checks\n\nUse GitHub in a well-developed way by making at least a few commits and using descriptive commit messages.\nDo not include p-values, stars, or similar, in tables. If you invoke statistical significance, then you should draw on and integrate Fisher (1926) and others.\n\n\n\nD.7.3 FAQ\n\nHow much should I write? Most students submit something in the 10-to-15-page range, but it is up to you. Be precise and thorough.\n\n\n\nD.7.4 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Model\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe model should be nicely written out, well-explained, justified, and appropriate.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Parquet\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe analysis dataset is saved as a parquet file (optionally also as a CSV).\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.7.5 Previous examples\n\n2020: Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz (this paper received an “Honorable Mention” in the ASA December 2020 Undergraduate Statistics Research Project competition.)"
  },
  {
    "objectID": "23-assessment.html#sec-final-paper",
    "href": "23-assessment.html#sec-final-paper",
    "title": "Online Appendix D — Papers",
    "section": "D.8 Final paper",
    "text": "D.8 Final paper\n\nD.8.1 Task\n\nWorking individually and in an entirely reproducible way please write a paper that involves original work to tell a story with data.\nOptions include (pick one):\n\nDevelop a research question that is of interest to you based on your own interests, background, and expertise, then obtain or create a relevant dataset.\nA reproduction, being sure to use the paper as a foundation rather than as an end-in-itself.\n\nCreate a well-organized folder with appropriate sub-folders, add it to GitHub, and then prepare a PDF using Quarto with these sections (you are welcome to use this starter folder):\n\nTitle, date, author, abstract, introduction, data, model, results, discussion, appendix (optional, for supporting, but not critical, material), and a reference list.\nIt must also include an enhancement, and this would either be contained, or linked to, in the appendix.\n\n\n\n\nD.8.2 Peer review submission\n\nThis is an initial “submission” where you get comments and feedback on a draft.\nSubmit a PDF of your draft.\nThe paper does not have to be finished at this point, but the following sections must be filled out: title, author, date, abstract, and introduction.\nAll other sections must be present in the paper, but do not have to be filled out (e.g. you must have a “Data” heading, but you do not need to have content in that section).\nTo be clear, it is fine to later change any aspect of what you submit at this checkpoint.\nYou will be awarded one percentage point just for submitting a draft that meets this minimum.\nThere are no extensions possible for this submission because the following submission is dependent on this date.\n\n\n\nD.8.3 Conduct peer-review\n\nAs an individual, you will randomly be assigned a handful of rough drafts to provide feedback. You have three days to provide feedback to your peers.\nYou should use GitHub Issues, or make a pull request, to provide the feedback.\nIf you provide feedback to one peer you will receive one percentage point, if you provide feedback to two peers you will receive two percentage points, etc.\nYour feedback must include at least five comments (meaningful and useful bullet points). These must be well-written and thoughtful.\nThere are no extensions granted for this submission since the following submission is dependent on this date.\nPlease remember that you are providing feedback here to help your colleagues. All comments should be professional and kind. It is challenging to receive criticism. Please remember that your goal here is to help your peers advance their writing/analysis.\nSubmit the links to the GitHub Issues or pull requests that you created.\n\n\n\nD.8.4 FAQ\n\nCan I work as part of a team? No. You must have some work that is entirely your own. You really need your own work to show off for job applications etc.\nHow much should I write? Most students submit something that has 10-to-20-pages of main content, with additional pages devoted to appendices, but it is up to you. Be precise and thorough.\nDo I have to submit an initial paper in order to do the peer-review? Yes.\nCan I use the same paper for the reproduction as in the Howrah Paper? No.\nCan I use any model? You are welcome to use any model, but you need to thoroughly explain it and this can be difficult for more complicated models. Start small. Pick one or two explanatory variables. Once you get that working, then complicate it. Remember that every explanatory variable, and the dependents for that matter, needs to be graphed.\n\n\n\nD.8.5 Rubric\n\n\n\n\n\n\n  \n    \n    \n      Component\n      Range\n      Requirement\n    \n  \n  \n    R is appropriately cited\n0 - 'No'; 1 - 'Yes'\nMust be referred to in the main content and included in the reference list. If not, no need to continue marking, paper gets 0 overall.\n    Class paper\n0 - 'No'; 1 - 'Yes'\nCheck meta data such as project and folder names, as well as other aspect such as title etc. If there is any sign this is a class paper then no need to continue marking, paper gets 0 overall.\n    Title\n0 - 'Poor or not done'; 1 - 'Yes'; 2 - 'Exceptional'\nAn informative title is included that explains the story, and ideally tells the reader what happens at the end of it. 'Paper X' is not an informative title. There should be no evidence this is a school paper.\n    Author, date, and repo\n0 - 'Poor or not done'; 2 - 'Yes'\nThe author, date of submission in unambiguous format, and a link to a GitHub repo are clearly included. (The later likely, but not necessarily, through a statement such as: 'Code and data supporting this analysis is available at: LINK').\n    Abstract\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nAn abstract is included and appropriately pitched to a non-specialist audience. The abstract answers: 1) what was done, 2) what was found, and 3) why this matters (all at a high level). Likely four sentences. Abstract must make clear what we learn about the world because of this paper.\n    Introduction\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe introduction is self-contained and tells a reader everything they need to know including: 1) broader context to motivate; 2) some detail about what the paper is about; 3) a clear gap that needs to be filled; 4) what was done; 5) what was found; 6) why it is important; 7) the structure of the paper. A reader should be able to read only the introduction and know what was done, why, and what was found. Likely 3 or 4 paragraphs, or 10 per cent of total.\n    Estimand\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe estimand is clearly stated in the introduction.\n    Data\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nA sense of the dataset should be communicated to the reader. All variables should be thoroughly examined and explained. Explain if there were similar datasets that could have been used and why they were not. If variables were constructed then this should be mentioned, and high-level cleaning aspects of note should be mentioned, but this section should focus on the destination, not the journey. It is important to understand what the variables look like by including graphs, and possibly tables, of all observations, along with discussion of those graphs and the other features of these data. Summary statistics should also be included, and well as any relationships between the variables. If this becomes too detailed, then appendices could be used.\n    Measurement\n0 - 'Poor or not done'; 1 - 'Exceptional'\nSome aspect of measurement, relating to the dataset, is mentioned in the data section.\n    Model\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nThe model should be nicely written out, well-explained, justified, and appropriate.\n    Results\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nResults will likely require summary statistics, tables, graphs, images, and possibly statistical analysis or maps. There should also be text associated with all these aspects. Show the reader the results by plotting them where possible. Talk about them. Explain them. That said, this section should strictly relay results. Regression tables must not contain stars.\n    Discussion\n0 - 'Poor or not done'; 2 - 'Many issues'; 4 - 'Some issues'; 6 - 'Good'; 8 - 'Great'; 10 - 'Exceptional'\nSome questions that a good discussion would cover include (each of these would be a sub-section of something like half a page to a page): What is done in this paper? What is something that we learn about the world? What is another thing that we learn about the world? What are some weaknesses of what was done? What is left to learn or how should we proceed in the future?\n    Cross-references\n0 - 'Poor or not done'; 2 - 'Yes'\nAll figures, tables, and equations, should be numbered, and referred to in the text using cross-references.\n    Prose\n0 - 'Poor or not done'; 2 - 'Yes'\nAll aspects of submission should be free of noticeable typos, spelling mistakes, and be grammatically correct. Prose should be coherent, concise, and clear.\n    Graphs/tables/etc\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nGraphs and tables must be included in the paper and should be to well-formatted, clear, and digestible. They should: 1) serve a clear purpose; 2) be fully self-contained through appropriate use of captions and sub-captions; 3) appropriately sized and colored; and 4) have appropriate significant figures, in the case of tables.\n    Reference list\n0 - 'Poor or not done'; 3 - 'One minor issue'; 4 - 'Perfect'\nAll data, software, literature, and any other relevant material, should be cited in-text and included in a reference list made using BibTeX. A few lines of code from Stack Overflow or similar, would be acknowledged just with a comment in the script immediately preceding the use of the code rather than here. But larger chunks of code should be fully acknowledged with an in-text citation and appear in the reference list.\n    Commits\n0 - 'Poor or not done'; 2 - 'Excellent'\nThere are at least two different commits, and they have meaningful commit messages.\n    Simulation\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe script is clearly commented and structured. All variables are appropriately simulated.\n    Tests\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nData and code tests are appropriately used.\n    Parquet\n0 - 'Poor or not done'; 1 - 'Exceptional'\nThe analysis dataset is saved as a parquet file (optionally also as a CSV).\n    Reproducibility\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nThe paper and analysis should be fully reproducible. The repo should have a detailed README. All code should be thoroughly documented. An R project should be used. Code should be used to do all steps including appropriately read data, prepare it, create plots, conduct analysis, and generate documents. Seeds should be used where needed. Code should have a preamble and be well-documented including comments and layout. The repo should be appropriately organized and not contain extraneous files. setwd() and hard coded file paths must not be used.\n    Code style\n0 - 'Poor or not done'; 1 - 'Exceptional'\nCode is appropriately styled.\n    Enhancements\n0 - 'Poor or not done'; 1 - 'Gets job done'; 2 - 'Fine'; 3 - 'Great'; 4 - 'Exceptional'\nYou should pick at least one of the following and include it to enhance your submission: 1) A datasheet for the dataset; 2) A model card for the model; 3) A Shiny application; 4) An R package; or 5) API for the model.\n    General excellence\n0 - 'None'; 1 - 'Huh, that's interesting'; 2 - 'Wow'; 3 - 'Exceptional'\nThere are always students that excel in a way that is not anticipated in the rubric. This item accounts for that.\n  \n  \n  \n\n\n\n\n\n\nD.8.6 Previous examples\n\n2023: Aliyah Maxine Ramos; Chloe Thierstein; Jason Ngo; Jenny Shen; Laura Lee-Chu; and Sebastian Rodriguez.\n2022: Alicia Yang; Ethan Sansom; Ivan Li; Jack McKay; Olaedo Okpareke; Sidharth Gupta; and Tian Yi Zhang.\n2021: Amy Farrow; Jia Jia Ji; Laura Cline; Lorena Almaraz De La Garza; and Rachael Lam.\n2020: Annie Collins.\n\n\n\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible Research.” https://arxiv.org/abs/1802.03311.\n\n\nFisher, Ronald. 1926. “The Arrangement of Field Experiments.” Journal of the Ministry of Agriculture, 503–15. https://doi.org/10.23637/rothamsted.8v61q.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. “Datasheets for Datasets.” Communications of the ACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGelfand, Sharla. 2022. opendatatoronto: Access the City of Toronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nHuntington-Klein, Nick. 2022. “Library of Statistical Techniques.” https://lost-stats.github.io.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, and JooYoung Seo. 2022. gt: Easily Create Presentation-Ready Display Tables.\n\n\nWickham, Hadley. 2016. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nXie, Yihui. 2023. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/."
  },
  {
    "objectID": "23-assessment.html#footnotes",
    "href": "23-assessment.html#footnotes",
    "title": "Online Appendix D — Papers",
    "section": "",
    "text": "This terminology is used following Barba (2018), but it is the opposite of that used by BITSS.↩︎\nThe US GSS is recommended here because individual-level data are publicly available, and the dataset is well-documented. But, often university students in particular countries have access to individual level data that are not available to the public, and if this is the case then you are welcome to use that instead. Students at Australian universities will likely have access to individual-level data from the Australian General Social Survey, and could use that. Students at Canadian universities will likely have access to individual-level data from the Canadian General Social and may like to use that.↩︎"
  },
  {
    "objectID": "24-interaction.html#introduction",
    "href": "24-interaction.html#introduction",
    "title": "Online Appendix E — Interaction",
    "section": "E.1 Introduction",
    "text": "E.1 Introduction\nBooks and papers have been the primary mediums for communication for thousands of years. But with the rise of computers, and especially the internet, in recent decades, these static approaches have been complemented with interactive approaches. Fundamentally, the internet is about making files available others. If we additionally allow them to do something with what we make available, then we need to take a variety of additional aspects into consideration. Interactive communication is also important as models become more complex. For instance, da Silva, Cook, and Lee (2023) develop interactive graphics which they use to better understand their random forest models.\nIn this chapter we begin by covering how to create and publish a website. This serves as a place to host a portfolio of work. After that we cover adding interaction to maps and graphs, which are two that nicely lend themselves to this."
  },
  {
    "objectID": "24-interaction.html#quarto-websites",
    "href": "24-interaction.html#quarto-websites",
    "title": "Online Appendix E — Interaction",
    "section": "E.2 Quarto websites",
    "text": "E.2 Quarto websites\nA website is a critical part of communication. For instance, it is a place to make a portfolio of work publicly available. One way to make a website is to use Quarto’s built in websites.   Having set-up GitHub in RStudio, it is possible to have a website online in five minutes.\n\n\n\n\n\n\nOne nice feature of a Quarto website is that it enables us to have a multi-page, rather than single-page, website. As it is fundamentally a Quarto document, it also allows us to use the skills that we developed in Chapter 3.\nGet started by creating a new project (“File” -&gt; “New project” -&gt; “New Directory” -&gt; “Quarto Website”), give it a name, and select “Open in new session” -&gt; “Create Project” (Figure E.1 (a)).\n\n\n\n\n\n\n\n(a) Example settings for setting up a Quarto website\n\n\n\n\n\n\n\n(b) Building a Quarto website\n\n\n\n\n\n\n\n\n\n(c) Personalizing the Quarto website\n\n\n\n\n\n\n\n(d) Updating the Quarto yaml\n\n\n\n\nFigure E.1: Using Quarto to make a website\n\n\nThe default basic website can be produced with “Build” -&gt; “Render Website” (Figure E.1 (b)). By default it may show in the “Viewer” pane, but can also be shown in a New Window, by . Again, at this point we may like to change the details to reflect our own. In particular, we may like to change the title of “index.qmd”, and add our own details (Figure E.1 (c)).\nContent that is included in the primary menu is specified in “_quarto.yml”. We could add another page to this, such as “contact.qmd” and to create the content that would be included we may like to duplicate, say, “about.qmd” and then edit that (Figure E.1 (d)). Another aspect that we can change in “_quarto.yml” is the theme. The default is “cosmo”, but there are many other options specified here.\nAfter the details are personalized and we are not unhappy with the website it can be pushed to GitHub and then hosted with GitHub Pages. To take advantage of that we need to first do two things. Firstly, we should slightly modify “_quarto.yml” to specify that we should build to a “docs” folder rather than “_site” (Figure E.1 (d)).\n\nproject:\n  type: website \n  output-dir: docs\n\nThe other aspect to know is that when we use this service, by default, GitHub would try to build the site, which we do not want, so we need to first add a hidden file to turn that off, by running this in the console:\n\nfile.create(\".nojekyll\")\n\nThen, assuming GitHub was set-up in Chapter 3, we can use usethis to get our newly created project onto GitHub. We use use_git() to initialize a Git repository, and then use_github() pushes it to GitHub.\n\nuse_git()\nuse_github()\n\nThe project will then be on GitHub. We can use GitHub pages to host it: “Settings -&gt; Pages” and then change the source to “main” or “master”, depending on your settings, and finally “docs”. After a few minutes to run through various checks, GitHub will let you know the address that you can share to visit your site.\nTo update the site, work locally. First pull, to ensure that any changes that GitHub made are present locally, then edit the site, re-render it and push it to GitHub in the usual manner. After the checks are completed the live website will update."
  },
  {
    "objectID": "24-interaction.html#client-side-interactivity",
    "href": "24-interaction.html#client-side-interactivity",
    "title": "Online Appendix E — Interaction",
    "section": "E.3 Client-side interactivity",
    "text": "E.3 Client-side interactivity\nOnce we have a hosted website, one nice thing is that we can use it to “ship” some “light” interactivity. We will discuss more onerous approaches, such as a shiny app later, but these require a different skill set to share and deploy. Here we introduce a client-side solution, crosstalk and plotly, which will provide some interactivity, such as a tooltip, with little additional overhead."
  },
  {
    "objectID": "24-interaction.html#interactive-maps",
    "href": "24-interaction.html#interactive-maps",
    "title": "Online Appendix E — Interaction",
    "section": "E.4 Interactive maps",
    "text": "E.4 Interactive maps\nThe nice thing about interactive maps is that we can let our user decide what they are interested in. For instance, in the case of a map, some people will be interested in, say, Toronto, while others will be interested in Chennai or even Auckland. But it would be difficult to present a map that focused on all of those, so an interactive map is a way to allow users to focus on what they want.\nThat said, we should be cognizant of what we are doing when we build maps, and more broadly, what is being done at scale to enable us to be able to build our own maps. For instance, with regard to Google, McQuire (2019) says:\n\nGoogle began life in 1998 as a company famously dedicated to organising the vast amounts of data on the Internet. But over the last two decades its ambitions have changed in a crucial way. Extracting data such as words and numbers from the physical world is now merely a stepping-stone towards apprehending and organizing the physical world as data. Perhaps this shift is not surprising at a moment when it has become possible to comprehend human identity as a form of (genetic) ‘code’. However, apprehending and organizing the world as data under current settings is likely to take us well beyond Heidegger’s ‘standing reserve’ in which modern technology enframed ‘nature’ as productive resource. In the 21st century, it is the stuff of human life itself—from genetics to bodily appearances, mobility, gestures, speech, and behaviour—that is being progressively rendered as productive resource that can not only be harvested continuously but subject to modulation over time.\n\nDoes this mean that we should not use or build interactive maps? Of course not. But it is important to be aware of the fact that this is a frontier, and the boundaries of appropriate use are still being determined. Indeed, the literal boundaries of the maps themselves are being consistently determined and updated. The move to digital maps, compared with physical printed maps, means that it is possible for different users to be presented with different realities. For instance, “…Google routinely takes sides in border disputes. Take, for instance, the representation of the border between Ukraine and Russia. In Russia, the Crimean Peninsula is represented with a hard-line border as Russian-controlled, whereas Ukrainians and others see a dotted-line border. The strategically important peninsula is claimed by both nations and was violently seized by Russia in 2014, one of many skirmishes over control” (Bensinger 2020).\n\nE.4.1 Leaflet\nWe can use leaflet (Cheng, Karambelkar, and Xie 2021) to make interactive maps. The essentials are similar to ggmap (Kahle and Wickham 2013), but there are many additional aspects beyond that. We can redo the US military deployments map from Chapter 5 that used troopdata (Flynn 2022). The advantage with an interactive map is that we can plot all the bases and allow the user to focus on which area they want, in comparison with Chapter 5 where we just picked a few particular countries. A great example of why this might be useful is provided by The Economist (2022b) where they are able to show 2022 French Presidential results for the entire country by commune.\nIn the same way as a graph in ggplot2 begins with ggplot(), a map in leaflet begins with leaflet(). Here we can specify data, and other options such as width and height. After this, we add “layers” in the same way that we added them in ggplot2. The first layer that we add is a tile, using addTiles(). In this case, the default is from OpenStreeMap. After that we add markers with addMarkers() to show the location of each base (Figure E.2).\n\nbases &lt;- get_basedata()\n\n# Some of the bases include unexpected characters which we need to address\nEncoding(bases$basename) &lt;- \"latin1\"\n\nleaflet(data = bases) |&gt;\n  addTiles() |&gt; # Add default OpenStreetMap map tiles\n  addMarkers(\n    lng = bases$lon,\n    lat = bases$lat,\n    popup = bases$basename,\n    label = bases$countryname\n  )\n\n\n\n\nFigure E.2: Interactive map of US bases\n\n\n\nThere are two new arguments, compared with ggmap. The first is “popup”, which is the behavior that occurs when the user clicks on the marker. In this case, the name of the base is provided. The second is “label”, which is what happens when the user hovers on the marker. In this case it is the name of the country.\nWe can try another example, this time of the amount spent building those bases. We will introduce a different type of marker here, which is circles. This will allow us to use different colors for the outcomes of each type. There are four possible outcomes: “More than $100,000,000”, “More than $10,000,000”, “More than $1,000,000”, “$1,000,000 or less” Figure E.3.\n\nbuild &lt;-\n  get_builddata(startyear = 2008, endyear = 2019) |&gt;\n  filter(!is.na(lon)) |&gt;\n  mutate(\n    cost = case_when(\n      spend_construction &gt; 100000 ~ \"More than $100,000,000\",\n      spend_construction &gt; 10000 ~ \"More than $10,000,000\",\n      spend_construction &gt; 1000 ~ \"More than $1,000,000\",\n      TRUE ~ \"$1,000,000 or less\"\n    )\n  )\n\npal &lt;-\n  colorFactor(\"Dark2\", domain = build$cost |&gt; unique())\n\nleaflet() |&gt;\n  addTiles() |&gt; # Add default OpenStreetMap map tiles\n  addCircleMarkers(\n    data = build,\n    lng = build$lon,\n    lat = build$lat,\n    color = pal(build$cost),\n    popup = paste(\n      \"&lt;b&gt;Location:&lt;/b&gt;\",\n      as.character(build$location),\n      \"&lt;br&gt;\",\n      \"&lt;b&gt;Amount:&lt;/b&gt;\",\n      as.character(build$spend_construction),\n      \"&lt;br&gt;\"\n    )\n  ) |&gt;\n  addLegend(\n    \"bottomright\",\n    pal = pal,\n    values = build$cost |&gt; unique(),\n    title = \"Type\",\n    opacity = 1\n  )\n\n\n\n\nFigure E.3: Interactive map of US bases with colored circules to indicate spend\n\n\n\n\n\nE.4.2 Mapdeck\nmapdeck (Cooley 2020) is based on WebGL. This means the web browser will do a lot of work for us. This enables us to accomplish things with mapdeck that leaflet struggles with, such as larger datasets.\nTo this point we have used “stamen maps” as our underlying tile, but mapdeck uses Mapbox. This requires registering an account and obtaining a token. This is free and only needs to be done once. Once we have that token we add it to our R environment (the details of this process are covered in Chapter 7) by running edit_r_environ(), which will open a text file, which is where we should add our Mapbox secret token.\n\nMAPBOX_TOKEN &lt;- \"PUT_YOUR_MAPBOX_SECRET_HERE\"\n\nWe then save this “.Renviron” file, and restart R (“Session” -&gt; “Restart R”).\nHaving obtained a token, we can create a plot of our base spend data from earlier (Figure E.4).\n\nmapdeck(style = mapdeck_style(\"light\")) |&gt;\n  add_scatterplot(\n    data = build,\n    lat = \"lat\",\n    lon = \"lon\",\n    layer_id = \"scatter_layer\",\n    radius = 10,\n    radius_min_pixels = 5,\n    radius_max_pixels = 100,\n    tooltip = \"location\"\n  )\n\n\n\n\nFigure E.4: Interactive map of US bases using Mapdeck"
  },
  {
    "objectID": "24-interaction.html#shiny",
    "href": "24-interaction.html#shiny",
    "title": "Online Appendix E — Interaction",
    "section": "E.5 Shiny",
    "text": "E.5 Shiny\nshiny (Chang et al. 2021) is a way of making interactive web applications using R. It is fun, but can be a little fiddly. Here we are going to step through one way to take advantage of shiny, which is to quickly add some interactivity to our graphs. This sounds like a small thing, but a great example of why it is so powerful is provided by The Economist (2022a) where they show how their forecasts of the 2022 French Presidential Election changed over time. We will return to shiny in Chapter 12.\nWe are going to make an interactive graph based on the “babynames” dataset from babynames (Wickham 2021a). First, we will build a static version (Figure E.5).\n\ntop_five_names_by_year &lt;-\n  babynames |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 5, by = c(year, sex))\n\ntop_five_names_by_year |&gt;\n  ggplot(aes(x = n, fill = sex)) +\n  geom_histogram(position = \"dodge\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    x = \"Babies with that name\",\n    y = \"Occurrences\",\n    fill = \"Sex\"\n  )\n\n\n\n\nFigure E.5: Popular baby names\n\n\n\n\nOne thing that we might be interested in is how the effect of the “bins” parameter shapes what we see. We might like to use interactivity to explore different values.\nTo get started, create a new shiny app (“File” -&gt; “New File” -&gt; “Shiny Web App”). Give it a name, such as “not_my_first_shiny” and then leave all the other options as the default. A new file “app.R” will open and we click “Run app” to see what it looks like.\nNow replace the content in that file, “app.R”, with the content below, and then again click “Run app”.\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  # Application title\n  titlePanel(\"Count of names for five most popular names each year.\"),\n\n  # Sidebar with a slider input for number of bins\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"number_of_bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n\n    # Show a plot of the generated distribution\n    mainPanel(plotOutput(\"distPlot\"))\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    # Draw the histogram with the specified number of bins\n    top_five_names_by_year |&gt;\n      ggplot(aes(x = n, fill = sex)) +\n      geom_histogram(position = \"dodge\", bins = input$number_of_bins) +\n      theme_minimal() +\n      scale_fill_brewer(palette = \"Set1\") +\n      labs(\n        x = \"Babies with that name\",\n        y = \"Occurrences\",\n        fill = \"Sex\"\n      )\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nWe have just build an interactive graph where the number of bins can be changed. It should look like Figure E.6.\n\n\n\nFigure E.6: Example of Shiny app where the user controls the number of bins"
  },
  {
    "objectID": "24-interaction.html#exercises",
    "href": "24-interaction.html#exercises",
    "title": "Online Appendix E — Interaction",
    "section": "E.6 Exercises",
    "text": "E.6 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Everyday a baby wakes at one of: 4am, 5am, or 6am, and wakes up a parent who has a choice of whether to have coffee or tea. Imagine you have daily data for a year about the time the baby wakes up and which drink the parent had. Please sketch what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation where each of the two variables are independent. After that, please simulate another situation where there is some relationship, of your choice, between the time the baby wakes up, and the choice of drink.\n(Acquire) Please describe a possible source of such a dataset.\n(Explore) Please use shiny to build an interactive version of the graph that you sketched.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\nBased on Lovelace, Nowosad, and Muenchow (2019), please explain in a paragraph or two, what is the difference between vector data and raster data in the context of geographic data?\nBased on Wickham (2021b), shiny uses:\n\nObject-oriented programming\nFunctional programming\nReactive programming\n\nIn a paragraph or two, why is it important to have a website?\nWhich function should we use to stop GitHub itself from trying to build our site instead of just serving it (pick one)?\n\nfile.create(\".nojekyll\")\nfile.remove(\".nojekyll\")\nfile.create(\".jekyll\")\nfile.remove(\".jekyll\")\n\nWhich argument to addMarkers() is used to specify the behavior that occurs after a marker is clicked (pick one)?\n\nlayerId\nicon\npopup\nlabel\n\n\n\n\nTutorial\nPlease obtain data on the ethnic origins and number of Holocaust victims killed at Auschwitz concentration camp. Then use shiny to create an interactive graph and an interactive table. These should show the number of people murdered by nationality/category and should allow the user to specify the groups they are interested in seeing data for. Publish them. Then, based on the themes brought up in Bouie (2022), discuss your work in at least two pages. Submit a PDF created using Quarto, and ensure that it contains a link to your app and the GitHub repo that contains all code and data.\n\n\n\n\nBensinger, Greg. 2020. “Google Redraws the Borders on Maps Depending on Who’s Looking.” The Washington Post, February. https://www.washingtonpost.com/technology/2020/02/14/google-maps-political-borders/.\n\n\nBouie, Jamelle. 2022. “We Still Can’t See American Slavery for What It Was.” The New York Times, January. https://www.nytimes.com/2022/01/28/opinion/slavery-voyages-data-sets.html.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. shiny: Web Application Framework for R. https://CRAN.R-project.org/package=shiny.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. leaflet: Create Interactive Web Maps with the JavaScript “Leaflet” Library. https://CRAN.R-project.org/package=leaflet.\n\n\nCooley, David. 2020. mapdeck: Interactive Maps Using “Mapbox GL JS” and “Deck.gl”. https://CRAN.R-project.org/package=mapdeck.\n\n\nda Silva, Natalia, Dianne Cook, and Eun-Kyung Lee. 2023. “Interactive graphics for visually diagnosing forest classifiers in R.” Computational Statistics, January. https://doi.org/10.1007/s00180-023-01323-x.\n\n\nFlynn, Michael. 2022. troopdata: Tools for Analyzing Cross-National Military Deployment and Basing Data. https://CRAN.R-project.org/package=troopdata.\n\n\nKahle, David, and Hadley Wickham. 2013. “ggmap: Spatial Visualization with ggplot2.” The R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman; Hall/CRC. https://geocompr.robinlovelace.net.\n\n\nMcQuire, Scott. 2019. “One Map to Rule Them All? Google Maps as Digital Technical Object.” Communication and the Public 4 (2): 150–65. https://doi.org/10.1177/2057047319850192.\n\n\nThe Economist. 2022a. “Will Emmanuel Macron Win a Second Term?” April. https://www.economist.com/interactive/france-2022/forecast.\n\n\n———. 2022b. “France’s Presidential Election: The Second Round in Detail,” April. https://www.economist.com/interactive/france-2022/results-round-two.\n\n\nWickham, Hadley. 2021a. babynames: US Baby Names 1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2021b. Mastering Shiny. 1st ed. O’Reilly Media. https://mastering-shiny.org.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis."
  },
  {
    "objectID": "25-datasheet.html",
    "href": "25-datasheet.html",
    "title": "Online Appendix F — Datasheets",
    "section": "",
    "text": "In Chapter 10 we introduced the notion of a datasheet and explained why they are so important. Here we provide an example of one that underpins Alexander and Hodgetts (2021).\nMotivation\n\nFor what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.\n\nThe dataset was created to enable analysis of Australian politicians. We were unable to find a publicly available dataset in a structured format that had the biographical and political information on Australian politicians that was needed for modelling.\n\nWho created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?\n\nRohan Alexander, while working at the Australian National University and the University of Toronto.\n\nWho funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\nNo direct funding was received for this project, but Rohan received a salary from University of Toronto.\n\nAny other comments?\n\nNo.\n\n\nComposition\n\nWhat do the instances that comprise the dataset represent (for example, documents, photos, people, countries)? Are there multiple types of instances (for example, movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.\n\nEach row of the main dataset is an individual, and these then link to other datasets where each row refers to various information about that person.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nA little more than 1700.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (for example, geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (for example, to cover a more diverse range of instances, because instances were withheld or unavailable).\n\nAll individuals elected or appointed to the Australian Federal Parliament are in the dataset.\n\nWhat data does each instance consist of? “Raw” data (for example, unprocessed text or images) or features? In either case, please provide a description.\n\nEach instance consists of biographical information such as birthdate, or political information, such as political party membership.\n\nIs there a label or target associated with each instance? If so, please provide a description.\n\nYes there is a unique key comprising the surname and year of birth, with a few individuals needing additional demarcation.\n\nIs any information missing from individual instances? If so, please provide a description, explaining why this information is missing (for example, because it was unavailable). This does not include intentionally removed information, but might include, for example, redacted text.\n\nBirthdate is not available in all cases, especially earlier in the dataset.\n\nAre relationships between individual instances made explicit (for example, users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit.\n\nYes, through the uniqueID.\n\nAre there recommended data splits (for example, training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.\n\nNo.\n\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.\n\nThere is some uncertainty about cabinet and ministries. For instance, different sources differ. There is also a little bit of uncertainty about birthdates.\n\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (for example, websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (that is, including the external resources as they existed at the time the dataset was created); c) are there any restrictions (for example, licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.\n\nSelf-contained.\n\nDoes the dataset contain data that might be considered confidential (for example, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals’ non-public communications)? If so, please provide a description.\n\nNo, all data were gathered from public sources.\n\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.\n\nNo.\n\nDoes the dataset identify any sub-populations (for example, by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.\n\nYes, age and gender.\n\nIs it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset? If so, please describe how.\n\nYes, individuals are identified by name.\n\nDoes the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.\n\nThe dataset contains sensitive information, such as political membership, however this is all public knowledge as they are federal politicians.\n\nAny other comments?\n\nNo.\n\n\nCollection process\n\nHow was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/derived from other data (for example, part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n\nThe data were gathered from the Australian Parliamentary Handbook in the first instance, and this was augmented with information from other parliaments, especially Victoria and New South Wales, and Wikipedia.\n\nWhat mechanisms or procedures were used to collect the data (for example, hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?\n\nScraping and parsing using R.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (for example, deterministic, probabilistic with specific sampling probabilities)?\n\nThe dataset is not a sample.\n\nWho was involved in the data collection process (for example, students, crowdworkers, contractors) and how were they compensated (for example, how much were crowdworkers paid)?\n\nRohan Alexander. Paid as a post-doc and an assistant professor, although this was not tied to this specific project.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (for example, recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\n\nThree years, and then updated from time to time.\n\nWere any ethical review processes conducted (for example, by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\n\nNo.\n\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (for example, websites)?\n\nThird parties in almost all cases.\n\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\n\nNo.\n\nDid the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\n\nNo.\n\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\n\nConsent was not obtained.\n\nHas an analysis of the potential impact of the dataset and its use on data subjects (for example, a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\n\nNo.\n\nAny other comments?\n\nNo.\n\n\nPreprocessing/cleaning/labeling\n\nWas any preprocessing/cleaning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.\n\nYes cleaning of the data was done.\n\nWas the “raw” data saved in addition to the preprocessed/cleaned/labeled data (for example, to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data.\n\nIn general, no. The scripts that got the data from the parliamentary handbook to CSV are not available. There are scripts that go through Wikipedia and check things and these are available.\n\nIs the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.\n\nR was used.\n\nAny other comments?\n\nNo\n\n\nUses\n\nHas the dataset been used for any tasks already? If so, please provide a description.\n\nYes, a few papers about Australian politics, for instance, https://arxiv.org/abs/2111.09299.\n\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\n\nNo\n\nWhat (other) tasks could the dataset be used for?\n\nLinking with elections would be interesting.\n\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (for example, stereotyping, quality of service issues) or other risks or harms (for example, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?\n\nNo.\n\nAre there tasks for which the dataset should not be used? If so, please provide a description.\n\nNo.\n\nAny other comments?\n\nNo.\n\n\nDistribution\n\nWill the dataset be distributed to third parties outside of the entity (for example, company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\n\nThe dataset is available through GitHub.\n\nHow will the dataset be distributed (for example, tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\n\nGitHub for now, and eventually a deposit.\n\nWhen will the dataset be distributed?\n\nThe dataset is available now.\n\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/ or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\nNo. MIT license.\n\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\n\nNone that are known.\n\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\n\nNone that are known.\n\nAny other comments?\n\nNo.\n\n\nMaintenance\n\nWho will be supporting/hosting/maintaining the dataset?\n\nRohan Alexander\n\nHow can the owner/curator/manager of the dataset be contacted (for example, email address)?\n\nrohan.alexander@utoronto\n\nIs there an erratum? If so, please provide a link or other access point.\n\nNo, the dataset is just updated.\n\nWill the dataset be updated (for example, to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (for example, mailing list, GitHub)?\n\nYes, roughly quarterly.\n\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (for example, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\n\nNo.\n\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.\n\nNo the dataset is just updated. Although a history is available through GitHub.\n\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.\n\nPull request on GitHub.\n\nAny other comments?\n\nNo\n\n\n\n\n\n\nAlexander, Rohan, and Paul Hodgetts. 2021. AustralianPoliticians: Provides Datasets About Australian Politicians. https://CRAN.R-project.org/package=AustralianPoliticians."
  },
  {
    "objectID": "26-sql.html#introduction",
    "href": "26-sql.html#introduction",
    "title": "Online Appendix G — SQL essentials",
    "section": "G.1 Introduction",
    "text": "G.1 Introduction\nStructured Query Language (SQL) (“see-quell” or “S.Q.L.”) is used with relational databases. A relational database is a collection of at least one table, and a table is just some data organized into rows and columns. If there is more than one table in the database, then there should be some column that links them. An example is the AustralianPoliticians datasets that are used in Appendix A. Using SQL feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that, by convention, commands are written in upper case. Another is that line spaces mean nothing: include them or do not, but always end a SQL command in a semicolon;\nSQL was developed in the 1970s at IBM. SQL is an especially popular way of working with data. There are many “flavors” of SQL, including both closed and open options. Here we introduce SQLite, which is open source, and pre-installed on Macs. Windows users can install it from here.\nAdvanced SQL users do a lot with it alone, but even just having a working knowledge of SQL increases the number of datasets that we can access. A working knowledge of SQL is especially useful for our efficiency because a large number of datasets are stored on SQL servers, and being able to get data from them ourselves is handy.\nWe could use SQL within RStudio, especially drawing on DBI (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller 2022). Although given the demand for SQL skills, independent of demand for R skills, it may be a better idea, from a career perspective to have a working knowledge of it that is independent of RStudio. We can consider many SQL commands as straightforward variants of the dplyr verbs that we have used throughout this book. Indeed, if we wanted to stay within R, then dbplyr (Wickham, Girlich, and Ruiz 2022) would explicitly allow us to use dplyr functions and would then automatically translate them into SQL. Having used mutate(), filter(), and left_join() in the tidyverse means that many of the core SQL commands will be familiar. That means that the main difficulty will be getting on top of the order of operations because SQL can be pedantic."
  },
  {
    "objectID": "26-sql.html#getting-started",
    "href": "26-sql.html#getting-started",
    "title": "Online Appendix G — SQL essentials",
    "section": "G.2 Getting started",
    "text": "G.2 Getting started\nTo get started with SQL, download DB Browser for SQLite (DB4S), which is free and open source, and open it (Figure G.1).\n\n\n\nFigure G.1: Opening DB Browser for SQLite\n\n\nDownload “AustralianPoliticians.db” here and then open it with “Open Database” and navigate to where you downloaded the database.\nThere are three key SQL commands that we now cover: SELECT, FROM, and WHERE. SELECT allows us to specify particular columns of the data, and we can consider SELECT in a similar way to select(). In the same way that we need to specify a dataset with select() and did that using a pipe operator, we specify a dataset with FROM. For instance, we could open “Execute SQL”, and then type the following, and click “Execute”.\n\nSELECT\n    surname   \nFROM\n    politicians;\n\nThe result is that we obtain the column of surnames. We could select multiple columns by separating them with commas, or all of them by using an asterisk, although this is not best practice because if the dataset were to change without us knowing then our result would differ.\n\nSELECT\n    uniqueID,\n    surname   \nFROM\n    politicians;\n\n\nSELECT\n    *\nFROM\n    politicians;\n\nAnd, finally, if there were repeated rows, then we could just look at the unique ones using DISTINCT, in a similar way to distinct().\n\nSELECT\n    DISTINCT surname   \nFROM\n    politicians;\n\nSo far we have used SELECT along with FROM. The third command that is commonly used is WHERE, and this will allow us to focus on particular rows, in a similar way to filter().\n\nSELECT\n    uniqueID,\n    surname,\n    firstName   \nFROM\n    politicians     \nWHERE\n    firstName = \"Myles\";\n\nAll the usual logical operators are fine with WHERE, such as “=”, “!=”, “&gt;”, “&lt;”, “&gt;=”, and “&lt;=”. We could combine conditions using AND and OR.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName   \nFROM\n    politicians     \nWHERE\n    firstName = \"Myles\" \n    OR firstName = \"Ruth\";\n\nIf we have a query that gave a lot of results, then we could limit the number of them with LIMIT.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName        \nFROM\n    politicians          \nWHERE\n    firstName = \"Robert\"       LIMIT 5;\n\nAnd we could specify the order of the results with ORDER.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName \nFROM\n    politicians \nWHERE\n    firstName = \"Robert\" \nORDER BY\n    surname DESC;\n\nSee the rows that are pretty close to a criteria:\n\nSELECT\n    uniqueID,\n    surname,\n    firstName      \nFROM\n    politicians      \nWHERE\n    firstName LIKE \"Ma__\";\n\nThe “_” above is a wildcard that matches to any character. This provides results that include “Mary” and “Mark”. LIKE is not case-sensitive: “Ma__” and “ma__” both return the same results.\nFocusing on missing data is possible using “NULL” or “NOT NULL”.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName,\n    comment\nFROM\n    politicians      \nWHERE\n    comment     IS NULL;\n\nAn ordering is applied to number, date, and text fields that means we can use BETWEEN on all those, not just numeric. For instance, we could look for all surnames that start with a letter between X and Z (not including Z).\n\nSELECT\n    uniqueID,\n    surname,\n    firstName\nFROM\n    politicians      \nWHERE\n    surname     BETWEEN \"X\" AND \"Z\";\n\nUsing WHERE with a numeric variable means that BETWEEN is inclusive, compared with the example with letters which is not.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName,\n    birthYear\nFROM\n    politicians      \nWHERE\n    birthYear     BETWEEN 1980 AND 1990;\n\nIn addition to providing us with dataset observations that match what we asked for, we can modify the dataset. For instance, we could edit a value using UPDATE and SET.\n\nUPDATE\n    politicians   \nSET\n    displayName = \"John Gilbert Alexander\"\nWHERE\n    uniqueID = \"Alexander1951\";\n\nWe can integrate if-else logic with CASE and ELSE. For instance, we add a column called “wasTreasurer”, which is “Yes” in the case of “Josh Frydenberg”, and “No” in the case of “Kevin Rudd”, and “Unsure” for all other cases.\n\nSELECT\n    uniqueID,\n    surname,\n    firstName,\n    birthYear,\n    CASE                  \n        WHEN uniqueID = \"Frydenberg1971\" THEN \"Yes\"                  \n        WHEN surname = \"Rudd\" THEN \"No\"                  \n        ELSE \"Unsure\"            \n    END AS \"wasTreasurer\"        \nFROM\n    politicians;\n\nWe can create summary statistics using commands such as COUNT, SUM, MAX, MIN, AVG, and ROUND in the place of summarize(). COUNT counts the number of rows that are not empty for some column by passing the column name, and this is similarly how MIN, etc, work.\n\nSELECT\n    COUNT(uniqueID)   \nFROM\n    politicians;\n\n\nSELECT\n    MIN(birthYear)   \nFROM\n    politicians;\n\nWe can get results based on different groups in our dataset using GROUP BY, in a similar manner to group_by in R.\n\nSELECT\n    COUNT(uniqueID)   \nFROM\n    politicians     \nGROUP BY\n    gender;\n\nAnd finally, we can combine two tables using LEFT JOIN. We need to be careful to specify the matching columns using dot notation.\n\nSELECT\n    politicians.uniqueID,\n    politicians.firstName,\n    politicians.surname,\n    party.partySimplifiedName   \nFROM\n    politicians \nLEFT JOIN\n    party     \n        ON politicians.uniqueID = party.uniqueID;\n\nAs SQL is not our focus we have only provided a brief overview of some essential commands. From a career perspective you should develop a comfort with SQL. It is so integrated into data science that it would be “difficult to get too far without it” (Robinson and Nolis 2020, 8) and that “almost any” data science interview will include questions about SQL (Robinson and Nolis 2020, 110)."
  },
  {
    "objectID": "26-sql.html#exercises",
    "href": "26-sql.html#exercises",
    "title": "Online Appendix G — SQL essentials",
    "section": "G.3 Exercises",
    "text": "G.3 Exercises\n\nQuestions\n\nPlease submit a screenshot showing you got at least 70 per cent in the free w3school SQL Quiz available here. You may like to go through their tutorial here. But the SQL content in this chapter is sufficient to get 70 per cent. Please include the time and date in the screenshot i.e. take a screenshot of your whole screen, not just the browser.\n\n\n\n\n\nChamberlin, Donald. 2012. “Early History of SQL.” IEEE Annals of the History of Computing 34 (4): 78–82. https://doi.org/10.1109/mahc.2012.61.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and Kirill Müller. 2022. DBI: R Database Interface. https://CRAN.R-project.org/package=DBI.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data Science. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2022. dbplyr: A “dplyr” Back End for Databases. https://CRAN.R-project.org/package=dbplyr."
  },
  {
    "objectID": "27-prediction.html#introduction",
    "href": "27-prediction.html#introduction",
    "title": "Online Appendix H — Prediction",
    "section": "H.1 Introduction",
    "text": "H.1 Introduction\nAs discussed in Chapter 12, models tend to be focused on either inference or prediction. There are, in general, different cultures depending on your focus. One reason for this is a different emphasis of causality, introduced in Chapter 14. I am talking very generally here, but often with inference we will be very concerned about causality, and with prediction we will be less so. That means the quality of our predictions will break-down when conditions are quite different from what our model was expecting—but how do we know when conditions are sufficiently different for us to be worried?\nAnother way for this cultural difference is because the rise of data science and machine learning in particular, has been substantially driven by the development of models in Python by people with a computer science or engineering background. This means there is an additional vocabulary difference because much of inference came out of statistics. Again, this is all speaking very broadly.\nIn this chapter, I begin with a focus on prediction using the R approach of tidymodels. I then introduce one of those grey areas—and the reason that I have been trying to speak broadly—lasso regression. That was developed by statisticians, but is mostly used for prediction. Finally, I introduce all of this in Python."
  },
  {
    "objectID": "27-prediction.html#prediction-with-tidymodels",
    "href": "27-prediction.html#prediction-with-tidymodels",
    "title": "Online Appendix H — Prediction",
    "section": "H.2 Prediction with tidymodels",
    "text": "H.2 Prediction with tidymodels\n\nH.2.1 Linear models\nWhen we are focused on prediction, we will often want to fit many models. One way to do this is to copy and paste code many times. This is okay, and it is the way that most people get started but it is prone to making errors that are hard to find. A better approach will:\n\nscale more easily;\nenable us to think carefully about over-fitting; and\nadd model evaluation.\n\nThe use of tidymodels (Kuhn and Wickham 2020) satisfies these criteria by providing a coherent grammar that allows us to easily fit a variety of models. Like the tidyverse, it is a package of packages.\nBy way of illustration, we want to estimate the following model for the simulated running data:\n\\[\n\\begin{aligned}\ny_i | \\mu_i &\\sim \\mbox{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 +\\beta_1x_i\n\\end{aligned}\n\\]\nwhere \\(y_i\\) refers to the marathon time of some individual \\(i\\) and \\(x_i\\) refers to their five-kilometer time. Here we say that the marathon time of some individual \\(i\\) is normally distributed with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\), where the mean depends on two parameters \\(\\beta_0\\) and \\(\\beta_1\\) and their five-kilometer time. Here “~” means “is distributed as”. We use this slightly different notation from earlier to be more explicit about the distributions being used, but this model is equivalent to \\(y_i=\\beta_0+\\beta_1 x_i + \\epsilon_i\\), where \\(\\epsilon\\) is normally distributed.\nAs we are focused on prediction, we are worried about over-fitting our data, which would limit our ability to make claims about other datasets. One way to partially address this is to split our dataset in two using initial_split().\n\nsim_run_data &lt;- \n  read_parquet(file = \"outputs/data/running_data.parquet\")\n\nset.seed(853)\n\nsim_run_data_split &lt;-\n  initial_split(\n    data = sim_run_data,\n    prop = 0.80\n  )\n\nsim_run_data_split\n\n&lt;Training/Testing/Total&gt;\n&lt;160/40/200&gt;\n\n\nHaving split the data, we then create training and test datasets with training() and testing().\n\nsim_run_data_train &lt;- training(sim_run_data_split)\n\nsim_run_data_test &lt;- testing(sim_run_data_split)\n\nWe have placed 80 per cent of our dataset into the training dataset. We will use that to estimate the parameters of our model. We have kept the remaining 20 per cent of it back, and we will use that to evaluate our model. Why might we do this? Our concern is the bias-variance trade-off, which haunts all aspects of modeling. We are concerned that our results may be too particular to the dataset that we have, such that they are not applicable to other datasets. To take an extreme example, consider a dataset with ten observations. We could come up with a model that perfectly hits those observations. But when we took that model to other datasets, even those generated by the same underlying process, it would not be accurate.\nOne way to deal with this concern is to split the data in this way. We use the training data to inform our estimates of the coefficients, and then use the test data to evaluate the model. A model that too closely matched the data in the training data would not do well in the test data, because it would be too specific to the training data. The use of this test-training split enables us the opportunity to build an appropriate model.\nIt is more difficult to do this separation appropriately than one might initially think. We want to avoid the situation where aspects of the test dataset are present in the training dataset because this inappropriately telegraphs what is about to happen. This is called data leakage. But if we consider data cleaning and preparation, which likely involves the entire dataset, it may be that some features of each are influencing each other. Kapoor and Narayanan (2022) find extensive data leakage in applications of machine learning that could invalidate much research.\nTo use tidymodels we first specify that we are interested in linear regression with linear_reg(). We then specify the type of linear regression, in this case multiple linear regression, with set_engine(). Finally, we specify the model with fit(). While this requires considerably more infrastructure than the base R approach detailed above, the advantage of this approach is that it can be used to fit many models; we have created a model factory, as it were.\n\nsim_run_data_first_model_tidymodels &lt;-\n  linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  fit(\n    marathon_time ~ five_km_time + was_raining,\n    data = sim_run_data_train\n  )\n\nThe estimated coefficients are summarized in the first column of Table 12.4. For instance, we find that on average in our dataset, five-kilometer run times that are one minute longer are associated with marathon times that are about eight minutes longer.\n\n\nH.2.2 Logistic regression\nWe can also use tidymodels for logistic regression problems. To accomplish this, we first need to change the class of our dependent variable into a factor because this is required for classification models.\n\nweek_or_weekday &lt;- \n  read_parquet(file = \"outputs/data/week_or_weekday.parquet\")\n\nset.seed(853)\n\nweek_or_weekday &lt;-\n  week_or_weekday |&gt;\n  mutate(is_weekday = as_factor(is_weekday))\n\nweek_or_weekday_split &lt;- initial_split(week_or_weekday, prop = 0.80)\nweek_or_weekday_train &lt;- training(week_or_weekday_split)\nweek_or_weekday_test &lt;- testing(week_or_weekday_split)\n\nweek_or_weekday_tidymodels &lt;-\n  logistic_reg(mode = \"classification\") |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(\n    is_weekday ~ number_of_cars,\n    data = week_or_weekday_train\n  )\n\nAs before, we can make a graph of the actual results compared with our estimates. But one nice aspect of this is that we could use our test dataset to evaluate our model’s prediction ability more thoroughly, for instance through a confusion matrix, which specifies the count of each prediction by what the truth was. We find that the model does well on the held-out dataset. There were 90 observations where the model predicted it was a weekday, and it was actually a weekday, and 95 where the model predicted it was a weekend, and it was a weekend. It was wrong for 15 observations, and these were split across seven where it predicted a weekday, but it was a weekend, and eight where it was the opposite case.1\n\nweek_or_weekday_tidymodels |&gt;\n  predict(new_data = week_or_weekday_test) |&gt;\n  cbind(week_or_weekday_test) |&gt;\n  conf_mat(truth = is_weekday, estimate = .pred_class)\n\n          Truth\nPrediction  0  1\n         0 90  8\n         1  7 95\n\n\n\nH.2.2.1 US political support\nOne approach is to use tidymodels to build a prediction-focused logistic regression model in the same way as before, i.e. a validation set approach (James et al. [2013] 2021, 176). In this case, the probability will be that of voting for Biden.\n\nces2020 &lt;- \n  read_parquet(file = \"outputs/data/ces2020.parquet\")\n\nset.seed(853)\n\nces2020_split &lt;- initial_split(ces2020, prop = 0.80)\nces2020_train &lt;- training(ces2020_split)\nces2020_test &lt;- testing(ces2020_split)\n\nces_tidymodels &lt;-\n  logistic_reg(mode = \"classification\") |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(\n    voted_for ~ gender + education,\n    data = ces2020_train\n  )\n\nces_tidymodels\n\nparsnip model object\n\n\nCall:  stats::glm(formula = voted_for ~ gender + education, family = stats::binomial, \n    data = data)\n\nCoefficients:\n                  (Intercept)                     genderMale  \n                       0.2157                        -0.4697  \neducationHigh school graduate          educationSome college  \n                      -0.1857                         0.3502  \n              education2-year                education4-year  \n                       0.2311                         0.6700  \n           educationPost-grad  \n                       0.9898  \n\nDegrees of Freedom: 34842 Total (i.e. Null);  34836 Residual\nNull Deviance:      47000 \nResidual Deviance: 45430    AIC: 45440\n\n\nAnd then evaluate it on the test set. It appears as though the model is having difficulty identifying Trump supporters.\n\nces_tidymodels |&gt;\n  predict(new_data = ces2020_test) |&gt;\n  cbind(ces2020_test) |&gt;\n  conf_mat(truth = voted_for, estimate = .pred_class)\n\n          Truth\nPrediction Trump Biden\n     Trump   656   519\n     Biden  2834  4702\n\n\nWhen we introduced tidymodels, we discussed the importance of randomly constructing training and test sets. We use the training dataset to estimate parameters, and then evaluate the model on the test set. It is natural to ask why we should be subject to the whims of randomness and whether we are making the most of our data. For instance, what if a good model is poorly evaluated because of some random inclusion in the test set? Further, what if we do not have a large test set?\nOne commonly used resampling method that goes some way to addressing this is \\(k\\)-fold cross-validation. In this approach we create \\(k\\) different samples, or “folds”, from the dataset without replacement. We then fit the model to the first \\(k-1\\) folds, and evaluate it on the last fold. We do this \\(k\\) times, once for every fold, such that every observation will be used for training \\(k-1\\) times and for testing once. The \\(k\\)-fold cross-validation estimate is then the average mean squared error (James et al. [2013] 2021, 181). For instance, vfold_cv() from tidymodels can be used to create, say, ten folds.\n\nset.seed(853)\n\nces2020_10_folds &lt;- vfold_cv(ces2020, v = 10)\n\nThe model can then be fit across the different combinations of folds with fit_resamples(). In this case, the model will be fit ten times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH.2.3 Poisson regression\nWe can use tidymodels to estimate Poisson models with poissonreg (Kuhn and Frick 2022) (Table 13.4).\n\ncount_of_A &lt;- \n  read_parquet(file = \"outputs/data/count_of_A.parquet\")\n\nset.seed(853)\n\ncount_of_A_split &lt;-\n  initial_split(count_of_A, prop = 0.80)\ncount_of_A_train &lt;- training(count_of_A_split)\ncount_of_A_test &lt;- testing(count_of_A_split)\n\ngrades_tidymodels &lt;-\n  poisson_reg(mode = \"regression\") |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(\n    number_of_As ~ department,\n    data = count_of_A_train\n  )\n\nThe results of this estimation are in the second column of Table 13.4. They are similar to the estimates from glm(), but the number of observations is less because of the split."
  },
  {
    "objectID": "27-prediction.html#lasso-regression",
    "href": "27-prediction.html#lasso-regression",
    "title": "Online Appendix H — Prediction",
    "section": "H.3 Lasso regression",
    "text": "H.3 Lasso regression\n\n\n\n\n\n\n\nShoulders of giants\n\n\n\nDr Robert Tibshirani is Professor in the Departments of Statistics and Biomedical Data Science at Stanford University. After earning a PhD in Statistics from Stanford University in 1981, he joined the University of Toronto as an assistant professor. He was promoted to full professor in 1994 and moved to Stanford in 1998. He made fundamental contributions including GAMs, mentioned above, and lasso regression, which is a way of automated variable selection. He is an author of James et al. ([2013] 2021). He was awarded the COPSS Presidents’ Award in 1996 and was appointed a Fellow of the Royal Society in 2019."
  },
  {
    "objectID": "27-prediction.html#prediction-with-python",
    "href": "27-prediction.html#prediction-with-python",
    "title": "Online Appendix H — Prediction",
    "section": "H.4 Prediction with Python",
    "text": "H.4 Prediction with Python\n\nH.4.1 Setup\nWe will use Python within VSCode, which is a free IDE from Microsoft that you can download here. You then install the Quarto and Python extensions.\n\n\nH.4.2 Data\nRead in data using parquet.\nManipulate using pandas\n\n\nH.4.3 Model\n\nH.4.3.1 scikit-learn\n\n\nH.4.3.2 TensorFlow"
  },
  {
    "objectID": "27-prediction.html#exercises",
    "href": "27-prediction.html#exercises",
    "title": "Online Appendix H — Prediction",
    "section": "H.5 Exercises",
    "text": "H.5 Exercises\n\nScales\n\n(Plan) Consider the following scenario: Each day for a year your uncle and you play darts. Each round consists of throwing three darts each. At the end of each round you add the points that your three darts hit. So if you hit 3, 5, and 10, then your total score for that round is 18. Your uncle is somewhat benevolent, and if you hit a number less than 5, he pretends not to see that, allowing you the chance to re-throw that dart. Pretend that each day you play 15 rounds. Please sketch out what that dataset could look like and then sketch a graph that you could build to show all observations.\n(Simulate) Please further consider the scenario described and simulate the situation. Compare your uncle’s total with your total if you didn’t get the chance to re-throw, and the total that actually end up with. Please include at least ten tests based on the simulated data.\n(Acquire) Please describe one possible source of such a dataset (or an equivalent sport or situation of interest to you).\n(Explore) Please use ggplot2 to build the graph that you sketched. Then use rstanarm to build a model of who wins.\n(Communicate) Please write two paragraphs about what you did.\n\n\n\nQuestions\n\n\nTutorial\n\n\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and Hadley Wickham. 2022. rsample: General Resampling Infrastructure. https://CRAN.R-project.org/package=rsample.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. (2013) 2021. An Introduction to Statistical Learning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nKapoor, Sayash, and Arvind Narayanan. 2022. “Leakage and the Reproducibility Crisis in ML-Based Science.” arXiv. https://doi.org/10.48550/ARXIV.2207.07048.\n\n\nKuhn, Max. 2022. tune: Tidy Tuning Tools. https://CRAN.R-project.org/package=tune.\n\n\nKuhn, Max, and Hannah Frick. 2022. poissonreg: Model Wrappers for Poisson Regression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, and Davis Vaughan. 2022. parsnip: A Common API to Modeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2022. yardstick: Tidy Characterizations of Model Performance. https://CRAN.R-project.org/package=yardstick.\n\n\nKuhn, Max, and Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n\n\n———. 2022. recipes: Preprocessing and Feature Engineering Steps for Modeling. https://CRAN.R-project.org/package=recipes.\n\n\nMcKinney, Wes. (2011) 2022. Python for Data Analysis. 3rd ed. https://wesmckinney.com/book/.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and Apache Arrow. 2023. arrow: Integration to Apache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "27-prediction.html#footnotes",
    "href": "27-prediction.html#footnotes",
    "title": "Online Appendix H — Prediction",
    "section": "",
    "text": "STUDENTS PROBABLY WON’T UNDERSTAND THAT IMPICIT IN THE PREDICTED LABEL IS AN OPERATING THRESHOLD, I WOULD HAVE AT LEAST ONE SENTENCE EXPLAINING THAT THIS IS A BUILT IN ASSUMPION IN THE PREDICT() METHOD THAT CAN BE ADJUSTED AND ADJUSTING THIS WILL IMPACT SENSITIVITY VS PRECISION WHICH CAN BE DISCUSSED LATER↩︎"
  },
  {
    "objectID": "28-deploy.html#introduction",
    "href": "28-deploy.html#introduction",
    "title": "Online Appendix I — Production",
    "section": "I.1 Introduction",
    "text": "I.1 Introduction\nHaving done the work to develop a dataset and explore it with a model that we are confident can be used, we may wish to enable this to be used more widely than just our own computer. There are a variety of ways of doing this, including:\n\nusing the cloud;\ncreating R packages;\nmaking shiny applications; and\nusing plumber to create an API.\n\nThe general idea here is that we need to know, and allow others to come to trust, the whole workflow. That is what our approach to this point brings. After this, then we may like to use our model more broadly. Say we have been able to scrape some data from a website, bring some order to that chaos, make some charts, appropriately model it, and write this all up. In most academic settings that is more than enough. But in many industry settings we would like to use the model to do something. For instance, setting up a website that allows a model to be used to generate an insurance quote given several inputs.\nIn this chapter, we begin by moving our compute from our local computer to the cloud. We then describe the use of R packages and Shiny for sharing models. That works well, but in some settings other users may like to interact with our model in ways that we are not focused on. One way to allow this is to make our results available to other computers, and for that we will want to make an API. Hence, we introduce plumber (Schloerke and Allen 2022), which is a way of creating APIs."
  },
  {
    "objectID": "28-deploy.html#amazon-web-services",
    "href": "28-deploy.html#amazon-web-services",
    "title": "Online Appendix I — Production",
    "section": "I.2 Amazon Web Services",
    "text": "I.2 Amazon Web Services\nApocryphally the cloud is just another name for someone else’s computer. And while that is true to various degrees, for our purposes that is enough. Learning to use someone else’s computer can be great for a number of reasons including:\n\nScalability: It can be quite expensive to buy a new computer, especially if we only need it to run something every now and then, but by using the cloud, we can just rent for a few hours or days. This allows use to amortize this cost and work out what we actually need before committing to a purchase. It also allows us to easily increase or decrease the compute scale if we suddenly have a substantial increase in demand.\nPortability: If we can shift our analysis workflow from a local computer to the cloud, then that suggests that we are likely doing good things in terms of reproducibility and portability. At the very least, code can run both locally and on the cloud, which is a big step in terms of reproducibility.\nSet-and-forget: If we are doing something that will take a while, then it can be great to not have to worry about our own computer needing to run overnight. Additionally, on many cloud options, open-source statistical software, such as R and Python, is either already available, or relatively easy to set up.\n\nThat said, there are downsides, including:\n\nCost: While most cloud options are cheap, they are rarely free. To provide an idea of cost, using a well-featured AWS instance for a few days may end up being a few dollars. It is also easy to accidentally forget about something, and generate unexpectedly large bills, especially initially.\nPublic: It can be easy to make mistakes and accidentally make everything public.\nTime: It takes time to get set up and comfortable on the cloud.\n\nWhen we use the cloud, we are typically running code on a “virtual machine” (VM). This is an allocation that is part of a larger collection of computers that has been designed to act like a computer with specific features. For instance, we may specify that our virtual machine has, say, 8GB RAM, 128GB storage, and 4 CPUs. The VM would then act like a computer with those specifications. The cost to use cloud options increases based on the VM specifications.\nIn a sense, we started with a cloud option, through our initial recommendation in Chapter 2 of using Posit Cloud, before we moved to our local computer in Appendix A. That cloud option was specifically designed for beginners. We will now introduce a more general cloud option: Amazon Web Services (AWS). Often a particular business will use a particular cloud option, such as Google, AWS, or Azure, but developing familiarity with one will make the use of the others easier.\nAmazon Web Services is a cloud service from Amazon. To get started we need to create an AWS Developer account here (Figure I.1 (a)).\n\n\n\n\n\n\n\n(a) AWS Developer website\n\n\n\n\n\n\n\n(b) AWS Developer console\n\n\n\n\n\n\n\n\n\n(c) Launching an AWSinstance\n\n\n\n\n\n\n\n(d) Establishing a key pair\n\n\n\n\nFigure I.1: Overview of getting Amazon AWS set up\n\n\nAfter we have created an account, we need to select a region where the computer that we will access is located. After this, we want to “Launch a virtual machine” with EC2 (Figure I.1 (b)).\nThe first step is to choose an Amazon Machine Image (AMI). This provides the details of the computer that you will be using. For instance, a local computer may be a MacBook running Monterey. Louis Aslett provides AMIs that are already set up with RStudio and much else here. We can either search for the AMI of the region that we registered for, or click on the relevant link on Aslett’s website. For instance, to use the AMI set-up for the Canadian central region we search for “ami-0bdd24fd36f07b638”. The benefit of using these AMIs is that they are set up specifically for RStudio, but the trade-off is that they are a little outdated, as they were compiled in August 2020.\nIn the next step we can choose how powerful the computer will be. The free tier is a basic computer, but we can choose better ones when we need them. At this point we can pretty much just launch the instance (Figure I.1 (c)). If we start using AWS more seriously, then we could go back and select different options, especially around the security of the account. AWS relies on key pairs. And so, we will need to create a Privacy Enhanced Mail (PEM) and save it locally (Figure I.1 (d)). We can then launch the instance.\nAfter a few minutes, the instance will be running. We can use it by pasting the “public DNS” into a browser. The username is “rstudio” and the password is the instance ID.\nWe should have RStudio running, which is exciting. The first thing to do is probably to change the default password using the instructions in the instance.\nWe do not need to install, say, the tidyverse, instead we can just call the library and keep going. This is because this AMI comes with many packages already installed. We can see the list of packages that are installed with installed.packages(). For instance, rstan is already installed, and we could set up an instance with GPUs if we needed.\nPerhaps as important as being able to start a AWS instance is being able to stop it (so that we do not get billed). The free tier is useful, but we do need to turn it off. To stop an instance, in the AWS instances page, select it, then “Actions -&gt; Instance State -&gt; Terminate”."
  },
  {
    "objectID": "28-deploy.html#plumber-and-model-apis",
    "href": "28-deploy.html#plumber-and-model-apis",
    "title": "Online Appendix I — Production",
    "section": "I.3 Plumber and model APIs",
    "text": "I.3 Plumber and model APIs\nThe general idea behind the plumber package (Schloerke and Allen 2022) is that we can train a model and make it available via an API that we can call when we want a forecast. Recall in Chapter 7 that we informally defined an API in the context of data gathering as a website that was set-up for another computer to access it, rather than a person. Here, we broaden that to enable data to encompass a model.\nJust to get something working, let us make a function that returns “Hello Toronto” regardless of the output. Open a new R file, add the following, and then save it as “plumber.R” (you may need to install the plumber package if you have not done that yet).\n\n#* @get /print_toronto\nprint_toronto &lt;- function() {\n  result &lt;- \"Hello Toronto\"\n  return(result)\n}\n\nAfter that is saved, in the top right of the editor you should get a button to “Run API”. Click that, and your API should load. It will be a “Swagger” application, which provides a GUI around our API. Expand the GET method, and then click “Try it out” and “Execute”. In the response body, you should get “Hello Toronto”.\nTo more closely reflect the fact that this is an API designed for computers, you can copy/paste the “Request URL” into a browser and it should return “Hello Toronto”.\n\nI.3.0.1 Local model\nNow, we are going to update the API so that it serves a model output, given some input. This follows Buhr (2017).\nAt this point, we should start a new R Project. To get started, let us simulate some data and then train a model on it. In this case we are interested in forecasting how long a baby may sleep overnight, given we know how long they slept during their afternoon nap.\n\nset.seed(853)\n\nnumber_of_observations &lt;- 1000\n\nbaby_sleep &lt;-\n  tibble(\n    afternoon_nap_length = rnorm(number_of_observations, 120, 5) |&gt; abs(),\n    noise = rnorm(number_of_observations, 0, 120),\n    night_sleep_length = afternoon_nap_length * 4 + noise,\n  )\n\nbaby_sleep |&gt;\n  ggplot(aes(x = afternoon_nap_length, y = night_sleep_length)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    x = \"Baby's afternoon nap length (minutes)\",\n    y = \"Baby's overnight sleep length (minutes)\"\n  ) +\n  theme_classic()\n\n\n\n\nLet us now use tidymodels to quickly make a model.\n\nset.seed(853)\n\nbaby_sleep_split &lt;- initial_split(baby_sleep, prop = 0.80)\nbaby_sleep_train &lt;- training(baby_sleep_split)\nbaby_sleep_test &lt;- testing(baby_sleep_split)\n\nmodel &lt;-\n  linear_reg() |&gt;\n  set_engine(engine = \"lm\") |&gt;\n  fit(\n    night_sleep_length ~ afternoon_nap_length,\n    data = baby_sleep_train\n  )\n\nwrite_rds(x = model, file = \"baby_sleep.rds\")\n\nAt this point, we have a model. One difference from what you might be used to is that we have saved the model as an “.rds” file. We are going to read that in.\nNow that we have our model we want to put that into a file that we will use the API to access, again called “plumber.R”. And we also want a file that sets up the API, called “server.R”. Make an R script called “server.R” and add the following content:\n\nlibrary(plumber)\n\nserve_model &lt;- plumb(\"plumber.R\")\nserve_model$run(port = 8000)\n\nThen in “plumber.R” add the following content:\n\nlibrary(plumber)\nlibrary(tidyverse)\n\nmodel &lt;- readRDS(\"baby_sleep.rds\")\n\nversion_number &lt;- \"0.0.1\"\n\nvariables &lt;-\n  list(\n    afternoon_nap_length = \"A value in minutes, likely between 0 and 240.\",\n    night_sleep_length = \"A forecast, in minutes, likely between 0 and 1000.\"\n  )\n\n#* @param afternoon_nap_length\n#* @get /survival\npredict_sleep &lt;- function(afternoon_nap_length = 0) {\n  afternoon_nap_length &lt;- as.integer(afternoon_nap_length)\n\n  payload &lt;- data.frame(afternoon_nap_length = afternoon_nap_length)\n\n  prediction &lt;- predict(model, payload)\n\n  result &lt;- list(\n    input = list(payload),\n    response = list(\"estimated_night_sleep\" = prediction),\n    status = 200,\n    model_version = version_number\n  )\n\n  return(result)\n}\n\nAgain, after we save the “plumber.R” file we should have an option to “Run API”. Click that and you can try out the API locally in the same way as before. In this case, click “Try It Out” and then input an afternoon nap length in minutes. The response body will contain the prediction based on the data and model we set up.\n\n\nI.3.0.2 Cloud model\nTo this point, we have got an API working on our own machine, but what we really want to do is to get it working on a computer such that the API can be accessed by anyone. To do this we are going to use DigitalOcean. It is a charged service, but when you create an account, it will come with $200 in credit, which will be enough to get started.\nThis set-up process will take some time, but we only need to do it once. Two additional packages that will assist here are plumberDeploy (Allen 2021) and analogsea (Chamberlain et al. 2022) (which will need to be installed from GitHub: install_github(\"sckott/analogsea\")).\nNow we need to connect the local computer with the DigitalOcean account.\n\naccount()\n\nNow we need to authenticate the connection, and this is done using a SSH public key.\n\nkey_create()\n\nWhat you want is to have a “.pub” file on our computer. Then copy the public key aspect in that file, and add it to the SSH keys section in the account security settings. When we have the key on our local computer, then we can check this using ssh.\n\nssh_key_info()\n\nAgain, this will all take a while to validate. DigitalOcean calls every computer that we start a “droplet”. If we start three computers, then we will have started three droplets. We can check the droplets that are running.\n\ndroplets()\n\nIf everything is set up properly, then this will print the information about all droplets that you have associated with the account (which at this point, is probably none). We must first create a droplet.\n\nid &lt;- do_provision(example = FALSE)\n\nThen we get asked for the SSH passphrase and then it will set up a bunch of things. After this we are going to need to install a whole bunch of things onto our droplet.\n\ninstall_r_package(\n  droplet = id,\n  c(\n    \"plumber\",\n    \"remotes\",\n    \"here\"\n  )\n)\n\ndebian_apt_get_install(\n  id,\n  \"libssl-dev\",\n  \"libsodium-dev\",\n  \"libcurl4-openssl-dev\"\n)\n\ndebian_apt_get_install(\n  id,\n  \"libxml2-dev\"\n)\n\ninstall_r_package(\n  id,\n  c(\n    \"config\",\n    \"httr\",\n    \"urltools\",\n    \"plumber\"\n  )\n)\n\ninstall_r_package(id, c(\"xml2\"))\ninstall_r_package(id, c(\"tidyverse\"))\ninstall_r_package(id, c(\"tidymodels\"))\n\nAnd then when that is finally set up (it will take 30 minutes or so) we can deploy our API.\n\ndo_deploy_api(\n  droplet = id,\n  path = \"example\",\n  localPath = getwd(),\n  port = 8000,\n  docs = TRUE,\n  overwrite = TRUE\n)"
  },
  {
    "objectID": "28-deploy.html#exercises",
    "href": "28-deploy.html#exercises",
    "title": "Online Appendix I — Production",
    "section": "I.4 Exercises",
    "text": "I.4 Exercises\n\nScales\n\n(Plan)\n(Simulate)\n(Acquire)\n(Explore)\n(Communicate)\n\n\n\nQuestions\n\n\nTutorial\n\n\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber Deployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nBlair, James. 2019. Democratizing R with Plumber APIs. https://posit.co/resources/videos/democratizing-r-with-plumber-apis/.\n\n\nBuhr, Ray. 2017. Using R as a Production Machine Learning Language (Part I). https://raybuhr.github.io/blog/posts/making-predictions-over-http/.\n\n\nChamberlain, Scott, Hadley Wickham, Winston Chang, and Mauricio Vargas. 2022. Analogsea: Interface to “Digital Ocean”. https://CRAN.R-project.org/package=analogsea.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan, and Dan Tenenbaum. 2021. remotes: R Package Installation from Remote Repositories, Including “GitHub”. https://CRAN.R-project.org/package=remotes.\n\n\nGentemann, Chelle Leigh, Chris Holdgraf, Ryan Abernathey, Daniel Crichton, James Colliander, Edward Joseph Kearns, Yuvi Panda, and Richard Signell. 2021. “Science Storms the Cloud.” AGU Advances 2 (2). https://doi.org/10.1029/2020av000354.\n\n\nHuyen, Chip. 2020. “Machine Learning Is Going Real-Time,” December. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\nKuhn, Max, and Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org.\n\n\nOoms, Jeroen. 2022. ssh: Secure Shell (SSH) Client for R. https://CRAN.R-project.org/package=ssh.\n\n\nSchloerke, Barret, and Jeff Allen. 2022. plumber: An API Generator for R. https://CRAN.R-project.org/package=plumber.\n\n\nShankar, Shreya, Rolando Garcia, Joseph Hellerstein, and Aditya Parameswaran. 2022. “Operationalizing Machine Learning: An Interview Study.” arXiv. https://doi.org/10.48550/ARXIV.2209.09125.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "29-activities.html#telling-stories-with-data",
    "href": "29-activities.html#telling-stories-with-data",
    "title": "Online Appendix J — Class activities",
    "section": "J.1 Telling stories with data",
    "text": "J.1 Telling stories with data\n\nWrite three dot points about what you expect to get, and how you expect to change, by taking this course?\nWrite a one-paragraph response to: “What is data science?”. Strong answers would include references and draw on your own experience.\n[The instructor should take a photo of the class, then display the photo.] Write three aspects about what the photo shows (faces, class composition, etc), and three aspects about what the photo does not (context, thoughts, emotions, motivations, students who are not present, etc). Discuss how this relates to data science.\nPlease discuss one of the ten elements of telling convincing stories with data that you are particularly experienced with, and another that you are less experienced with.\n[The instructor should give each group a different item to use for measurement, some of which are more useful than others, for instance, measuring tape, paper, ruler, markers, scales, etc.] Using the item you were given, please answer the following question: “How long is your hair?”. Relate your experience to data science."
  },
  {
    "objectID": "29-activities.html#drinking-from-a-fire-hose",
    "href": "29-activities.html#drinking-from-a-fire-hose",
    "title": "Online Appendix J — Class activities",
    "section": "J.2 Drinking from a fire hose",
    "text": "J.2 Drinking from a fire hose\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\nPick one of the dplyr verbs – mutate(), select(), filter(), arrange(), summarize(). Explain what it does and the context, and then livecode an example of its use.\nExplain what class is, with an example.\nSimulate 100 draws from the uniform distribution with mean 5 and standard deviation 2 in the simulation.R script. Write one test for this dataset in the tests.R script.\nSimulate 50 draws from the Poisson distribution with lambda 10 in the simulation.R script. Write two tests for this dataset in the tests.R script.\nGather some data on Marriage Licence Statistics in Toronto using Open Data Toronto in the gather.R script. Clean it in the cleaning.R script.1 Graph it in the Quarto document.\nThe following code produces an error. Please add it to a GitHub Gist, and email it to the instructor asking for help:\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) |&gt;\n  geom_point()\n\n\nThe following code creates an odd-looking graph in terms of dates. Please identify the issue and fix it, by adding functions before ggplot().\n\n\nset.seed(853)\n\ndata &lt;-\n  tibble(date = as.character(sample(seq(\n    as.Date(\"2022-01-01\"),\n    as.Date(\"2022-12-31\"),\n    by = \"day\"\n  ),\n  10)), # https://stackoverflow.com/a/21502397\n  number = rcauchy(n = 10, location = 1) |&gt; round(0))\n\ndata |&gt; \n  # MAKE CHANGE HERE\n  ggplot(aes(x = date, y = number)) +\n  geom_col()\n\n\nConsider the following code to make a graph. You want to move the legend to the bottom but cannot remember the ggplot2 function to do that. Please find the answer on Stack Overflow.\n\n\npenguins |&gt; \n  drop_na() |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point()"
  },
  {
    "objectID": "29-activities.html#reproducible-workflows",
    "href": "29-activities.html#reproducible-workflows",
    "title": "Online Appendix J — Class activities",
    "section": "J.3 Reproducible workflows",
    "text": "J.3 Reproducible workflows\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\nUse Quarto to make a PDF with a title, author, and an abstract.2\nAdd three sections and some code that produces the mean bill length, by species, for palmerpenguins::penguins (with the code itself hidden).\nAdd a citation of R and palmerpenguins, then add a graph of body mass, by sex.\nAdd a paragraph of text about the graph and a cross-reference. Also add a table about the number of species, by year.\n[The instructor should (very slowly) live code all this and have students code-along.] Set up git on your local computer.3 Make a GitHub repo, then make a local copy, make some changes, and push.4\nFind the GitHub repo of a partner, fork it, make a change, and make a pull request.\nThe following code produces an error. Please follow the strategies in Section 3.5.1 to fix it.\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) |&gt;\n  geom_point()\n\n\nThe following code produces an error. Please follow the strategies in Section 3.5.1 to fix it.\n\n\ntibble(year = 1871:1970,\n       annual_nile_flow = as.character(datasets::Nile)) |&gt;\n  ggplot(aes(x = annual_nile_flow)) +\n  geom_histogram()\n\n\nThe following code produces an error. Following Section 3.5.2 create a reprex (change the example to use a more common dataset such as mtcars), add it to a GitHub Gist, and email it to the instructor.\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) |&gt;\n  geom_point()\n\n\nThe following code produces an error. Please use ChatGPT, or an equivalent LLM, to correct it. Discuss: 1) the prompt, and 2) the corrected code.\n\n\npenguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) |&gt; \n  geom_point()"
  },
  {
    "objectID": "29-activities.html#writing-research",
    "href": "29-activities.html#writing-research",
    "title": "Online Appendix J — Class activities",
    "section": "J.4 Writing research",
    "text": "J.4 Writing research\n\nDiscuss your preferred approach (data-first/question-first/other) to research and why.\nExplain, with reference to examples, what is an estimand, estimator, and estimate.\nPlease consider “selection bias” and include the definition in a sentence in the same way that Alexander (2019) does for the Gini coefficient.\nPlease use ChatGPT, or an equivalent LLM, to create a prompt that answers the question “What is a selection effect?”. With a partner, improve the response by adding context, references, and making it true (if necessary). Discuss three aspects: 1) the prompt, 2) the original answer, 3) your augmented answer.\nPick one of the well-written quantitative papers:\n\nWrite out the original title. What do you like, and not like, about it? Write an alternative title for it.\nWrite out the abstract. What do you like, and not like, about it?\nPlease prompt ChatGPT, or an equivalent LLM, to create an alternative abstract (copy the prompt so you can discuss it).\nDraw on all of this to put together an improved abstract and then discuss everything.\n\nMake a plan, based on King (2006), for how you will write a meaningful paper by the end of this class. (For PhD students: Detail three journals/conferences, in order, that you will submit it to, and why the paper would be a good fit at each.)\nPaper review: Please read Gerring (2012) and write a review of one page."
  },
  {
    "objectID": "29-activities.html#static-communication",
    "href": "29-activities.html#static-communication",
    "title": "Online Appendix J — Class activities",
    "section": "J.5 Static communication",
    "text": "J.5 Static communication\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc. Do all the following in paper.qmd.\nThe following produces a scatterplot showing the level, in feet, of Lake Huron between 1875 and 1972. Please improve it.\n\n\ntibble(year = 1875:1972,\n       level = as.numeric(datasets::LakeHuron)) |&gt;\n  ggplot(aes(x = year, y = level)) +\n  geom_point()\n\n\nThe following produces a bar chart of the height of 31 Black Cherry Trees. Please improve it.\n\n\ndatasets::trees |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = Height)) +\n  geom_bar()\n\n\nThe following produces a line plot showing the weight of chicks, in grams, by how many days old they were. Please improve it.\n\n\ndatasets::ChickWeight |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = Time, y = weight, group = Chick)) +\n  geom_line()\n\n\nThe following produces a histogram showing the annual number of sunspots between 1700 and 1988. Please improve it.\n\n\ntibble(year = 1700:1988,\n       sunspots = as.numeric(datasets::sunspot.year) |&gt; round(0)) |&gt;\n  ggplot(aes(x = sunspots)) +\n  geom_histogram()\n\n\nPlease follow this code from Saloni Dattani, and make a graph for two countries of interest to you.\nThe following code, taken from the palmerpenguins vignette, produces a beautiful graph. Please modify it to create the ugliest graph that you can.5\n\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = body_mass_g)) +\n  geom_point(aes(color = species,\n                 shape = species),\n             size = 3,\n             alpha = 0.8) +\n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) +\n  labs(\n    title = \"Penguin size, Palmer Station LTER\",\n    subtitle = \"Flipper length and body mass for Adelie, Chinstrap and Gentoo Penguins\",\n    x = \"Flipper length (mm)\",\n    y = \"Body mass (g)\",\n    color = \"Penguin species\",\n    shape = \"Penguin species\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.2, 0.7),\n    plot.title.position = \"plot\",\n    plot.caption = element_text(hjust = 0, face = \"italic\"),\n    plot.caption.position = \"plot\"\n  )\n\n\nThe following code provides estimates for the speed of light, from three experiments, each of 20 runs. Please create an average speed of light, per experiment, then use knitr::kable() to create a cross-referenced table, with specified column names, and no significant digits.\n\n\ndatasets::morley |&gt; \n  tibble()"
  },
  {
    "objectID": "29-activities.html#farm-data",
    "href": "29-activities.html#farm-data",
    "title": "Online Appendix J — Class activities",
    "section": "J.6 Farm data",
    "text": "J.6 Farm data\n\nWhat does usethis::git_vaccinate() do?\nReflect on the quote from Amia Srinivasan in Appendix A, and discuss with regard to the measurement of intelligence.\nWhy is it critical to use set.seed() when simulating?\nDiscuss to what extent the measurement of happiness is valid, being sure to define the term as part of your discussion.\nDiscuss to what extent the measurement of beauty is reliable, being sure to define the term as part of your discussion.\nHow is it possible that Country A’s measure of imports of a given good from Country B do not equal Country B’s measure of exports of that same good to Country A? Discuss, in detail, a different measurement that this makes you question.\nPlease discuss Table J.1 with respect to missing data.\n\n\n\n\n\nTable J.1: Members of various bands and their associated birth year\n\n\nBand\nPerson\nYear of birth\n\n\n\n\nBeatles\nRingo Starr\n1940\n\n\nBeatles\nJohn Lennon\n1940\n\n\nBeatles\nGeorge Harrison\nNA\n\n\nSpice Girls\nVictoria Beckham\n1975\n\n\nSpice Girls\nMel B\n1975\n\n\nSpice Girls\nNA\n1976\n\n\nNA\nMel C\n1974\n\n\nGirls’ Generation\nTaeyeon\n1989\n\n\nGirls’ Generation\nSunny\n1989\n\n\nGirls’ Generation\nNA\nNA\n\n\nGirls’ Generation\nHyoyeon\nNA\n\n\nGirls’ Generation\nYuri\n1989\n\n\nGirls’ Generation\nSooyoung\n1990\n\n\nGirls’ Generation\nYoona\n1990\n\n\nGirls’ Generation\nNA\n1991\n\n\n\n\n\n\n\nPlease discuss sampling bias in the context of the response by Katherine Rundell, a fellow at All Souls College, in response to a question about whether the word “emancipation” is from John Donne:\n\n\nIt is, although, of course, I think it would be amiss of me not to offer the caveat that often, the OED has always found first uses in canonical authors, in part because they’re just the ones who survived fire. So of course, he may have just been noting down a word in common parlance rather than being its inventor\nKatherine Rundell (Cowen 2023)\n\n\nThe dean wants to understand the average statistical ability of students in the faculty. She agrees to provide you with some funding, but not so much that you can do a census, and asks you to report back in a week. Please define the target population, sampling frame, and sample. Please further discuss whether you will use probability or non-probability sampling, and within these, which approach you want to use. Discuss the strengths and weaknesses of your approach.\nPlease use a probabilistic sampling approach to determine the average height of the class.\nPlease use a large language model to generate an initial response to ‘Pretend that we have conducted a survey of everyone in Canada, where we asked for age, sex, and gender. Your friend claims that there is no need to worry about uncertainty “because we have the whole population”. Is your friend right or wrong, and why?’. Then use what we have covered in class, your own research to correct and expand it. In particular you should add context, nuance, and citations, as well as other aspects. Submit: 1) your prompt, 2) the initial LLM response, and 3) your augmented response.6\n“Rabois’ mistake”: Imagine your sample consists of the entire population for some question of interest. Do you have to worry about bias? (Hint: Think about having a dataset for some year of everyone in some country and whether they went to a hospital and whether they died, and what that might lead you to conclude about the effect of hospitals on death.) To what extent would you be willing to give up some number of observations for better quality data (strong answers would reference Meng (2018) and Bradley et al. (2021))? What data would you like (strong answers would establish an estimand)?\nPaper review: Please read Bradley et al. (2021) and write a review of at least one page, drawing on an example that you are familiar with."
  },
  {
    "objectID": "29-activities.html#gather-data",
    "href": "29-activities.html#gather-data",
    "title": "Online Appendix J — Class activities",
    "section": "J.7 Gather data",
    "text": "J.7 Gather data\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\nObtain the NASA APOD for today using the API and then add it to the Quarto document in the repo.\nWhich Beyonce album has the highest average “danceability”?\nPlease make a graph to answer the question of whether Camila Cabello’s departure from Fifth Harmony in December 2016 affected the valence of the songs in their studio albums.7 Some helpful cleaning code is below.\n\n\nfifth_harmony |&gt;\n  filter(album_name %in% c(\"Reflection\", \"7/27 (Deluxe)\", \"Fifth Harmony\")) |&gt; \n  mutate(album_release_date = ymd(album_release_date)) |&gt;\n  filter(album_release_date != \"2017-10-29\") # There is a essentially duplicate album\n\n\nBuild an equivalent to Figure 7.11, but for Canada.\nPaper review: Please read Kish (1959) and write a review of at least one page, drawing on an example that you are familiar with."
  },
  {
    "objectID": "29-activities.html#hunt-data",
    "href": "29-activities.html#hunt-data",
    "title": "Online Appendix J — Class activities",
    "section": "J.8 Hunt data",
    "text": "J.8 Hunt data\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\nConsider Fisher’s tea tasting experiment. First, pretend that you had the results of a handful of tea tasting experiments each conducted separately. Sketch the table of data and a graph that you might make with the results. Then simulate these. Then in a small group, do the experiment (this will be more difficult than you think). Add your group’s results to those of the whole class, and then make a graph.\nFollow Appendix E to build a quick personal website and deploy it using GitHub Pages.\nBuild another website, but this time add Google Analytics. Deploy it using Netlify. Change some aspect of the website, add a different tracker, and push it to a new branch. Then use Netlify to conduct an A/B test.\nPaper review: With reference to Hammond et al. (2022), please discuss experimental design, informed consent and equipoise. Please write at least two pages."
  },
  {
    "objectID": "29-activities.html#clean-and-prepare",
    "href": "29-activities.html#clean-and-prepare",
    "title": "Online Appendix J — Class activities",
    "section": "J.9 Clean and prepare",
    "text": "J.9 Clean and prepare\n\nPick a topic that you know well, then:\n\nReproducibly simulate an idealized dataset in an R script.\nWrite five tests for it in a different R script.\nGive the code to create the dataset (not the tests) to someone else, via a GitHub Gist. Have them use code to deliberately create three different data issues in the dataset and then send it back to you (some ideas include: inconsistent date formatting, adding missing values, adding negative values, changing the decimal place)\nDo your tests identify the issues?\n\nDiscuss, with the help of simulation, the following claim: “This house believes that strings are better than factors”.8\nI obtain some data about income. Why would I be concerned if there were many respondents with income “999999”?\nCreate a new R project and use an R script to download and load the original “Adélie penguin data” (Palmer Station Antarctica LTER and Gorman, Kristen 2020), (some code9 is included below to help with this) then:\n\nUse rename() to improve the names.\nAdd the folder to a GitHub repo.\nExchange the repo with another student.\nCreate a GitHub issue with two dot points about where the names could be improved.\nDiscuss how your names compare with palmerpenguins::penguins.\n\n\n\nraw_penguin_data &lt;-\n  read_csv(file = \"https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-pal.219.5&entityid=002f3893385f710df69eeebe893144ff\",\n           show_col_types = FALSE)\n\n\nProduce a tidy version of Anscombe’s Quartet, available using anscombe.\nHow does the US General Social Survey code “don’t know”?\nDiscuss the advantages and disadvantages of each option for coding missing data:\n\n\ntibble(\n  income_1 = c(88515, 103608, -99, 87644, 118279, 107342, 97300, 97226, 73367, 101528),\n  income_2 = c(88515, 103608, 102582, 87644, 118279, 107342, \"missing\", 97226, 73367, 101528),\n  income_3 = c(999999, 103608, 102582, 87644, 118279, 107342, 97300, 97226, 73367, 101528),\n  income_4 = c(88515, 103608, 102582, 87644, 0, 107342, 97300, 97226, 73367, 101528),\n  income_5 = c(88515, 103608, 102582, 87644, 118279, 107342, 97300, 97226, 73367, NA_integer_)\n)\n\n\nHow do you think missing data should be coded in a dataset?\nUsing a new R project, for a city that you are interested in:10\n\nWrite reproducible code to download a ShotSpotter dataset from here and clean up the dataset.\nAdd your code to GitHub.\nExchange your repo with someone else in the class. They are to make a pull request that uses your dataset to make a graph.\n\nGet the ImageNet dataset and open ten random images. Apply your own label. To what extent does your label agree with the actual label? What are the effects of this on the outcomes of models trained on this dataset?\nImagine you are interested in understanding intergenerational wealth, and one aspect of this is linking the educational outcomes of people today with those of people one hundred years ago, and in different countries. Please make the dataset consistent, and document why you made the choices that you made.\n\n\ntibble(\n  period = c(1901, 1901, 1901, 2023, 2023, 2023),\n  country = c(\n    \"Prussia\",\n    \"Austro-Hungarian Empire\",\n    \"Kingdom of Hungary\",\n    \"Canada\",\n    \"Australia\",\n    \"UK\"\n  ),\n  highest_education = c(\n    \"Gymnasium\",\n    \"Higher Polytechnic School\",\n    \"Matura\",\n    \"High school\",\n    \"Undergraduate Honours\",\n    \"A-levels\"\n  )\n)\n\n\nMake tidy data from datasets::UCBAdmissions.\nUsing datasets::LifeCycleSavings:\n\nFirst use pivot_longer() to make it long.\nMake a graph using ggplot.\nThen use pivot_wider() to change it back to wide.\nMake a table using knitr::kable for Australia, Canada, New Zealand, and a country that you choose.\n\nFix the following:\n\n\ntibble(\n  date = c(\"20-02-2023\",\n           \"20 February 2023\",\n           \"02-20-2023\",\n           \"2023-20-02\",\n           \"February 20, 2023\")\n  )\n\n\nFix the following:\n\n\ntibble(\n  date = c(\"01-02-2023\",\n           \"02-01-2023\",\n           \"2023-01-02\")\n  )\n\n\nAssume you live in Toronto, Canada. What is wrong with the following date?11 2023-03-12 02:01:00.\nFollowing the example in Chapter 15 read in a “.dta” file from the US General Social Survey, add the labels, and build a graph of some variable. What happens to the missing data?"
  },
  {
    "objectID": "29-activities.html#store-and-share",
    "href": "29-activities.html#store-and-share",
    "title": "Online Appendix J — Class activities",
    "section": "J.10 Store and share",
    "text": "J.10 Store and share\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\nAdd the following code to a simulation R script, then lint it. What do you think about the recommendations?\n\n\nset.seed(853)\ntibble(\n  age_days=runif(n=10,min=0,max=36500),\n  age_years=age_days%/%365\n)\n\n\nSimulate a dataset with ten million observations and at least five variables, one of which must be a date. Save it in both CSV and parquet formats. What is the file size difference?\nDiscuss datasheets in the context of the dataset that you simulated.\nPretend you were joining two datasets with left_join(). When joining datasets it is easy to accidentally duplicate or remove rows. Please add some tests that might put your mind at ease.\n\n\nset.seed(853)\n\n# CONSIDER A CHANGE HERE\n\nmain_data &lt;-\n  tibble(\n    participant_id = stringi::stri_rand_strings(n = 100, length = 5),\n    education_value = sample(\n      x = 1:5,\n      size = 100,\n      replace = TRUE\n    )\n  )\n\n# CONSIDER A CHANGE HERE\n\neducation_labels &lt;-\n  tibble(\n    education_value = 1:5,\n    education_label = c(\n      \"Some high school\",\n      \"High school\",\n      \"Some post secondary\",\n      \"Post secondary degree\",\n      \"Graduate degree\"\n    )\n  )\n\n# CONSIDER A CHANGE HERE\n\njoined_data &lt;-\n  main_data |&gt;\n  left_join(education_labels, by = join_by(education_value))\n\n# CONSIDER A CHANGE HERE\n\n\nModify the following code to show why using “T” instead of “TRUE” should generally not be done (hint: assign “T” to “FALSE”)?\n\n\nset.seed(853)\n# MAKE CHANGE HERE\nsample(x = 1:5, size = 5, replace = T)\n\n[1] 1 2 5 1 5\n\n\n\nWorking with the instructor, pick a chapter from Lewis (2023) and create a five-slide summary of the key take-aways from the chapter. Present to the class.\nWorking with the instructor, make a pull request that fixes some small aspect of a work-in-progress book.12 Options include:13 Lewis (2023) or Wickham, Çetinkaya-Rundel, and Grolemund ([2016] 2023).\nPretend that you are in a small class, and have some results from an assessment (Table J.2). Use the code, but change the seed to generate your own dataset.\n\nHash, but do not salt, the names, and then exchange with another group. Can they work out what the names are?\nContinuing with the results that you generated, please write code that simulates the dataset. You will need to decide which features are important and which are not. Note two interesting aspects of this and then share with the class.\nContinuing with the results that you generated, please: 1) work out the class mean, 2) remove the mark of one student, 3) provide the mean and the off-by-one dataset to another group. Can they work out the mark of the student who opted not to share?\nFinally, please do the same exercise, but create a differentially private mean. What are they able to figure out now?\n\n\n\nset.seed(853)\n\nclass_marks &lt;-\n  tibble(\n    student = sample(\n      x = babynames |&gt; filter(prop &gt; 0.01) |&gt;\n        select(name) |&gt; unique() |&gt; unlist(),\n      size = 10,\n      replace = FALSE\n    ),\n    mark = rnorm(n = 10, mean = 50, sd = 10) |&gt; round(0)\n  )\n\nclass_marks |&gt;\n  knitr::kable(col.names = c(\"Student\", \"Mark\"))\n\n\n\nTable J.2: Simulated students and their mark (out of 100) in a particular class paper\n\n\nStudent\nMark\n\n\n\n\nBertha\n37\n\n\nTyler\n32\n\n\nKevin\n48\n\n\nRyan\n39\n\n\nRobert\n34\n\n\nJennifer\n52\n\n\nDonna\n48\n\n\nKaren\n43\n\n\nEmma\n61\n\n\nArthur\n55"
  },
  {
    "objectID": "29-activities.html#exploratory-data-analysis",
    "href": "29-activities.html#exploratory-data-analysis",
    "title": "Online Appendix J — Class activities",
    "section": "J.11 Exploratory data analysis",
    "text": "J.11 Exploratory data analysis\n\nFix the following file names.\n\nexample_project/\n├── .gitignore\n├── Example project.Rproj\n├── scripts\n│   ├── simulate data.R\n│   ├── DownloadData.R\n│   ├── data-cleaning.R\n│   ├── test(new)data.R\n\nConsider Anscombe’s Quartet, introduced in Chapter 5. We will randomly remove certain observations. Please pretend you are given the dataset with missing data. Pick one of the approaches to dealing with missing data from Chapter 6 and Chapter 11, then write code to implement your choice. Compare:\n\nthe results with the actual observations;\nthe summary statistics with the actual summary statistics; and\nbuild a graph that shows the missing and actual data on the one graph.\n\n\n\nset.seed(853)\n\ntidy_anscombe &lt;-\n  anscombe |&gt;\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\n\ntidy_anscombe_MCAR &lt;-\n  tidy_anscombe |&gt;\n  mutate(row_number = row_number()) |&gt;\n  mutate(\n    x = if_else(row_number %in% sample(\n      x = 1:nrow(tidy_anscombe), size = 10\n    ), NA_real_, x),\n    y = if_else(row_number %in% sample(\n      x = 1:nrow(tidy_anscombe), size = 10\n    ), NA_real_, y)\n  ) |&gt;\n  select(-row_number)\n\ntidy_anscombe_MCAR\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        NA NA   \n 4 4         8 NA   \n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        NA  7.58\n10 2        13  8.74\n# ℹ 34 more rows\n\n# ADD CODE HERE\n\n\nRedo the exercise, but with the following dataset. What is the main difference in this case?\n\n\ntidy_anscombe_MNAR &lt;-\n  tidy_anscombe |&gt;\n  arrange(desc(x)) |&gt;\n  mutate(\n    ordered_x_rows = 1:nrow(tidy_anscombe),\n    x = if_else(ordered_x_rows %in% 1:10, NA_real_, x)\n  ) |&gt;\n  select(-ordered_x_rows) |&gt;\n  arrange(desc(y)) |&gt;\n  mutate(\n    ordered_y_rows = 1:nrow(tidy_anscombe),\n    y = if_else(ordered_y_rows %in% 1:10, NA_real_, y)\n  ) |&gt;\n  arrange(set) |&gt;\n  select(-ordered_y_rows)\n\ntidy_anscombe_MNAR\n\n# A tibble: 44 × 3\n   set       x     y\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        NA NA   \n 2 1        NA NA   \n 3 1         9 NA   \n 4 1        11  8.33\n 5 1        10  8.04\n 6 1        NA  7.58\n 7 1         6  7.24\n 8 1         8  6.95\n 9 1         5  5.68\n10 1         7  4.82\n# ℹ 34 more rows\n\n# ADD CODE HERE\n\n\nUsing pair programming (being sure to switch every 5 minutes), create a new R project, then read in the following dataset from Bombieri et al. (2023) and explore it by adding code and notes to a Quarto document.\n\n\ndownload.file(url = \"https://doi.org/10.1371/journal.pbio.3001946.s005\",\n              destfile = \"data.xlsx\")\n\ndata &lt;-\n  read_xlsx(path = \"data.xlsx\",\n            col_types = \"text\") |&gt;\n  clean_names() |&gt;\n  mutate(date = convert_to_date(date))\n\n\nPlay the role of a data scientist partnering with a subject expert by pairing up with another student. Your partner gets to pick a topic, and a question, and it should be something they know well but you do not (perhaps something about their country, if they are an international student). You need to work with them to develop an analysis plan, simulate some data, and create a graph that they can use."
  },
  {
    "objectID": "29-activities.html#linear-models",
    "href": "29-activities.html#linear-models",
    "title": "Online Appendix J — Class activities",
    "section": "J.12 Linear models",
    "text": "J.12 Linear models\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\n\nWe are interested in understanding the relationship between bill length and depth for Adelie penguins using palmerpenguins. Begin by sketching and simulating.\nThen add three graphs to the data section: one of each variable separately and a third of the relationship between the two.\nThen in the model section, write out the model. Use rstanarm to estimate a linear model between the two variables in an R script.\nRead that model into the results section and use modelsummary to create a summary table.\n\n\n\npalmerpenguins::penguins |&gt; \n  filter(species == \"Adelie\") |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n(This question is based on Gelman and Vehtari (2023, 32).) Please follow the instructions here to make a paper plane. Measure: 1) the width of the wings; 2) the length of the wings; 3) the height of the winglets; 4) in a safe space, fly your plane and measure for how long it stays in the air. Combine your data with those of the rest of the class in a table that looks like Table J.3. Then use linear regression to explore the relationship between the dependent and independent variables. Based on the results, how would you change your plane design if you were to do this exercise again?\n\n\n\n\n\nTable J.3: Data gathered on the relationship between features of a paper plane and the length of time it stays in the air\n\n\n\n\n\n\n\n\nWing width (mm)\nWing length (mm)\nWinglet height (mm)\nFlying time (sec)\n\n\n\n\n…\n…\n…\n…\n\n\n\n\n\n\n\nImagine that we add FIPS code as an explanatory variable in a linear regression model explaining inflation, by US county. Please discuss the likely effect of this and whether it is a good idea. You may want to simulate the situation to help you think it through more clearly.\nImagine that we add latitude and longitude as explanatory variables in a regression trying to explain flu prevalence in each UK city and town. Please discuss the likely effect of this and whether it is a good idea. You may want to simulate the situation to help you think it through more clearly.\nIn Shakespeare’s Julius Caesar, the character Cassius famously says: “The fault, dear Brutus, is not in our stars, / But in ourselves, that we are underlings.” Please discuss p-values, the common, but misleading, use of significance stars in regression tables, and Ioannidis (2005), with reference to this quote.\nImagine that a variance estimate ends up negative. Please discuss."
  },
  {
    "objectID": "29-activities.html#generalized-linear-models",
    "href": "29-activities.html#generalized-linear-models",
    "title": "Online Appendix J — Class activities",
    "section": "J.13 Generalized linear models",
    "text": "J.13 Generalized linear models\n\nDiscuss how you would build a Bayesian regression model to look at the association between whether someone prefers football or hockey, and their age, gender, and location. Write out:\n\nThe outcome of interest and the likelihood\nThe regression model for the outcome of interest\nThe priors on any parameters to be estimated in the model.\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\n\nWe are interested in explaining support for either Trump or Clinton, based on education, age-group, and gender, and state. Please sketch and simulate the situation.\nPlease obtain the data underpinning Cohn (2016), available here. Save the unedited data, and construct an analysis dataset (there is some code below to get you started). Add graphs of each of the variables, individually, into the data section, as well as graphs of how they relate.\nPlease build one model explaining “vt_pres_2”, as a function of “gender”, “educ”, and “age”; and another which additionally considers “state”. Write up the two models in a model section, and add the results into the results section (again, there is some code below to get you started).\n\n\n\nvote_data &lt;-\n  read_csv(\n    \"https://raw.githubusercontent.com/TheUpshot/2016-upshot-siena-polls/master/upshot-siena-polls.csv\"\n  )\n\ncleaned_vote_data &lt;-\n  vote_data |&gt;\n  select(vt_pres_2, gender, educ, age, state) |&gt;\n  rename(vote = vt_pres_2) |&gt;\n  mutate(\n    gender = factor(gender),\n    educ = factor(educ),\n    state = factor(state),\n    age = as.integer(age)\n  ) |&gt;\n  mutate(\n    vote =\n      case_when(\n        vote == \"Donald Trump, the Republican\" ~ \"0\",\n        vote == \"Hillary Clinton, the Democrat\" ~ \"1\",\n        TRUE ~ vote\n      )\n  ) |&gt;\n  filter(vote %in% c(\"0\", \"1\")) |&gt;\n  mutate(vote = as.integer(vote))\n\n\nvote_model &lt;-\n  stan_glm(\n    formula = vote ~ age + educ,\n    data = cleaned_vote_data,\n    family = gaussian(),\n    prior = normal(location = 0, scale = 2.5),\n    prior_intercept = normal(location = 0, scale = 2.5),\n    prior_aux = exponential(rate = 1),\n    seed = 853\n  )"
  },
  {
    "objectID": "29-activities.html#causality-from-observational-data",
    "href": "29-activities.html#causality-from-observational-data",
    "title": "Online Appendix J — Class activities",
    "section": "J.14 Causality from observational data",
    "text": "J.14 Causality from observational data\n\n“Look Daddy, I’m shaving”. A three-year-old stands next to his father. The father shaves, and then has no beard. The three-year-old “shaves”, and then also has no beard. Use a DAG to illustrate the situation and what might be missing.\nConsider the following situation: “There are two surgeons at a hospital. One is considered great, and so triage, on average, tends to send hard cases to them, hence many of their patients die. The other surgeon is considered good, and so triage, on average, tends to send moderate cases to them, hence an average number of their patients die.” Simulate the situation, including only patient name, surgeon name, and patient outcome. Run a regression. What conclusion do you draw? Then draw a DAG that specifies the data that were missed in that regression. Add those variables to the simulation and re-run the regression. How does your conclusion differ?\nPick either Simpson’s paradox or Berkson’s paradox and then please think of an example and discuss an analytical approach to adjusting for it."
  },
  {
    "objectID": "29-activities.html#multilevel-regression-with-post-stratification",
    "href": "29-activities.html#multilevel-regression-with-post-stratification",
    "title": "Online Appendix J — Class activities",
    "section": "J.15 Multilevel regression with post-stratification",
    "text": "J.15 Multilevel regression with post-stratification\n\nUse the starter folder and create a new repo. Add a link to the GitHub repo in the class’s shared Google Doc.\nLike in “Linear Models”, we are again interested in understanding the relationship between bill length and depth using palmerpenguins, but this time for all three species. Begin by estimating separate models for each. Then estimate one model for all three species. Finally, estimate a model with partial pooling.\nPaper review: Please read Wang et al. (2015) and write a review of at least two pages."
  },
  {
    "objectID": "29-activities.html#text-as-data",
    "href": "29-activities.html#text-as-data",
    "title": "Online Appendix J — Class activities",
    "section": "J.16 Text as data",
    "text": "J.16 Text as data\n\nDo children learn “dog”, “cat”, or “bird” first? Use the Wordbank database."
  },
  {
    "objectID": "29-activities.html#concluding-remarks",
    "href": "29-activities.html#concluding-remarks",
    "title": "Online Appendix J — Class activities",
    "section": "J.17 Concluding remarks",
    "text": "J.17 Concluding remarks\n\nClean up your GitHub: delete unnecessary repos, pin your best ones, update your bio, add a profile README.\nIn small groups, using the STAR approach, please take turns asking and answering the following:\n\n“Tell me about a time that a data science project you were part of did not go well. What did you do?”\n“Tell me about a time that you worked in a team. How did it go?”\n“Tell me about a project that you disliked working on.”\n“How do you incorporate feedback into your work?”\n“Tell me about a time you needed help on a project and how did you get it?”\n“What issues have your managers had with you in the past?”\n“What are your weaknesses?”\n“What are your strengths?”\n“Where do you see yourself in five years?”\n\nReview Riley et al. (2022), then pick one of the twelve issues to be the hill you will die on, when it comes to statistics, and discuss why. You can only pick one.\nWrite three dot points about what you got, and how you changed, by taking this course?\n\n\n\n\n\nAlexander, Monica. 2019. “The Concentration and Uniqueness of Baby Names in Australia and the US,” January. https://www.monicaalexander.com/posts/2019-20-01-babynames/.\n\n\nBombieri, Giulia, Vincenzo Penteriani, Kamran Almasieh, Hüseyin Ambarlı, Mohammad Reza Ashrafzadeh, Chandan Surabhi Das, Nishith Dharaiya, et al. 2023. “A Worldwide Perspective on Large Carnivore Attacks on Humans.” PLOS Biology 21 (1): e3001946. https://doi.org/10.1371/journal.pbio.3001946.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic, Xiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big Surveys Significantly Overestimated US Vaccine Uptake.” Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nCohn, Nate. 2016. “We Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results.” The New York Times, September. https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html.\n\n\nCowen, Tyler. 2023. “Episode 168: Katherine Rundell on the Art of Words.” Conversations with Tyler, January. https://conversationswithtyler.com/episodes/katherine-rundell/.\n\n\nGelman, Andrew, and Aki Vehtari. 2023. Learn Statistics: Hundreds of Stories, Activities, and Examples.\n\n\nGerring, John. 2012. “Mere Description.” British Journal of Political Science 42 (4): 721–46. https://doi.org/10.1017/s0007123412000130.\n\n\nHammond, Jennifer, Heidi Leister-Tebbe, Annie Gardner, Paula Abreu, Weihang Bao, Wayne Wisemandle, MaryLynn Baniecki, et al. 2022. “Oral Nirmatrelvir for High-Risk, Nonhospitalized Adults with Covid-19.” New England Journal of Medicine 386 (15): 1397–1408. https://doi.org/10.1056/nejmoa2118542.\n\n\nIoannidis, John. 2005. “Why Most Published Research Findings Are False.” PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS: Political Science & Politics 39 (1): 119–25. https://doi.org/10.1017/S1049096506060252.\n\n\nKish, Leslie. 1959. “Some Statistical Problems in Research Design.” American Sociological Review 24 (3): 328–38. https://doi.org/10.2307/2089381.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education Research. https://datamgmtinedresearch.com/index.html.\n\n\nLyman, Frank. 1981. “The Responsive Classroom Discussion: The Inclusion of All Students.” Mainstreaming Digest 109: 109–13.\n\n\nMeng, Xiao-Li. 2018. “Statistical Paradises and Paradoxes in Big Data (i): Law of Large Populations, Big Data Paradox, and the 2016 US Presidential Election.” The Annals of Applied Statistics 12 (2): 685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\nPalmer Station Antarctica LTER, and Gorman, Kristen. 2020. “Structural Size Measurements and Isotopic Signatures of Foraging Among Adult Male and Female Adélie Penguins (Pygoscelis Adeliae) Nesting Along the Palmer Archipelago Near Palmer Station, 2007-2009.” https://doi.org/10.6073/PASTA/98B16D7D563F265CB52372C8CA99E60F.\n\n\nRiley, Richard, Tim Cole, Jon Deeks, Jamie Kirkham, Julie Morris, Rafael Perera, Angie Wade, and Gary Collins. 2022. “On the 12th Day of Christmas, a Statistician Sent to Me...” BMJ, December, e072883. https://doi.org/10.1136/bmj-2022-072883.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015. “Forecasting Elections with Non-Representative Polls.” International Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz."
  },
  {
    "objectID": "29-activities.html#footnotes",
    "href": "29-activities.html#footnotes",
    "title": "Online Appendix J — Class activities",
    "section": "",
    "text": "Consider separate() and then lubridate::ymd() for the dates.↩︎\nThere are always a small number of students who struggle with getting the PDF set up locally. Worst case, do everything else locally as a html, then build the PDF in Posit Cloud.↩︎\nIf you have hidden your GitHub email then make sure you use the alias when you add an email address locally.↩︎\nThere will always be a few students that cannot get git working locally. I find the best approach is to triage by pairing them with an advanced student while you demonstrate, and if there are remaining issues then deal with them individually at an office hour.↩︎\nThe idea for this exercise is from Liza Bolton.↩︎\nDamien Patrick Williams came up with the idea behind this question.↩︎\nStudents who finish quickly should similarly look at Jessica’s departure from Girls’ Generation in September 2014, and then attempt to compare the two situations.↩︎\nThe underlying idea for this exercise is from Michael Donnelly.↩︎\nThis code is from Christina Wei.↩︎\nThe idea for this exercise is from Taylor John Wright.↩︎\nThe idea for this exercise is from Derek Beaton.↩︎\nClosely supervise the students, and especially check the pull requests before they are made to ensure they are reasonable; we don’t want to annoy people. A good option might a fix to a couple of typos or similar.↩︎\nThese will need to change each year.↩︎"
  },
  {
    "objectID": "97-errata.html#errata",
    "href": "97-errata.html#errata",
    "title": "Online Appendix K — Errata and updates",
    "section": "K.1 Errata",
    "text": "K.1 Errata\nThe following errata exist in the print version, but have been updated in the online version. If you notice an error not mentioned below, please submit an issue or send an email: rohan.alexander@utoronto.ca.\n\n\nxxi: Add Alex Hayes to the Acknowledgments.\n\np. 20: Add “packages” to “use the tidyverse and janitor packages.”\np. 34: \"daily-shelter-overnight-service-occupancy-capacity-2021\" should be \"daily-shelter-overnight-service-occupancy-capacity-2021.csv\" (note the “.csv” added).\np. 34: Replace the first code chunk with the second:\n\n\ntoronto_shelters_clean &lt;-\n     clean_names(toronto_shelters) |&gt;\n     select(occupancy_date, id, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n\ntoronto_shelters_clean &lt;-\n  clean_names(toronto_shelters) |&gt;\n  mutate(occupancy_date = ymd(occupancy_date)) |&gt; \n  select(occupancy_date, occupied_beds)\n\nhead(toronto_shelters_clean)\n\n\np. 41: Remove a stray “:::”.\np. 66: “New Project$dots” should be “New Project…”.\np. 138: scale_color_brewer(palette = \"Set1\") is unnecesary and should be removed.\np. 138: The figure caption should refer to inflation not unemployment.\np. 347: The “Exploratory data analysis” chapter of R for Data Science is 11, not 12.\np. 353: Fix a “the the”.\np. 587: The link should be: https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/"
  },
  {
    "objectID": "97-errata.html#updates",
    "href": "97-errata.html#updates",
    "title": "Online Appendix K — Errata and updates",
    "section": "K.2 Updates",
    "text": "K.2 Updates\nWhile I have tried to limit changes to what was printed, the following have been made:\n2023-07-24\n\nUpdated links and citations for: Barrett (2021); Blair (2019); Chase (2020); Kasy and Teytelboym (2023); and World Health Organization (2019).\n\n2023-08-10\n\nAdded a reference to Matsumoto (2007).\n\n\n\n\n\nBarrett, Malcolm. 2021. “Data Science as an Atomic Habit,” January. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\nBlair, James. 2019. Democratizing R with Plumber APIs. https://posit.co/resources/videos/democratizing-r-with-plumber-apis/.\n\n\nChase, William. 2020. “The Glamour of Graphics.” RStudio Conference, January. https://posit.co/resources/videos/the-glamour-of-graphics/.\n\n\nKasy, Maximilian, and Alexander Teytelboym. 2023. “Matching with Semi-Bandits.” The Econometrics Journal 26 (1): 45–66. https://doi.org/10.1093/ectj/utac021.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as an Essay.” In Beautiful Code, edited by Andy Oram and Greg Wilson, 477–81. O’Reilly.\n\n\nWorld Health Organization. 2019. “Trends in Maternal Mortality 2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the United Nations Population Division.” https://apps.who.int/iris/handle/10665/327596."
  },
  {
    "objectID": "98-cocktails.html",
    "href": "98-cocktails.html",
    "title": "Online Appendix L — Cocktails",
    "section": "",
    "text": "Each of the chapters inspired a cocktail created by Monica. (Most of) the names are Radiohead songs.\nChapter 1—Telling stories with data\nEverything in its right place\n\\(1\\) oz Bacardi Superior\n\\(\\frac{1}{2}\\) oz lime juice\n\\(\\frac{1}{2}\\) oz mango syrup\n1 oz champagne\nChapter 2—Drinking from a fire hose\nI jumped in the river and what did I see?\n\\(2\\) oz Hennessy\n\\(1\\) oz lemon juice\n\\(1\\) oz strawberry syrup\nTop with soda water\n\nChapter 3—Reproducible workflows\nNo alarms and no surprises\n\\(1\\frac{1}{2}\\) oz Empress gin\n\\(\\frac{1}{2}\\) oz St Germain\n\\(\\frac{1}{2}\\) oz lime juice\n\\(\\frac{1}{4}\\) oz raspberry syrup\nOne dash Angostura bitters\nChapter 4—Writing research\nDon’t get sentimental it always ends up drivel\n\\(1\\) oz Hennessy\n\\(\\frac{1}{2}\\) oz Grand Marnier\n\\(\\frac{1}{2}\\) oz Amaro Nonino\n\\(\\frac{1}{4}\\) oz lemon juice\n\\(\\frac{1}{4}\\) oz honey syrup\nCherry garnish\nChapter 5—Static communication\nJigsaw falling into place\n\\(1\\) oz Bulleit rye\n\\(1\\) oz lemon juice\n\\(1\\) oz green chartreuse\n\\(\\frac{1}{2}\\) oz st germain\n\\(\\frac{1}{2}\\) oz Luxardo Maraschino Originale\nOne dash Angostura bitters\nTop with lemonade\nChapter 6—Farm data\nIn the neon sign; scrolling up and down\n\\(1\\frac{1}{4}\\) oz Bulleit rye\n\\(\\frac{1}{2}\\) oz Grand Marnier\n\\(\\frac{3}{4}\\) oz lemon juice\n\\(\\frac{3}{4}\\) oz orange juice\n\\(\\frac{1}{2}\\) oz grenadine\n\\(\\frac{1}{4}\\) oz raspberry syrup\nChapter 7—Gather data\nJanuary has April showers\n\\(2\\) oz Buffalo Trace bourbon\n\\(1\\) oz Galliano Vanilla\n\\(1\\) oz raspberry syrup\n\\(1\\) oz lemon juice\nOne dash Angostura bitters\n\nChapter 8—Hunt data\nHere I’m alive, everything all of the time\n\\(2\\) oz Lillet\n\\(1\\) oz Cointreau\n\\(2-3\\) oz tonic\n\nChapter 9—Clean and prepare\nThe infrastructure will collapse\n\\(1\\frac{1}{2}\\) oz cherry-infused vodka (Iceberg)\n\\(1\\) oz sweet vermouth\n\\(1\\) oz raspberry syrup\n\\(1\\) oz lime juice\nOne dash Angostura bitters\nChapter 10—Store and share\nAnd you realize you’re looking in, looking in the wrong place\n\\(1\\frac{1}{4}\\) oz Four Roses bourbon\n\\(1\\) oz apple cider\n\\(\\frac{1}{2}\\) oz Grand Marnier\n\\(\\frac{1}{4}\\) oz ameretto\n\\(\\frac{1}{2}\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Angostura bitters\n\nChapter 11—Exploratory data analysis\nJust cause you feel it doesn’t mean it’s there\n\\(1\\frac{1}{2}\\) oz Glynnevan Canadian whisky\n\\(\\frac{3}{4}\\) oz Sweet vermouth\n\\(\\frac{3}{4}\\) oz Benedictine\nOne dash absinthe\nTwo dashes Peychaud’s bitters\nTwo dashes Angostura bitters\n\nChapter 12—Linear models\nYou can try the best you can, the best you can is good enough \n\\(2\\) oz Bulleit rye\n\\(1\\) oz sweet vermouth\n\\(\\frac{1}{2}\\) oz blackberry syrup\nOne dash Angostura bitters\n\nChapter 13–Generalized linear models\nThe don’t speak for us\n\\(1\\frac{1}{2}\\) oz Captain Morgan dark rum\n\\(1\\) oz honey syrup\n\\(1\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Fee Brothers Old Fashioned bitters\nCherry garnish\n\nChapter 14—Causality from observational data\nWe are accidents waiting to happen\n\\(1\\frac{1}{2}\\) oz Hennessey\n\\(\\frac{3}{4}\\) oz Cointreau\n\\(\\frac{3}{4}\\) oz Verjus\n1 barspoon simple syrup\nLemon garnish\nChapter 15—Multilevel regression with post-stratification\nLittle by little by hook or by crook\n\\(1\\frac{1}{2}\\) oz Captain Morgan dark rum\n\\(\\frac{3}{4}\\) oz cane sugar syrup\n\\(\\frac{1}{2}\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz orange juice\n\\(\\frac{1}{2}\\) oz ginger syrup\nOne dash Old Fashioned bitters\nChapter 16—Text as data\nWords are blunt instruments; words are a sawed-off shotgun\n\\(2\\) oz Buffalo Trace bourbon\n\\(\\frac{1}{4}\\) oz Luxardo Maraschino Originale\n1 sugar cube\nOne dash Angostura bitters\nOne dash absinthe\nCherry garnish\nChapter 17—Concluding remarks\nI’m the next act, waiting in the wings\n\\(1\\) oz Plymouth gin\n\\(\\frac{1}{2}\\) oz lemon juice\n\\(\\frac{1}{2}\\) oz St Germain\nOne dash Angostura bitters\nTop with champagne\nAppendix A—R essentials\nGravity always wins\n\\(2\\) oz Buffalo Trace bourbon\n\\(1\\) oz sweet vermouth\n\\(1\\frac{1}{2}\\) oz cherry syrup\n\\(1\\frac{1}{2}\\) oz lemon juice\nOne dash Fee Brothers Old Fashioned bitters\n\nAppendix B—Datasets\nTBD\nAppendix C—R Markdown*\nTBD\nAppendix D—Papers*\nTBD\nAppendix E—Interaction*\nTBD\nAppendix F—Datasheets*\nTBD\nAppendix G—SQL*\nTBD\nAppendix H—Prediction*\n2+2 always makes up 5\nTBD\nAppendix I—Production*\nNo love interest required\n1 3/4 oz Bombay Sapphire Gin\n1 oz Lillet Blanc which was sitting in mashed cucumber for 10 min with ice\n1/4 oz Benedictine\n2 dashes Angostura\nStirred and strained\nOptional: Served with fresh Borage as garnish\nAppendix J—Class activities*\nTBD"
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "Abadie, Alberto, Susan Athey, Guido Imbens, and Jeffrey Wooldridge.\n2017. “When Should You Adjust Standard Errors for\nClustering?” Working Paper 24003. Working Paper Series. National\nBureau of Economic Research. https://doi.org/10.3386/w24003.\n\n\nAbelson, Harold, and Gerald Jay Sussman. 1996. Structure and\nInterpretation of Computer Programs. Cambridge: The MIT Press.\n\n\nAbeysooriya, Mandhri, Megan Soria, Mary Sravya Kasu, and Mark Ziemann.\n2021. “Gene Name Errors: Lessons Not Learned.” PLOS\nComputational Biology 17 (7): 1–13. https://doi.org/10.1371/journal.pcbi.1008984.\n\n\nAcemoglu, Daron, Simon Johnson, and James Robinson. 2001. “The\nColonial Origins of Comparative Development: An Empirical\nInvestigation.” American Economic Review 91\n(5): 1369–1401. https://doi.org/10.1257/aer.91.5.1369.\n\n\nAchen, Christopher. 1978. “Measuring Representation.”\nAmerican Journal of Political Science 22 (3): 475–510. https://doi.org/10.2307/2110458.\n\n\nAkerlof, George. 1970. “The Market for ‘Lemons’:\nQuality Uncertainty and the Market Mechanism.” The Quarterly\nJournal of Economics 84 (3): 488–500. https://doi.org/10.2307/1879431.\n\n\nAlexander, Monica. 2019a. “Reproducibility in Demographic\nResearch.” https://www.monicaalexander.com/posts/2019-10-20-reproducibility/.\n\n\n———. 2019b. “The Concentration and Uniqueness of Baby Names in\nAustralia and the US,” January. https://www.monicaalexander.com/posts/2019-20-01-babynames/.\n\n\n———. 2019c. “Analyzing Name Changes After Marriage Using a\nNon-Representative Survey,” August. https://www.monicaalexander.com/posts/2019-08-07-mrp/.\n\n\n———. 2021. “Overcoming Barriers to Sharing Code.”\nYouTube, February. https://youtu.be/yvM2C6aZ94k.\n\n\nAlexander, Monica, and Leontine Alkema. 2022. “A Bayesian Cohort Component Projection Model to Estimate\nWomen of Reproductive Age at the Subnational Level in Data-Sparse\nSettings.” Demography 59 (5): 1713–37. https://doi.org/10.1215/00703370-10216406.\n\n\nAlexander, Monica, Mathew Kiang, and Magali Barbieri. 2018.\n“Trends in Black and White Opioid Mortality in the United States,\n1979–2015.” Epidemiology 29 (5): 707–15. https://doi.org/10.1097/EDE.0000000000000858.\n\n\nAlexander, Rohan, and Monica Alexander. 2021. “The Increased\nEffect of Elections and Changing Prime Ministers on Topics Discussed in\nthe Australian Federal Parliament Between 1901 and 2018.” https://doi.org/10.48550/arXiv.2111.09299.\n\n\nAlexander, Rohan, and Paul Hodgetts. 2021.\nAustralianPoliticians: Provides Datasets About Australian\nPoliticians. https://CRAN.R-project.org/package=AustralianPoliticians.\n\n\nAlexander, Rohan, and A Mahfouz. 2021. heapsofpapers: Easily Download Heaps of PDF and CSV\nFiles. https://CRAN.R-project.org/package=heapsofpapers.\n\n\nAlexander, Rohan, and Zachary Ward. 2018. “Age at Arrival and\nAssimilation During the Age of Mass Migration.” The Journal\nof Economic History 78 (3): 904–37. https://doi.org/10.1017/S0022050718000335.\n\n\nAlexopoulos, Michelle, and Jon Cohen. 2015. “The power of print: Uncertainty shocks, markets, and the\neconomy.” International Review of Economics\n& Finance 40 (November): 8–28. https://doi.org/10.1016/j.iref.2015.02.002.\n\n\nAllen, Jeff. 2021. plumberDeploy: Plumber\nDeployment. https://CRAN.R-project.org/package=plumberDeploy.\n\n\nAlsan, Marcella, and Amy Finkelstein. 2021. “Beyond Causality:\nAdditional Benefits of Randomized Controlled Trials for Improving Health\nCare Delivery.” The Milbank Quarterly 99 (4): 864–81. https://doi.org/10.1111/1468-0009.12521.\n\n\nAlsan, Marcella, and Marianne Wanamaker. 2018. “Tuskegee and the\nHealth of Black Men.” The Quarterly Journal of Economics\n133 (1): 407–55. https://doi.org/10.1093/qje/qjx029.\n\n\nAltman, Douglas, and Martin Bland. 1995. “Statistics notes: The normal distribution.”\nBMJ 310 (6975): 298–98. https://doi.org/10.1136/bmj.310.6975.298.\n\n\nAmaka, Ofunne, and Amber Thomas. 2021. “The Naked Truth: How the\nNames of 6,816 Complexion Products Can Reveal Bias in Beauty.”\nThe Pudding, March. https://pudding.cool/2021/03/foundation-names/.\n\n\nAmerican Medical Association and New York Academy of Medicine. 1848.\nCode of Medical Ethics. Academy of Medicine. https://hdl.handle.net/2027/chi.57108026.\n\n\nAndersen, Robert, and David Armstrong. 2021. Presenting Statistical\nResults Effectively. London: Sage.\n\n\nAnderson, Margo, and Stephen Fienberg. 1999. Who Counts?: The Politics of Census-Taking in\nContemporary America. Russell Sage Foundation. http://www.jstor.org/stable/10.7758/9781610440059.\n\n\nAndrews, David, and Agnes Herzberg. 2012. Data: A Collection of\nProblems from Many Fields for the Student and Research Worker. New\nYork: Springer Science & Business Media.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of\nLow Advertising Revenues.” American Economic Journal:\nMicroeconomics 11 (3): 319–64. https://doi.org/10.1257/mic.20170306.\n\n\nAngrist, Joshua, and Alan Krueger. 2001. “Instrumental Variables\nand the Search for Identification: From Supply and Demand to Natural\nExperiments.” Journal of Economic Perspectives 15 (4):\n69–85. https://doi.org/10.1257/jep.15.4.69.\n\n\nAngrist, Joshua, and Jörn-Steffen Pischke. 2010. “The Credibility\nRevolution in Empirical Economics: How Better Research Design Is Taking\nthe Con Out of Econometrics.” Journal of Economic\nPerspectives 24 (2): 3–30. https://doi.org/10.1257/jep.24.2.3.\n\n\nAnnas, George. 2003. “HIPAA Regulations: A New Era of\nMedical-Record Privacy?” New England Journal of Medicine\n348 (15): 1486–90. https://doi.org/10.1056/NEJMlim035027.\n\n\nAnsolabehere, Stephen, Brian Schaffner, and Sam Luks. 2021. “Guide to the 2020 Cooperative Election\nStudy.” https://doi.org/10.7910/DVN/E9N6PH.\n\n\nAprameya, Lavanya. 2020. “Improving Duolingo, One Experiment at a\nTime.” Duolingo Blog, January. https://blog.duolingo.com/improving-duolingo-one-experiment-at-a-time/.\n\n\nArel-Bundock, Vincent. 2021. WDI: World\nDevelopment Indicators and Other World Bank Data. https://CRAN.R-project.org/package=WDI.\n\n\n———. 2022. “modelsummary: Data and\nModel Summaries in R.” Journal of Statistical\nSoftware 103 (1): 1–23. https://doi.org/10.18637/jss.v103.i01.\n\n\n———. 2023. marginaleffects: Predictions,\nComparisons, Slopes, Marginal Means, and Hypothesis Tests.\nhttps://vincentarelbundock.github.io/marginaleffects/.\n\n\nArel-Bundock, Vincent, Ryan Briggs, Hristos Doucouliagos, Marco Mendoza\nAviña, and T. D. Stanley. 2022. “Quantitative Political Science\nResearch Is Greatly Underpowered.” https://osf.io/bzj9y/.\n\n\nArmstrong, Zan. 2022. “Stop Aggregating Away the Signal in Your\nData.” The Overflow, March. https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/.\n\n\nArnold, Jeffrey. 2021. ggthemes: Extra Themes,\nScales and Geoms for “ggplot2”. https://CRAN.R-project.org/package=ggthemes.\n\n\nAsher, Sam, Tobias Lunt, Ryu Matsuura, and Paul Novosad. 2021.\n“Development Research at High Geographic Resolution: An Analysis\nof Night Lights, Firms, and Poverty in India Using the SHRUG Open Data\nPlatform.” World Bank Economic Review 35 (4). https://shrug-assets-ddl.s3.amazonaws.com/static/main/assets/other/almn-shrug.pdf.\n\n\nAthey, Susan, and Guido Imbens. 2017a. “The Econometrics of\nRandomized Experiments.” In Handbook of Field\nExperiments, 73–140. Elsevier. https://doi.org/10.1016/bs.hefe.2016.10.003.\n\n\n———. 2017b. “The State of Applied Econometrics: Causality and\nPolicy Evaluation.” Journal of Economic Perspectives 31\n(2): 3–32. https://doi.org/10.1257/jep.31.2.3.\n\n\nAthey, Susan, Guido Imbens, Jonas Metzger, and Evan Munro. 2021.\n“Using Wasserstein Generative Adversarial Networks for the Design\nof Monte Carlo Simulations.” Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2020.09.013.\n\n\nAu, Randy. 2020. “Data Cleaning IS Analysis, Not Grunt\nWork,” September. https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt.\n\n\n———. 2022. “Celebrating Everyone Counting Things,”\nFebruary. https://counting.substack.com/p/celebrating-everyone-counting-things.\n\n\nBååth, Rasmus. 2018. beepr: Easily Play\nNotification Sounds on any Platform. https://CRAN.R-project.org/package=beepr.\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. magrittr: A Forward-Pipe Operator for R. https://CRAN.R-project.org/package=magrittr.\n\n\nBackus, John. 1981. “The History of FORTRAN\nI, II, and III.” In History of Programming\nLanguages, edited by Richard Wexelblat, 25–74. Academic Press.\n\n\nBailey, Rosemary. 2008. Design of Comparative Experiments.\nCambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511611483.\n\n\nBaio, Gianluca, and Marta Blangiardo. 2010. “Bayesian Hierarchical\nModel for the Prediction of Football Results.” Journal of\nApplied Statistics 37 (2): 253–64. https://doi.org/10.1080/02664760802684177.\n\n\nBaker, Dominique. 2023. “Scams Will Not Save Us (Tuition\nDollars),” February. http://www.dominiquebaker.com/blog/2023/2/16/scams-will-not-save-us-tuition-dollars.\n\n\nBaker, Reg, Michael Brick, Nancy Bates, Mike Battaglia, Mick Couper,\nJill Dever, Krista Gile, and Roger Tourangeau. 2013. “Summary Report of the AAPOR Task Force on Non-Probability\nSampling.” Journal of Survey Statistics and\nMethodology 1 (2): 90–143. https://doi.org/10.1093/jssam/smt008.\n\n\nBandy, Jack, and Nicholas Vincent. 2021. “Addressing\n‘Documentation Debt’ in Machine Learning Research: A\nRetrospective Datasheet for BookCorpus.” arXiv. https://doi.org/10.48550/arXiv.2105.05241.\n\n\nBanerjee, Abhijit, and Esther Duflo. 2011. Poor Economics: A Radical\nRethinking of the Way to Fight Global Poverty. New York:\nPublicAffairs.\n\n\nBanerjee, Abhijit, Esther Duflo, Rachel Glennerster, and Cynthia Kinnan.\n2015. “The Miracle of Microfinance? Evidence from a Randomized\nEvaluation.” American Economic Journal: Applied\nEconomics 7 (1): 22–53. https://doi.org/10.1257/app.20130533.\n\n\nBanes, Graham, Emily Fountain, Alyssa Karklus, Robert Fulton, Lucinda\nAntonacci-Fulton, and Joanne Nelson. 2022. “Nine out of ten samples were mistakenly switched by The\nOrang-utan Genome Consortium.” Scientific Data 9\n(1). https://doi.org/10.1038/s41597-022-01602-0.\n\n\nBarba, Lorena. 2018. “Terminologies for Reproducible\nResearch.” https://arxiv.org/abs/1802.03311.\n\n\nBarrett, Malcolm. 2021a. Data Science as an Atomic Habit. https://malco.io/2021/01/04/data-science-as-an-atomic-habit/.\n\n\n———. 2021b. ggdag: Analyze and Create Elegant\nDirected Acyclic Graphs. https://CRAN.R-project.org/package=ggdag.\n\n\n———. 2021c. “Data Science as an Atomic Habit,” January. https://malco.io/articles/2021-01-04-data-science-as-an-atomic-habit.\n\n\nBarron, Alexander, Jenny Huang, Rebecca Spang, and Simon DeDeo. 2018.\n“Individuals, Institutions, and Innovation in the Debates of the\nFrench Revolution.” Proceedings of the National Academy of\nSciences 115 (18): 4607–12. https://doi.org/10.1073/pnas.1717729115.\n\n\nBaumer, Benjamin, Daniel Kaplan, and Nicholas Horton. 2021.\nModern Data Science With R. 2nd ed. Chapman;\nHall/CRC. https://mdsr-book.github.io/mdsr2e/.\n\n\nBaumgartner, Jason, Savvas Zannettou, Brian Keegan, Megan Squire, and\nJeremy Blackburn. 2020. “The Pushshift Reddit Dataset.”\narXiv. https://doi.org/10.48550/arxiv.2001.08435.\n\n\nBaumgartner, Peter. 2021. “Ways I Use Testing\nas a Data Scientist,” December. https://www.peterbaumgartner.com/blog/testing-for-data-science/.\n\n\nBeaumont, Jean-Francois. 2020. “Are Probability Surveys Bound to\nDisappear for the Production of Official Statistics?” Survey\nMethodology 46 (1): 1–29.\n\n\nBeauregard, Katrine, and Jill Sheppard. 2021. “Antiwomen but\nProquota: Disaggregating Sexism and Support for Gender Quota\nPolicies.” Political Psychology 42 (2): 219–37. https://doi.org/10.1111/pops.12696.\n\n\nBecker, Richard, Allan Wilks, Ray Brownrigg, Thomas Minka, and Alex\nDeckmyn. 2022. maps: Draw Geographical\nMaps. https://CRAN.R-project.org/package=maps.\n\n\nBeelen, Kaspar, Timothy Alberdingk Thim, Christopher Cochrane, Kees\nHalvemaan, Graeme Hirst, Michael Kimmins, Sander Lijbrink, et al. 2017.\n“Digitization of the Canadian Parliamentary Debates.”\nCanadian Journal of Political Science 50 (3): 849–64.\n\n\nBender, Emily, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. “On the Dangers of Stochastic Parrots: Can\nLanguage Models Be Too Big?” In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and\nTransparency. ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBengtsson, Henrik. 2021. “A Unifying\nFramework for Parallel and Distributed Processing in R using\nFutures.” The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBenoit, Kenneth. 2020. “Text as Data: An Overview.” In\nThe SAGE Handbook of Research Methods in Political Science and\nInternational Relations, edited by Luigi Curini and Robert\nFranzese, 461–97. London: SAGE Publishing. https://doi.org/10.4135/9781526486387.n29.\n\n\nBenoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,\nStefan Müller, and Akitaka Matsuo. 2018. “quanteda: An R package for the quantitative analysis of\ntextual data.” Journal of Open Source Software 3\n(30): 774. https://doi.org/10.21105/joss.00774.\n\n\nBensinger, Greg. 2020. “Google Redraws the Borders on Maps\nDepending on Who’s Looking.” The Washington Post,\nFebruary. https://www.washingtonpost.com/technology/2020/02/14/google-maps-political-borders/.\n\n\nBerdine, Gilbert, Vincent Geloso, and Benjamin Powell. 2018.\n“Cuban Infant Mortality and Longevity: Health Care or\nRepression?” Health Policy and Planning 33 (6): 755–57.\nhttps://doi.org/10.1093/heapol/czy033.\n\n\nBerkson, Joseph. 1946. “Limitations of the Application of Fourfold\nTable Analysis to Hospital Data.” Biometrics Bulletin 2\n(3): 47–53. https://doi.org/10.2307/3002000.\n\n\nBerners-Lee, Timothy. 1989. “Information Management: A\nProposal.” https://www.w3.org/History/1989/proposal.html.\n\n\nBerry, Donald. 1989. “Comment: Ethics and ECMO.”\nStatistical Science 4 (4): 306–10. https://www.jstor.org/stable/2245830.\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and\nGreg More Employable Than Lakisha and Jamal? A Field Experiment on Labor\nMarket Discrimination.” American Economic Review 94 (4):\n991–1013. https://doi.org/10.1257/0002828042002561.\n\n\nBethlehem, R. A. I., J. Seidlitz, S. R. White, J. W. Vogel, K. M.\nAnderson, C. Adamson, S. Adler, et al. 2022. “Brain Charts for the\nHuman Lifespan.” Nature 604 (7906): 525–33. https://doi.org/10.1038/s41586-022-04554-y.\n\n\nBetz, Timm, Scott Cook, and Florian Hollenbach. 2018. “On the Use\nand Abuse of Spatial Instruments.” Political Analysis 26\n(4): 474–79. https://doi.org/10.1017/pan.2018.10.\n\n\nBickel, Peter, Eugene Hammel, and William O’Connell. 1975. “Sex\nBias in Graduate Admissions: Data from Berkeley: Measuring Bias Is\nHarder Than Is Usually Assumed, and the Evidence Is Sometimes Contrary\nto Expectation.” Science 187 (4175): 398–404. https://doi.org/10.1126/science.187.4175.398.\n\n\nBiderman, Stella, Kieran Bicheno, and Leo Gao. 2022. “Datasheet\nfor the Pile.” https://arxiv.org/abs/2201.07311.\n\n\nBirkmeyer, John, Jonathan Finks, Amanda O’Reilly, Mary Oerline, Arthur\nCarlin, Andre Nunn, Justin Dimick, Mousumi Banerjee, and Nancy\nBirkmeyer. 2013. “Surgical Skill and Complication Rates After\nBariatric Surgery.” New England Journal of Medicine 369\n(15): 1434–42. https://doi.org/10.1056/nejmsa1300625.\n\n\nBlair, Ed, Seymour Sudman, Norman M Bradburn, and Carol Stocking. 1977.\n“How to Ask Questions about Drinking and Sex: Response Effects in\nMeasuring Consumer Behavior.” Journal of Marketing\nResearch 14 (3): 316–21. https://doi.org/10.2307/3150769.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys.\n2019. “Declaring and Diagnosing Research Designs.”\nAmerican Political Science Review 113 (3): 838–59. https://doi.org/10.1017/S0003055419000194.\n\n\nBlair, Graeme, Jasper Cooper, Alexander Coppock, Macartan Humphreys, and\nLuke Sonnet. 2021. estimatr: Fast Estimators\nfor Design-Based Inference. https://CRAN.R-project.org/package=estimatr.\n\n\nBlair, James. 2019. Democratizing R with\nPlumber APIs. https://posit.co/resources/videos/democratizing-r-with-plumber-apis/.\n\n\nBland, Martin, and Douglas Altman. 1986. “Statistical Methods for\nAssessing Agreement Between Two Methods of Clinical Measurement.”\nThe Lancet 327 (8476): 307–10. https://doi.org/10.1016/S0140-6736(86)90837-8.\n\n\nBlei, David. 2012. “Probabilistic Topic Models.”\nCommunications of the ACM 55 (4): 77–84. https://doi.org/10.1145/2133806.2133826.\n\n\nBlei, David, Andrew Ng, and Michael Jordan. 2003. “Latent\nDirichlet Allocation.” Journal of Machine Learning\nResearch 3 (Jan): 993–1022. https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf.\n\n\nBloom, Howard, Andrew Bell, and Kayla Reiman. 2020. “Using Data\nfrom Randomized Trials to Assess the Likely Generalizability of\nEducational Treatment-Effect Estimates from Regression Discontinuity\nDesigns.” Journal of Research on Educational\nEffectiveness 13 (3): 488–517. https://doi.org/10.1080/19345747.2019.1634169.\n\n\nBoland, Philip. 1984. “A Biographical Glimpse of William Sealy\nGosset.” The American Statistician 38 (3): 179–83. https://doi.org/10.2307/2683648.\n\n\nBolker, Ben, and David Robinson. 2022. broom.mixed: Tidying Methods for Mixed\nModels. https://CRAN.R-project.org/package=broom.mixed.\n\n\nBolton, Ruth, and Randall Chapman. 1986. “Searching for Positive\nReturns at the Track.” Management Science 32 (August):\n1040–60. https://doi.org/10.1287/mnsc.32.8.1040.\n\n\nBombieri, Giulia, Vincenzo Penteriani, Kamran Almasieh, Hüseyin Ambarlı,\nMohammad Reza Ashrafzadeh, Chandan Surabhi Das, Nishith Dharaiya, et al.\n2023. “A Worldwide Perspective on Large Carnivore Attacks on\nHumans.” PLOS Biology 21 (1): e3001946. https://doi.org/10.1371/journal.pbio.3001946.\n\n\nBor, Jacob, Atheendar Venkataramani, David Williams, and Alexander Tsai.\n2018. “Police Killings and Their Spillover Effects on the Mental\nHealth of Black Americans: A Population-Based, Quasi-Experimental\nStudy.” The Lancet 392 (10144): 302–10. https://doi.org/10.1016/s0140-6736(18)31130-9.\n\n\nBorer, Elizabeth T., Eric W. Seabloom, Matthew B. Jones, and Mark\nSchildhauer. 2009. “Some Simple Guidelines for Effective Data\nManagement.” Bulletin of the Ecological Society of\nAmerica 90 (2): 205–14. https://doi.org/10.1890/0012-9623-90.2.205.\n\n\nBorghi, John, and Ana Van Gulick. 2022. “Promoting Open Science\nThrough Research Data Management.” Harvard Data Science\nReview 4 (3). https://doi.org/10.1162/99608f92.9497f68e.\n\n\nBorkin, Michelle, Zoya Bylinskii, Nam Wook Kim, Constance May\nBainbridge, Chelsea Yeh, Daniel Borkin, Hanspeter Pfister, and Aude\nOliva. 2015. “Beyond Memorability: Visualization Recognition and\nRecall.” IEEE Transactions on Visualization and Computer\nGraphics 22 (1): 519–28. https://doi.org/10.1109/TVCG.2015.2467732.\n\n\nBosch, Oriol, and Melanie Revilla. 2022. “When survey science met web tracking: Presenting an error\nframework for metered data.” Journal of the Royal\nStatistical Society: Series A (Statistics in Society), November,\n1–29. https://doi.org/10.1111/rssa.12956.\n\n\nBouguen, Adrien, Yue Huang, Michael Kremer, and Edward Miguel. 2019.\n“Using Randomized Controlled Trials to Estimate Long-Run Impacts\nin Development Economics.” Annual Review of Economics 11\n(1): 523–61. https://doi.org/10.1146/annurev-economics-080218-030333.\n\n\nBouie, Jamelle. 2022. “We Still Can’t See American Slavery for\nWhat It Was.” The New York Times, January. https://www.nytimes.com/2022/01/28/opinion/slavery-voyages-data-sets.html.\n\n\nBowen, Claire McKay. 2022. Protecting Your\nPrivacy in a Data-Driven World. 1st ed. Chapman; Hall/CRC.\nhttps://doi.org/10.1201/9781003122043.\n\n\nBowers, Jake, and Maarten Voors. 2016. “How to Improve Your\nRelationship with Your Future Self.” Revista de Ciencia\nPolı́tica 36 (3): 829–48. https://doi.org/10.4067/S0718-090X2016000300011.\n\n\nBowley, Arthur Lyon. 1901. Elements of Statistics. London: P.\nS. King.\n\n\n———. 1913. “Working-Class Households in Reading.”\nJournal of the Royal Statistical Society 76 (7): 672–701. https://doi.org/10.2307/2339708.\n\n\nBox, George E. P. 1976. “Science and Statistics.”\nJournal of the American Statistical Association 71 (356):\n791–99. https://doi.org/10.1080/01621459.1976.10480949.\n\n\nBoykis, Vicki. 2019. “A Deep Dive on Python Type Hints,”\nJuly. https://vickiboykis.com/2019/07/08/a-deep-dive-on-python-type-hints/.\n\n\nBoysel, Sam, and Davis Vaughan. 2021. fredr: An\nR Client for the “FRED” API. https://CRAN.R-project.org/package=fredr.\n\n\nBradley, Valerie, Shiro Kuriwaki, Michael Isakov, Dino Sejdinovic,\nXiao-Li Meng, and Seth Flaxman. 2021. “Unrepresentative Big\nSurveys Significantly Overestimated US Vaccine\nUptake.” Nature 600 (7890): 695–700. https://doi.org/10.1038/s41586-021-04198-4.\n\n\nBraginsky, Mika. 2020. wordbankr: Accessing the\nWordbank Database. https://CRAN.R-project.org/package=wordbankr.\n\n\nBrandt, Allan. 1978. “Racism and Research: The Case of the\nTuskegee Syphilis Study.” Hastings Center Report, 21–29.\nhttps://doi.org/10.2307/3561468.\n\n\nBreiman, Leo. 1994. “The 1991 Census Adjustment: Undercount or Bad\nData?” Statistical Science 9 (4). https://doi.org/10.1214/ss/1177010259.\n\n\n———. 2001. “Statistical Modeling: The Two Cultures.”\nStatistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nBremer, Nadieh, and Shirley Wu. 2021. Data Sketches. A K\nPeters/CRC Press. https://doi.org/10.1201/9780429445019.\n\n\nBrewer, Cynthia. 2015. Designing Better Maps: A Guide for GIS\nUsers. 2nd ed.\n\n\nBrewer, Ken. 2013. “Three Controversies in the History of Survey\nSampling.” Survey Methodology 39 (2): 249–63.\n\n\nBreznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung HV Nguyen, Muna\nAdem, Jule Adriaans, Amalia Alvarez-Benjumea, et al. 2022.\n“Observing Many Researchers Using the Same Data and Hypothesis\nReveals a Hidden Universe of Uncertainty.” Proceedings of the\nNational Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.2203150119.\n\n\nBriggs, Ryan. 2021. “Why Does Aid Not Target the Poorest?”\nInternational Studies Quarterly 65 (3): 739–52. https://doi.org/10.1093/isq/sqab035.\n\n\nBrodeur, Abel, Nikolai Cook, and Anthony Heyes. 2020. “Methods Matter: p-Hacking and Publication Bias in Causal\nAnalysis in Economics.” American Economic Review\n110 (11): 3634–60. https://doi.org/10.1257/aer.20190687.\n\n\nBrokowski, Carolyn, and Mazhar Adli. 2019. “CRISPR Ethics: Moral\nConsiderations for Applications of a Powerful Tool.” Journal\nof Molecular Biology 431 (1): 88–101. https://doi.org/10.1016/j.jmb.2018.05.044.\n\n\nBronner, Laura. 2020. “Why Statistics Don’t Capture the Full\nExtent of the Systemic Bias in Policing.”\nFiveThirtyEight, June. https://fivethirtyeight.com/features/why-statistics-dont-capture-the-full-extent-of-the-systemic-bias-in-policing/.\n\n\n———. 2021. “Quantitative Editing.” YouTube, June.\nhttps://youtu.be/LI5m9RzJgWc.\n\n\nBrontë, Charlotte. 1847. Jane Eyre. https://www.gutenberg.org/files/1260/1260-h/1260-h.htm.\n\n\n———. 1857. The Professor. https://www.gutenberg.org/files/1028/1028-h/1028-h.htm.\n\n\nBrook, Robert, John Ware, William Rogers, Emmett Keeler, Allyson Ross\nDavies, Cathy Sherbourne, George Goldberg, Kathleen Lohr, Patricia Camp,\nand Joseph Newhouse. 1984. “The Effect of Coinsurance on the\nHealth of Adults: Results from the RAND Health Insurance\nExperiment.” https://www.rand.org/pubs/reports/R3055.html.\n\n\nBrown, Zack. 2018. “A Git Origin Story.” Linux\nJournal, July. https://www.linuxjournal.com/content/git-origin-story.\n\n\nBryan, Jenny. 2015. “Naming Things.” Reproducible\nScience Workshop, May. https://speakerdeck.com/jennybc/how-to-name-files.\n\n\n———. 2018a. “Excuse Me, Do You Have a Moment to Talk about Version\nControl?” The American Statistician 72 (1): 20–27. https://doi.org/10.1080/00031305.2017.1399928.\n\n\n———. 2018b. “Code Smells and Feels.” YouTube,\nJuly. https://youtu.be/7oyiPBjLAWY.\n\n\n———. 2020. Happy Git and GitHub for the\nuseR. https://happygitwithr.com.\n\n\nBryan, Jenny, and Jim Hester. 2020. What They\nForgot to Teach You About R. https://rstats.wtf/index.html.\n\n\nBryan, Jenny, Jim Hester, David Robinson, Hadley Wickham, and Christophe\nDervieux. 2022. reprex: Prepare Reproducible\nExample Code via the Clipboard. https://CRAN.R-project.org/package=reprex.\n\n\nBryan, Jenny, and Hadley Wickham. 2021. gh:\nGitHub API. https://CRAN.R-project.org/package=gh.\n\n\nBuckheit, Jonathan, and David Donoho. 1995. “Wavelab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer. https://doi.org/10.1007/978-1-4612-2544-7_5.\n\n\nBueno de Mesquita, Ethan, and Anthony Fowler. 2021. Thinking Clearly\nwith Data: A Guide to Quantitative Reasoning and Analysis. New\nJersey: Princeton University Press.\n\n\nBuhr, Ray. 2017. Using R as a Production\nMachine Learning Language (Part I). https://raybuhr.github.io/blog/posts/making-predictions-over-http/.\n\n\nBuja, Andreas, Dianne Cook, and Deborah Swayne. 1996. “Interactive\nHigh-Dimensional Data Visualization.” Journal of\nComputational and Graphical Statistics 5 (1): 78–99. https://doi.org/10.2307/1390754.\n\n\nBuneman, Peter, Sanjeev Khanna, and Tan Wang-Chiew. 2001. “Why and\nWhere: A Characterization of Data Provenance.” In Database\nTheory  ICDT 2001, 316–30. Springer\nBerlin Heidelberg. https://doi.org/10.1007/3-540-44503-x_20.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Conference on Fairness, Accountability\nand Transparency, 77–91.\n\n\nBurch, Tyler James. 2023. “2023 NHL Playoff\nPredictions,” April. https://tylerjamesburch.com/blog/misc/nhl-predictions.\n\n\nBurton, Jason, Nicole Cruz, and Ulrike Hahn. 2021. “Reconsidering\nEvidence of Moral Contagion in Online Social Networks.”\nNature Human Behaviour 5 (12): 1629–35. https://doi.org/10.1038/s41562-021-01133-5.\n\n\nBush, Vannevar. 1945. “As We May Think.” The Atlantic\nMonthly, July. https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/.\n\n\nByrd, James Brian, Anna Greene, Deepashree Venkatesh Prasad, Xiaoqian\nJiang, and Casey Greene. 2020. “Responsible, Practical Genomic\nData Sharing That Accelerates Research.” Nature Reviews\nGenetics 21 (10): 615–29. https://doi.org/10.1038/s41576-020-0257-5.\n\n\nCahill, Niamh, Michelle Weinberger, and Leontine Alkema. 2020.\n“What Increase in Modern Contraceptive Use Is Needed in FP2020\nCountries to Reach 75% Demand Satisfied by 2030? An Assessment Using the\nAccelerated Transition Method and Family Planning Estimation\nModel.” Gates Open Research 4. https://doi.org/10.12688/gatesopenres.13125.1.\n\n\nCalonico, Sebastian, Matias Cattaneo, Max Farrell, and Rocio Titiunik.\n2021. rdrobust: Robust Data-Driven Statistical\nInference in Regression-Discontinuity Designs. https://CRAN.R-project.org/package=rdrobust.\n\n\nCambon, Jesse, and Christopher Belanger. 2021. “tidygeocoder: Geocoding Made Easy.” Zenodo.\nhttps://doi.org/10.5281/zenodo.3981510.\n\n\nCanty, Angelo, and B. D. Ripley. 2021. boot:\nBootstrap R (S-Plus) Functions.\n\n\nCardoso, Tom. 2020. “Bias behind bars: A\nGlobe investigation finds a prison system stacked against Black and\nIndigenous inmates.” The Globe and Mail, October.\nhttps://www.theglobeandmail.com/canada/article-investigation-racial-bias-in-canadian-prison-risk-assessments/.\n\n\nCarleton, Chris. 2021. “wccarleton/conflict-europe: Acce.” Zenodo.\nhttps://doi.org/10.5281/zenodo.4550688.\n\n\nCarleton, Chris, Dave Campbell, and Mark Collard. 2021. “A\nReassessment of the Impact of Temperature Change on European Conflict\nDuring the Second Millennium CE Using a Bespoke Bayesian Time-Series\nModel.” Climatic Change 165 (1): 1–16. https://doi.org/10.1007/s10584-021-03022-2.\n\n\nCaro, Robert. 2019. Working. 1st ed. New York: Knopf.\n\n\nCarpenter, Christopher, and Carlos Dobkin. 2014. “Replication data for: The Minimum Legal Drinking Age and\nCrime.” https://doi.org/10.7910/DVN/27070.\n\n\n———. 2015. “The Minimum Legal Drinking Age\nand Crime.” The Review of Economics and\nStatistics 97 (2): 521–24. https://doi.org/10.1162/REST_a_00489.\n\n\nCarroll, Lewis. 1871. Through the Looking-Glass. Macmillan. https://www.gutenberg.org/files/12/12-h/12-h.htm.\n\n\nCastro, Marcia, Susie Gurzenda, Cassio Turra, Sun Kim, Theresa\nAndrasfay, and Noreen Goldman. 2023. “Research Note:\nCOVID-19 Is Not an Independent Cause of Death.”\nDemography, February. https://doi.org/10.1215/00703370-10575276.\n\n\nCaughey, Devin, and Jasjeet Sekhon. 2011. “Elections and the Regression Discontinuity Design:\nLessons from Close U.S. House Races, 1942–2008.”\nPolitical Analysis 19 (4): 385–408. https://doi.org/10.1093/pan/mpr032.\n\n\nChamberlain, Scott, Hadley Wickham, Winston Chang, and Mauricio Vargas.\n2022. Analogsea: Interface to “Digital Ocean”. https://CRAN.R-project.org/package=analogsea.\n\n\nChamberlin, Donald. 2012. “Early History of\nSQL.” IEEE Annals of the History of\nComputing 34 (4): 78–82. https://doi.org/10.1109/mahc.2012.61.\n\n\nChambliss, Daniel. 1989. “The Mundanity of Excellence: An\nEthnographic Report on Stratification and Olympic Swimmers.”\nSociological Theory 7 (1): 70–86. https://doi.org/10.2307/202063.\n\n\nChambru, Cédric, and Paul Maneuvrier-Hervieu. 2022. “Introducing HiSCoD: A new gateway for the study of\nhistorical social conflict.” Working Paper Series,\nDepartment of Economics, University of Zurich. https://doi.org/10.5167/uzh-217109.\n\n\nChan, Duo. 2021. “Combining Statistical, Physical, and Historical\nEvidence to Improve Historical Sea-Surface Temperature Records.”\nHarvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.edcee38f.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2021. shiny: Web Application Framework\nfor R. https://CRAN.R-project.org/package=shiny.\n\n\nChase, William. 2020. “The Glamour of Graphics.”\nRStudio Conference, January. https://posit.co/resources/videos/the-glamour-of-graphics/.\n\n\nChawla, Dalmeet Singh. 2020. “Critiqued Coronavirus Simulation\nGets Thumbs up from Code-Checking Efforts.” Nature 582:\n323–24. https://doi.org/10.1038/d41586-020-01685-y.\n\n\nChellel, Kit. 2018. “The Gambler Who Cracked the Horse-Racing\nCode.” Bloomberg Businessweek, May. https://www.bloomberg.com/news/features/2018-05-03/the-gambler-who-cracked-the-horse-racing-code.\n\n\nChen, Heng, Marie-Hélène Felt, and Christopher Henry. 2018. “2017\nMethods-of-Payment Survey: Sample Calibration and Variance\nEstimation.” Bank of Canada. https://doi.org/10.34989/tr-114.\n\n\nChen, Wei, Xilu Chen, Chang-Tai Hsieh, and Zheng Song. 2019. “A\nForensic Examination of China’s National Accounts.” Brookings\nPapers on Economic Activity, 77–127. https://www.jstor.org/stable/26798817.\n\n\nChen, Weijun, Yan Qi, Yuwen Zhang, Christina Brown, Akos Lada, and\nHarivardan Jayaraman. 2022. “Notifications: Why Less Is\nMore,” December. https://medium.com/@AnalyticsAtMeta/notifications-why-less-is-more-how-facebook-has-been-increasing-both-user-satisfaction-and-app-9463f7325e7d.\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2021. leaflet: Create Interactive Web Maps with the JavaScript\n“Leaflet” Library. https://CRAN.R-project.org/package=leaflet.\n\n\nCheriet, Mohamed, Nawwaf Kharma, Cheng-Lin Liu, and Ching Suen. 2007.\nCharacter Recognition Systems: A Guide for Students and\nPractitioner. Wiley.\n\n\nChouldechova, Alexandra, Diana Benavides-Prado, Oleksandr Fialko, and\nRhema Vaithianathan. 2018. “A Case Study of Algorithm-Assisted\nDecision Making in Child Maltreatment Hotline Screening\nDecisions.” In Proceedings of the 1st Conference on Fairness,\nAccountability and Transparency, edited by Sorelle Friedler and\nChristo Wilson, 81:134–48. Proceedings of Machine Learning Research. https://proceedings.mlr.press/v81/chouldechova18a.html.\n\n\nChrétien, Jean. 2007. My Years as Prime Minister. 1st ed.\nToronto: Knopf Canada.\n\n\nChristensen, Garret, Allan Dafoe, Edward Miguel, Don Moore, and Andrew\nRose. 2019. “A Study of the Impact of Data Sharing on Article\nCitations Using Journal Policies as a Natural Experiment.”\nPLOS ONE 14 (12): e0225883. https://doi.org/10.1371/journal.pone.0225883.\n\n\nChristensen, Garret, Jeremy Freese, and Edward Miguel. 2019.\nTransparent and Reproducible Social Science Research.\nCalifornia: University of California Press.\n\n\nChristian, Brian. 2012. “The A/B Test: Inside\nthe Technology That’s Changing the Rules of Business.”\nWired, April. https://www.wired.com/2012/04/ff-abtesting/.\n\n\nCirone, Alexandra, and Arthur Spirling. 2021. “Turning History\ninto Data: Data Collection, Measurement, and Inference in HPE.”\nJournal of Historical Political Economy 1 (1): 127–54. https://doi.org/10.1561/115.00000005.\n\n\nCity of Toronto. 2021. 2021 Street Needs Assessment. https://www.toronto.ca/city-government/data-research-maps/research-reports/housing-and-homelessness-research-and-reports/.\n\n\nCleveland, William. (1985) 1994. The Elements of Graphing Data.\n2nd ed. New Jersey: Hobart Press.\n\n\nClinton, Joshua, John Lapinski, and Marc Trussler. 2022.\n“Reluctant Republicans, Eager Democrats?” Public\nOpinion Quarterly 86 (2): 247–69. https://doi.org/10.1093/poq/nfac011.\n\n\nCohen, Glenn, and Michelle Mello. 2018. “HIPAA and\nProtecting Health Information in the 21st Century.”\nJAMA 320 (3): 231. https://doi.org/10.1001/jama.2018.5630.\n\n\nCohen, Jason, Steven Teleki, and Eric Brown. 2006. Best Kept Secrets\nof Peer Code Review. Smart Bear Incorporated.\n\n\nCohn, Alain. 2019. “Data and code for: Civic\nHonesty Around the Globe.” Harvard Dataverse. https://doi.org/10.7910/dvn/ykbodn.\n\n\nCohn, Alain, Michel André Maréchal, David Tannenbaum, and Christian\nLukas Zünd. 2019a. “Civic Honesty Around the Globe.”\nScience 365 (6448): 70–73. https://doi.org/10.1126/science.aau8712.\n\n\n———. 2019b. “Supplementary Materials for: Civic Honesty Around the\nGlobe.” Science 365 (6448): 70–73.\n\n\nCohn, Nate. 2016. “We Gave Four Good Pollsters the Same Raw Data.\nThey Had Four Different Results.” The New York Times,\nSeptember. https://www.nytimes.com/interactive/2016/09/20/upshot/the-error-the-polling-world-rarely-talks-about.html.\n\n\nCollins, Annie, and Rohan Alexander. 2022. “Reproducibility of\nCOVID-19 Pre-Prints.” Scientometrics 127: 4655–73. https://doi.org/10.1007/s11192-022-04418-2.\n\n\nColombo, Tommaso, Holger Fröning, Pedro Javier Garcı̀a, and Wainer\nVandelli. 2016. “Optimizing the Data-Collection Time of a\nLarge-Scale Data-Acquisition System Through a Simulation\nFramework.” The Journal of Supercomputing 72 (12):\n4546–72. https://doi.org/10.1007/s11227-016-1764-1.\n\n\nComer, Benjamin P., and Jason R. Ingram. 2022. “Comparing Fatal\nEncounters, Mapping Police Violence, and Washington Post Fatal Police\nShooting Data from 2015-2019: A Research Note.” Criminal\nJustice Review, January, 073401682110710. https://doi.org/10.1177/07340168211071014.\n\n\nCook, Dianne, Nancy Reid, and Emi Tanaka. 2021. “The Foundation Is\nAvailable for Thinking about Data Visualization Inferentially.”\nHarvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.8453435d.\n\n\nCooley, David. 2020. mapdeck: Interactive Maps\nUsing “Mapbox GL JS” and\n“Deck.gl”. https://CRAN.R-project.org/package=mapdeck.\n\n\nCouncil of European Union. 2016. “General Data Protection\nRegulation 2016/679.” https://eur-lex.europa.eu/eli/reg/2016/679/oj.\n\n\nCowen, Tyler. 2021. “Episode 132: Amia Srinivasan on Utopian\nFeminism.” Conversations with Tyler, September. https://conversationswithtyler.com/episodes/amia-srinivasan/.\n\n\n———. 2023. “Episode 168: Katherine Rundell on the Art of\nWords.” Conversations with Tyler, January. https://conversationswithtyler.com/episodes/katherine-rundell/.\n\n\nCox, David. 2018. “In Gentle Praise of Significance Tests.”\nYouTube, October. https://youtu.be/txLj%5FP9UlCQ.\n\n\nCox, David, and Nancy Reid. 1987. “Parameter Orthogonality and\nApproximate Conditional Inference.” Journal of the Royal\nStatistical Society: Series B (Methodological) 49 (1): 1–18. https://doi.org/10.1111/j.2517-6161.1987.tb01422.x.\n\n\nCox, Murray. 2021. “Inside Airbnb—Toronto\nData.” http://insideairbnb.com/get-the-data.html.\n\n\nCoyle, Edward, Andrew Coggan, Mari Hopper, and Thomas Walters. 1988.\n“Determinants of Endurance in Well-Trained\nCyclists.” Journal of Applied Physiology 64 (6):\n2622–30. https://doi.org/10.1152/jappl.1988.64.6.2622.\n\n\nCraiu, Radu. 2019. “The Hiring Gambit: In Search of the Twofer\nData Scientist.” Harvard Data Science Review 1 (1). https://doi.org/10.1162/99608f92.440445cb.\n\n\nCramer, Jan Salomon. 2003. “The Origins of Logistic\nRegression.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.360300.\n\n\nCrane, Nicola, Stephanie Hazlitt, and Apache Arrow. 2023.\nApache Arrow R Cookbook. https://arrow.apache.org/cookbook/r/.\n\n\nCrawford, Kate. 2021. Atlas of AI.\n1st ed. New Haven: Yale University Press.\n\n\nCrosby, Alfred. 1997. The Measure of Reality: Quantification in\nWestern Europe, 1250-1600. Cambridge: Cambridge University Press.\n\n\nCsárdi, Gábor. 2022. gitcreds: Query\n“git” Credentials from “R”. https://CRAN.R-project.org/package=gitcreds.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan,\nand Dan Tenenbaum. 2021. remotes: R Package\nInstallation from Remote Repositories, Including\n“GitHub”. https://CRAN.R-project.org/package=remotes.\n\n\nCummins, Neil. 2022. “The Hidden Wealth of English Dynasties,\n1892–2016.” The Economic History Review 75 (3): 667–702.\nhttps://doi.org/10.1111/ehr.13120.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. 1st ed.\nNew Haven: Yale Press. https://mixtape.scunning.com.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. Data Feminism.\nMassachusetts: The MIT Press. https://data-feminism.mitpress.mit.edu.\n\n\nda Silva, Natalia, Dianne Cook, and Eun-Kyung Lee. 2023. “Interactive graphics for visually diagnosing forest\nclassifiers in R.” Computational Statistics,\nJanuary. https://doi.org/10.1007/s00180-023-01323-x.\n\n\nDagan, Noa, Noam Barda, Eldad Kepten, Oren Miron, Shay Perchik, Mark\nKatz, Miguel Hernán, Marc Lipsitch, Ben Reis, and Ran Balicer. 2021.\n“BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination\nSetting.” New England Journal of Medicine 384 (15):\n1412–23. https://doi.org/10.1056/NEJMoa2101765.\n\n\nDaston, Lorraine. 2000. “Why Statistics Tend Not Only to Describe\nthe World but to Change It.” London Review of Books 22\n(8). https://www.lrb.co.uk/the-paper/v22/n08/lorraine-daston/why-statistics-tend-not-only-to-describe-the-world-but-to-change-it.\n\n\nData and Justice Criminology Lab, Institute of Criminology and Criminal\nJustice, Carleton University; The Centre for Research & Innovation\nfor Black Survivors of Homicide Victims (The CRIB), at the\nFactor-Inwentash Faculty of Social Work, University of Toronto; Canadian\nCivil Liberties Association; Ethics and Technology Lab, Queen’s\nUniversity. 2022. “Tracking (in)justice: A Living Data Set\nTracking Canadian Police-Involved Deaths.” https://trackinginjustice.ca.\n\n\nDavidson, Thomas, Debasmita Bhattacharya, and Ingmar Weber. 2019.\n“Racial Bias in Hate Speech and Abusive Language Detection\nDatasets.” In Proceedings of the Third Workshop on Abusive\nLanguage Online, 25–35.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus\nDozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nDavis, Darren. 1997. “Nonrandom Measurement Error and Race of\nInterviewer Effects Among African Americans.” The Public\nOpinion Quarterly 61 (1): 183–207. https://doi.org/10.1086/297792.\n\n\nDavison, A. C., and D. V. Hinkley. 1997. Bootstrap Methods and Their\nApplications. Cambridge: Cambridge University Press. http://statwww.epfl.ch/davison/BMA/.\n\n\nDe Jonge, Edwin, and Mark van der Loo. 2013. An\nintroduction to data cleaning with R. Statistics Netherlands\nHeerlen. https://cran.r-project.org/doc/contrib/de%5FJonge+van%5Fder%5FLoo-Introduction%5Fto%5Fdata%5Fcleaning%5Fwith%5FR.pdf.\n\n\nDean, Natalie. 2022. “Tracking COVID-19 Infections:\nTime for Change.” Nature 602 (7896): 185. https://doi.org/10.1038/d41586-022-00336-8.\n\n\nDeaton, Angus. 2010. “Instruments, Randomization, and Learning\nabout Development.” Journal of Economic Literature 48\n(2): 424–55. https://doi.org/10.1257/jel.48.2.424.\n\n\nDenby, Lorraine, and Colin Mallows. 2009. “Variations on the\nHistogram.” Journal of Computational and Graphical\nStatistics 18 (1): 21–31. https://doi.org/10.1198/jcgs.2009.0002.\n\n\nDeWitt, Helen. 2000. The Last Samurai. 1st ed. United States:\nTalk Mirimax Books.\n\n\nDillman, Don, Jolene Smyth, and Leah Christian. (1978) 2014.\nInternet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design\nMethod. 4th ed. Wiley.\n\n\nDoggers, Peter. 2021. “Carlsen Wins Game 6, Longest World Chess\nChampionship Game of All Time,” December. https://www.chess.com/news/view/fide-world-chess-championship-2021-game-6.\n\n\nDolatsara, Hamidreza Ahady, Ying-Ju Chen, Robert Leonard, Fadel Megahed,\nand Allison Jones-Farmer. 2021. “Explaining Predictive Model\nPerformance: An Experimental Study of Data Preparation and Model\nChoice.” Big Data, October. https://doi.org/10.1089/big.2021.0067.\n\n\nDoll, Richard, and Bradford Hill. 1950. “Smoking and Carcinoma of\nthe Lung.” British Medical Journal 2 (4682): 739–48. https://doi.org/10.1136/bmj.2.4682.739.\n\n\nDruckman, James, and Donald Green. 2021. “A New Era of\nExperimental Political Science.” In Advances in Experimental\nPolitical Science, 1–16. Cambridge: Cambridge University Press. https://doi.org/10.1017/9781108777919.002.\n\n\nDu, Kai, Steven Huddart, and Xin Daniel Jiang. 2022. “Lost in\nStandardization: Effects of Financial Statement Database Discrepancies\non Inference.” Journal of Accounting and Economics,\nDecember, 101573. https://doi.org/10.1016/j.jacceco.2022.101573.\n\n\nDuflo, Esther. 2020. “Field Experiments and the Practice of\nPolicy.” American Economic Review 110 (7): 1952–73. https://doi.org/10.1257/aer.110.7.1952.\n\n\nDwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006.\n“Calibrating Noise to Sensitivity in Private Data\nAnalysis.” In Theory of Cryptography Conference, 265–84.\nSpringer. https://doi.org/10.1007/11681878_14.\n\n\nDwork, Cynthia, and Aaron Roth. 2013. “The Algorithmic Foundations\nof Differential Privacy.” Foundations and Trends in\nTheoretical Computer Science 9 (3-4): 211–407. https://doi.org/10.1561/0400000042.\n\n\nEdelman, Murray, Liberty Vittert, and Xiao-Li Meng. 2021. “An\nInterview with Murray Edelman on the History of the Exit Poll.”\nHarvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.3a25cd24.\n\n\nEdgeworth, Francis Ysidro. 1885. “Methods of Statistics.”\nJournal of the Statistical Society of London, 181–217.\n\n\nEdwards, Jonathan. 2017. “PACE team response\nshows a disregard for the principles of science.”\nJournal of Health Psychology 22 (9): 1155–58. https://doi.org/10.1177/1359105317700886.\n\n\nEfron, Bradley, and Carl Morris. 1977. “Stein’s Paradox in\nStatistics.” Scientific American 236 (May): 119–27. https://doi.org/10.1038/scientificamerican0577-119.\n\n\nEghbal, Nadia. 2020. Working in Public: The Making and Maintenance\nof Open Source Software. California: Stripe Press.\n\n\nEisenstein, Michael. 2022. “Need Web Data? Here’s How to Harvest\nThem.” Nature 607: 200–201. https://doi.org/10.1038/d41586-022-01830-9.\n\n\nElliott, Michael, Brady West, Xinyu Zhang, and Stephanie Coffey. 2022.\n“The Anchoring Method: Estimation of Interviewer Effects in the\nAbsence of Interpenetrated Sample Assignment.” Survey\nMethodology 48 (1): 25–48. http://www.statcan.gc.ca/pub/12-001-x/2022001/article/00005-eng.htm.\n\n\nElson, Malte. 2018. “Question Wording and Item\nFormulation.” https://doi.org/10.31234/osf.io/e4ktc.\n\n\nEnns, Peter, and Jake Rothschild. 2022. “Do You Know Where Your\nSurvey Data Come From?” May. https://medium.com/3streams/surveys-3ec95995dde2.\n\n\nFarrugia, Patricia, Bradley Petrisor, Forough Farrokhyar, and Mohit\nBhandari. 2010. “Research Questions, Hypotheses and\nObjectives.” Canadian Journal of Surgery 53 (4): 278.\n\n\nFinkelstein, Amy, Sarah Taubman, Bill Wright, Mira Bernstein, Jonathan\nGruber, Joseph Newhouse, Heidi Allen, Katherine Baicker, and Oregon\nHealth Study Group. 2012. “The Oregon Health Insurance Experiment:\nEvidence from the First Year.” The Quarterly Journal of\nEconomics 127 (3): 1057–1106. https://doi.org/10.1093/qje/qjs020.\n\n\nFirke, Sam. 2023. janitor: Simple Tools for\nExamining and Cleaning Dirty Data. https://CRAN.R-project.org/package=janitor.\n\n\nFisher, Ronald. 1926. “The Arrangement of\nField Experiments.” Journal of the Ministry of\nAgriculture, 503–15. https://doi.org/10.23637/rothamsted.8v61q.\n\n\n———. (1925) 1928. Statistical Methods for Research Workers. 2nd\ned. London: Oliver; Boyd.\n\n\n———. (1935) 1949. The Design of Experiments. 5th ed. London:\nOliver; Boyd.\n\n\nFiske, Susan, and Shiro Kuriwaki. 2021. “Words to the Wise on\nWriting Scientific Papers,” November. https://doi.org/10.31234/osf.io/n32qw.\n\n\nFitts, Alexis Sobel. 2014. “The King of Content: How Upworthy Aims\nto Alter the Web, and Could End up Altering the World.”\nColumbia Journalism Review 53: 34–38. https://archives.cjr.org/feature/the%5Fking%5Fof%5Fcontent.php.\n\n\nFlake, Jessica, and Eiko Fried. 2020. “Measurement Schmeasurement:\nQuestionable Measurement Practices and How to Avoid Them.”\nAdvances in Methods and Practices in Psychological Science 3\n(4): 456–65. https://doi.org/10.1177/2515245920952393.\n\n\nFlynn, Michael. 2022. troopdata: Tools for\nAnalyzing Cross-National Military Deployment and Basing\nData. https://CRAN.R-project.org/package=troopdata.\n\n\nFord, Paul. 2015. “What Is Code?” Bloomberg\nBusinessweek, June. https://www.bloomberg.com/graphics/2015-paul-ford-what-is-code/.\n\n\nForster, Edward Morgan. 1927. Aspects of the Novel. London:\nEdward Arnold.\n\n\nFoster, Gordon. 1968. “Computers, Statistics and Planning: Systems\nor Chaos?” Geary Lecture. https://www.esri.ie/system/files/publications/GLS2.pdf.\n\n\nFourcade, Marion, and Kieran Healy. 2017. “Seeing Like a\nMarket.” Socio-Economic Review 15 (1): 9–29. https://doi.org/10.1093/ser/mww033.\n\n\nFowler, Martin, and Kent Beck. 2018. Refactoring: Improving the Design of Existing\nCode. 2nd ed. New York: Addison-Wesley Professional.\n\n\nFox, John, and Robert Andersen. 2006. “Effect Displays for\nMultinomial and Proportional-Odds Logit Models.” Sociological\nMethodology 36 (1): 225–55. https://doi.org/10.1111/j.1467-9531.2006.00180.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2022. carData:\nCompanion to Applied Regression Data Sets. https://CRAN.R-project.org/package=carData.\n\n\nFranconeri, Steven, Lace Padilla, Priti Shah, Jeffrey Zacks, and Jessica\nHullman. 2021. “The Science of Visual Data Communication: What\nWorks.” Psychological Science in the Public Interest 22\n(3): 110–61. https://doi.org/10.1177/15291006211051956.\n\n\nFrandell, Ashlee, Mary Feeney, Timothy Johnson, Eric Welch, Lesley\nMichalegko, and Heyjie Jung. 2021. “The Effects of Electronic\nAlert Letters for Internet Surveys of Academic Scientists.”\nScientometrics 126 (8): 7167–81. https://doi.org/10.1007/s11192-021-04029-3.\n\n\nFranklin, Laura. 2005. “Exploratory Experiments.”\nPhilosophy of Science 72 (5): 888–99. https://doi.org/10.1086/508117.\n\n\nFrei, Christoph, and Liam Welsh. 2022. “How\nthe Closure of a U.S. Tax Loophole May Affect Investor\nPortfolios.” Journal of Risk and Financial\nManagement 15 (5): 209. https://doi.org/10.3390/jrfm15050209.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2022. rsample: General\nResampling Infrastructure. https://CRAN.R-project.org/package=rsample.\n\n\nFried, Eiko, Jessica Flake, and Donald Robinaugh. 2022.\n“Revisiting the Theoretical and Methodological Foundations of\nDepression Measurement.” Nature Reviews Psychology 1\n(6): 358–68. https://doi.org/10.1038/s44159-022-00050-2.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2009. The\nElements of Statistical Learning. 2nd ed. Springer. https://hastie.su.domains/ElemStatLearn/.\n\n\nFriendly, Michael. 2021. HistData: Data Sets from the History of\nStatistics and Data Visualization. https://CRAN.R-project.org/package=HistData.\n\n\nFriendly, Michael, and Howard Wainer. 2021. A History of Data\nVisualization and Graphic Communication. 1st ed. Massachusetts:\nHarvard University Press.\n\n\nFry, Hannah. 2020. “Big Tech Is Testing You.” The New\nYorker, February, 61–65. https://www.newyorker.com/magazine/2020/03/02/big-tech-is-testing-you.\n\n\nFuller, Mark, and James Mosher. 1987. “Raptor Survey\nTechniques.” In Raptor Management Techniques Manual,\nedited by Beth Pendleton, Brian Millsap, Keith Cline, and David Bird,\n37–65. National Wildlife Federation. https://www.sandiegocounty.gov/content/dam/sdc/pds/ceqa/JVR/AdminRecord/IncorporatedByReference/Appendices/Appendix-D---Biological-Resources-Report/Fuller%20and%20Mosher%201987.pdf.\n\n\nFunkhouser, Gray. 1937. “Historical Development of the Graphical\nRepresentation of Statistical Data.” Osiris 3: 269–404.\nhttps://doi.org/10.1086/368480.\n\n\nGagolewski, Marek. 2022. “stringi:\nFast and Portable Character String Processing in\nR.” Journal of Statistical Software 103\n(2): 1–59. https://doi.org/10.18637/jss.v103.i02.\n\n\nGalef, Julia. 2020. “Episode 248: Are Democrats Being Irrational?\n(David Shor).” Rationally Speaking, December. http://rationallyspeakingpodcast.org/248-are-democrats-being-irrational-david-shor/.\n\n\nGao, Lucy, Jacob Bien, and Daniela Witten. 2022. “Selective\nInference for Hierarchical Clustering.” Journal of the\nAmerican Statistical Association, October, 1–11. https://doi.org/10.1080/01621459.2022.2116331.\n\n\nGao, Zheng, Christian Bird, and Earl T. Barr. 2017. “To Type or\nNot to Type: Quantifying Detectable Bugs in\nJavaScript.” In 2017\nIEEE/ACM 39th International Conference on\nSoftware Engineering (ICSE). IEEE. https://doi.org/10.1109/icse.2017.75.\n\n\nGarfinkel, Irwin, Lee Rainwater, and Timothy Smeeding. 2006. “A\nRe-Examination of Welfare States and Inequality in Rich Nations: How\nin-Kind Transfers and Indirect Taxes Change the Story.”\nJournal of Policy Analysis and Management 25 (4): 897–919. https://doi.org/10.1002/pam.20213.\n\n\nGargiulo, Maria. 2022. “Statistical Biases, Measurement\nChallenges, and Recommendations for Studying Patterns of Femicide in\nConflict.” Peace Review 34 (2): 163–76. https://doi.org/10.1080/10402659.2022.2049002.\n\n\nGarnier, Simon, Noam Ross, Robert Rudis, Antônio Camargo, Marco Sciaini,\nand Cédric Scherer. 2021. viridis –\nColorblind-Friendly Color Maps for R. https://doi.org/10.5281/zenodo.4679424.\n\n\nGazeley, Ursula, Georges Reniers, Hallie Eilerts-Spinelli, Julio Romero\nPrieto, Momodou Jasseh, Sammy Khagayi, and Veronique Filippi. 2022.\n“Women’s Risk of Death Beyond 42 Days Post Partum: A Pooled\nAnalysis of Longitudinal Health and Demographic Surveillance System Data\nin Sub-Saharan Africa.” The Lancet Global Health 10\n(11): e1582–89. https://doi.org/10.1016/s2214-109x(22)00339-4.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.\n“Datasheets for Datasets.” Communications of the\nACM 64 (12): 86–92. https://doi.org/10.1145/3458723.\n\n\nGelfand, Sharla. 2021. “Make a ReprEx... Please.”\nYouTube, February. https://youtu.be/G5Nm-GpmrLw.\n\n\n———. 2022a. Astrologer: Chani Nicholas Weekly Horoscopes\n(2013-2017). http://github.com/sharlagelfand/astrologer.\n\n\n———. 2022b. opendatatoronto: Access the City of\nToronto Open Data Portal. https://CRAN.R-project.org/package=opendatatoronto.\n\n\nGelman, Andrew. 2016. “What has happened down\nhere is the winds have changed,” September. https://statmodeling.stat.columbia.edu/2016/09/21/what-has-happened-down-here-is-the-winds-have-changed/.\n\n\n———. 2019. “Another Regression Discontinuity Disaster and What Can\nWe Learn from It,” June. https://statmodeling.stat.columbia.edu/2019/06/25/another-regression-discontinuity-disaster-and-what-can-we-learn-from-it/.\n\n\n———. 2020. “Statistical Models of Election Outcomes.”\nYouTube, August. https://youtu.be/7gjDnrbLQ4k.\n\n\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and\nDonald Rubin. (1995) 2014. Bayesian Data Analysis. 3rd ed.\nChapman; Hall/CRC.\n\n\nGelman, Andrew, Sharad Goel, Douglas Rivers, and David Rothschild. 2016.\n“The Mythical Swing Voter.” Quarterly Journal of\nPolitical Science 11 (1): 103–30. https://doi.org/10.1561/100.00015031.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. 1st ed. Cambridge\nUniversity Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press. https://avehtari.github.io/ROS-Examples/.\n\n\nGelman, Andrew, and Guido Imbens. 2019. “Why High-Order\nPolynomials Should Not Be Used in Regression Discontinuity\nDesigns.” Journal of Business & Economic Statistics\n37 (3): 447–56. https://doi.org/10.1080/07350015.2017.1366909.\n\n\nGelman, Andrew, and Eric Loken. 2013. “The Garden of Forking\nPaths: Why Multiple Comparisons Can Be a Problem, Even When There Is No\n‘Fishing Expedition’ or ‘p-Hacking’ and the\nResearch Hypothesis Was Posited Ahead of Time.” Department of\nStatistics, Columbia University. http://www.stat.columbia.edu/~gelman/research/unpublished/p%5Fhacking.pdf.\n\n\nGelman, Andrew, Greggor Mattson, and Daniel Simpson. 2018. “Gaydar\nand the Fallacy of Decontextualized Measurement.”\nSociological Science 5 (12): 270–80. https://doi.org/10.15195/v5.a12.\n\n\nGelman, Andrew, Cristian Pasarica, and Rahul Dodhia. 2002. “Let’s\nPractice What We Preach: Turning Tables into Graphs.” The\nAmerican Statistician 56 (2): 121–30. https://doi.org/10.1198/000313002317572790.\n\n\nGelman, Andrew, and Aki Vehtari. 2021. “What Are the Most\nImportant Statistical Ideas of the Past 50 Years?” Journal of\nthe American Statistical Association 116 (536): 2087–97. https://doi.org/10.1080/01621459.2021.1938081.\n\n\n———. 2023. Learn Statistics: Hundreds of Stories, Activities, and\nExamples.\n\n\nGelman, Andrew, Aki Vehtari, Daniel Simpson, Charles Margossian, Bob\nCarpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian\nBürkner, and Martin Modrák. 2020. “Bayesian Workflow.”\narXiv. https://doi.org/10.48550/arXiv.2011.01808.\n\n\nGentemann, Chelle Leigh, Chris Holdgraf, Ryan Abernathey, Daniel\nCrichton, James Colliander, Edward Joseph Kearns, Yuvi Panda, and\nRichard Signell. 2021. “Science Storms the Cloud.”\nAGU Advances 2 (2). https://doi.org/10.1029/2020av000354.\n\n\nGerber, Alan, and Donald Green. 2012. Field Experiments: Design,\nAnalysis, and Interpretation. New York: WW Norton.\n\n\nGerring, John. 2012. “Mere Description.” British\nJournal of Political Science 42 (4): 721–46. https://doi.org/10.1017/s0007123412000130.\n\n\nGertler, Paul, Sebastian Martinez, Patrick Premand, Laura Rawlings, and\nChristel Vermeersch. 2016. Impact Evaluation in Practice. 2nd\ned. The World Bank. https://doi.org/10.1596/978-1-4648-0779-4.\n\n\nGeuenich, Michael, Jinyu Hou, Sunyun Lee, Shanza Ayub, Hartland Jackson,\nand Kieran Campbell. 2021a. “Automated Assignment of Cell Identity\nfrom Single-Cell Multiplexed Imaging and Proteomic Data.”\nCell Systems 12 (12): 1173–86. https://doi.org/10.1016/j.cels.2021.08.012.\n\n\n———. 2021b. “Replication Materials: \"Automated Assignment of Cell\nIdentity from Single-Cell Multiplexed Imaging and Proteomic\nData\".” https://doi.org/10.5281/ZENODO.5156049.\n\n\nGhitza, Yair, and Andrew Gelman. 2020. “Voter Registration\nDatabases and MRP: Toward the Use of Large-Scale Databases in Public\nOpinion Research.” Political Analysis 28 (4): 507–31. https://doi.org/10.1017/pan.2020.3.\n\n\nGibney, Elizabeth. 2022. “The leap second’s\ntime is up: world votes to stop pausing clocks.”\nNature 612 (7938): 18–18. https://doi.org/10.1038/d41586-022-03783-5.\n\n\nGleick, James. 1990. “The Census: Why We Can’t Count.”\nThe New York Times, July. https://www.nytimes.com/1990/07/15/magazine/the-census-why-we-can-t-count.html.\n\n\nGodfrey, Ernest. 1918. “History and Development of Statistics in\nCanada.” In The History of Statistics–Their Development and\nProgress in Many Countries. New York: Macmillan, edited by John\nKoren, 179–98. Macmillan Company of New York.\n\n\nGoodman, Leo. 1961. “Snowball Sampling.” The Annals of\nMathematical Statistics 32 (1): 148–70. https://doi.org/10.1214/aoms/1177705148.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2023.\n“rstanarm: Bayesian applied\nregression modeling via Stan.” https://mc-stan.org/rstanarm.\n\n\nGoogle. 2022. “What to Look for in a Code Review.” Google\nEngineering Practices Documentation. https://google.github.io/eng-practices/review/reviewer/looking-for.html.\n\n\nGordon, Brett, Robert Moakler, and Florian Zettelmeyer. 2022.\n“Close Enough? A Large-Scale Exploration of Non-Experimental\nApproaches to Advertising Measurement.” Marketing\nScience, November. https://doi.org/10.1287/mksc.2022.1413.\n\n\nGordon, Brett, Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky.\n2019. “A Comparison of Approaches to Advertising Measurement:\nEvidence from Big Field Experiments at Facebook.” Marketing\nScience 38 (2): 193–225. https://doi.org/10.1287/mksc.2018.1135.\n\n\nGraham, Paul. 2020. “How to Write Usefully,” February. http://paulgraham.com/useful.html.\n\n\nGray, Charles T., and Ben Marwick. 2019. “Truth, Proof, and\nReproducibility: There’s No Counter-Attack for the Codeless.” In\nCommunications in Computer and Information Science, 111–29.\nSpringer Singapore. https://doi.org/10.1007/978-981-15-1960-4_8.\n\n\nGreen, Donald, Terence Leong, Holger Kern, Alan Gerber, and Christopher\nLarimer. 2009. “Testing the Accuracy of Regression Discontinuity\nAnalysis Using Experimental Benchmarks.” Political\nAnalysis 17 (4): 400–417. https://doi.org/10.1093/pan/mpp018.\n\n\nGreen, Eric. 2020. “Nivi Research: Mister P\nhelps us understand vaccine hesitancy,” December. https://research.nivi.io/posts/2020-12-08-mister-p-helps-us-understand-vaccine-hesitancy/.\n\n\nGreenberg, Bernard, Abdel-Latif Abul-Ela, Walt Simmons, and Daniel\nHorvitz. 1969. “The Unrelated Question Randomized Response Model:\nTheoretical Framework.” Journal of the American Statistical\nAssociation 64 (326): 520–39. https://doi.org/10.1080/01621459.1969.10500991.\n\n\nGreenland, Sander, Stephen Senn, Kenneth Rothman, John Carlin, Charles\nPoole, Steven Goodman, and Douglas Altman. 2016. “Statistical Tests, P values, Confidence Intervals, and\nPower: A Guide to Misinterpretations.” European\nJournal of Epidemiology 31 (4): 337–50. https://doi.org/10.1007/s10654-016-0149-3.\n\n\nGreifer, Noah. 2021. “Why Do We Do Matching for Causal Inference\nVs Regressing on Confounders?” Cross Validated,\nSeptember. https://stats.stackexchange.com/q/544958.\n\n\nGrimmer, Justin, Margaret Roberts, and Brandon Stewart. 2022. Text As Data: A New Framework for Machine Learning and\nthe Social Sciences. New Jersey: Princeton University Press.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times\nMade Easy with lubridate.”\nJournal of Statistical Software 40 (3): 1–25. https://doi.org/10.18637/jss.v040.i03.\n\n\nGronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, and\nTianxi Cai. 2019. “Automated Feature Selection of Predictors in\nElectronic Medical Records Data.” Biometrics 75 (1):\n268–77. https://doi.org/10.1111/biom.12987.\n\n\nGroves, Robert. 2011. “Three Eras of Survey Research.”\nPublic Opinion Quarterly 75 (5): 861–71. https://doi.org/10.1093/poq/nfr057.\n\n\nGroves, Robert, and Lars Lyberg. 2010. “Total\nSurvey Error: Past, Present, and Future.” Public\nOpinion Quarterly 74 (5): 849–79. https://doi.org/10.1093/poq/nfq065.\n\n\nGrün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R Package for Fitting\nTopic Models.” Journal of Statistical Software 40 (13):\n1–30. https://doi.org/10.18637/jss.v040.i13.\n\n\nGustafsson, Karl, and Linus Hagström. 2017. “What Is the Point?\nTeaching Graduate Students How to Construct Political Science Research\nPuzzles.” European Political Science 17 (4): 634–48. https://doi.org/10.1057/s41304-017-0130-y.\n\n\nGutman, Robert. 1958. “Birth and Death Registration in\nMassachusetts: II. The Inauguration of a Modern System,\n1800-1849.” The Milbank Memorial Fund Quarterly 36 (4):\n373–402.\n\n\nHackett, Robert. 2016. “Researchers Caused an\nUproar By Publishing Data From 70,000 OkCupid Users.”\nFortune, May. https://fortune.com/2016/05/18/okcupid-data-research/.\n\n\nHalberstam, David. 1972. The Best and the\nBrightest. 1st ed. New York: Random House.\n\n\nHamming, Richard. (1997) 2020. The Art of Doing\nScience and Engineering. 2nd ed. Stripe Press.\n\n\nHammond, Jennifer, Heidi Leister-Tebbe, Annie Gardner, Paula Abreu,\nWeihang Bao, Wayne Wisemandle, MaryLynn Baniecki, et al. 2022.\n“Oral Nirmatrelvir for High-Risk, Nonhospitalized Adults with\nCovid-19.” New England Journal of Medicine 386 (15):\n1397–1408. https://doi.org/10.1056/nejmoa2118542.\n\n\nHand, David. 2018. “Statistical Challenges of Administrative and\nTransaction Data.” Journal of the Royal Statistical Society:\nSeries A (Statistics in Society) 181 (3): 555–605. https://doi.org/10.1111/rssa.12315.\n\n\nHandcock, Mark, and Krista Gile. 2011. “Comment: On the Concept of\nSnowball Sampling.” Sociological Methodology 41 (1):\n367–71. https://doi.org/10.1111/j.1467-9531.2011.01243.x.\n\n\nHangartner, Dominik, Daniel Kopp, and Michael Siegenthaler. 2021.\n“Monitoring Hiring Discrimination Through Online Recruitment\nPlatforms.” Nature 589 (7843): 572–76. https://doi.org/10.1038/s41586-020-03136-0.\n\n\nHanretty, Chris. 2020. “An Introduction to Multilevel Regression\nand Post-Stratification for Estimating Constituency Opinion.”\nPolitical Studies Review 18 (4): 630–45. https://doi.org/10.1177/1478929919864773.\n\n\nHao, Karen. 2019. “This is How AI Bias Really\nHappens—And Why It’s So Hard To Fix.” MIT Technology\nReview, February. https://www.technologyreview.com/2019/02/04/137602/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/.\n\n\nHart, Edmund, Pauline Barmby, David LeBauer, François Michonneau, Sarah\nMount, Patrick Mulrooney, Timothée Poisot, Kara Woo, Naupaka Zimmerman,\nand Jeffrey Hollister. 2016. “Ten Simple Rules for Digital Data\nStorage.” PLOS Computational Biology 12\n(10): e1005097. https://doi.org/10.1371/journal.pcbi.1005097.\n\n\nHartocollis, Anemona. 2022. “U.S. News Ranked\nColumbia No. 2, but a Math Professor Has His Doubts.”\nThe New York Times, March. https://www.nytimes.com/2022/03/17/us/columbia-university-rank.html.\n\n\nHassan, Mai. 2022. “New Insights on Africa’s Autocratic\nPast.” African Affairs 121 (483): 321–33. https://doi.org/10.1093/afraf/adac002.\n\n\nHastie, Trevor, and Robert Tibshirani. 1990. Generalized Additive\nModels. 1st ed. Boca Raton: Chapman; Hall/CRC.\n\n\nHawes, Michael. 2020. “Implementing Differential\nPrivacy: Seven Lessons From the\n2020 United States\nCensus.” Harvard Data Science Review 2 (2).\nhttps://doi.org/10.1162/99608f92.353c6f99.\n\n\nHayot, Eric. 2014. The Elements of Academic Style. New York:\nColumbia University Press.\n\n\nHealy, Kieran. 2018. Data Visualization. New Jersey: Princeton\nUniversity Press. https://socviz.co.\n\n\n———. 2020. “The Kitchen Counter Observatory,” May. https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\n\n\n———. 2022. “Unhappy in Its Own Way,” July. https://kieranhealy.org/blog/archives/2022/07/22/unhappy-in-its-own-way/.\n\n\nHeckathorn, Douglas. 1997. “Respondent-Driven Sampling: A New\nApproach to the Study of Hidden Populations.” Social\nProblems 44 (2): 174–99. https://doi.org/10.2307/3096941.\n\n\nHeil, Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey\nGreene, and Stephanie Hicks. 2021. “Reproducibility Standards for\nMachine Learning in the Life Sciences.” Nature Methods\n18 (10): 1132–35. https://doi.org/10.1038/s41592-021-01256-7.\n\n\nHeller, Jean. 2022. “AP Exposes the Tuskegee Syphilis Study: The\n50th Anniversary.” AP, July. https://apnews.com/article/tuskegee-study-ap-story-investigation-syphilis-53403657e77d76f52df6c2e2892788c9.\n\n\nHermans, Felienne. 2017. “Peter Hilton on Naming.” IEEE\nSoftware 34 (3): 117–20. https://doi.org/10.1109/MS.2017.81.\n\n\n———. 2021. The Programmer’s Brain: What Every Programmer Needs to\nKnow about Cognition. 1st ed. New York: Simon; Schuster. https://www.manning.com/books/the-programmers-brain.\n\n\nHernán, Miguel, David Clayton, and Niels Keiding. 2011. “The\nSimpson’s Paradox Unraveled.” International Journal of\nEpidemiology 40 (3): 780–85. https://doi.org/10.1093/ije/dyr041.\n\n\nHernán, Miguel, and James Robins. 2023. What If. 1st ed. Boca\nRaton: Chapman & Hall/CRC. https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High\nPublic Debt Consistently Stifle Economic Growth? A Critique of Reinhart\nand Rogoff.” Cambridge Journal of Economics 38 (2):\n257–79. https://doi.org/10.1093/cje/bet075.\n\n\nHester, Jim, Florent Angly, Russ Hyde, Michael Chirico, Kun Ren,\nAlexander Rosenstock, and Indrajeet Patil. 2022. lintr: A “Linter” for R Code. https://CRAN.R-project.org/package=lintr.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2021. fs: Cross-Platform File System Operations Based on\n“libuv”. https://CRAN.R-project.org/package=fs.\n\n\nHill, Austin Bradford. 1965. “The Environment and Disease:\nAssociation or Causation?” Proceedings of the Royal Society\nof Medicine 58 (5): 295–300.\n\n\nHillel, Wayne. 2017. How Do We Trust Our Science Code? https://www.hillelwayne.com/how-do-we-trust-science-code/.\n\n\nHo, Daniel, Kosuke Imai, Gary King, and Elizabeth Stuart. 2011.\n“MatchIt: Nonparametric Preprocessing for Parametric\nCausal Inference.” Journal of Statistical Software 42\n(8): 1–28. https://doi.org/10.18637/jss.v042.i08.\n\n\nHodgetts, Paul. 2022. “The Negative Space of Data,” March.\nhttps://hodgettsp.netlify.app/post/data-negativespace/.\n\n\nHofmeister, Johannes, Janet Siegmund, and Daniel Holt. 2017.\n“Shorter Identifier Names Take Longer to Comprehend.” In\n2017 IEEE 24th International Conference on Software Analysis,\nEvolution and Reengineering (SANER), 217–27. https://doi.org/10.1109/saner.2017.7884623.\n\n\nHolland, Paul. 1986. “Statistics and Causal Inference.”\nJournal of the American Statistical Association 81 (396):\n945–60. https://doi.org/10.2307/2289064.\n\n\nHolliday, Derek, Tyler Reny, Alex Rossell Hayes, Aaron Rudkin, Chris\nTausanovitch, and Lynn Vavreck. 2021. “Democracy Fund + UCLA Nationscape Methodology and\nRepresentativeness Assessment.”\n\n\nHopper, Nate. 2022. “The Thorny Problem of Keeping the Internet’s\nTime.” The New Yorker, September. https://www.newyorker.com/tech/annals-of-technology/the-thorny-problem-of-keeping-the-internets-time.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen Gorman. 2020.\npalmerpenguins: Palmer Archipelago (Antarctica)\npenguin data. https://doi.org/10.5281/zenodo.3960218.\n\n\nHorton, Nicholas, Rohan Alexander, Micaela Parker, Aneta Piekut, and\nColin Rundel. 2022. “The Growing Importance of Reproducibility and\nResponsible Workflow in the Data Science and Statistics\nCurriculum.” Journal of Statistics and Data Science\nEducation 30 (3): 207–8. https://doi.org/10.1080/26939169.2022.2141001.\n\n\nHorton, Nicholas, and Stuart Lipsitz. 2001. “Multiple Imputation\nin Practice.” The American Statistician 55 (3): 244–54.\nhttps://doi.org/10.1198/000313001317098266.\n\n\nHotz, Joseph, Christopher Bollinger, Tatiana Komarova, Charles Manski,\nRobert Moffitt, Denis Nekipelov, Aaron Sojourner, and Bruce Spencer.\n2022. “Balancing Data Privacy and Usability in the Federal\nStatistical System.” Proceedings of the National Academy of\nSciences 119 (31): 1–10. https://doi.org/10.1073/pnas.2104906119.\n\n\nHowes, Adam. 2022. “Representing Uncertainty Using Significant\nFigures,” April. https://athowes.github.io/posts/2022-04-24-representing-uncertainty-using-significant-figures/.\n\n\nHug, Lucia, Monica Alexander, Danzhen You, Leontine Alkema, and UN\nInter-agency Group for Child. 2019. “National, Regional, and\nGlobal Levels and Trends in Neonatal Mortality Between 1990 and 2017,\nwith Scenario-Based Projections to 2030: A Systematic Analysis.”\nLancet Global Health 7 (6): e710–20. https://doi.org/10.1016/S2214-109X(19)30163-9.\n\n\nHughes, Nicola, and Jill Rutter. 2016. “Ministers Reflect:\nInterview with Oliver Letwin,” December. https://www.instituteforgovernment.org.uk/ministers-reflect/person/oliver-letwin/.\n\n\nHulley, Stephen, Steven Cummings, Warren Browner, Deborah Grady, and\nThomas Newman. 2007. Designing Clinical Research. 3rd ed.\nLippincott Williams & Wilkins.\n\n\nHullman, Jessica, and Andrew Gelman. 2021. “Designing for\nInteractive Exploratory Data Analysis Requires Theories of Graphical\nInference.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.3ab8a587.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to\nResearch Design and Causality. 1st ed. Chapman & Hall. https://theeffectbook.net.\n\n\n———. 2022. “Library of Statistical Techniques.” https://lost-stats.github.io.\n\n\nHuntington-Klein, Nick, Andreu Arenas, Emily Beam, Marco Bertoni,\nJeffrey Bloem, Pralhad Burli, Naibin Chen, et al. 2021. “The\nInfluence of Hidden Researcher Decisions in Applied\nMicroeconomics.” Economic Inquiry 59: 944–60. https://doi.org/10.1111/ecin.12992.\n\n\nHuyen, Chip. 2020. “Machine Learning Is Going Real-Time,”\nDecember. https://huyenchip.com/2020/12/27/real-time-machine-learning.html.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in\nR. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781003093459.\n\n\nHyman, Michael, Luca Sartore, and Linda J Young. 2021. “Capture-Recapture Estimation of Characteristics of U.S.\nLocal Food Farms Using a Web-Scraped List Frame.”\nJournal of Survey Statistics and Methodology 10 (4): 979–1004.\nhttps://doi.org/10.1093/jssam/smab008.\n\n\nHyndman, Rob, Timothy Hyndman, Charles Gray, Sayani Gupta, and Jacquie\nTran. 2022. cricketdata: International Cricket\nData. https://CRAN.R-project.org/package=cricketdata.\n\n\nIannone, Richard. 2022. DiagrammeR: Graph/Network\nVisualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra\nLauer, and JooYoung Seo. 2022. gt: Easily\nCreate Presentation-Ready Display Tables.\n\n\nIannone, Richard, and Mauricio Vargas. 2022. pointblank: Data Validation and Organization of Metadata\nfor Local and Remote Tables. https://CRAN.R-project.org/package=pointblank.\n\n\nInternational Organization Of Legal Metrology. 2007. International\nVocabulary of Metrology – Basic and General Concepts and Associated\nTerms. 3rd ed. https://www.oiml.org/en/files/pdf%5Fv/v002-200-e07.pdf.\n\n\nIoannidis, John. 2005. “Why Most Published Research Findings Are\nFalse.” PLOS Medicine 2 (8): e124. https://doi.org/10.1371/journal.pmed.0020124.\n\n\nIrizarry, Rafael. 2020. “The Role of Academia\nin Data Science Education.” Harvard Data Science\nReview 2 (1). https://doi.org/10.1162/99608f92.dd363929.\n\n\nIrving, Damien, Kate Hertweck, Luke Johnston, Joel Ostblom, Charlotte\nWickham, and Greg Wilson. 2021. Research Software Engineering with\nPython. Chapman; Hall/CRC.\n\n\nIsaacson, Walter. 2011. Steve Jobs. 1st ed. Simon &\nSchuster.\n\n\nIshiguro, Kazuo. 1989. The Remains of the Day. 1st ed. Faber;\nFaber.\n\n\nIzrailev, Sergei. 2022. tictoc: Functions for\nTiming R Scripts, as Well as Implementations of “Stack” and\n“List” Structures. https://CRAN.R-project.org/package=tictoc.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n(2013) 2021. An Introduction to Statistical\nLearning with Applications in R. 2nd ed. Springer. https://www.statlearning.com.\n\n\nJenkins, Jennifer, Steven Rich, Andrew Ba Tran, Paige Moody, Julie Tate,\nand Ted Mellnik. 2022. “How the Washington Post Examines Police\nShootings in the United States.” https://www.washingtonpost.com/investigations/2022/12/05/washington-post-fatal-police-shootings-methodology/.\n\n\nJet Propulsion Laboratory. 2009. “JPL\nInstitutional Coding Standard for the C Programming\nLanguage.” Document Number D-60411, March. https://web.archive.org/web/20111015064908/http://lars-lab.jpl.nasa.gov/JPL_Coding_Standard_C.pdf.\n\n\nJohnson, Alicia, Miles Ott, and Mine Dogucu. 2022. Bayes Rules! An Introduction to Bayesian Modeling with\nR. 1st ed. Chapman; Hall/CRC. https://www.bayesrulesbook.com.\n\n\nJohnson, Kaneesha. 2021. “Two Regimes of Prison Data\nCollection.” Harvard Data Science Review 3 (3). https://doi.org/10.1162/99608f92.72825001.\n\n\nJohnston, Myfanwy, and David Robinson. 2022. gutenbergr: Download and Process Public Domain Works from\nProject Gutenberg. https://CRAN.R-project.org/package=gutenbergr.\n\n\nJones, Arnold. 1953. “Census Records of the Later Roman\nEmpire.” The Journal of Roman Studies 43: 49–64. https://doi.org/10.2307/297781.\n\n\nJordan, Michael. 2004. “Graphical Models.” Statistical\nScience 19 (1). https://doi.org/10.1214/088342304000000026.\n\n\n———. 2019. “Artificial Intelligence–The\nRevolution Hasn’t Happened Yet.” Harvard Data Science\nReview 1 (1). https://doi.org/10.1162/99608f92.f06c6e61.\n\n\nJoyner, Michael. 1991. “Modeling: Optimal Marathon Performance on\nthe Basis of Physiological Factors.” Journal of Applied\nPhysiology 70 (2): 683–87. https://doi.org/10.1152/jappl.1991.70.2.683.\n\n\nJurafsky, Dan, and James Martin. (2000) 2023. Speech and Language\nProcessing. 3rd ed. https://web.stanford.edu/~jurafsky/slp3/.\n\n\nKahan, Brennan, Suzie Cro, Fan Li, and Michael Harhay. 2023.\n“Eliminating Ambiguous Treatment Effects Using Estimands.”\nAmerican Journal of Epidemiology, February. https://doi.org/10.1093/aje/kwad036.\n\n\nKahan, Brennan, Fan Li, Andrew Copas, and Michael Harhay. 2022.\n“Estimands in Cluster-Randomized Trials: Choosing Analyses That\nAnswer the Right Question.” International Journal of\nEpidemiology, July. https://doi.org/10.1093/ije/dyac131.\n\n\nKahle, David, and Hadley Wickham. 2013. “ggmap: Spatial Visualization with ggplot2.”\nThe R Journal 5 (1): 144–61. http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf.\n\n\nKahneman, Daniel, Olivier Sibony, and Cass Sunstein. 2021. Noise: A\nFlaw in Human Judgment. William Collins.\n\n\nKalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and\nSujit Kapadia. 2022. “Making text count:\nEconomic forecasting using newspaper text.”\nJournal of Applied Econometrics 37 (5): 896–919.\nhttps://doi.org/10.1002/jae.2907.\n\n\nKalgin, Alexander. 2014. “Implementation of\nPerformance Management in Regional Government in Russia: Evidence of\nData Manipulation.” Public Management Review 18\n(1): 110–38. https://doi.org/10.1080/14719037.2014.965271.\n\n\nKapoor, Sayash, and Arvind Narayanan. 2022. “Leakage and the\nReproducibility Crisis in ML-Based Science.” arXiv. https://doi.org/10.48550/ARXIV.2207.07048.\n\n\nKarsten, Karl. 1923. Charts and Graphs. New York:\nPrentice-Hall.\n\n\nKasy, Maximilian, and Alexander Teytelboym. 2023. “Matching with\nSemi-Bandits.” The Econometrics Journal 26 (1): 45–66.\nhttps://doi.org/10.1093/ectj/utac021.\n\n\nKatz, Lindsay, and Rohan Alexander. 2023a. “Digitization of the\nAustralian Parliamentary Debates, 1998-2022.” arXiv. https://doi.org/10.48550/arXiv.2304.04561.\n\n\n———. 2023b. “A new, comprehensive database of\nall proceedings of the Australian Parliamentary Debates\n(1998-2022).” Zenodo. https://doi.org/10.5281/zenodo.7799678.\n\n\nKay, Matthew. 2022. tidybayes: Tidy Data\nand Geoms for Bayesian Models. https://doi.org/10.5281/zenodo.1308151.\n\n\nKennedy, Lauren, and Jonah Gabry. 2020. “MRP\nwith rstanarm,” July. https://mc-stan.org/rstanarm/articles/mrp.html.\n\n\nKennedy, Lauren, and Andrew Gelman. 2021. “Know Your Population\nand Know Your Model: Using Model-Based Regression and Poststratification\nto Generalize Findings Beyond the Observed Sample.”\nPsychological Methods 26 (5): 547–58. https://doi.org/10.1037/met0000362.\n\n\nKennedy, Lauren, Katharine Khanna, Daniel Simpson, Andrew Gelman, Yajun\nJia, and Julien Teitler. 2022. “He, She, They: Using Sex and\nGender in Survey Adjustment.” https://arxiv.org/abs/2009.14401.\n\n\nKenny, Christopher T., Shiro Kuriwaki, Cory McCartan, Evan T. R.\nRosenman, Tyler Simko, and Kosuke Imai. 2021. “The use of differential privacy for census data and its\nimpact on redistricting: The case of the 2020 U.S.\nCensus.” Science Advances 7 (41). https://doi.org/10.1126/sciadv.abk3283.\n\n\n———. 2022. “Comment: The Essential Role of Policy Evaluation for\nthe 2020 Census Disclosure Avoidance System.” Harvard Data\nScience Review. https://doi.org/10.48550/arXiv.2210.08383.\n\n\nKeshav, Srinivasan. 2007. “How to Read a Paper.”\nACM SIGCOMM Computer Communication\nReview 37 (3): 83–84. https://doi.org/10.1145/1273445.1273458.\n\n\nKeyes, Os. 2019. “Counting the Countless.” Real\nLife. https://reallifemag.com/counting-the-countless/.\n\n\nKharecha, Pushker, and James Hansen. 2013. “Prevented Mortality\nand Greenhouse Gas Emissions from Historical and Projected Nuclear\nPower.” Environmental Science & Technology 47 (9):\n4889–95. https://doi.org/10.1021/es3051197.\n\n\nKiang, Mathew, Alexander Tsai, Monica Alexander, David Rehkopf, and\nSanjay Basu. 2021. “Racial/Ethnic Disparities in Opioid-Related\nMortality in the USA, 1999–2019: The Extreme Case of Washington\nDC.” Journal of Urban Health 98 (5): 589–95. https://doi.org/10.1007/s11524-021-00573-8.\n\n\nKing, Gary. 2006. “Publication, Publication.” PS:\nPolitical Science & Politics 39 (1): 119–25. https://doi.org/10.1017/S1049096506060252.\n\n\nKing, Gary, and Richard Nielsen. 2019. “Why Propensity Scores\nShould Not Be Used for Matching.” Political Analysis 27\n(4): 435–54. https://doi.org/10.1017/pan.2019.11.\n\n\nKing, Stephen. 2000. On Writing: A Memoir of the Craft. 1st ed.\nScribner.\n\n\nKirkegaard, Emil, and Julius Bjerrekær. 2016. “The OKCupid\nDataset: A Very Large Public Dataset of Dating Site Users.”\nOpen Differential Psychology, 1–10. https://doi.org/10.26775/ODP.2016.11.03.\n\n\nKish, Leslie. 1959. “Some Statistical Problems in Research\nDesign.” American Sociological Review 24 (3): 328–38. https://doi.org/10.2307/2089381.\n\n\nKleiber, Christian, and Achim Zeileis. 2008. Applied Econometrics\nwith R. New York: Springer-Verlag. https://CRAN.R-project.org/package=AER.\n\n\nKnuth, Donald. 1984. “Literate Programming.” The\nComputer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\n———. 1998. Art of Computer Programming, Volume 2: Seminumerical\nAlgorithms. 2nd ed.\n\n\nKnutson, Victoria, Serge Aleshin-Guendel, Ariel Karlinsky, William\nMsemburi, and Jon Wakefield. 2022. “Estimating Global and\nCountry-Specific Excess Mortality During the COVID-19 Pandemic,”\nMay. https://cdn.who.int/media/docs/default-source/world-health-data-platform/covid-19-excessmortality/covid-methods-paper-revision.pdf.\n\n\nKoenecke, Allison, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey,\nZion Mengesha, Connor Toups, John Rickford, Dan Jurafsky, and Sharad\nGoel. 2020. “Racial Disparities in Automated Speech\nRecognition.” Proceedings of the National Academy of\nSciences 117 (14): 7684–89. https://doi.org/10.1073/pnas.1915768117.\n\n\nKoenecke, Allison, and Hal Varian. 2020. “Synthetic Data\nGeneration for Economists.” https://arxiv.org/abs/2011.01374.\n\n\nKoenker, Roger, and Achim Zeileis. 2009. “On Reproducible\nEconometric Research.” Journal of Applied Econometrics\n24 (5): 833–47. https://doi.org/10.1002/jae.1083.\n\n\nKoerner, Lisbet. 2000. Linnaeus: Nature and Nation. Cambridge:\nHarvard University Press.\n\n\nKohavi, Ron, Alex Deng, Brian Frasca, Roger Longbotham, Toby Walker, and\nYa Xu. 2012. “Trustworthy Online Controlled Experiments.”\nIn Proceedings of the 18th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining -\nKDD 12, 1st ed. ACM Press.\nhttps://doi.org/10.1145/2339530.2339653.\n\n\nKohavi, Ron, Diane Tang, and Ya Xu. 2020. Trustworthy Online Controlled Experiments: A Practical\nGuide to A/B Testing. Cambridge University Press.\n\n\nKoitsalu, Marie, Martin Eklund, Jan Adolfsson, Henrik Grönberg, and\nYvonne Brandberg. 2018. “Effects of Pre-Notification, Invitation\nLength, Questionnaire Length and Reminder on Participation Rate: A\nQuasi-Randomised Controlled Trial.” BMC Medical Research\nMethodology 18 (3): 1–5. https://doi.org/10.1186/s12874-017-0467-5.\n\n\nKrantz, Sebastian. 2023. collapse: Advanced and\nFast Data Transformation. https://CRAN.R-project.org/package=collapse.\n\n\nKuhn, Max. 2022. tune: Tidy Tuning\nTools. https://CRAN.R-project.org/package=tune.\n\n\nKuhn, Max, and Hannah Frick. 2022. poissonreg:\nModel Wrappers for Poisson Regression. https://CRAN.R-project.org/package=poissonreg.\n\n\nKuhn, Max, and Davis Vaughan. 2022. parsnip: A\nCommon API to Modeling and Analysis Functions. https://CRAN.R-project.org/package=parsnip.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2022. yardstick: Tidy Characterizations of Model\nPerformance. https://CRAN.R-project.org/package=yardstick.\n\n\nKuhn, Max, and Hadley Wickham. 2020. tidymodels: a collection of packages for modeling and\nmachine learning using tidyverse principles. https://www.tidymodels.org.\n\n\n———. 2022. recipes: Preprocessing and Feature\nEngineering Steps for Modeling. https://CRAN.R-project.org/package=recipes.\n\n\nKuriwaki, Shiro, Will Beasley, and Thomas Leeper. 2023. dataverse: R Client for Dataverse 4+\nRepositories.\n\n\nKuznets, Simon, Lillian Epstein, and Elizabeth Jenks. 1941. National Income and Its Composition,\n1919-1938. National Bureau of Economic Research.\n\n\nLamott, Anne. 1994. Bird by Bird: Some Instructions on Writing and\nLife. Anchor Books.\n\n\nLandau, William Michael. 2021. “The targets R\nPackage: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for\nReproducibility and High-Performance Computing.”\nJournal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nLane, Nick. 2015. “The Unseen World: Reflections on Leeuwenhoek\n(1677) ‘Concerning Little Animals’.”\nPhilosophical Transactions of the Royal Society B: Biological\nSciences 370 (1666): 20140344. https://doi.org/10.1098/rstb.2014.0344.\n\n\nLaouenan, Morgane, Palaash Bhargava, Jean-Benoı̂t Eyméoud, Olivier\nGergaud, Guillaume Plique, and Etienne Wasmer. 2022. “A Cross-Verified Database of Notable People,\n3500BC–2018AD.” Scientific Data 9 (290). https://doi.org/10.1038/s41597-022-01369-4.\n\n\nLarmarange, Joseph. 2023. labelled:\nManipulating Labelled Data. https://CRAN.R-project.org/package=labelled.\n\n\nLatour, Bruno. 1996. “On Actor-Network Theory: A Few\nClarifications.” Soziale Welt 47 (4): 369–81. http://www.jstor.org/stable/40878163.\n\n\nLauderdale, Benjamin, Delia Bailey, Jack Blumenau, and Douglas Rivers.\n2020. “Model-Based Pre-Election Polling for National and\nSub-National Outcomes in the US and UK.” International\nJournal of Forecasting 36 (2): 399–413. https://doi.org/10.1016/j.ijforecast.2019.05.012.\n\n\nLeek, Jeff, Blakeley McShane, Andrew Gelman, David Colquhoun, Michèle\nNuijten, and Steven Goodman. 2017. “Five Ways to Fix\nStatistics.” Nature 551 (7682): 557–59. https://doi.org/10.1038/d41586-017-07522-z.\n\n\nLeek, Jeff, and Roger Peng. 2020. “Advanced Data Science\n2020.” http://jtleek.com/ads2020/index.html.\n\n\nLeonelli, Sabina. 2020. “Learning from Data Journeys.” In\nData Journeys in the Sciences, 1–24. Springer International\nPublishing. https://doi.org/10.1007/978-3-030-37177-7_1.\n\n\nLeos-Barajas, Vianey, Theoni Photopoulou, Roland Langrock, Toby\nPatterson, Yuuki Watanabe, Megan Murgatroyd, and Yannis Papastamatiou.\n2016. “Analysis of Animal Accelerometer Data Using Hidden Markov\nModels.” Methods in Ecology and Evolution 8 (2): 161–73.\nhttps://doi.org/10.1111/2041-210x.12657.\n\n\nLetterman, Clark. 2021. “Q&A: How Pew\nResearch Center surveyed nearly 30,000 people in India,”\nJuly. https://medium.com/pew-research-center-decoded/q-a-how-pew-research-center-surveyed-nearly-30-000-people-in-india-7c778f6d650e.\n\n\nLevay, Kevin, Jeremy Freese, and James Druckman. 2016. “The\nDemographic and Political Composition of Mechanical Turk\nSamples.” SAGE Open 6 (1): 1–17. https://doi.org/10.1177/2158244016636433.\n\n\nLevine, Judah, Patrizia Tavella, and Martin Milton. 2022. “Towards\na Consensus on a Continuous Coordinated Universal Time.”\nMetrologia 60 (1): 014001. https://doi.org/10.1088/1681-7575/ac9da5.\n\n\nLewis, Crystal. 2023. Data Management in Large-Scale Education\nResearch. https://datamgmtinedresearch.com/index.html.\n\n\nLichand, Guilherme, and Sharon Wolf. 2022. “Measuring Child Labor:\nWhom Should Be Asked, and Why It Matters,” March. https://doi.org/10.21203/rs.3.rs-1474562/v1.\n\n\nLight, Richard, Judith Singer, and John Willett. 1990. By Design: Planning Research on Higher\nEducation. 1st ed. Cambridge: Harvard University Press.\n\n\nLima, Renato de, Oliver Phillips, Alvaro Duque, Sebastian Tello, Stuart\nDavies, Alexandre Adalardo de Oliveira, Sandra Muller, et al. 2022.\n“Making Forest Data Fair and Open.” Nature Ecology\n& Evolution 6 (April): 656–58. https://doi.org/10.1038/s41559-022-01738-7.\n\n\nLin, Herbert. 2014. “A Proposal to Reduce Government\nOverclassification of Information Related to National Security.”\nJournal of National Security Law and Policy 7: 443–63.\n\n\nLin, Sarah, Ibraheem Ali, and Greg Wilson. 2021. “Ten Quick Tips\nfor Making Things Findable.” PLOS Computational Biology\n16 (12): 1–10. https://doi.org/10.1371/journal.pcbi.1008469.\n\n\nLips, Hilary. 2020. Sex and Gender: An Introduction. 7th ed.\nIllinois: Waveland Press.\n\n\nLittle, Roderick, and Roger Lewis. 2021. “Estimands, Estimators,\nand Estimates.” JAMA 326 (10): 967. https://doi.org/10.1001/jama.2021.2886.\n\n\nLiu, Emily, Lenny Bronner, and Jeremy Bowers. 2022. “What the\nWashington Post Elections Engineering Team Had to Learn about Election\nData.” Washington Post Engineering, April. https://washpost.engineering/what-the-washington-post-elections-engineering-team-had-to-learn-about-election-data-a41603daf9ca.\n\n\nLockheed Martin. 2005. “Joint Strike Fighter Air Vehicle C++\nCoding Standards For The System Development And Demonstration\nProgram.” Document Number 2RDU00001 Rev C,\nDecember. https://www.stroustrup.com/JSF-AV-rules.pdf.\n\n\nLohr, Sharon. (1999) 2022. Sampling: Design and Analysis. 3rd\ned. Chapman; Hall/CRC.\n\n\nLoken, Meredith, and Hilary Matfess. 2023. “Introducing the\nWomen’s Activities in Armed Rebellion (WAAR) Project, 1946-2015.”\nJournal of Peace Research.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. Chapman;\nHall/CRC. https://geocompr.robinlovelace.net.\n\n\nLucas, Jack, Reed Merrill, Kelly Blidook, Sandra Breux, Laura Conrad,\nGabriel Eidelman, Royce Koop, et al. 2020. “Canadian\nMunicipal Elections Database.” Scholars Portal Dataverse.\nhttps://doi.org/10.5683/sp2/4mzjpq.\n\n\nLucas, Robert. 1978. “Asset Prices in an Exchange Economy.”\nEconometrica 46 (6): 1429–45. https://doi.org/10.2307/1913837.\n\n\nLuebke, David Martin, and Sybil Milton. 1994. “Locating the\nVictim: An Overview of Census-Taking, Tabulation Technology, and\nPersecution in Nazi Germany.” IEEE Annals of the History of\nComputing 16 (3): 25–39. https://doi.org/10.1109/MAHC.1994.298418.\n\n\nLumley, Thomas. 2020. “survey: analysis of\ncomplex survey samples.” https://cran.r-project.org/web/packages/survey/index.html.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon Stewart. 2021. “What\nIs Your Estimand? Defining the Target Quantity Connects Statistical\nEvidence to Theory.” American Sociological Review 86\n(3): 532–65. https://doi.org/10.1177/00031224211004187.\n\n\nLuscombe, Alex, Kevin Dick, and Kevin Walby. 2021. “Algorithmic\nThinking in the Public Interest: Navigating Technical, Legal, and\nEthical Hurdles to Web Scraping in the Social Sciences.”\nQuality & Quantity 56 (3): 1–22. https://doi.org/10.1007/s11135-021-01164-0.\n\n\nLuscombe, Alex, Jamie Duncan, and Kevin Walby. 2022. “Jumpstarting\nthe Justice Disciplines: A Computational-Qualitative Approach to\nCollecting and Analyzing Text and Image Data in Criminology and Criminal\nJustice Studies.” Journal of Criminal Justice Education\n33 (2): 151–71. https://doi.org/10.1080/10511253.2022.2027477.\n\n\nLuscombe, Alex, and Alexander McClelland. 2020. “Policing the\nPandemic: Tracking the Policing of Covid-19 Across Canada,”\nApril. https://doi.org/10.31235/osf.io/9pn27.\n\n\nLyman, Frank. 1981. “The Responsive Classroom Discussion: The\nInclusion of All Students.” Mainstreaming Digest 109:\n109–13.\n\n\nMacDorman, Marian, and Eugene Declercq. 2018. “The Failure of\nUnited States Maternal Mortality Reporting and Its Impact on Women’s\nLives.” Birth 45 (2): 105–8. https://doi.org/1111/birt.12333.\n\n\nMaher, Michael. 1982. “Modelling Association Football\nScores.” Statistica Neerlandica 36 (3): 109–18. https://doi.org/10.1111/j.1467-9574.1982.tb00782.x.\n\n\nMaier, Maximilian, František Bartoš, Tom Stanley, David Shanks, Adam\nHarris, and Eric-Jan Wagenmakers. 2022. “No Evidence for Nudging\nAfter Adjusting for Publication Bias.” Proceedings of the\nNational Academy of Sciences 119 (31): e2200300119. https://doi.org/10.1073/pnas.2200300119.\n\n\nMammoliti, Anthony, Petr Smirnov, Minoru Nakano, Zhaleh Safikhani,\nChristopher Eeles, Heewon Seo, Sisira Kadambat Nair, et al. 2021.\n“Orchestrating and Sharing Large Multimodal Data for Transparent\nand Reproducible Research.” Nature Communications 12\n(1). https://doi.org/10.1038/s41467-021-25974-w.\n\n\nManski, Charles. 2022. “Inference with Imputed Data: The Allure of\nMaking Stuff Up.” arXiv. https://doi.org/10.48550/arXiv.2205.07388.\n\n\nMarchese, David. 2022. “Her Discovery Changed the World. How Does\nShe Think We Should Use It?” The New York Times, August.\nhttps://www.nytimes.com/interactive/2022/08/15/magazine/jennifer-doudna-crispr-interview.html.\n\n\nMartin, Charles, and Ben Popper. 2021. “Don’t Push That Button:\nExploring the Software That Flies SpaceX Rockets and Starships.”\nThe Overflow, December. https://stackoverflow.blog/2021/12/27/dont-push-that-button-exploring-the-software-that-flies-spacex-starships/.\n\n\nMartı́nez, Luis. 2022. “How Much Should We Trust the Dictator’s\nGDP Growth Estimates?” Journal of Political\nEconomy 130 (10): 2731–69. https://doi.org/10.1086/720458.\n\n\nMatias, Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles\nEbersole. 2021. “The Upworthy Research\nArchive, a time series of 32,487 experiments in U.S.\nmedia.” Scientific Data 8 (1): 1–8. https://doi.org/10.1038/s41597-021-00934-7.\n\n\nMatsumoto, Yukihiro. 2007. “Treating Code as\nan Essay.” In Beautiful Code, edited by Andy Oram\nand Greg Wilson, 477–81. O’Reilly.\n\n\nMattson, Greggor. 2017. “Artificial Intelligence Discovers\nGayface. Sigh.” https://greggormattson.com/2017/09/09/artificial-intelligence-discovers-gayface/amp/.\n\n\nMcCarthy, Fiona M., Tamsin E. M. Jones, Anne E. Kwitek, Cynthia L.\nSmith, Peter D. Vize, Monte Westerfield, and Elspeth A. Bruford. 2023.\n“The Case for Standardizing Gene Nomenclature in\nVertebrates.” Nature 614 (7948): E31–32. https://doi.org/10.1038/s41586-022-05633-w.\n\n\nMcClelland, Alexander. 2019. “‘Lock This Whore up’:\nLegal Violence and Flows of Information Precipitating Personal Violence\nAgainst People Criminalised for HIV-Related Crimes in Canada.”\nEuropean Journal of Risk Regulation 10 (1): 132–47. https://doi.org/10.1017/err.2019.20.\n\n\nMcElreath, Richard. (2015) 2020. Statistical\nRethinking: A Bayesian Course with Examples in R and Stan.\n2nd ed. Chapman; Hall/CRC.\n\n\n———. 2020. “Science as Amateur Software Development.”\nYouTube, September. https://youtu.be/zwRdO9%5FGGhY.\n\n\nMcIlroy, Doug, Ray Brownrigg, Thomas Minka, and Roger Bivand. 2023.\nmapproj: Map Projections. https://CRAN.R-project.org/package=mapproj.\n\n\nMcKenzie, David. 2021. “What Do You Need To\nDo To Make A Matching Estimator Convincing? Rhetorical vs Statistical\nChecks.” World Bank Blogs—Development Impact,\nFebruary. https://blogs.worldbank.org/impactevaluations/what-do-you-need-do-make-matching-estimator-convincing-rhetorical-vs-statistical.\n\n\nMcKinney, Wes. (2011) 2022. Python for Data Analysis. 3rd ed.\nhttps://wesmckinney.com/book/.\n\n\nMcPhee, John. 2017. Draft No. 4. 1st ed. Farrar, Straus;\nGiroux.\n\n\nMcQuire, Scott. 2019. “One Map to Rule Them All? Google Maps as\nDigital Technical Object.” Communication and the Public\n4 (2): 150–65. https://doi.org/10.1177/2057047319850192.\n\n\nMellon, Jonathan. 2023. “Rain, Rain, Go Away: 195 Potential\nExclusion-Restriction Violations for Studies Using Weather as an\nInstrumental Variable.” SocArXiv. https://doi.org/10.31235/osf.io/9qj4f.\n\n\nMeng, Xiao-Li. 1994. “Multiple-Imputation Inferences with\nUncongenial Sources of Input.” Statistical Science 9\n(4): 538–58. https://doi.org/10.1214/ss/1177010269.\n\n\n———. 2012. “You Want Me to Analyze Data i Don’t Have? Are You\nInsane?” Shanghai Archives of Psychiatry 24 (5):\n297–301. https://doi.org/10.3969/j.issn.1002-0829.2012.05.011.\n\n\n———. 2018. “Statistical Paradises and Paradoxes in Big Data (i):\nLaw of Large Populations, Big Data Paradox, and the 2016 US Presidential\nElection.” The Annals of Applied Statistics 12 (2):\n685–726. https://doi.org/10.1214/18-AOAS1161SF.\n\n\n———. 2021. “What Are the Values of Data, Data Science, or Data\nScientists?” Harvard Data Science Review 3 (1). https://doi.org/10.1162/99608f92.ee717cf7.\n\n\nMerali, Zeeya. 2010. “Computational Science:... Error.”\nNature 467 (7317): 775–77. https://doi.org/10.1038/467775a.\n\n\nMiceli, Milagros, Julian Posada, and Tianling Yang. 2022.\n“Studying up Machine Learning Data.” Proceedings of the\nACM on Human-Computer Interaction 6 (January): 1–14.\nhttps://doi.org/10.1145/3492853.\n\n\nMichener, William. 2015. “Ten Simple Rules for Creating a Good\nData Management Plan.” PLOS Computational Biology 11\n(10): e1004525. https://doi.org/10.1371/journal.pcbi.1004525.\n\n\nMill, James. 1817. The History of British India. 1st ed. https://books.google.ca/books?id=Orw_AAAAcAAJ.\n\n\nMiller, Greg. 2014. “The Cartographer Who’s\nTransforming Map Design.” Wired, October. https://www.wired.com/2014/10/cindy-brewer-map-design/.\n\n\nMiller, Michael, and Joseph Sutherland. 2022. “The Effect of\nGender on Interruptions at Congressional Hearings.” American\nPolitical Science Review, 1–19. https://doi.org/10.1017/S0003055422000260.\n\n\nMills, David L. 1991. “Internet Time Synchronization: The Network\nTime Protocol.” IEEE Transactions on Communications 39\n(10): 1482–93.\n\n\nMindell, David. 2008. Digital Apollo: Human and\nMachine in Spaceflight. 1st ed. New York: The MIT Press.\n\n\nMineault, Patrick, and The Good Research Code Handbook Community. 2021.\n“The Good Research Code Handbook.” https://doi.org/10.5281/zenodo.5796873.\n\n\nMinsky, Yaron. 2011. “OCaml for the\nmasses.” Communications of the ACM 54 (11):\n53–58. https://doi.org/10.1145/2018396.2018413.\n\n\n———. 2015. “Automated Trading and OCaml with Yaron Minsky.”\nHackers — Software Engineering Daily, November. https://softwareengineeringdaily.com/2015/11/09/automated-trading-and-ocaml-with-yaron-minsky/.\n\n\nMitchell, Alanna. 2022a. “Get Ready for the New, Improved\nSecond.” The New York Times, April. https://www.nytimes.com/2022/04/25/science/time-second-measurement.html.\n\n\n———. 2022b. “Time Has Run Out for the Leap Second.” The\nNew York Times, November. https://www.nytimes.com/2022/11/14/science/time-leap-second.html.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for Model Reporting.”\nProceedings of the Conference on Fairness, Accountability, and\nTransparency, January. https://doi.org/10.1145/3287560.3287596.\n\n\nMitrovski, Alen, Xiaoyan Yang, and Matthew Wankiewicz. 2020. “Joe\nBiden Projected to Win Popular Vote in 2020 US Election.” https://github.com/matthewwankiewicz/US_election_forecast.\n\n\nMiyakawa, Tsuyoshi. 2020. “No Raw Data, No Science: Another\nPossible Source of the Reproducibility Crisis.” Molecular\nBrain 13 (1): 1–6. https://doi.org/10.1186/s13041-020-0552-2.\n\n\nMok, Lillio, Samuel Way, Lucas Maystre, and Ashton Anderson. 2022.\n“The Dynamics of Exploration on Spotify.” In\nProceedings of the International AAAI Conference on Web and Social\nMedia, 16:663–74. https://doi.org/10.1609/icwsm.v16i1.19324.\n\n\nMolanphy, Chris. 2012. “100 & Single: Three Rules to Define\nthe Term ‘One-Hit Wonder’ in 2012.” The Village\nVoice, September. https://www.villagevoice.com/2012/09/10/100-single-three-rules-to-define-the-term-one-hit-wonder-in-2012/.\n\n\nMorange, Michel. 2016. A History of Biology. New Jersey:\nPrinceton University Press.\n\n\nMoyer, Brian, and Abe Dunn. 2020. “Measuring the\nGross Domestic Product\n(GDP): The Ultimate Data\nScience Project.” Harvard Data\nScience Review 2 (1). https://doi.org/10.1162/99608f92.414caadb.\n\n\nMüller, Kirill. 2020. here: A Simpler Way to\nFind Your Files. https://CRAN.R-project.org/package=here.\n\n\nMüller, Kirill, Tobias Schieferdecker, and Patrick Schratz. 2019.\nVisualization, Transformation and Reporting with the Tidyverse.\nhttps://krlmlr.github.io/vistransrep/.\n\n\nMüller, Kirill, and Lorenz Walthert. 2022. styler: Non-Invasive Pretty Printing of R\nCode. https://CRAN.R-project.org/package=styler.\n\n\nMüller, Kirill, and Hadley Wickham. 2022. tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nMurphy, Heather. 2017. “Why Stanford Researchers Tried to Create a\n‘Gaydar’ Machine.” The New York Times,\nOctober. https://www.nytimes.com/2017/10/09/science/stanford-sexual-orientation-study.html.\n\n\nNavarro, Danielle. 2022. “Binding Apache\nArrow to R,” January. https://blog.djnavarro.net/posts/2022-01-18%5Fbinding-arrow-to-r/.\n\n\nNavarro, Danielle, Jonathan Keane, and Stephanie Hazlitt. 2022.\n“Larger-Than-Memory Data Workflows with\nApache Arrow,” June. https://arrow-user2022.netlify.app.\n\n\nNelder, John. 1999. “From Statistics to Statistical\nScience.” Journal of the Royal Statistical Society: Series D\n(The Statistician) 48 (2): 257–69. https://doi.org/10.1111/1467-9884.00187.\n\n\nNelder, John, and Robert Wedderburn. 1972. “Generalized Linear\nModels.” Journal of the Royal Statistical Society: Series A\n(General) 135 (3): 370–84. https://doi.org/10.2307/2344614.\n\n\nNeufeld, Anna, and Daniela Witten. 2021. “Discussion of Breiman’s\n\"Two Cultures\": From Two Cultures to One.” Observational\nStudies 7 (1): 171–74. https://doi.org/10.1353/obs.2021.0004.\n\n\nNeufeld, Michael. 2002. “Wernher von Braun, the SS, and\nConcentration Camp Labor: Questions of Moral, Political, and Criminal\nResponsibility.” German Studies Review 25 (1): 57–78. https://doi.org/10.2307/1433245.\n\n\nNeuwirth, Erich. 2022. RColorBrewer: ColorBrewer\nPalettes. https://CRAN.R-project.org/package=RColorBrewer.\n\n\nNewman, Daniel. 2014. “Missing Data: Five Practical\nGuidelines.” Organizational Research Methods 17 (4):\n372–411. https://doi.org/10.1177/1094428114548590.\n\n\nNeyman, Jerzy. 1934. “On the Two Different Aspects of the\nRepresentative Method: The Method of Stratified Sampling and the Method\nof Purposive Selection.” Journal of the Royal Statistical\nSociety 97 (4): 558–625. https://doi.org/10.2307/2342192.\n\n\nNix, Justin, and M. James Lozada. 2020. “Police Killings of\nUnarmed Black Americans: A Reassessment of Community Mental Health\nSpillover Effects,” January. https://doi.org/10.31235/osf.io/ajz2q.\n\n\nNobles, Melissa. 2002. “Racial Categorization and\nCensuses.” In Census and Identity: The Politics of Race,\nEthnicity, and Language in National Censuses, edited by David\nKertzer and Dominique Arel, 43–70. Cambridge: Cambridge University\nPress. https://doi.org/10.1017/CBO9780511606045.003.\n\n\nNorthcutt, Curtis, Anish Athalye, and Jonas Mueller. 2021.\n“Pervasive Label Errors in Test Sets Destabilize Machine Learning\nBenchmarks.” https://doi.org/10.48550/arXiv.2103.14749.\n\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil\nMullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used\nto Manage the Health of Populations.” Science 366\n(6464): 447–53. https://doi.org/10.1126/science.aax2342.\n\n\nOberski, Daniel, and Frauke Kreuter. 2020. “Differential Privacy\nand Social Science: An Urgent\nPuzzle.” Harvard Data Science Review 2 (1).\nhttps://doi.org/10.1162/99608f92.63a22079.\n\n\nOECD. 2014. “The Essential Macroeconomic Aggregates.” In\nUnderstanding National Accounts, 13–46. OECD. https://doi.org/10.1787/9789264214637-2-en.\n\n\n———. 2022. Quarterly GDP. https://data.oecd.org/gdp/quarterly-gdp.htm.\n\n\nOoms, Jeroen. 2014. “The jsonlite Package: A\nPractical and Consistent Mapping Between JSON Data and R\nObjects.” arXiv:1403.2805 [Stat.CO]. https://arxiv.org/abs/1403.2805.\n\n\n———. 2022a. openssl: Toolkit for Encryption,\nSignatures and Certificates Based on OpenSSL. https://CRAN.R-project.org/package=openssl.\n\n\n———. 2022b. pdftools: Text Extraction,\nRendering and Converting of PDF Documents. https://CRAN.R-project.org/package=pdftools.\n\n\n———. 2022c. ssh: Secure Shell (SSH) Client for\nR. https://CRAN.R-project.org/package=ssh.\n\n\n———. 2022d. tesseract: Open Source OCR\nEngine. https://CRAN.R-project.org/package=tesseract.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility\nof Psychological Science.” Science 349 (6251): aac4716.\nhttps://doi.org/10.1126/science.aac4716.\n\n\nOrwell, George. 1946. Politics and the English Language. https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/.\n\n\nOsborne, Jason. 2012. Best Practices in Data\nCleaning: A Complete Guide to Everything You Need to Do Before and After\nCollecting Your Data. SAGE Publications.\n\n\nOsgood, D. Wayne. 2000. “Poisson-Based Regression Analysis of\nAggregate Crime Rates.” Journal of Quantitative\nCriminology 16 (1): 21–43. https://doi.org/10.1023/a:1007521427059.\n\n\nPalmer Station Antarctica LTER, and Gorman, Kristen. 2020.\n“Structural Size Measurements and Isotopic Signatures of Foraging\nAmong Adult Male and Female Adélie Penguins (Pygoscelis Adeliae) Nesting\nAlong the Palmer Archipelago Near Palmer Station, 2007-2009.” https://doi.org/10.6073/PASTA/98B16D7D563F265CB52372C8CA99E60F.\n\n\nPatki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The\nSynthetic Data Vault.” In 2016 IEEE International Conference\non Data Science and Advanced Analytics (DSAA), 399–410. https://doi.org/10.1109/DSAA.2016.49.\n\n\nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily Bender, Emily\nDenton, and Alex Hanna. 2021. “Data and Its (Dis)contents: A\nSurvey of Dataset Development and Use in Machine Learning\nResearch.” Patterns 2 (11): 100336. https://doi.org/10.1016/j.patter.2021.100336.\n\n\nPavlik, Kaylin. 2019. “Understanding + Classifying Genres Using\nSpotify Audio Features.” https://www.kaylinpavlik.com/classifying-songs-genres/.\n\n\nPedersen, Thomas Lin. 2022. patchwork: The\nComposer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPerepolkin, Dmytro. 2022. polite: Be Nice on\nthe Web. https://CRAN.R-project.org/package=polite.\n\n\nPerkel, Jeffrey. 2021. “Ten Computer Codes That Transformed\nScience.” Nature 589 (7842): 344–48. https://doi.org/10.1038/d41586-021-00075-2.\n\n\nPhillips, Alban. 1958. “The Relation Between Unemployment and the\nRate of Change of Money Wage Rates in the United Kingdom,\n1861-1957.” Economica 25 (100): 283–99. https://doi.org/10.1111/j.1468-0335.1958.tb00003.x.\n\n\nPiller, Charles. 2022. “Blots on a Field?” Science\n377 (6604): 358–63. https://doi.org/10.1126/science.ade0209.\n\n\nPineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent\nLarivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo\nLarochelle. 2021. “Improving Reproducibility in Machine Learning\nResearch (a Report from the NeurIPS 2019 Reproducibility\nProgram).” Journal of Machine Learning Research 22\n(164): 1–20. http://jmlr.org/papers/v22/20-303.html.\n\n\nPitman, Jim. 1993. Probability. 1st ed. New York: Springer. https://doi.org/10.1007/978-1-4612-4374-8.\n\n\nPlant, Anne, and Robert Hanisch. 2020. “Reproducibility in\nScience: A Metrology Perspective.” Harvard Data Science\nReview 2 (4). https://doi.org/10.1162/99608f92.eb6ddee4.\n\n\nPodlogar, Tim, Peter Leo, and James Spragg. 2022. “Using VO2max as a marker of training status in\nathletes—Can we do better?” Journal of Applied\nPhysiology 133 (6): 144–47. https://doi.org/10.1152/japplphysiol.00723.2021.\n\n\nPreece, Donald Arthur. 1981. “Distributions of Final Digits in\nData.” The Statistician 30 (1): 31. https://doi.org/10.2307/2987702.\n\n\nPrévost, Jean-Guy, and Jean-Pierre Beaud. 2015. Statistics, Public\nDebate and the State, 1800–1945: A Social, Political and Intellectual\nHistory of Numbers. Routledge.\n\n\nR Core Team. 2023. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and\nKirill Müller. 2022. DBI: R Database Interface. https://CRAN.R-project.org/package=DBI.\n\n\nRadcliffe, Nicholas. 2023. Test-Driven Data\nAnalysis (Python TDDA library). https://tdda.readthedocs.io/en/latest/index.html.\n\n\nRegister, Yim. 2020a. “Introduction to Sampling and\nRandomization.” YouTube, November. https://youtu.be/U272FFxG8LE.\n\n\n———. 2020b. “Data Science Ethics in 6 Minutes.”\nYouTube, December. https://youtu.be/mA4gypAiRYU.\n\n\nRehaag, Sean. 2023. “Supreme Court of Canada Bulk Decisions\nDataset.” Refugee Law Laboratory. https://refugeelab.ca/bulk-data/scc.\n\n\nReid, Nancy. 2003. “Asymptotics and the Theory of\nInference.” The Annals of Statistics 31 (6): 1695–1731.\nhttps://doi.org/10.1214/aos/1074290325.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain\nFrançois, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and\nApache Arrow. 2023. arrow: Integration to\nApache Arrow. https://CRAN.R-project.org/package=arrow.\n\n\nRiederer, Emily. 2020. “Column Names as Contracts,”\nSeptember. https://emilyriederer.netlify.app/post/column-name-contracts/.\n\n\n———. 2021. “Causal Design Patterns for Data Analysts,”\nJanuary. https://emilyriederer.netlify.app/post/causal-design-patterns/.\n\n\nRiffe, Tim, Enrique Acosta, Enrique José Acosta, Diego Manuel Aburto,\nAnna Alburez-Gutierrez, Ainhoa Altová, Ugofilippo Alustiza, et al. 2021.\n“Data Resource Profile: COVerAGE-DB: A\nGlobal Demographic Database of COVID-19 Cases and\nDeaths.” International Journal of Epidemiology 50 (2):\n390–390f. https://doi.org/10.1093/ije/dyab027.\n\n\nRiley, Richard, Tim Cole, Jon Deeks, Jamie Kirkham, Julie Morris, Rafael\nPerera, Angie Wade, and Gary Collins. 2022. “On the 12th Day of\nChristmas, a Statistician Sent to Me...”\nBMJ, December, e072883. https://doi.org/10.1136/bmj-2022-072883.\n\n\nRilke, Rainer Maria. (1929) 2014. Letters to a Young Poet.\nPenguin Classics.\n\n\nRoberts, Margaret, Brandon Stewart, and Dustin Tingley. 2019.\n“stm: An R Package for\nStructural Topic Models.” Journal of Statistical\nSoftware 91 (2): 1–40. https://doi.org/10.18637/jss.v091.i02.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2022. broom: Convert Statistical Objects into Tidy\nTibbles. https://CRAN.R-project.org/package=broom.\n\n\nRobinson, Emily, and Jacqueline Nolis. 2020. Build a Career in Data\nScience. Shelter Island: Manning Publications. https://livebook.manning.com/book/build-a-career-in-data-science.\n\n\nRockoff, Hugh. 2019. “On the Controversies Behind the Origins of\nthe Federal Economic Statistics.” Journal of Economic\nPerspectives 33 (1): 147–64. https://doi.org/10.1257/jep.33.1.147.\n\n\nRomer, Paul. 2018. “Jupyter, Mathematica, and the Future of the\nResearch Paper,” April. https://paulromer.net/jupyter-mathematica-and-the-future-of-the-research-paper/.\n\n\nRose, Angela, Rebecca Grais, Denis Coulombier, and Helga Ritter. 2006.\n“A Comparison of Cluster and Systematic Sampling Methods for\nMeasuring Crude Mortality.” Bulletin of the World Health\nOrganization 84: 290–96. https://doi.org/10.2471/blt.05.029181.\n\n\nRosenau, James N. 1999. “A Transformed Observer in a Transforming\nWorld.” Studia Diplomatica 52 (1/2): 5–14. http://www.jstor.org/stable/44838096.\n\n\nRoss, Casey. 2022. “How a Decades-Old Database Became a Hugely\nProfitable Dossier on the Health of 270 Million Americans.”\nStat, February. https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/.\n\n\nRubinstein, Benjamin, and Francesco Alda. 2017. “Pain-Free Random\nDifferential Privacy with Sensitivity Sampling.” In 34th\nInternational Conference on Machine Learning (ICML’2017).\n\n\nRudis, Bob. 2020. hrbrthemes: Additional\nThemes, Theme Components and Utilities for\n“ggplot2”. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nRuggles, Steven, Catherine Fitch, Diana Magnuson, and Jonathan\nSchroeder. 2019. “Differential Privacy and Census Data:\nImplications for Social and Economic Research.” AEA Papers\nand Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nRuggles, Steven, Sarah Flood, Sophia Foster, Ronald Goeken, Jose Pacas,\nMegan Schouweiler, and Matthew Sobek. 2021. “IPUMS USA: Version\n11.0.” Minneapolis, MN: IPUMS. https://doi.org/10.18128/d010.v11.0.\n\n\nRyan, Philip. 2015. “Keeping a Lab Notebook.”\nYouTube, May. https://youtu.be/-MAIuaOL64I.\n\n\nSadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and\nAlberto Bacchelli. 2018. “Modern Code Review: A Case Study at\nGoogle.” In Proceedings of the 40th International Conference\non Software Engineering: Software Engineering in Practice, 181–90.\nICSE-SEIP ’18. New York, NY, USA: Association for Computing Machinery.\nhttps://doi.org/10.1145/3183519.3183525.\n\n\nSakshaug, Joseph, Ting Yan, and Roger Tourangeau. 2010.\n“Nonresponse Error, Measurement Error, and Mode of Data\nCollection: Tradeoffs in a Multi-Mode Survey of Sensitive and\nNon-Sensitive Items.” Public Opinion Quarterly 74 (5):\n907–33. https://doi.org/10.1093/poq/nfq057.\n\n\nSalganik, Matthew. 2018. Bit by Bit: Social Research in the Digital\nAge. New Jersey: Princeton University Press.\n\n\nSalganik, Matthew, Peter Sheridan Dodds, and Duncan Watts. 2006.\n“Experimental Study of Inequality and Unpredictability in an\nArtificial Cultural Market.” Science 311 (5762): 854–56.\nhttps://doi.org/10.1126/science.1121066.\n\n\nSalganik, Matthew, and Douglas Heckathorn. 2004. “Sampling and\nEstimation in Hidden Populations Using Respondent-Driven\nSampling.” Sociological Methodology 34 (1): 193–240. https://doi.org/10.1111/j.0081-1750.2004.00152.x.\n\n\nSambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,\nPraveen Paritosh, and Lora Aroyo. 2021. “‘Everyone Wants to\nDo the Model Work, Not the Data Work’: Data Cascades in\nHigh-Stakes AI.” In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems.\nACM. https://doi.org/10.1145/3411764.3445518.\n\n\nSamuel, Arthur. 1959. “Some Studies in Machine Learning Using the\nGame of Checkers.” IBM Journal of Research and\nDevelopment 3 (3): 210–29. https://doi.org/10.1147/rd.33.0210.\n\n\nSaulnier, Lucile, Siddharth Karamcheti, Hugo Laurençon, Léo Tronchon,\nThomas Wang, Victor Sanh, Amanpreet Singh, et al. 2022. “Putting\nEthical Principles at the Core of the Research Lifecycle.” https://huggingface.co/blog/ethical-charter-multimodal.\n\n\nSavage, Van, and Pamela Yeh. 2019. “Novelist Cormac\nMcCarthy’s Tips on How to Write a Great Science\nPaper.” Nature 574 (7778): 441–42. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nSchaffner, Brian, Stephen Ansolabehere, and Sam Luks. 2021.\n“Cooperative Election Study Common Content,\n2020.” Harvard Dataverse. https://doi.org/10.7910/DVN/E9N6PH.\n\n\nSchloerke, Barret, and Jeff Allen. 2022. plumber: An API Generator for R. https://CRAN.R-project.org/package=plumber.\n\n\nSchmertmann, Carl. 2022. “UN API Test,” July. https://bonecave.schmert.net/un-api-example.html.\n\n\nSchofield, Alexandra, Måns Magnusson, and David Mimno. 2017.\n“Pulling Out the Stops: Rethinking Stopword Removal for Topic\nModels.” In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, 432–36. Valencia, Spain:\nAssociation for Computational Linguistics. https://aclanthology.org/E17-2069.\n\n\nSchofield, Alexandra, Måns Magnusson, Laure Thompson, and David Mimno.\n2017. “Understanding Text Pre-Processing for Latent Dirichlet\nAllocation.” In ACL Workshop for Women in NLP (WiNLP).\nhttps://www.cs.cornell.edu/~xanda/winlp2017.pdf.\n\n\nSchofield, Alexandra, Laure Thompson, and David Mimno. 2017.\n“Quantifying the Effects of Text Duplication on Semantic\nModels.” In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, 2737–47. Copenhagen,\nDenmark: Association for Computational Linguistics. https://doi.org/10.18653/v1/D17-1290.\n\n\nScott, James. 1998. Seeing Like a State. Yale University Press.\n\n\nSekhon, Jasjeet, and Rocío Titiunik. 2017. “Understanding\nRegression Discontinuity Designs as Observational Studies.”\nObservational Studies 3 (2): 174–82. https://doi.org/10.1353/obs.2017.0005.\n\n\nSen, Amartya. 1980. “Description as\nChoice.” Oxford Economic Papers 32 (3): 353–69.\nhttps://doi.org/10.1093/oxfordjournals.oep.a041484.\n\n\nShankar, Shreya, Rolando Garcia, Joseph Hellerstein, and Aditya\nParameswaran. 2022. “Operationalizing Machine Learning: An\nInterview Study.” arXiv. https://doi.org/10.48550/ARXIV.2209.09125.\n\n\nSi, Yajuan. 2020. “On the Use of Auxiliary Variables in Multilevel\nRegression and Poststratification.” https://arxiv.org/abs/2011.00360.\n\n\nSides, John, Lynn Vavreck, and Christopher Warshaw. 2021. “The\nEffect of Television Advertising in United States Elections.”\nAmerican Political Science Review, 1–17. https://doi.org/10.1017/s000305542100112x.\n\n\nSilberzahn, Raphael, Eric Uhlmann, Daniel Martin, Pasquale Anselmi,\nFrederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. “Many\nAnalysts, One Data Set: Making Transparent How Variations in Analytic\nChoices Affect Results.” Advances in Methods and Practices in\nPsychological Science 1 (3): 337–56. https://doi.org/10.1177/2515245917747646.\n\n\nSilge, Julia, and David Robinson. 2016. “tidytext: Text Mining and Analysis Using Tidy Data\nPrinciples in R.” The Journal of Open Source\nSoftware 1 (3). https://doi.org/10.21105/joss.00037.\n\n\nSilver, Nate. 2020. “We Fixed an Issue with How Our Primary\nForecast Was Calculating Candidates’ Demographic Strengths.”\nFiveThirtyEight, February. https://fivethirtyeight.com/features/we-fixed-a-mistake-in-how-our-primary-forecast-was-calculating-candidates-demographic-strengths/.\n\n\nSimonsohn, Uri. 2013. “Just Post It: The Lesson from Two Cases of\nFabricated Data Detected by Statistics Alone.” Psychological\nScience 24 (10): 1875–88. https://doi.org/10.1177/0956797613480366.\n\n\nSimpson, Edward. 1951. “The Interpretation of Interaction in\nContingency Tables.” Journal of the Royal Statistical\nSociety: Series B (Methodological) 13 (2): 238–41. https://doi.org/10.1111/j.2517-6161.1951.tb00088.x.\n\n\nSmith, Jessie, Saleema Amershi, Solon Barocas, Hanna Wallach, and\nJennifer Wortman Vaughan. 2022. “REAL ML: Recognizing, Exploring,\nand Articulating Limitations of Machine Learning Research.”\n2022 ACM Conference on Fairness, Accountability, and Transparency\n(FAccT ’22). https://doi.org/10.1145/3531146.3533122.\n\n\nSmith, Matthew. 2018. “Should Milk Go in a Cup of Tea First or\nLast?” July. https://yougov.co.uk/topics/consumer/articles-reports/2018/07/30/should-milk-go-cup-tea-first-or-last.\n\n\nSmith, Richard. 2002. “A Statistical Assessment of Buchanan’s Vote\nin Palm Beach County.” Statistical Science 17 (4):\n441–57. https://doi.org/10.1214/ss/1049993203.\n\n\nSobek, Matthew, and Steven Ruggles. 1999. “The IPUMS Project: An\nUpdate.” Historical Methods: A Journal of Quantitative and\nInterdisciplinary History 32 (3): 102–10. https://doi.org/10.1080/01615449909598930.\n\n\nSomers, James. 2015. “Toolkits for the\nMind.” MIT Technology Review, April. https://www.technologyreview.com/2015/04/02/168469/toolkits-for-the-mind/.\n\n\n———. 2017. “Torching the Modern-Day Library of Alexandria.”\nThe Atlantic, April. https://www.theatlantic.com/technology/archive/2017/04/the-tragedy-of-google-books/523320/.\n\n\n———. 2018. “The Scientific Paper Is Obsolete.” The\nAtlantic, April. https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/.\n\n\nSpear, Mary Eleanor. 1952. Charting Statistics. https://archive.org/details/ChartingStatistics_201801/.\n\n\nSprint, Gina, and Jason Conci. 2019. “Mining GitHub Classroom\nCommit Behavior in Elective and Introductory Computer Science\nCourses.” Journal of Computing Sciences in Colleges 35\n(1): 76–84.\n\n\nStaicu, Ana-Maria. 2017. “Interview with Nancy Reid.”\nInternational Statistical Review 85 (3): 381–403. https://doi.org/10.1111/insr.12237.\n\n\nStaniak, Mateusz, and Przemysław Biecek. 2019. “The Landscape of R Packages for Automated Exploratory\nData Analysis.” The R Journal 11\n(2): 347–69. https://doi.org/10.32614/RJ-2019-033.\n\n\nStantcheva, Stefanie. 2023. “How to Run Surveys: A Guide to\nCreating Your Own Identifying Variation and Revealing the\nInvisible.” Annual Review of Economics. https://scholar.harvard.edu/files/stantcheva/files/How_to_run_surveys_Stantcheva.pdf.\n\n\nStatistics Canada. 2017. “Guide to the Census of Population,\n2016.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2016/ref/98-304/98-304-x2016001-eng.pdf.\n\n\n———. 2020. “Sex at Birth and Gender: Technical Report on Changes\nfor the 2021 Census.” Statistics Canada. https://www12.statcan.gc.ca/census-recensement/2021/ref/98-20-0002/982000022020002-eng.pdf.\n\n\nSteckel, Richard. 1991. “The Quality of Census Data for Historical\nInquiry: A Research Agenda.” Social Science History 15\n(4): 579–99. https://doi.org/10.2307/1171470.\n\n\nSteele, Fiona. 2007. “Multilevel Models for Longitudinal\nData.” Journal of the Royal Statistical Society Series\nA: Statistics in Society 171 (1): 5–19. https://doi.org/10.1111/j.1467-985x.2007.00509.x.\n\n\nSteele, Fiona, Anna Vignoles, and Andrew Jenkins. 2007. “The\nEffect of School Resources on Pupil Attainment: A Multilevel\nSimultaneous Equation Modelling Approach.” Journal of the\nRoyal Statistical Society Series A: Statistics in Society 170 (3):\n801–24. https://doi.org/10.1111/j.1467-985x.2007.00476.x.\n\n\nStevens, Wallace. 1934. The Idea of Order at Key West. https://www.poetryfoundation.org/poems/43431/the-idea-of-order-at-key-west.\n\n\nSteyvers, Mark, and Tom Griffiths. 2006. “Probabilistic Topic\nModels.” In Latent Semantic Analysis: A Road to Meaning,\nedited by T. Landauer, D McNamara, S. Dennis, and W. Kintsch. https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf.\n\n\nStigler, Stephen. 1978. “Francis Ysidro Edgeworth,\nStatistician.” Journal of the Royal Statistical\nSociety. Series A (General) 141 (3): 287–322. https://doi.org/10.2307/2344804.\n\n\n———. 1986. The History of Statistics. Massachusetts: Belknap\nHarvard.\n\n\nStock, James, and Francesco Trebbi. 2003. “Retrospectives: Who\nInvented Instrumental Variable Regression?” Journal of\nEconomic Perspectives 17 (3): 177–94. https://doi.org/10.1257/089533003769204416.\n\n\nStolberg, Michael. 2006. “Inventing the Randomized Double-Blind\nTrial: The Nuremberg Salt Test of 1835.” Journal of the Royal\nSociety of Medicine 99 (12): 642–43. https://doi.org/10.1177/014107680609901216.\n\n\nStoler, Ann Laura. 2002. “Colonial Archives and the Arts of\nGovernance.” Archival Science 2 (March): 87–109. https://doi.org/10.1007/bf02435632.\n\n\nStolley, Paul. 1991. “When Genius Errs: R. A. Fisher and the Lung\nCancer Controversy.” American Journal of Epidemiology\n133 (5): 416–25. https://doi.org/10.1093/oxfordjournals.aje.a115904.\n\n\nStommes, Drew, P. M. Aronow, and Fredrik Sävje. 2021. “On the\nReliability of Published Findings Using the Regression Discontinuity\nDesign in Political Science.” arXiv. https://doi.org/10.48550/ARXIV.2109.14526.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika 6 (1): 1–25. https://doi.org/10.2307/2331554.\n\n\nSunstein, Cass, and Lucia Reisch. 2017. The Economics of Nudge.\nRoutledge.\n\n\nSuriyakumar, Vinith, Nicolas Papernot, Anna Goldenberg, and Marzyeh\nGhassemi. 2021. “Chasing Your Long Tails.” In\nProceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency. https://doi.org/10.1145/3442188.3445934.\n\n\nSwain, Larry. 1985. “Basic Principles of Questionnaire\nDesign.” Survey Methodology 11 (2): 161–70.\n\n\nSylvester, Christine, Anastasia Ershova, Aleksandra Khokhlova, Nikoleta\nYordanova, and Zachary Greene. 2023. “ParlEE\nplenary speeches V2 data set: Annotated full-text of 15.1 million\nsentence-level plenary speeches of six EU legislative\nchambers.” Harvard Dataverse. https://doi.org/10.7910/DVN/VOPK0E.\n\n\nSzaszi, Barnabas, Anthony Higney, Aaron Charlton, Andrew Gelman, Ignazio\nZiano, Balazs Aczel, Daniel Goldstein, David Yeager, and Elizabeth\nTipton. 2022. “No Reason to Expect Large and Consistent Effects of\nNudge Interventions.” Proceedings of the National Academy of\nSciences 119 (31): e2200732119. https://doi.org/10.1073/pnas.2200732119.\n\n\nTaddy, Matt. 2019. Business Data Science. 1st ed. McGraw Hill.\n\n\nTaflaga, Marija, and Matthew Kerby. 2019. “Who Does What Work in a\nMinisterial Office: Politically Appointed Staff and the Descriptive\nRepresentation of Women in Australian Political Offices,\n19792010.” Political Studies 68 (2):\n463–85. https://doi.org/10.1177/0032321719853459.\n\n\nTal, Eran. 2020. “Measurement in\nScience.” In The Stanford Encyclopedia of\nPhilosophy, edited by Edward Zalta, Fall 2020. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/;\nMetaphysics Research Lab, Stanford University.\n\n\nTang, John. 2015. “Pollution havens and the\ntrade in toxic chemicals: Evidence from U.S. trade flows.”\nEcological Economics 112 (April): 150–60. https://doi.org/10.1016/j.ecolecon.2015.02.022.\n\n\nTang, Jun, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, and\nXiaofeng Wang. 2017. “Privacy Loss in Apple’s Implementation of\nDifferential Privacy on MacOS 10.12.” arXiv. https://doi.org/10.48550/arXiv.1709.02753.\n\n\nTausanovitch, Chris, and Lynn Vavreck. 2021. “Democracy Fund\n+ UCLA Nationscape Project.” https://www.voterstudygroup.org/data/nationscape.\n\n\nTaylor, Adam. 2015. “New Zealand Says No to Jedis.” The\nWashington Post, September. https://www.washingtonpost.com/news/worldviews/wp/2015/09/29/new-zealand-says-no-to-jedis/.\n\n\nTeate, Renée. 2022. SQL for Data Scientists. Wiley.\n\n\nThe Economist. 2013. “Johnson: Those Six Little Rules: George\nOrwell on Writing,” July. https://www.economist.com/prospero/2013/07/29/johnson-those-six-little-rules.\n\n\n———. 2022a. “What Spotify Data Show about the Decline of\nEnglish,” January. https://www.economist.com/interactives/graphic-detail/2022/01/29/what-spotify-data-show-about-the-decline-of-english.\n\n\n———. 2022b. “Will Emmanuel Macron Win a Second Term?”\nApril. https://www.economist.com/interactive/france-2022/forecast.\n\n\n———. 2022c. “France’s Presidential Election: The Second Round in\nDetail,” April. https://www.economist.com/interactive/france-2022/results-round-two.\n\n\nThe Washington Post. 2023. “Fatal Force Database.” https://github.com/washingtonpost/data-police-shootings.\n\n\nThe White House. 2023. “Recommendations on the Best Practices for\nthe Collection of Sexual Orientation and Gender Identity Data on Federal\nStatistical Survey,” January. https://www.whitehouse.gov/wp-content/uploads/2023/01/SOGI-Best-Practices.pdf.\n\n\nThieme, Nick. 2018. “R Generation.” Significance\n15 (4): 14–19. https://doi.org/10.1111/j.1740-9713.2018.01169.x.\n\n\nThistlethwaite, Donald, and Donald Campbell. 1960.\n“Regression-Discontinuity Analysis: An Alternative to the Ex Post\nFacto Experiment.” Journal of Educational Psychology 51\n(6): 309–17. https://doi.org/10.1037/h0044319.\n\n\nThompson, Charlie, Daniel Antal, Josiah Parry, Donal Phipps, and Tom\nWolff. 2022. spotifyr: R Wrapper for the\n“Spotify” Web API. https://CRAN.R-project.org/package=spotifyr.\n\n\nThomson-DeVeaux, Amelia, Laura Bronner, and Damini Sharma. 2021.\n“Cities Spend Millions On Police Misconduct\nEvery Year. Here’s Why It’s So Difficult to Hold Departments\nAccountable.” FiveThirtyEight, February. https://fivethirtyeight.com/features/police-misconduct-costs-cities-millions-every-year-but-thats-where-the-accountability-ends/.\n\n\nThornhill, John. 2021. “Lunch with the FT: Mathematician Hannah\nFry.” Financial Times, July. https://www.ft.com/content/a5e33e5a-99b9-4bbc-948f-8a527c7675c3.\n\n\nTierney, Nicholas, Di Cook, Miles McBain, and Colin Fay. 2021. naniar: Data Structures, Summaries, and Visualisations\nfor Missing Data. https://CRAN.R-project.org/package=naniar.\n\n\nTierney, Nicholas, and Karthik Ram. 2020. “A Realistic Guide to\nMaking Data Available Alongside Code to Improve Reproducibility.”\nhttps://arxiv.org/abs/2002.11626.\n\n\n———. 2021. “Common-Sense Approaches to Sharing Tabular Data\nAlongside Publication.” Patterns 2 (12): 100368. https://doi.org/10.1016/j.patter.2021.100368.\n\n\nTimbers, Tiffany. 2020. canlang: Canadian\nCensus language data. https://ttimbers.github.io/canlang/.\n\n\nTimbers, Tiffany, Trevor Campbell, and Melissa Lee. 2022. Data\nScience: A First Introduction. Chapman; Hall/CRC. https://datasciencebook.ca.\n\n\nTolley, Erin, and Mireille Paquet. 2021. “Gender, Municipal Party\nPolitics, and Montreal’s First Woman Mayor.” Canadian Journal\nof Urban Research 30 (1): 40–52. https://cjur.uwinnipeg.ca/index.php/cjur/article/view/323.\n\n\nTourangeau, Roger, Lance Rips, and Kenneth Rasinski. 2000. The\nPsychology of Survey Response. 1st ed. Cambridge University Press.\nhttps://doi.org/10.1017/CBO9780511819322.003.\n\n\nTrisovic, Ana, Matthew Lau, Thomas Pasquier, and Mercè Crosas. 2022.\n“A Large-Scale Study on Research Code Quality and\nExecution.” Scientific Data 9 (1). https://doi.org/10.1038/s41597-022-01143-6.\n\n\nTukey, John. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67. https://doi.org/10.1214/aoms/1177704711.\n\n\n———. 1977. Exploratory Data Analysis.\n\n\nTurcotte, Alexi, Aviral Goel, Filip Křikava, and Jan Vitek. 2020.\n“Designing Types for r, Empirically.” Proceedings of\nthe ACM on Programming Languages 4\n(OOPSLA): 1–25. https://doi.org/10.1145/3428249.\n\n\nUN IGME. 2021. “Levels and Trends in Child Mortality,\n2021.” https://childmortality.org/wp-content/uploads/2021/12/UNICEF-2021-Child-Mortality-Report.pdf.\n\n\nUrban, Steve, Rangarajan Sreenivasan, and Vineet Kannan. 2016.\n“It’s All A/Bout Testing: The Netflix\nExperimentation Platform.” Netflix Technology\nBlog, April. https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15.\n\n\nUshey, Kevin. 2022. renv: Project\nEnvironments. https://CRAN.R-project.org/package=renv.\n\n\nvan Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in\nR.” Journal of Statistical Software 45 (3): 1–67.\nhttps://doi.org/10.18637/jss.v045.i03.\n\n\nVan den Broeck, Jan, Solveig Argeseanu Cunningham, Roger Eeckels, and\nKobus Herbst. 2005. “Data Cleaning: Detecting, Diagnosing, and\nEditing Data Abnormalities.” PLOS Medicine 2 (10): e267.\nhttps://doi.org/10.1371/journal.pmed.0020267.\n\n\nvan der Loo, Mark. 2022. The Data Validation Cookbook. https://data-cleaning.github.io/validate/.\n\n\nvan der Loo, Mark, and Edwin De Jonge. 2021. “Data Validation Infrastructure for R.”\nJournal of Statistical Software 97 (10): 1–33. https://doi.org/10.18637/jss.v097.i10.\n\n\nVanderplas, Susan, Dianne Cook, and Heike Hofmann. 2020. “Testing\nStatistical Charts: What Makes a Good Graph?” Annual Review\nof Statistics and Its Application 7: 61–88. https://doi.org/10.1146/annurev-statistics-031219-041252.\n\n\nVanhoenacker, Mark. 2015. Skyfaring: A Journey with a Pilot.\n1st ed. Alfred A. Knopf.\n\n\nVarin, Cristiano, Nancy Reid, and David Firth. 2011. “An Overview\nof Composite Likelihood Methods.” Statistica Sinica,\n5–42. https://www.jstor.org/stable/24309261.\n\n\nVarner, Maddy, and Aaron Sankin. 2020. “Suckers List: How\nAllstate’s Secret Auto Insurance Algorithm Squeezes Big\nSpenders.” The Markup, February. https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list.\n\n\nVavreck, Lynn, and Chris Tausanovitch. 2021. “Democracy Fund\n+ UCLA Nationscape Project User Guide.” https://www.voterstudygroup.org/data/nationscape.\n\n\nVickers, Andrew, and Emily Vertosick. 2016. “An Empirical Study of\nRace Times in Recreational Endurance Runners.”\nBMC Sports Science, Medicine and Rehabilitation 8\n(1). https://doi.org/10.1186/s13102-016-0052-y.\n\n\nVidoni, Melina. 2021. “Evaluating Unit\nTesting Practices in R Packages.” In 2021 IEEE/ACM\n43rd International Conference on Software Engineering (ICSE),\n1523–34. https://doi.org/10.1109/ICSE43902.2021.00136.\n\n\nvon Bergmann, Jens, Dmitry Shkolnik, and Aaron Jacobs. 2021. cancensus: R package to access, retrieve, and work with\nCanadian Census data and geography. https://mountainmath.github.io/cancensus/.\n\n\nWalby, Kevin, and Alex Luscombe. 2019. Freedom of Information and\nSocial Science Research Design. Routledge.\n\n\nWalker, Kyle. 2022. Analyzing US Census Data. Chapman;\nHall/CRC. https://walker-data.com/census-r/index.html.\n\n\nWalker, Kyle, and Matt Herman. 2022. tidycensus: Load US Census Boundary and Attribute Data as\n“tidyverse” and “sf”-Ready Data\nFrames. https://CRAN.R-project.org/package=tidycensus.\n\n\nWallach, Hanna. 2018. “Computational Social Science ≠ Computer Science + Social Data.”\nCommunications of the ACM 61 (3): 42–44. https://doi.org/10.1145/3132698.\n\n\nWan, Mengting, and Julian J. McAuley. 2018. “Item Recommendation\non Monotonic Behavior Chains.” In Proceedings of the 12th\nACM Conference on Recommender Systems, RecSys 2018,\nVancouver, BC, Canada, October 2-7, 2018, edited by Sole Pera,\nMichael D. Ekstrand, Xavier Amatriain, and John O’Donovan, 86–94.\nACM. https://doi.org/10.1145/3240323.3240369.\n\n\nWan, Mengting, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley.\n2019. “Fine-Grained Spoiler Detection from Large-Scale Review\nCorpora.” In Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\nedited by Anna Korhonen, David R. Traum, and Lluı́s Màrquez, 2605–10.\nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/p19-1248.\n\n\nWang, Wei, David Rothschild, Sharad Goel, and Andrew Gelman. 2015.\n“Forecasting Elections with Non-Representative Polls.”\nInternational Journal of Forecasting 31 (3): 980–91. https://doi.org/10.1016/j.ijforecast.2014.06.001.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are\nMore Accurate Than Humans at Detecting Sexual Orientation from Facial\nImages.” Journal of Personality and Social Psychology\n114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\nWardrop, Robert. 1995. “Simpson’s Paradox and the Hot Hand in\nBasketball.” The American Statistician 49 (1): 24–28. https://doi.org/10.2307/2684806.\n\n\nWare, James. 1989. “Investigating Therapies of Potentially Great\nBenefit: ECMO.” Statistical Science 4 (4): 298–306. https://doi.org/10.1214/ss/1177012384.\n\n\nWasserman, Larry. 2005. All of Statistics. Springer.\n\n\nWei, LJ, and S Durham. 1978. “The Randomized Play-the-Winner Rule\nin Medical Trials.” Journal of the American Statistical\nAssociation 73 (364): 840–43. https://doi.org/10.2307/2286290.\n\n\nWeinberg, Gerald. 1971. The Psychology of Computer Programming.\nNew York: Van Nostrand Reinhold Company.\n\n\nWeissgerber, Tracey, Natasa Milic, Stacey Winham, and Vesna Garovic.\n2015. “Beyond Bar and Line Graphs: Time for a New Data\nPresentation Paradigm.” PLoS Biology 13 (4): e1002128.\nhttps://doi.org/10.1371/journal.pbio.1002128.\n\n\nWhitby, Andrew. 2020. The Sum of the\nPeople. New York: Basic Books.\n\n\nWhitelaw, James. 1805. An Essay on the Population of Dublin. Being\nthe Result of an Actual Survey Taken in 1798, with Great Care and\nPrecision, and Arranged in a Manner Entirely New. Graisberry;\nCampbell.\n\n\nWicherts, Jelte, Marjan Bakker, and Dylan Molenaar. 2011.\n“Willingness to Share Research Data Is Related to the Strength of\nthe Evidence and the Quality of Reporting of Statistical\nResults.” PLOS ONE 6 (11): e26828. https://doi.org/10.1371/journal.pone.0026828.\n\n\nWickham, Hadley. 2009. “Manipulating Data.” In ggplot2, 157–75. Springer New York. https://doi.org/10.1007/978-0-387-98141-3_9.\n\n\n———. 2010. “A Layered Grammar of Graphics.” Journal of\nComputational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\n———. 2011. “testthat: Get Started with\nTesting.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal%5F2011-1%5FWickham.pdf.\n\n\n———. 2014. “Tidy Data.” Journal of Statistical\nSoftware 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2016. ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2017. tidyverse: Easily Install and Load\nthe “Tidyverse”. https://CRAN.R-project.org/package=tidyverse.\n\n\n———. 2018. “Whole Game.” YouTube, January. https://youtu.be/go5Au01Jrvs.\n\n\n———. 2019. Advanced R. 2nd ed. Chapman; Hall/CRC.\nhttps://adv-r.hadley.nz.\n\n\n———. 2020. Tidyverse. https://www.tidyverse.org/.\n\n\n———. 2021a. babynames: US Baby Names\n1880-2017. https://CRAN.R-project.org/package=babynames.\n\n\n———. 2021b. Mastering Shiny. 1st ed. O’Reilly Media. https://mastering-shiny.org.\n\n\n———. 2021c. The Tidyverse Style Guide. https://style.tidyverse.org/index.html.\n\n\n———. 2022a. R Packages. 2nd ed. O’Reilly Media. https://r-pkgs.org.\n\n\n———. 2022b. rvest: Easily Harvest (Scrape) Web\nPages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2022c. stringr: Simple, Consistent\nWrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\n———. 2023a. forcats: Tools for Working with\nCategorical Variables (Factors). https://CRAN.R-project.org/package=forcats.\n\n\n———. 2023b. httr: Tools for Working with URLs\nand HTTP. https://CRAN.R-project.org/package=httr.\n\n\nWickham, Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the Tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Jennifer Bryan, and Malcolm Barrett. 2022. usethis: Automate Package and Project Setup.\nhttps://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016)\n2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\ndplyr: A Grammar of Data\nManipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2022. dbplyr: A “dplyr” Back End for\nDatabases. https://CRAN.R-project.org/package=dbplyr.\n\n\nWickham, Hadley, and Lionel Henry. 2022. purrr:\nFunctional Programming Tools. https://CRAN.R-project.org/package=purrr.\n\n\nWickham, Hadley, Jim Hester, and Jenny Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jenny Bryan. 2022.\ndevtools: Tools to Make Developing R Packages\nEasier. https://CRAN.R-project.org/package=devtools.\n\n\nWickham, Hadley, Jim Hester, and Jeroen Ooms. 2021. xml2: Parse XML. https://CRAN.R-project.org/package=xml2.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. haven: Import and Export “SPSS”\n“Stata” and “SAS” Files. https://CRAN.R-project.org/package=haven.\n\n\nWickham, Hadley, and Dana Seidel. 2022. scales:\nScale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWickham, Hadley, and Lisa Stryjewski. 2011. “40 Years of\nBoxplots,” November. https://vita.had.co.nz/papers/boxplots.pdf.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWiessner, Polly. 2014. “Embers of Society: Firelight Talk Among\nthe Ju/’hoansi Bushmen.” Proceedings of the National Academy\nof Sciences 111 (39): 14027–35. https://doi.org/10.1073/pnas.1404212111.\n\n\nWilde, Oscar. 1891. The Picture of Dorian Gray. https://www.gutenberg.org/files/174/174-h/174-h.htm.\n\n\nWilford, John Noble. 1977. “Wernher von Braun, Rocket Pioneer,\nDies.” The New York Times, June. https://www.nytimes.com/1977/06/18/archives/wernher-von-braun-rocket-pioneer-dies-wernher-von-braun-pioneer-in.html.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed.\nSpringer.\n\n\nWilkinson, Mark, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle\nAppleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016.\n“The FAIR Guiding Principles for Scientific Data Management and\nStewardship.” Scientific Data 3 (1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, Greg, Jenny Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy Teal. 2017. “Good Enough Practices in\nScientific Computing.” PLOS Computational Biology 13\n(6): 1–20. https://doi.org/10.1371/journal.pcbi.1005510.\n\n\nWong, Julia Carrie. 2020. “One Year Inside Trump’s Monumental\nFacebook Campaign.” The Guardian, January. https://www.theguardian.com/us-news/2020/jan/28/donald-trump-facebook-ad-campaign-2020-election.\n\n\nWood, Simon. 2015. Core Statistics. Cambridge University Press.\nhttps://www.maths.ed.ac.uk/\\%7Eswood34/core-statistics.pdf.\n\n\nWorld Health Organization. 2019. “Trends in Maternal Mortality\n2000 to 2017: Estimates by WHO, UNICEF, UNFPA, World Bank Group and the\nUnited Nations Population Division.” https://apps.who.int/iris/handle/10665/327596.\n\n\nWright, Philip. 1928. The Tariff on Animal and Vegetable Oils.\nNew York: Macmillan Company.\n\n\nWu, Changbao, and Mary Thompson. 2020. Sampling Theory and\nPractice. Springer.\n\n\nXie, Yihui. 2019. “TinyTeX: A lightweight,\ncross-platform, and easy-to-maintain LaTeX distribution based on TeX\nLive.” TUGboat, no. 1: 30–32. https://tug.org/TUGboat/Contents/contents40-1.html.\n\n\n———. 2023. knitr: A General-Purpose Package for\nDynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nXu, Ya. 2020. “Causal Inference Challenges in Industry: A\nPerspective from Experiences at LinkedIn.” YouTube,\nJuly. https://youtu.be/OoKsLAvyIYA.\n\n\nYoshioka, Alan. 1998. “Use of Randomisation in the Medical\nResearch Council’s Clinical Trial of Streptomycin in Pulmonary\nTuberculosis in the 1940s.” BMJ 317 (7167): 1220–23. https://doi.org/10.1136/bmj.317.7167.1220.\n\n\nZhang, Ping, XunPeng Shi, YongPing Sun, Jingbo Cui, and Shuai Shao.\n2019. “Have China’s provinces achieved their\ntargets of energy intensity reduction? Reassessment based on nighttime\nlighting data.” Energy Policy 128 (May): 276–83.\nhttps://doi.org/10.1016/j.enpol.2019.01.014.\n\n\nZhang, Susan, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\nShuohui Chen, Christopher Dewan, et al. 2022. “OPT: Open\nPre-Trained Transformer Language Models.” arXiv. https://doi.org/10.48550/arXiv.2205.01068.\n\n\nZimmer, Michael. 2018. “Addressing Conceptual Gaps in Big Data\nResearch Ethics: An Application of Contextual Integrity.”\nSocial Media + Society 4 (2): 1–11. https://doi.org/10.1177/2056305118768300.\n\n\nZinsser, William. 1976. On Writing Well. New York:\nHarperCollins.\n\n\nZook, Matthew, Solon Barocas, danah boyd, Kate Crawford, Emily Keller,\nSeeta Peña Gangadharan, Alyssa Goodman, et al. 2017. “Ten Simple\nRules for Responsible Big Data Research.” PLOS Computational\nBiology 13 (3): e1005399. https://doi.org/10.1371/journal.pcbi.1005399."
  }
]